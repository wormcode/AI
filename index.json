[{"authors":["admin"],"categories":null,"content":"JM Liu is a engineer of artificial intelligence. He learns lots of artificial intelligence courses and practiced lots of hands on projects and had read many papers.\n","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1555459200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://wormcode.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"JM Liu is a engineer of artificial intelligence. He learns lots of artificial intelligence courses and practiced lots of hands on projects and had read many papers.","tags":null,"title":"JM Liu","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://wormcode.github.io/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://wormcode.github.io/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://wormcode.github.io/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://wormcode.github.io/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1559174400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559174400,"objectID":"4941ba764b0219f9da250ee9f774c48a","permalink":"https://wormcode.github.io/project/cvpr2019/","publishdate":"2019-05-30T00:00:00Z","relpermalink":"/project/cvpr2019/","section":"project","summary":"An example of using the in-built project page.","tags":["cvpr2019"],"title":"cvpr 2019 translation","type":"project"},{"authors":["JM Liu"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://wormcode.github.io/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":["JM Liu"],"categories":[],"content":" from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png')  print(\u0026quot;Welcome to Academic!\u0026quot;)  Welcome to Academic!  Install Python and Jupyter Install Anaconda which includes Python 3 and Jupyter notebook.\nOtherwise, for advanced users, install Jupyter notebook with pip3 install jupyter.\nCreate a new blog post as usual Run the following commands in your Terminal, substituting \u0026lt;MY_WEBSITE_FOLDER\u0026gt; and my-post with the file path to your Academic website folder and a name for your blog post (without spaces), respectively:\ncd \u0026lt;MY_WEBSITE_FOLDER\u0026gt; hugo new --kind post post/my-post cd \u0026lt;MY_WEBSITE_FOLDER\u0026gt;/content/post/my-post/  Create or upload a Jupyter notebook Run the following command to start Jupyter within your new blog post folder. Then create a new Jupyter notebook (New \u0026gt; Python Notebook) or upload a notebook.\njupyter notebook  Convert notebook to Markdown jupyter nbconvert Untitled.ipynb --to markdown --NbConvertApp.output_files_dir=. # Copy the contents of Untitled.md and append it to index.md: cat Untitled.md | tee -a index.md # Remove the temporary file: rm Untitled.md  Edit your post metadata Open index.md in your text editor and edit the title etc. in the front matter according to your preference.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"https://wormcode.github.io/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://wormcode.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["JM Liu"],"categories":[],"content":" from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png')  print(\u0026quot;Welcome to Academic!\u0026quot;)  Welcome to Academic!  Install Python and Jupyter Install Anaconda which includes Python 3 and Jupyter notebook.\nOtherwise, for advanced users, install Jupyter notebook with pip3 install jupyter.\nCreate a new blog post as usual Run the following commands in your Terminal, substituting \u0026lt;MY_WEBSITE_FOLDER\u0026gt; and my-post with the file path to your Academic website folder and a name for your blog post (without spaces), respectively:\ncd \u0026lt;MY_WEBSITE_FOLDER\u0026gt; hugo new --kind post post/my-post cd \u0026lt;MY_WEBSITE_FOLDER\u0026gt;/content/post/my-post/  Create or upload a Jupyter notebook Run the following command to start Jupyter within your new blog post folder. Then create a new Jupyter notebook (New \u0026gt; Python Notebook) or upload a notebook.\njupyter notebook  Convert notebook to Markdown jupyter nbconvert Untitled.ipynb --to markdown --NbConvertApp.output_files_dir=. # Copy the contents of Untitled.md and append it to index.md: cat Untitled.md | tee -a index.md # Remove the temporary file: rm Untitled.md  Edit your post metadata Open index.md in your text editor and edit the title etc. in the front matter according to your preference.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"e368e80244db4f7a9c74bc3236074d9e","permalink":"https://wormcode.github.io/post/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"test post","type":"post"},{"authors":["JM Liu"],"categories":[],"content":" 翻译者:wormcode, 如发现问题请邮件\nSC-FEGAN: Face Editing Generative Adversarial Network with User’s Sketch and Color Youngjoo Jo Jongyoul Park ETRI South Korea frun.youngjoo,jongyoulg@etri.re.kr\nSC-FEGAN 使用用户输入的草图和颜色进行脸部编辑生成对抗网络\nYoungjoo Jo Jongyoul Park\nETRI\nSouth Korea\nfrun.youngjoo,jongyoulg@etri.re.kr\n\n摘要 我们提出了一种新颖的图像编辑系统，可以在用户提供任意形状的蒙版，草图和颜色作为输入时生成图像。 我们的系统包括端到端可训练的卷积网络。 与现有方法相反，我们的系统完全利用具有颜色和形状的任意形状用户输入。 这允许系统响应用户的草图和颜色输入，使用它作为生成图像的指南。 在我们的特定工作中，我们训练的网络具有额外的风格损失[3]，这使得即使在图像的大部分被移除情况下可以生成更加逼真的结果。 我们提出的网络架构SC-FEGAN非常适合使用直观的用户输入生成高质量的合成图像。\n1. 引言 生成对抗网络（GAN）的图像补全是计算机视觉中高度认可的主题。 随着图像交换成为当今日常通信中的常见介质媒体，在最小图像补全特征（痕迹）上对生成图像中的真实感的需求增加。 这种需求反映在社交媒体统计数据上。 但是，大多数图像编辑软件都需要专业知识，例如知道在特定情况下使用哪些特定工具，以便按照我们想要的方式有效地修改图像。 相反，响应用户输入的图像补全方法将允许新手根据需要容易地修改图像。 类似地，即使图像中存在擦除部分，我们提出的系统也能够轻松生成高质量的人脸图像，前提是草图和颜色作为输入。\n在最近的工作中，已经使用基于深度学习的图像补全方法来恢复图像的擦除部分。最典型的方法是使用普通（方形）蒙版，然后使用编码器解码器生成器恢复遮挡区域。然后使用全局和局部鉴别器来估计结果是真实的还是假的[5,9]。然而，该系统限于低分辨率图像，并且所生成的图像在遮挡区域上具有令人尴尬的边缘。此外，修复区上合成图像经常达不到用户的期望，因为生成器从未被给予任何用户输入以用作指导。改进此限制的一些工作包括Deepfillv2 [17]，一种利用用户草图作为输入的工作，以及GuidedInpating [20]，它将另一个图像的一部分作为输入来恢复缺失的部分。但是，由于Deepfillv2不使用颜色输入，因此合成图像中的颜色通过来自从训练数据集学习的先前分布的推断来进行计算。 Guided-Inpating使用其他图像的一部分来恢复已删除的区域。然而，很难恢复细节，因为这样的过程需要推断用户偏好的参考图像。最近的另一项工作Ideepcolor [19]提出了一种系统，它接受用户输入的颜色作为参考，以创建黑白图像对应的彩色图像。但是，Ideepcolor中的系统不允许编辑对象结构或恢复图像上已删除的部分。在另一项工作中，引入了一个面部编辑系统FaceShop [12]，它接受草图和颜色作为用户输入。但是，FaceShop用作生成合成图像的交互系统有一些限制。首先，它利用随机矩形和 可旋转蒙版来擦除那些由局部和全局鉴别器中使用的区域。这意味着局部鉴别器必须调整 修复的局部补丁 的大小以接受拟合输入尺寸，并且调整大小的过程中将使图像的擦除部分和剩余部分中的信息失真。结果，所产生的图像在修复部分将具有尴尬（明显的？）的边缘。其次，如果太多区域被擦除，FaceShop会产生不合理的合成图像。通常，当给定整个头发部分被擦除的图像时，系统会以扭曲的形状恢复它。\n图1.我们系统的面部图像编辑结果。 它可以采取任意形状的输入，包括面具，草图和颜色。 对于每个示例，它表明我们的系统使用户可以轻松编辑脸部的形状和颜色，即使用户想要完全改变发型和眼睛（第三行）。 有趣的是，用户可以通过我们的系统编辑耳环（第四行）。\n为了解决上述限制，我们提出了一种具有全卷积网络的SC-FEGAN，能够进行端到端的训练。 我们提出的网络使用SN-patchGAN [17]鉴别器来解决和改善尴尬的边缘。 该系统不仅具有一般的GAN损失，而且还具有风格损失，即使在大面积缺失的情况下也可以编辑面部图像的各部分。 我们的系统使用用户的自由形状输入创建高质量的逼真合成图像。 草图和颜色的任意形状域输入也有一个有趣的叠加效应，如图1所示。总之，我们做出以下贡献：\n 我们提出一种类似于Unet [13]的网络体系结构，带有门控卷积层[17]。 对于训练和推理阶段，这种架构更容易，更快捷。 与我们案例中的粗糙-精细网络相比，它产生了优越而细致的结果。\n We created a free-form domain data of masks, color and sketch. This data is used for making incomplete image data for training instead of stereotyped form input.\n我们创建了蒙版，颜色和草图的自由格式域数据。 该数据用于 使 不完整图像数据用于训练 而不是刻板形式的输入。\n 我们应用了SN-patchGAN [17]鉴别器，并以额外的风格损失训练了我们的网络。 该应用程序涵盖了大部分被擦除的情况，并且在管理蒙版边缘时表现出稳健性。 它还允许生成所生成图像的细节，例如高质量的合成发型和耳环。\n  2. Related Work 交互式图像修改具有广泛的历史，主要涉及使用手工特征而非深度学习的技术。 这种优势反映在商业图像编辑软件和我们的使用实践中。 因为大多数商业图像编辑软件使用定义好的操作，所以典型的图像修改任务需要专业知识来策略性地应用图像的变换组合。 除了专业知识，用户还需要花费很长的工作时间来生产精致的产品。 因此，传统方法对于非专家来说是不利的，并且用于产生高质量结果是繁琐的。 除了这些传统的建模方法之外，通过使用大数据集训练生成模型，GAN研究方面的最新突破已经开发了几种补全，修改和转换图像的方法。\n在本节中，我们将讨论使用深度学习流行的图像编辑方法中的 图像补全和图像转换领域的几项工作。\n2.1. Image Translation 用于图像翻译的GAN首先被提出用于学习两个数据集[21,6]之间的图像域变换。 Pix2Pix [6]提出了一个系统使用了一种数据集，该数据集由成对图像组成，可用于创建模型，这种模型或将分割标签转换为原始图像，或将草图转换为图像，或将黑白图像转换为彩色图像。 但是该系统要求图像和目标图像必须成对存在于训练数据集中，以便学习域之间的变换。 CycleGAN [21]提出了对这种要求进行改进的建议。 给定没有目标图像的目标域，在转换原 域中图像时，目标域中存在虚拟结果。 如果再次反转虚拟结果，则2次反转后的结果必须是原始图像。 因此，它需要两个生成器来完成转换任务。\n最近，在域到域更改之后，一些研究工作已经展示了 可采用用户输入以将所需方向效果？添加到生成结果的系统。 StarGAN [2]使用单个生成器和鉴别器通过域标签训练将输入图像灵活地转换为任何期望的目标域。 Ideepcolor [19]是作为一种系统引入的，该系统通过将用户所需的颜色作为蒙版将单色图像转换为彩色图像。 在这些工作中，与用户输入交互的图像变换已经表明，可以通过将载有用户输入的图像输入到生成器来学习用户输入。\n2.2. Image Completion 图像补全领域有两个主要挑战：1）填充图像中的删除区域，2）在修复区域中正确反映用户输入。 在之前的研究中，GAN系统探索了生成原来有擦除区域的完整图像的可能性[5]。 它使用来自U-net [13]结构的发生器并利用局部和全局鉴别器。 鉴别器分别确定在新填充的部分图像和完整的重建图像上是真实的还是假的。 Deepfillv1 [18]也使用矩形蒙版和全局和局部鉴别器模型来表明上下文关注层广泛地改善了性能。 然而，全局和局部鉴别器仍然在已修复部分的边界上产生尴尬的区域。\n在后续研究中Deepfillv2 [17]，引入了任意形状蒙版和SN-patchGAN，代替现有的矩形蒙版，用单个鉴别器代替全局和局部鉴别器。此外，还提出了学习遮挡区域特征的门控卷积层。此图层可以通过训练自动从数据中显示蒙版，这使网络能够在结果上反映用户输入的草图\n我们在下一节中描述的网络不仅允许使用草图而且还使用颜色数据作为编辑图像的输入。即使我们使用U-net结构而不是像Deepfillv1,2 [5,17]那样的粗-细网结构，我们的网络也可以生成高质量的结果，而无需复杂的训练计划，也不需要其他复杂的层。\n3. Approach 在本文中，我们描述了所提出的SC-FEGAN，一种基于神经网络的人脸图像编辑系统，并且还描述了用于制作输入批量数据的方法。该网络可以端到端地进行训练，并生成具有逼真纹理细节的高质量合成图像。在3.1节中，我们讨论了制作训练数据的方法。在3.2节中，我们描述了我们的网络结构和损失函数，它们允许从草图和颜色输入中提取特征，同时实现训练的稳定性。\nThe color maps are generated by median color of segmented areas from using GFC [9]. 图2.草图和颜色域数据集以及批处理的输入。 我们使用HED边缘检测器提取草图[16]。 使用GFC [9]，通过分割区域的中间颜色生成颜色图。 网络的输入包括不完整的图像，蒙版，草图，颜色和噪声。\n3.1. Training Data 合适的训练数据是提高网络训练性能和增加对用户输入的响应性的非常重要的因素。为了训练我们的模型，我们在几个预处理步骤之后使用了CelebA-HQ [8]数据集，如下所述。我们首先随机选择2组29,000张用于训练的图像和1,000张用于测试的图像。在获得草图和颜色数据集之前，我们将图像的大小调整为512 x 512像素。\n为了更好地表达面部图像中眼睛的复杂性，我们使用基于眼睛位置的任意形状蒙版来训练网络。此外，我们通过使用任意形状的蒙版和面部分割GFC [9]创建了适当的草图域和颜色域。 这是一个至关重要的步骤，使我们的系统能够为用户输入手绘案例产生有说服力的结果。我们在输入数据中随机将蒙版应用于头发区域，因为它与脸部的其他部分相比具有不同的属性。我们在下面讨论更多细节。\nFree-form masking with eye-positions 具有眼睛位置的任意形状蒙版\n我们使用类似于Deepfillv2 [17]中提出的蒙版方法来制作不完整的图像。然而，当对面部图像进行训练时，我们随机应用一个以眼睛位置为起点的自由绘制的面具，以表达眼睛的复杂部分。我们还使用GFC[9]随机添加了头发蒙版。算法1中描述了细节。\nSketch \u0026amp; Color domain 草图和颜色域\n------------------------------------------------------------------------- Algorithm 1 Free-form masking with eye-positions ------------------------------------------------------------------------- maxDraw, maxLine, maxAngle, maxLength are hyperparameters GFCHair is the GFC for get hair mask of input image Mask=zeros(inputSize,inputSize) HairMask=GFCHair(IntputImage) numLine=random.range(maxDraw) for i=0 to numLine do startX = random.range(inputSize) startY = random.range(inputSize) startAngle = random.range(360) numV = random.range(maxLine) for j=0 to numV do angleP = random.range(-maxAngle,maxAngle) if j is even then angle = startAngle+angleP else angle = startAngle+angleP+180 end if length = random.range(maxLength) Draw a line on Mask from point (startX, startY) with angle and length. startX = startX + length * sin(angle) startY = stateY + length * cos(angle) end for Draw a line on Mask from eye postion randomly. end for Mask = Mask + HairMask (randomly) ---------------------------------------------------------------------------------------  对于这部分，我们使用了类似于FaceShop [12]中使用的方法。但是，我们排除了将草图数据的位图转换为矢量图形的AutoTrace [15]。我们使用HED [16]边缘检测器生成与用户输入相对应的草图数据，以修改面部图像。之后，我们平滑了曲线并擦除了小边缘。为了创建颜色域数据，我们首先通过应用大小为3的中值滤波创建模糊图像，we first created blurred images by applying a median filtering with size 3 followed by 20 application of bilateral filter.然后应用双边滤波器来。之后，使用GFC [9]对面部进行分割，并将每个分割的部分替换为相应部分的中间中值median颜色。在为色域创建数据时，没有使用直方图均衡，目的是为了避免光反射和阴影造成的颜色污染。然而，不考虑光干涉引起的模糊，因为用户在草图域中表达脸部的所有部分更加共鸣，所以在从域数据创建草图时使用了直方图均衡。更具体地说，在直方图均衡之后，我们应用HED从图像中获得边缘。然后，我们平滑了曲线并擦除了小的对象（objects）。最后，我们将蒙版相乘，采用类似于先前任意形状蒙版的处理，以及彩色图像并获得彩色刷图像。有关我们数据的示例，请参见图2。\n3.2. Network Architecture 受近期图像补全研究[5,17,12]的启发，我们的补全网络（即发生器）基于编码器 - 解码器架构，如U-net [13]，我们的鉴别网络基于SN-patchGAN [17]。我们的网络结构可产生高质量的合成结果，图像大小为512x 512，同时实现稳定和快速的训练。我们的网络也像其他网络一样同时训练生成器和鉴别器。生成器接收具有用户输入的不完整图像以在RGB通道中创建输出图像，并将输出图像的遮挡遮挡区域插入到不完整的输入图像中以创建完整图像。鉴别器接收完成的图像或原始图像（没有遮挡）以确定给定输入是真实的还是假的。在对抗训练中，鉴别器的额外用户输入也有助于提高性能。此外，我们发现与一般GAN损失不同的额外损失对于恢复大的擦除部分是有效的。我们的网络详情如下所示。\n图3. SC-FEGAN的网络架构。 在输入和输出之外的所有卷积层之后应用LRN。 我们使用tanh作为生成器输出的激活函数。 我们使用SN卷积层[11]作为鉴别器。\n生成器 图3详细显示了我们的网络架构。我们的发生器基于U-net [10]，所有卷积层都使用门控卷积[17]，使用3x3大小的卷积核。在除了其他软门之外的特征映射卷积层之后应用局部信号归一化（LRN）[8]。 LRN应用于除输入和输出层之外的所有卷积层。我们的发生器的编码器接收大小为512 x512 x 9的输入张量：具有要被编辑的有移除区域的不完整RGB通道图像，描述被移除部分的结构的二进制草图，RGB颜色笔划图，二值蒙版和噪音（见图2）。编码器使用2个步幅的卷积核对输入进行7次下采样，然后在上采样之前进行膨胀卷积。\n解码器使用转置卷积进行上采样。然后，添加跳线连接以允许与具有相同空间分辨率的先前层进行连接。除了使用tanh函数的输出层之外，我们在每一层之后使用了leaky ReLU激活函数。总的来说，我们的生成器由16个卷积层组成，网络的输出是与输入（512 x 512）相同大小的的RGB图像。在将损失函数应用于输入图像之前，我们用输入图像替换了蒙版之外的剩余图像部分。这种替换允许生成器专门在编辑区域上进行训练。我们的生成器使用在PartialConv [10]中引入的损失函数训练：逐个像素损失，感知损失，风格损失和总方差损失。还使用通用GAN损失函数。\n鉴别器\n我们的鉴别器具有SNPatchGAN [17]结构。与Deepfillv2 [17]不同，我们没有对GAN损失应用ReLu函数。我们还使用了3 x 3大小的卷积核并应用了梯度惩罚损失项。我们添加了一个额外的术语，以避免鉴别器输出接近零值的补丁。我们的整体损失函数如下所示：\n我们的生成器用LG训练，鉴别器用LD训练。 D（I）是给定输入I的鉴别器的输出。在编辑诸如发型的大区域时，额外的损失，Lsytle和Lpercept是关键的。 每种损失的细节描述如下。 真实图像Igt与发生器Igen的输出之间的L1距离的Lper-pixel计算为 其中，Na是特征a的元素个数，M是二元蒙版图，Igen是生成器的输出。 我们使用因子α\u0026gt; 1来增加擦除部分的损失的权重。 感知损失Lpercept也计算L=\u0026ndash;1距离，但是在使用在ImageNet上预先训练过的VGG-16 [14]将图像投影到特征空间之后。 它计算为 这里， 是VGG-16 [14]的第q层的特征图，给定输入x，Icomp是Igen的完成图像，非擦除部分直接设置为真实图像。 q是从 VGG-16中的选定的图层，我们使用了pool1， pool2和pool3的层;。 样式损失使用Gram矩阵比较两个图像的内容。 我们计算风格损失为\n其中是用于在VGG-16的每个特征图上执行自相关的Gram矩阵。 当特征具有形状时，Gram矩阵的输出具有形状 是快速神经风格[7]建议的总变异损失，用于改善感知损失项下的棋盘伪影。 它计算为\n图4.当移除眼睛区域时，我们使用U-net（左）和粗 - 精网（右）的结果。\n其中R是擦除部分的区域。 WGANGP [4]损失用于改进训练并计算为\n这里，U是沿着 来自Icomp的鉴别器输入和Igt之间的直线 均匀采样的数据点。 在我们的案例中，这个术语对合成图像的质量至关重要。 我们用= 0 4. Results 在本节中，我们将消融研究与最近的相关工作进行比较，然后是面部编辑结果。 所有实验均在具有Tensorflow [1] v1.12，CUDA v10，Cudnn v7和Python 3的NVIDIA（R）Tesla（R）V100 GPU和Power9@2.3GHz CPU上执行。 测试，无论输入的大小和形状如何，分辨率为512 X 512 的图片，GPU上平均需要44ms，CPU上平均需要53ms，。 源代码和更多结果显示在https://github.com/JoYoungjoo/SC-FEGAN。\nhttps://github.com/JoYoungjoo/SC-FEGAN.\n4.1. Ablation Studies and Comparisons Figure 5. 在有和没有VGG 损失的网络训练的结果。在没有VGG 损失的情况下，我们遇到了和FaceShop 类似的问题[12]. 图6.与CelebA-HQ验证集上的Deepfillv1 [18]的定性比较。\n我们首先将我们的结果与Coarse-Refined结构网络和U-net结构网络进行了比较。在Deepfillv2 [17]中，它表明Coarse-Refined结构和上下文注意模块对于生成是有效的。但是，我们测试了Coarse-Refined结构网络，并注意到精炼阶段使输出模糊。我们发现其原因是因为精炼网络输出的L1损失总是小于粗网络。粗网络通过使用不完整输入生成恢复区域的粗略估计。然后将该粗略图像传递到精炼网络。这种设置允许精细网络学习地面实况和粗略恢复的不完整输入之间的转换。为了通过卷积运算实现这种效果，使输入数据变模糊 被用作其他更复杂训练方法的变通方法。它可以改善棋盘格，但需要大量的记忆/内存？和时间进行训练。图4显示了我们的系统关于粗细结构网络的结果。\nThe system in FaceShop [12] has shown difficulty in modifying the huge erased image like whole hair regions. FaceShop [12]中的系统显示出难以修改像整个头发区域那样的巨大擦除的图像。\n图7.来自我们系统的面部图像编辑结果。 它表明我们的系统可以正确地改变面部的形状和颜色。 它还表明它可以用于改变眼睛的颜色或擦除不必要的部分。 特别是右下角的两个结果表明我们的系统也可以用于新的发型修饰。\n由于感知和风格损失，我们的系统在这方面表现更好。 图5显示了有和没有VGG损失的结果。 我们还与最近的研究Deepfillv1 [18]进行了比较，其中发布了测试系统。 图6显示我们的系统在结构和形状质量方面使用任意形状的蒙版产生更好的结果。\n4.2. Face Image Editing and Restoration 图7显示了草图和颜色输入的各种结果。它表明我们的系统允许用户直观地编辑脸部图像功能，如发型，脸型，眼睛，嘴巴等。即使整个头发区域被删除，我们的系统一旦提供了用户草图它就能够产生适当的结果。用户可以使用草图和颜色直观地编辑图像，同时网络可以容忍小的绘图错误。用户可以通过输入草图和颜色直观地修改面部图像，以获得逼真地反映阴影和形状的合成图像。图9显示了验证数据集的一些结果，它显示即使用户进行了大量修改，用户也可以获得的高质量合成图像在提供足够的用户输入情况下。此外，为了检查网络对学习所用数据集的依赖性，我们尝试输入所有区域的擦除图像。与Deepfillv1[18]相比，Deepfillv1会产生模糊的脸部图像，但我们的SC-FEGAN会产生模糊的头发图像（参见图10）。这意味着，如果没有草图和颜色等附加信息，面部元素的形状和位置具有一定的依赖值。因此，除非在期望的方向上修复图像，否则不需要提供附加信息。此外，即使输入图像被完全擦除，我们的SC-FEGAN也可以在仅具有草图和彩色任意形状输入的情况下生成人脸图像（参见图10）。\n4.3. Interesting results 由GAN生成的图像结果通常显示对训练数据集的高依赖性。 Deepfillv2 [17]使用相同的数据集CelebA-HQ，但仅使用真实图像来制作草图数据集。 在Faceshop [12]中，AutoTrace [15]删除了数据集图像中的小细节。 在我们的研究中，我们将HED应用于所有区域，并通过安排它来扩展遮蔽覆盖？遮盖区域，我们能够获得产生面部图像和耳环的特殊结果。 图8显示了这些选择的有趣结果。 这些例子表明，即使对于小输入，我们的网络也能够学习小细节并产生合理的结果。\n5. Conclusions 在本文中，我们提出了一种新颖的图像编辑系统，用于任意形状的蒙版，草图，颜色输入，它基于具有新颖GAN损失的端到端可训练生成网络。 我们发现，与其他研究相比，我们的网络架构和损失函数显著改善了修复效果。 我们基于celebA-HQ数据集的高分辨率图像对我们的系统进行了训练，并在许多情况下显示了各种成功和逼真的编辑结果。 我们已经证明，我们的系统能够一次性修改和恢复大区域，并且只需要用户付出最小的努力就可以产生高质量和逼真的结果。\nFigure 8. 我们的关于编辑耳环特殊 结果\n图9.我们关于面部修复的结果。 如果给出足够的输入信息，即使很多区域被删除，我们的系统也可以令人满意地恢复脸部。 图10.关于全区域修复的结果。 在左侧，它显示Deepfillv1 [18]和SC-FEGAN关于完全擦除的图像的结果。 在右侧，它表明SC-FEGAN可以像翻译一样工作。 它只能通过草图和颜色输入生成面部图像。\nReferences [1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, et al. Tensorflow: a system for large-scale machine learning. In OSDI, volume 16, pages 265–283, 2016. 6\n[2] Y. Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and J. Choo. Stargan: Unified generative adversarial networks for multidomain image-to-image translation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. 3\n[3] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer using convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2414–2423, 2016. 1\n[4] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pages 5767–5777, 2017. 6\n[5] S. Iizuka, E. Simo-Serra, and H. Ishikawa. Globally and locally consistent image completion. ACM Transactions on Graphics (TOG), 36(4):107, 2017. 1, 3, 4\n[6] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial networks. CVPR, 2017. 2\n[7] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision, pages 694–711. Springer, 2016. 5\n[8] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017. 3, 4\n[9] Y. Li, S. Liu, J. Yang, and M.-H. Yang. Generative face completion. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 1, page 3, 2017. 1, 3, 4\n[10] G. Liu, F. A. Reda, K. J. Shih, T.-C. Wang, A. Tao, and B. Catanzaro. Image inpainting for irregular holes using partial convolutions. arXiv preprint arXiv:1804.07723, 2018.4\n[11] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018. 5\n[12] T. Portenier, Q. Hu, A. Szabo, S. Bigdeli, P. Favaro, and M. Zwicker. Faceshop: Deep sketch-based face image editing. arXiv preprint arXiv:1804.08972, 2018. 2, 3, 4, 6, 7\n[13] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234–241. Springer, 2015. 2, 3, 4\n[14] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211–252, 2015. 5\n[15] M.Weber. Autotrace, 2018. http://autotrace.sourceforge.net. 3, 7\n[16] S. ”Xie and Z. Tu. Holistically-nested edge detection. In Proceedings of IEEE International Conference on Computer Vision, 2015. 3\n[17] J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, and T. S. Huang. Free-form image inpainting with gated convolution. arXiv preprint arXiv:1806.03589, 2018. 2, 3, 4, 6, 7\n[18] J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, and T. S. Huang. Generative image inpainting with contextual attention. arXiv preprint arXiv:1801.07892, 2018. 3, 6, 7, 9\n[19] R. Zhang, J.-Y. Zhu, P. Isola, X. Geng, A. S. Lin, T. Yu, and A. A. Efros. Real-time user-guided image colorization with learned deep priors. arXiv preprint arXiv:1705.02999, 2017. 2, 3\n[20] Y. Zhao, B. Price, S. Cohen, and D. Gurari. Guided image inpainting: Replacing an image region by pulling content from another image. arXiv preprint arXiv:1803.08435, 2018. 2\n[21] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired imageto- image translation using cycle-consistent adversarial networkss. In Computer Vision (ICCV), 2017 IEEE International Conference on, 2017. 2\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"ff7ce8d87d5df2b0fa146b855faeb92c","permalink":"https://wormcode.github.io/post/sc-fegan/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/sc-fegan/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"翻译 SC-FEGAN:Face Editing Generative Adversarial Network with User’s Sketch and Color","type":"post"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://wormcode.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://wormcode.github.io/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["JM Liu"],"categories":[],"content":" Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 widgets, themes, and language packs included!\nCheck out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\n Setup Academic Get Started View the documentation Ask a question Request a feature or report a bug Updating? View the Update Guide and Release Notes Support development of Academic:  Donate a coffee Become a backer on Patreon Decorate your laptop or journal with an Academic sticker Wear the T-shirt   \nKey features:\n Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 15+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Color Themes Academic comes with day (light) and night (dark) mode built-in. Click the sun/moon icon in the top right of the Demo to see it in action!\nChoose a stunning color and font theme for your site. Themes are fully customizable and include:\n         Ecosystem  Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site Academic Scripts: Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n one-click install using your web browser (recommended) install on your computer using Git with the Command Prompt/Terminal app install on your computer by downloading the ZIP files install on your computer with RStudio  Then personalize and deploy your new site.\nUpdating View the Update Guide.\nFeel free to star the project on Github to help keep track of updates.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://wormcode.github.io/post/getting-started/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Create a beautifully simple website in under 10 minutes.","tags":["Academic"],"title":"Academic: the website builder for Hugo","type":"post"},{"authors":["JM Liu","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://wormcode.github.io/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["JM Liu","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://wormcode.github.io/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"\r\rv\\:* {behavior:url(#default#VML);}\ro\\:* {behavior:url(#default#VML);}\rw\\:* {behavior:url(#default#VML);}\r.shape {behavior:url(#default#VML);}\r\r\r\rcommon\rNormal\rcommon\r2\r233\r2019-06-01T06:56:00Z\r2019-06-01T06:56:00Z\r544\r199385\r1136498\rMicrosoft Corporation\r9470\r2666\r1333217\r14.00\r\r\r\r\r\r\r\r\rfalse\r\r\r9.05 pt\r9.05 pt\r\rfalse\rfalse\rfalse\r\rEN-US\rZH-CN\rX-NONE\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r/* Style Definitions */\rtable.MsoNormalTable\r{mso-style-name:\"Table Normal\";\rmso-tstyle-rowband-size:0;\rmso-tstyle-colband-size:0;\rmso-style-noshow:yes;\rmso-style-priority:99;\rmso-style-parent:\"\";\rmso-padding-alt:0cm 5.4pt 0cm 5.4pt;\rmso-para-margin:0cm;\rmso-para-margin-bottom:.0001pt;\rmso-pagination:none;\rfont-size:12.0pt;\rfont-family:\"Courier New\";}\r\r\r\r\r\rReinforcement\rLearning An Introduction\nSecond edition, in\rprogress ��������Draft��������\nRichard S. Sutton and Andrew G. Barto \u0026copy;2014��2015��2016��2017\nA Bradford Book\nThe MIT Press Cambridge,\rMassachusetts London, England\n\r\rIn memory of A. Harry Klopf\n\r\rContents\n\u0026nbsp;TOC \\o\r\u0026quot;1-5\u0026quot; \\h \\z Preface to the\rFirst Edition\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ix\nPreface to the\rSecond Edition\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; xiii\nSummary of\rNotation\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; xvii\n1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe\u0026nbsp;\u0026nbsp; Reinforcement\rLearning Problem\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 1\n1.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rReinforcement\rLearning............................................................................... 1\n1.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rExamples..................................................................................................... 4\n1.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rElements\rof Reinforcement Learning............................................................ 6\n1.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rLimitations\rand Scope.................................................................................. 7\n1.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rAn\rExtended Example: Tic-Tac-Toe...................................................... \r10\n1.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rSummary................................................................................................ \r15\n1.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Early History of Reinforcement Learning................................................. \r15\n1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\r\u0026nbsp;Tabular Solution Methods\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 25\n2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\r\u0026nbsp;Multi-armed\rBandits\u0026nbsp;\u0026nbsp; 27\n2.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rA\rk-armed Bandit Problem...................................................................... 28\n2.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rAction-value\rMethods............................................................................ 29\n2.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe\r10-armed Testbed.......................................................................... 30\n2.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rIncremental\rImplementation .................................................................... \u0026nbsp;33\n2.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rTracking\ra Nonstationary Problem ......................................................... \u0026nbsp;34\n2.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rOptimistic\rInitial Values .......................................................................... \u0026nbsp;36\n2.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rUpper-Confidence-Bound\rActionSelection............................................. 37\n2.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rGradient\rBandit Algorithms .................................................................... \u0026nbsp;39\n2.9\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rAssociative\rSearch (Contextual Bandits) ............................................... \u0026nbsp;42\n2.10\u0026nbsp;\u0026nbsp;\u0026nbsp;\rSummary\r............................................................................................... \u0026nbsp;43\n3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rFinite Markov\rDecision\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Processes\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 49\n3.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe\rAgent-Environment Interface .......................................................... 49\n3.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rGoals\rand Rewards................................................................................ \r53\n3.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rReturns................................................................................................... \r54\n3.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rUnified\rNotation for Episodic and Continuing Tasks................................. \r57\n3.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\r*The\rMarkov Property............................................................................ \r58\n3.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rMarkov\rDecision Processes.................................................................. 62\n3.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rValue\rFunctions...................................................................................... 65\n3.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rOptimal Value Functions......................................................................... \r70\n3.9\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rOptimality\rand Approximation.................................................................. \r75\n3.10\u0026nbsp;\u0026nbsp;\u0026nbsp; Summary ............................................................................................... \u0026nbsp;76\n4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rDynamic\rProgramming\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 81\n4.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rPolicy\rEvaluation .................................................................................... \u0026nbsp;82\n4.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rPolicy\rImprovement ............................................................................... \u0026nbsp;86\n4.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rPolicy\rIteration ....................................................................................... \u0026nbsp;88\n4.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rValue\rIteration ........................................................................................ \u0026nbsp;91\n4.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rAsynchronous\rDynamic Programming .................................................... \u0026nbsp;93\n4.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rGeneralized\rPolicy Iteration ................................................................... \u0026nbsp;95\n4.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rEfficiency\rof Dynamic Programming........................................................ 96\n4.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Summary ............................................................................................... \u0026nbsp;97\n5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rMonte Carlo\rMethods\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 101\n5.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rMonte\rCarlo Prediction .......................................................................... \u0026nbsp;102\n5.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rMonte\rCarlo Estimation of Action Values ................................................ \u0026nbsp;106\n5.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rMonte Carlo Control ............................................................................... \u0026nbsp;107\n5.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rMonte\rCarlo Control without Exploring Starts .......................................... 110\n5.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rOff-policy\rPrediction via Importance Sampling........................................ 113\n5.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rIncremental\rImplementation .................................................................... \u0026nbsp;119\n5.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rOff-policy\rMonte Carlo Control............................................................... 120\n5.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\r*Discounting-aware\rImportance Sampling.............................................. 122\n5.9\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\r*Per-reward\rImportance Sampling.......................................................... 124\n5.10\u0026nbsp;\u0026nbsp;\u0026nbsp;\rOff-policy\rReturns.................................................................................. 125\n5.11\u0026nbsp;\u0026nbsp;\u0026nbsp; Summary ............................................................................................... \u0026nbsp;125\n6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rTemporal-Difference\rLearning\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 129\n6.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rTD\rPrediction ........................................................................................ \u0026nbsp;129\n6.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rAdvantages\rof TD Prediction Methods ................................................... \u0026nbsp;133\n6.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rOptimality\rof TD(0).................................................................................. 136\n6.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rSarsa:\rOn-policy TD Control................................................................... 139\n6.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rQ-learning: Off-policy TD\rControl........................................................... 142\n6.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rExpected\rSarsa...................................................................................... 144\n6.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rMaximization\rBias and Double Learning................................................... 145\n6.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rGames,\rAfterstates, and Other SpecialCases......................................... 147\n6.9\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Summary................................................................................................. 149\n7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rMulti-step\rBootstrapping\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 153\n7.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rn-step\rTD Prediction.............................................................................. 153\n7.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rn-step Sarsa.......................................................................................... 158\n7.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rn-step\rOff-policy Learning by Importance Sampling ............................... 160\n7.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\r*Per-reward Off-policy Methods ........................................................... \u0026nbsp;162\n7.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rOff-policy Learning Without\rImportanceSampling:\nThe n-step Tree Backup Algorithm......................................................... 163\n7.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\r*A Unifying Algorithm: n-step Q(��).......................................................... 166\n7.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Summary................................................................................................. 170\n8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rPlanning\rand Learning with Tabular\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Methods\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 173\n8.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rModels and Planning............................................................................... 173\n8.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rDyna: Integrating Planning, Acting,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; andLearning.................................... 176\n8.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rWhen the Model Is Wrong ....................................................................... \u0026nbsp;180\n8.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rPrioritized Sweeping .............................................................................. \u0026nbsp;183\n8.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rFull vs. Sample Backups ........................................................................ \u0026nbsp;187\n8.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rTrajectory Sampling ............................................................................... \u0026nbsp;190\n8.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rReal-time Dynamic Programming ............................................................ \u0026nbsp;193\n8.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rPlanning at Decision Time ...................................................................... \u0026nbsp;197\n8.9\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rHeuristic Search .................................................................................... \u0026nbsp;198\n8.10\u0026nbsp;\u0026nbsp;\u0026nbsp;\rRollout Algorithms .................................................................................. \u0026nbsp;200\n8.11\u0026nbsp;\u0026nbsp;\u0026nbsp;\rMonte Carlo Tree Search ....................................................................... \u0026nbsp;202\n8.12\u0026nbsp;\u0026nbsp;\u0026nbsp; Summary ................................................................................................ \u0026nbsp;205\nII\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rApproximate\rSolution Methods\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 208\n9\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rOn-policy Prediction with Approximation\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 211\n9.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rValue-function Approximation ................................................................ \u0026nbsp;211\n9.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe Prediction Objective (MSVE) ........................................................... \u0026nbsp;212\n9.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rStochastic-gradient and Semi-gradientMethods .................................... \u0026nbsp;214\n9.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rLinear Methods ...................................................................................... \u0026nbsp;218\n9.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rFeature\rConstruction for Linear Methods ................................................ \u0026nbsp;224\n9.5.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rPolynomials ................................................................................. \u0026nbsp;224\n9.5.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rFourier Basis .............................................................................. \u0026nbsp;225\n9.5.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rCoarse Coding ............................................................................ \u0026nbsp;228\n9.5.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rTile Coding .................................................................................. \u0026nbsp;231\n9.5.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rRadial Basis Functions ................................................................ \u0026nbsp;235\n9.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rNonlinear Function\rApproximation: Artificial Neural Networks . . . . 236\n9.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rLeast-Squares\rTD ................................................................................. 241\n9.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rMemory-based\rFunction Approximation ................................................. \u0026nbsp;243\n9.9\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rKernel-based\rFunction Approximation .................................................... \u0026nbsp;245\n9.10\u0026nbsp;\u0026nbsp;\rLooking Deeper at On-policy\rLearning: Interest and Emphasis . . . . 246\n9.11\u0026nbsp;\u0026nbsp; Summary ................................................................................................ \u0026nbsp;247\n10\u0026nbsp;\u0026nbsp;\rOn-policy\u0026nbsp;\u0026nbsp;\u0026nbsp; Controlwith Approximation\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 255\n10.1\u0026nbsp;\u0026nbsp;\rEpisodic\rSemi-gradient Control .............................................................. \u0026nbsp;255\n10.2\u0026nbsp;\u0026nbsp;\rn-step\rSemi-gradient Sarsa ................................................................... 259\n10.3\u0026nbsp;\u0026nbsp;\rAverage Reward: A New Problem\rSetting for Continuing Tasks . . . . 261\n10.4\u0026nbsp;\u0026nbsp;\rDeprecating\rthe Discounted Setting ........................................................ \u0026nbsp;264\n10.5\u0026nbsp;\u0026nbsp;\rn-step\rDifferential Semi-gradient Sarsa................................................... 266\n10.6\u0026nbsp;\u0026nbsp; Summary ................................................................................................ \u0026nbsp;267\n11\u0026nbsp;\u0026nbsp;\rOff-policy\u0026nbsp;\u0026nbsp;\u0026nbsp; Methods\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; with\u0026nbsp;\u0026nbsp;\u0026nbsp; Approximation\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 269\n11.1\u0026nbsp;\u0026nbsp;\rSemi-gradient\rMethods .......................................................................... \u0026nbsp;270\n11.2\u0026nbsp;\u0026nbsp;\rExamples\rof Off-policy Divergence ........................................................ \u0026nbsp;272\n11.3\u0026nbsp;\u0026nbsp;\rThe\rDeadly Triad .................................................................................... \u0026nbsp;276\n11.4\u0026nbsp;\u0026nbsp;\rLinear\rValue-function Geometry ............................................................. \u0026nbsp;278\n11.5\u0026nbsp;\u0026nbsp;\rStochastic Gradient Descent in the Bellman Error ................................... \u0026nbsp;282\n11.6\u0026nbsp;\u0026nbsp;\rLearnability of the Bellman Error ............................................................. \u0026nbsp;287\n11.7\u0026nbsp;\u0026nbsp;\rGradient-TD Methods............................................................................. 291\n11.8\u0026nbsp;\u0026nbsp;\rEmphatic-TD Methods ........................................................................... \u0026nbsp;295\n11.9\u0026nbsp;\u0026nbsp;\rReducing Variance ................................................................................. \u0026nbsp;296\n11.10Summary ................................................................................................. \u0026nbsp;298\n12\u0026nbsp;\u0026nbsp;\rEligibility\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Traces\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 301\n12.1\u0026nbsp;\u0026nbsp;\rThe ��-return........................................................................................... 302\n12.2\u0026nbsp;\u0026nbsp;\rTD(��)...................................................................................................... 306\n12.3\u0026nbsp;\u0026nbsp;\rn-step Truncated��-return Methods ...................................................... \u0026nbsp;310\n12.4\u0026nbsp;\u0026nbsp;\rRedoing Updates: The Online��-return Algorithm.................................... 311\n12.5\u0026nbsp;\u0026nbsp;\rTrue Online TD(��)................................................................................... 313\n12.6\u0026nbsp;\u0026nbsp;\rDutch Traces in Monte Carlo Learning .................................................... \u0026nbsp;315\n12.7\u0026nbsp;\u0026nbsp;\rSarsa(��) ................................................................................................. \u0026nbsp;317\n12.8\u0026nbsp;\u0026nbsp;\rVariable A and ��..................................................................................... 322\n12.9\u0026nbsp;\u0026nbsp;\rOff-policy Eligibility Traces..................................................................... 323\n12.10Watkins��s Q(��) to Tree-Backup(��)........................................................... 327\n12.11Stable Off-policy\rMethods with Traces ..................................................... 329\n12.12Implementation\rIssues ............................................................................. \u0026nbsp;330\n12.13Conclusions ............................................................................................. \u0026nbsp;331\n13\u0026nbsp;\u0026nbsp;\rPolicy\rGradient Methods\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 335\n13.1\u0026nbsp;\u0026nbsp;\rPolicy Approximation and its Advantages ............................................... \u0026nbsp;336\n13.2\u0026nbsp;\u0026nbsp;\rThe Policy Gradient Theorem ................................................................. \u0026nbsp;338\n13.3\u0026nbsp;\u0026nbsp;\rREINFORCE: Monte Carlo Policy Gradient ................................................ \u0026nbsp;340\n13.4\u0026nbsp;\u0026nbsp;\rREINFORCE with Baseline ....................................................................... \u0026nbsp;342\n13.5\u0026nbsp;\u0026nbsp;\rActor-Critic Methods ............................................................................. \u0026nbsp;343\n13.6\u0026nbsp;\u0026nbsp;\rPolicy\u0026nbsp;\u0026nbsp; Gradient for Continuing Problems ............................................... \u0026nbsp;345\n13.7\u0026nbsp;\u0026nbsp;\rPolicy\u0026nbsp;\u0026nbsp; Parameterization\rfor Continuous Actions..................................... 348\n13.8\u0026nbsp;\u0026nbsp;\rSummary .................................................................................................. \u0026nbsp;349\nIII\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rLooking Deeper\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 352\n14\u0026nbsp;\u0026nbsp;\rPsychology\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 353\n14.1\u0026nbsp;\u0026nbsp;\rPrediction and Control ............................................................................ \u0026nbsp;354\n14.2\u0026nbsp;\u0026nbsp;\rClassical Conditioning ............................................................................ \u0026nbsp;355\n14.2.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rBlocking and Higher-order Conditioning ..................................... \u0026nbsp;357\n14.2.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe Rescorla-Wagner Model ..................................................... \u0026nbsp;359\n14.2.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe TD Model ............................................................................ \u0026nbsp;361\n14.2.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rTD Model Simulations ................................................................. \u0026nbsp;363\n14.3\u0026nbsp;\u0026nbsp;\rInstrumental Conditioning......................................................................... 372\n14.4\u0026nbsp;\u0026nbsp;\rDelayed Reinforcement........................................................................... 376\n14.5\u0026nbsp;\u0026nbsp;\rCognitive Maps....................................................................................... 378\n14.6\u0026nbsp;\u0026nbsp;\rHabitual and Goal-directed Behavior....................................................... 379\n14.7\u0026nbsp;\u0026nbsp; Summary ................................................................................................ \u0026nbsp;384\n15\u0026nbsp;\u0026nbsp;\rNeuroscience\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 393\n15.1\u0026nbsp;\u0026nbsp;\rNeuroscience Basics .............................................................................. \u0026nbsp;394\n15.2\u0026nbsp;\u0026nbsp;\rReward Signals, Reinforcement\rSignals, Values, and Prediction Errors 396\n15.3\u0026nbsp;\u0026nbsp;\rThe Reward Prediction Error Hypothesis ................................................. \u0026nbsp;398\n15.4\u0026nbsp;\u0026nbsp;\rDopamine ................................................................................................ \u0026nbsp;399\n15.5\u0026nbsp;\u0026nbsp;\rExperimental Support for the\rReward Prediction Error Hypothesis . . 404\n15.6\u0026nbsp;\u0026nbsp;\rTD Error/Dopamine Correspondence ...................................................... \u0026nbsp;406\n15.7\u0026nbsp;\u0026nbsp;\rNeural Actor-Critic ................................................................................... 412\n15.8\u0026nbsp;\u0026nbsp;\rActor and Critic Learning Rules ............................................................... \u0026nbsp;415\n15.9\u0026nbsp;\u0026nbsp;\rHedonistic Neurons ................................................................................. \u0026nbsp;420\n15.10Collective\rReinforcement Learning .......................................................... \u0026nbsp;422\n15.11Model-based\rMethods in the Brain .......................................................... \u0026nbsp;425\n15.12Addiction ................................................................................................. \u0026nbsp;427\n15.13Summary ................................................................................................. \u0026nbsp;428\n16\u0026nbsp;\u0026nbsp;\rApplications\rand Case Studies\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 439\n16.1\u0026nbsp;\u0026nbsp;\rTD-Gammon ........................................................................................... \u0026nbsp;439\n16.2\u0026nbsp;\u0026nbsp;\rSamuel��s Checkers Player......................................................................... 444\n16.3\u0026nbsp;\u0026nbsp;\rThe Acrobot ............................................................................................ \u0026nbsp;447\n16.4\u0026nbsp;\u0026nbsp;\rWatson��s Daily-Double Wagering .............................................................. 451\n16.5\u0026nbsp;\u0026nbsp;\rOptimizing Memory Control........................................................................ 454\n16.6\u0026nbsp;\u0026nbsp;\rHuman-level Video Game Play................................................................... 458\n16.7\u0026nbsp;\u0026nbsp;\rMastering the Game of Go ....................................................................... \u0026nbsp;464\n16.8\u0026nbsp;\u0026nbsp;\rPersonalized Web Services ..................................................................... \u0026nbsp;469\n16.9\u0026nbsp;\u0026nbsp;\rThermal Soaring ...................................................................................... \u0026nbsp;473\n17\u0026nbsp;\u0026nbsp;\rFrontiers\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 477\n\r\r\r479\n\r\r\r\r\rReferences\n\r\rPreface to the First Edition\nWe first came to focus on what is now\rknown as reinforcement learning in late 1979. We were both at the University of\rMassachusetts, working on one of the earliest projects to revive the idea that\rnetworks of neuronlike adaptive elements might prove to be a promising approach\rto artificial adaptive intelligence. The project explored the ��heterostatic\rtheory of adaptive systems�� developed by A. Harry Klopf. Harry��s work was a\rrich source of ideas, and we were permitted to explore them critically and\rcompare them with the long history of prior work in adaptive systems. Our task\rbecame one of teasing the ideas apart and understanding their relationships and\rrelative importance. This continues today, but in 1979 we came to realize that\rperhaps the simplest of the ideas, which had long been taken for granted, had\rreceived surprisingly little attention from a computational perspective. This\rwas simply the idea of a learning system that wants\rsomething, that adapts its behavior in order to maximize a special signal from\rits environment. This was the idea of a ��hedonistic�� learning system, or, as we\rwould say now, the idea of reinforcement learning.\nLike others, we had a sense\rthat reinforcement learning had been thoroughly ex\u0026shy;plored in the early days of\rcybernetics and artificial intelligence. On closer inspection, though, we found\rthat it had been explored only slightly. While reinforcement learn\u0026shy;ing had\rclearly motivated some of the earliest computational studies of learning, most\rof these researchers had gone on to other things, such as pattern classifica\u0026shy;tion,\rsupervised learning, and adaptive control, or they had abandoned the study of\rlearning altogether. As a result, the special issues involved in learning how\rto get something from the environment received relatively little attention. In\rretrospect, focusing on this idea was the critical step that set this branch of\rresearch in motion. Little progress could be made in the computational study of\rreinforcement learning until it was recognized that such a fundamental idea had\rnot yet been thoroughly explored.\nThe field has come a long way\rsince then, evolving and maturing in several direc\u0026shy;tions. Reinforcement learning\rhas gradually become one of the most active research areas in machine learning,\rartificial intelligence, and neural network research. The field has developed\rstrong mathematical foundations and impressive applications. The computational\rstudy of reinforcement learning is now a large field, with hun\u0026shy;dreds of active\rresearchers around the world in diverse disciplines such as psychology, control\rtheory, artificial intelligence, and neuroscience. Particularly important have\rbeen the contributions establishing and developing the relationships to the\rtheory of optimal control and dynamic programming. The overall problem of\rlearning from interaction to achieve goals is still far from being solved, but\rour understanding of it has improved significantly. We can now place component\rideas, such as temporal- difference learning, dynamic programming, and function\rapproximation, within a coherent perspective with respect to the overall\rproblem.\n����ʱ�������������������˺�����һ��·���ڼ���������չ�ͳ��졣ǿ��ѧϰ������Ϊ����ѧϰ���˹����ܺ�������������������Ծ����������֮һ�������������γ���ǿ������ѧ����������ӡ�����̵�Ӧǿ��ѧϰ����������������һ�����������������緶Χ�ڻ�����������Ա������ѧ�������������˹����ܺ�������ѧ�Ȳ�ͬѧ�ƶ��С�������Ҫ���ǽ�������չ���������ƺ���̬�滮��������ϵ��������ѧϰ��ʵ��Ŀ�����������⻹ԶԶû�н����������Ƕ����������������������ߡ��������ڿ�����һ�������ĽǶ�������������������ʱ������ѧϰ����̬�滮�ͺ����ƽ�������˼������һ��\nOur goal in writing this book was to provide a\rclear and simple account of the key ideas and algorithms of reinforcement\rlearning. We wanted our treatment to be accessible to readers in all of the\rrelated disciplines, but we could not cover all of these perspectives in\rdetail. For the most part, our treatment takes the point of view of artificial\rintelligence and engineering. Coverage of connections to other fields we leave\rto others or to another time. We also chose not to produce a rigorous formal\rtreatment of reinforcement learning. We did not reach for the highest possible\rlevel of mathematical abstraction and did not rely on a theorem-proof format.\rWe tried to choose a level of mathematical detail that points the\rmathematically inclined in the right directions without distracting from the\rsimplicity and potential generality of the underlying ideas.\nThe book is largely self-contained. The only\rmathematical background assumed is familiarity with elementary concepts of\rprobability, such as expectations of random variables. Chapter 9 is\rsubstantially easier to digest if the reader has some knowledge of artificial\rneural networks or some other kind of supervised learning method, but it can be\rread without prior background. We strongly recommend working the exercises\rprovided throughout the book. Solution manuals are available to instructors.\rThis and other related and timely material is available via the Internet.\nAt the end of most chapters is a section entitled\r��Bibliographical and Histori\u0026shy;cal Remarks,�� wherein we credit the sources of the\rideas presented in that chapter, provide pointers to further reading and\rongoing research, and describe relevant his\u0026shy;torical background. Despite our\rattempts to make these sections authoritative and complete, we have undoubtedly\rleft out some important prior work. For that we apol\u0026shy;ogize, and welcome\rcorrections and extensions for incorporation into a subsequent edition.\nIn some sense we have been working toward this\rbook for thirty years, and we have lots of people to thank. First, we thank\rthose who have personally helped us develop the overall view presented in this\rbook: Harry Klopf, for helping us recognize that reinforcement learning needed\rto be revived; Chris Watkins, Dimitri Bertsekas, John Tsitsiklis, and Paul\rWerbos, for helping us see the value of the relationships to dynamic\rprogramming; John Moore and Jim Kehoe, for insights and inspirations from\ranimal learning theory; Oliver Selfridge, for emphasizing the breadth and im\u0026shy;portance\rof adaptation; and, more generally, our colleagues and students who have\rcontributed in countless ways: Ron Williams, Charles Anderson, Satinder Singh,\rSridhar Mahadevan, Steve Bradtke, Bob Crites, Peter Dayan, and Leemon Baird.\rOur view of reinforcement learning has been significantly enriched by\rdiscussions with Paul Cohen, Paul Utgoff, Martha Steenstrup, Gerry Tesauro,\rMike Jordan, Leslie Kaelbling, Andrew Moore, Chris Atkeson, Tom Mitchell, Nils\rNilsson, Stuart Russell, Tom Dietterich, Tom Dean, and Bob Narendra. We thank\rMichael Littman,\nGerry Tesauro, Bob Crites, Satinder\rSingh, and Wei Zhang for providing specifics of Sections 4.7, 15.1, 15.4, 15.5,\rand 15.6 respectively. We thank the Air Force Office of Scientific Research,\rthe National Science Foundation, and GTE Laboratories for their long and\rfarsighted support.\nWe also wish to thank the many\rpeople who have read drafts of this book and provided valuable comments,\rincluding Tom Kalt, John Tsitsiklis, Pawel Cichosz, Olle Gallmo, Chuck\rAnderson, Stuart Russell, Ben Van Roy, Paul Steenstrup, Paul Cohen, Sridhar\rMahadevan, Jette Randlov, Brian Sheppard, Thomas O��Connell, Richard Coggins,\rCristina Versino, John H. Hiett, Andreas Badelt, Jay Ponte, Joe Beck, Justus\rPiater, Martha Steenstrup, Satinder Singh, Tommi Jaakkola, Dimitri Bertsekas,\rTorbjorn Ekman, Christina Bjorkman, Jakob Carlstrom, and Olle Palm- gren.\rFinally, we thank Gwyn Mitchell for helping in many ways, and Harry Stanton and\rBob Prior for being our champions at MIT Press.\n\r\rxii\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Prefaceto\u0026nbsp; the\u0026nbsp; First\u0026nbsp; Edition\nPreface to the Second Edition\nThe nearly twenty years since the\rpublication of the first edition of this book have seen tremendous progress in\rartificial intelligence, propelled in large part by advances in machine\rlearning, including advances in reinforcement learning. Although the impressive\rcomputational power that became available is responsible for some of these\radvances, new developments in theory and algorithms have been driving forces as\rwell. In the face of this progress, we decided that a second edition of our\r1998 book was long overdue, and we finally began the project in 2013. Our goal\rfor the second edition was the same as our goal for the first: to provide a\rclear and simple account of the key ideas and algorithms of reinforcement\rlearning that is accessible to readers in all the related disciplines. The\redition remains an introduction, and we retain a focus on core, on-line\rlearning algorithms. This edition includes some new topics that rose to\rimportance over the intervening years, and we expanded coverage of topics that\rwe now understand better. But we made no attempt to provide comprehensive\rcoverage of the field, which has exploded in many different directions with outstanding\rcontributions by many active researchers. We apologize for having to leave out\rall but a handful of these contributions.\nAs for the first edition, we\rchose not to produce a rigorous formal treatment of reinforcement learning, or\rto formulate it in the most general terms. However, since the first edition,\rour deeper understanding of some topics required a bit more mathematics to\rexplain, though we have set off the more mathematical parts in shaded boxes\rthat the non-mathematically-inclined may choose to skip. We also use a slightly\rdifferent notation than we used in the first edition. In teaching, we have\rfound that the new notation helps to address some common points of confusion.\rIt emphasizes the difference between random variables, denoted with capital\rletters, and their instantiations, denoted in lower case. For example, the\rstate, action, and reward at time step t are denoted St, At, and Rt, while their possible\rvalues might be denoted s, a, and r.\rAlong with this, it is natural to use lower case for value functions (e.g., ����) and restrict capitals to their tabular estimates (e.g., Qt(s, a)).\rApproximate value functions are deterministic functions of random parameters\rand are thus also in lower case (e.g., V(s,wt) ~ vn(s)).\rVectors, such as wt and xt, are bold and written in lowercase even if they are\rrandom variables. Uppercase bold is reserved for matrices. All the changes in\rnotation are summarized in a table on page xvii.\nThe contents and scope of the book have been\rsignificantly enlarged compared to the first edition. The most obvious\radditions are the chapters on reinforcement learning��s relationships to\rpsychology (Chapter 14)\rand neuroscience (Chapter 15), and the much more\rextensive treatment of function approximation (Chapters 9-13). The latter\rcomprise the whole second part of the book, whereas the first part of the book\r(Chapters 2-8) is a comprehensive treatment of reinforcement learning while\rrestricting to the tabular case for which exact solutions can be found. More\rsubtly, the second edition significantly expands the treatment of off-policy\rlearning in Chapters 5-7, throughout Chapter 11, and in\rChapter 12(eligibility traces). Another subtle change is that we separate the\rforward-view idea of multi-step bootstrapping (now treated solely and more\rfully in Chapter 7) from the backward-view idea of eligibility traces (now\rgiven their own chapter in the function approximation part of the book). Many\radditional algorithms are presented in the tabular part of the book (e.g., UCB,\rExpected Sarsa, Double learning, n-step methods, tree-backup, Q(a),\rRLDP, and MCTS) and of course in the function approximation chapters (e.g.,\rartificial neural networks, the fourier basis, LSTD, kernel-based methods,\rGradient- TD and Emphatic-TD methods, average-reward methods, true online\rTD(A), and policy-gradient methods). The chapter on case studies has been\rupdated with a selection of recent applications including Atari game playing,\rWatson, and AlphaGo. Still, out of necessity we have included only a small\rsubset of all that is done in the field. Our choices reflect our long-standing\rinterests in inexpensive model-free methods that should scale well to large\rapplications. The final chapter now includes a discussion of the future\rsocietal impacts of reinforcement learning. For better or worse, the second\redition is about two-thirds longer than the first.\nThis book is designed to be\rused as the primary text for a one- or two-semester course on reinforcement\rlearning. For a one-semester course, the first ten chapters should be covered\rin order and form a good core, to which can be added material from the other\rchapters, from other books such as Bertsekas and Tsitsiklis (1996) or\rSzepesvari (2010), or from the literature, according to taste. Depending of the\rstudent��s background, some additional material on online supervised learning\rmay be helpful. I often cover the ideas of options and option models (Sutton,\rPrecup and Singh, 1999). A two-semester course can cover all the chapters as\rwell as supple\u0026shy;mentary material. The book can also be used as part of broader\rcourses on machine learning, artificial intelligence, or neural networks. In\rthis case, it may be desirable to cover only a subset of the material. We\rrecommend covering Chapter 1 for a brief overview, Chapter 2 through Section\r2.2, Chapter 3 except Sections 3.4, 3.5 and 3.9, and then selecting sections\rfrom the remaining chapters according to time and inter\u0026shy;ests. Chapter 6is the most important for the subject and for the rest of the book.\rA course focusing on machine learning or neural networks should cover Chapters\r9 and 10, and a course focusing on artificial intelligence or planning should\rcover Chapter 8. Throughout the book, sections that are more difficult and not\ressential to the rest of the book are marked with a *. These can be omitted on\rfirst reading without creating problems later on. Some exercises are also\rmarked with a * to indicate that they are more advanced and not essential to\runderstanding the basic material of the chapter.\n\r\rMost chapters end with a\rsection entitled ��Bibliographical and Historical Re\u0026shy;marks,�� wherein we credit\rthe sources of the ideas presented in that chapter, provide pointers to further\rreading and ongoing research, and describe relevant historical background.\rDespite our attempts to make these sections authoritative and com\u0026shy;plete, we\rhave undoubtedly left out some important prior work. For that we again apologize,\rand we welcome corrections and extensions for incorporation into the elec\u0026shy;tronic\rversion of the book.\nLike the first edition, this\redition of the book is dedicated to the memory of A. Harry Klopf. It was Harry\rwho introduced us to each other, and it was his ideas about the brain and\rartificial intelligence that launched our long excursion into re\u0026shy;inforcement\rlearning. Trained in neurophysiology and long interested in machine\rintelligence, Harry was a senior scientist affiliated with the Avionics Directorate\rof the Air Force Office of Scientific Research (AFOSR) at Wright-Patterson Air\rForce Base, Ohio. He was dissatisfied with the great importance attributed to\requilibrium- seeking processes, including homeostasis and error-correcting\rpattern classification methods, in explaining natural intelligence and in\rproviding a basis for machine in\u0026shy;telligence. He noted that systems that try to\rmaximize something (whatever that might be) are qualitatively different form\requilibrium-seeking systems, and he ar\u0026shy;gued that maximizing systems hold the\rkey to understanding important aspects of natural intelligence and for building\rartificial intelligences. Harry was instrumental in obtaining funding from\rAFOSR for a project to assess the scientific merit of these and related ideas.\rThe project was conducted in the late 1970s at the University of Massachusetts\rAmherst (UMass Amherst), initially under the direction of Michael Arbib,\rWilliam Kilmer, and Nico Spinelli, professors in the Department of Com\u0026shy;puter\rand Information Science at UMass Amherst, and founding members of the\rCybernetics Center for Systems Neuroscience at the University, a farsighted\rgroup focusing on the intersection of neuroscience and artificial intelligence.\rBarto, a re\u0026shy;cent Ph.D. from the University of Michigan, was hired as post\rdoctoral researcher on the project. Meanwhile, Sutton, an undergraduate\rstudying computer science and psychology at Stanford, had been corresponding\rwith Harry regarding their mutual interest in the role of stimulus timing in\rclassical conditioning. Harry suggested to the UMass group that Sutton would be\ra great addition to the project. Thus, Sut\u0026shy;ton became a UMass graduate student,\rwhose Ph.D. was directed by Barto, who had become an Associate Professor. The\rstudy of reinforcement learning as presented in this book is rightfully an\routcome of that project instigated by Harry and inspired by his ideas. Further,\rHarry was responsible for bringing us, the authors, together in what has been a\rlong and enjoyable interaction. By dedicating this book to Harry we honor his\ressential contributions, not only to the field of reinforcement learning, but\ralso to our collaboration. We also thank Professors Arbib, Kilmer, and Spinelli\rfor the opportunity they provided to us to begin exploring these ideas.\rFinally, we thank AFOSR for generous support over the early years of our\rresearch, and NSF for its generous support over many of the following years.\nWe have very many people to\rthank for their inspiration and help with this sec\u0026shy;ond edition. Everyone we\racknowledged for their inspiration and help with the first edition deserve our\rdeepest gratitude for this edition as well, which would not ex\u0026shy;ist were it not\rfor their contributions to edition number one. To that long list we must add\rmany others who contributed specifically to the second edition. Our stu\u0026shy;dents\rover the many years that we have taught from the first edition contributed in\rcountless ways: exposing errors, offering fixes, and��not the least��being\rconfused in places where we could have explained things better. The chapters on\rpsychology and neuroscience could not have been written without the help of\rmany experts in those fields. We thank John Moore for his patient tutoring over\rmany many years on animal learning experiments, theory, and neuroscience, and\rfor his careful reading of multiple drafts of Chapters 14 and 15. We also thank\rMatt Botvinick, Nathaniel Daw, Peter Dayan, and Yael Niv for their penetrating\rcomments on drafts of these chapter, their essential guidance through the\rmassive literature, and their intercep\u0026shy;tion of many of our errors in early\rdrafts. Of course, the remaining errors in these chapters��and there must still\rbe some��are totally our own. We owe Phil Thomas thanks for helping us make\rthese chapters accessible to non-psychologists and non\u0026shy;neuroscientists. We\rthank Jim Houk for introducing us to the subject of information processing in\rthe basal ganglia. Jose Martinez, Terry Sejnowski, David Silver, Gerry Tesauro,\rGeorgios Theocharous, and Phil Thomas generously helped us understand details\rof their reinforcement learning applications for inclusion in the case-studies\rchapter and commented on drafts of these sections. Special thanks is owed to\rDavid Silver for helping us better understand Monte Carlo Tree Search. We thank\rGeorge Konidaris for his help with the section on the Fourier basis. Emilio\rCartoni, Stefan Dernbach, Clemens Rosenbaum, and Patrick Taylor helped us in a\rnumber important ways for which we are most grateful.\nSutton would also like to thank the members of\rthe Reinforcement Learning and Artificial Intelligence (RLAI) laboratory at the\rUniversity of Alberta for contribu\u0026shy;tions to the second edition. We owe a\rparticular debt to Rupam Mahmood for essen\u0026shy;tial contributions to the treatment\rof off-policy Monte Carlo methods in Chapter 5, to Hamid Maei for helping\rdevelop the perspective on off-policy learning presented in Chapter 11, to Harm\rvan Seijen for insights that led to the separation of n-step methods from\religibility traces and, along with Hado van Hasselt, for the ideas involv\u0026shy;ing\rexact equivalence of forward and backward views of eligibility traces presented\rin Chapter 12. Sutton would also like to gratefully acknowledge the support and\rfreedom he has granted by the Government of Alberta and the National Science\rand Engineering Research Council of Canada throughout the period during which\rthe second edition was conceived and written. In particular, he would like to\rthank Randy Goebel for creating a supportive and far-sighted environment for\rresearch in Alberta.\n\r\r\rCapital let the values to be\rreal variables).\n\r\r\r\r\r\r\r\rSummary of Notation\n\r\r\r\r\rters are used for random variables, whereas lower\rcase letters are used for of random variables and for scalar functions.\rQuantities that are required -valued vectors are written in bold and in lower\rcase (even if random Matrices are bold capitals.\nan\requality relationship that is true by definition expectation of random variable\rX\nprobability that the random\rvariable X takes on the value x (a) a value of a at which f (a) takes its maximal\rvalue\nprobability\rof taking a random action in an ^-greedy policy\nstep-size\rparameters\ndiscount-rate\rparameter\ndecay-rate parameter for\religibility traces\nt problem:\nnumber of actions/arms true value of action a\restimate at time t of q^(a)\nthe number of times action a has been selected up through time t learned preference for\rselecting action a\nov Decision Process: states\raction reward\n\r\r\rs+^:Rt\rT��^t^S\n\r\r\r\r\rset of all nonterminal states\nset of\rall states, including the terminal state\nset of\rall actions possible in state s\nset of all possible rewards\ndiscrete\rtime step\nfinal\rtime step of an episode, or of the episode including time t action at time t\nstate at\rtime t, typically due, stochastically, to St-1and At-1\n\r\rreward at time t, typically due,\rstochastically, to St-i and At-i\n\r\r\rtt i:A AiAi:Ai:\nKG-GGGGG\n\r\r\r\r\rreturn (cumulative discounted reward) following time t (Section 3.3) flat return (uncorrected, undiscounted) from t\r+ 1 to h (Section 5.8) A-return, corrected by estimated\rstate values (Section 12.1)\nA-return, corrected by estimated action values (Section 12.1)\rtruncated, corrected A-return, with state values (Section 12.3) truncated,\rcorrected A-return, with action values (Section 12.3)\n\r\r\rn(s)\nn(a|\nV(s',\np(s'\\\n\r\r\r\r\rpolicy, decision-making rule\n\r\r\ra)\n\r\r\r\r\r\r\r\ra)\n\r\r\r\r\raction taken in state s under\rdeterministic policy n probability of taking action a in state s under stochastic\rpolicy n probability of transition to state sf\rwith reward r, from state s and\raction a probability of transition to state sf, from state s taking\raction a\nbehavior policy selecting actions while\restimating values for target policy n\nor a function giving a baseline b(s) for each\rstate s\n\r\r\rpt\npt:h\nr(n)\nRt\n\r\r\r\r\rimportance sampling ratio for time t\r(Section 5.5)\nimportance sampling ratio for time t to time h (Section 5.5)\naverage reward (reward rate) for policy n\nestimate at time t of r(n)\nvalue of state s under\rpolicy n (expected return)\nvalue of state s under the\roptimal policy\nvalue of taking action a\rin state s under policy n\nvalue of taking action a\rin state s under the optimal policy\narray estimates of state-value function vn or\n\r\r\rSt\n\rtemporal-difference\rerror at t(a random\rvariable) (secrefTD-prediction)\n\r\rw, w\n\rd-vector\rof weights underlying an approximate value function\n\r\r\u0026nbsp;\n\rith\rcomponent of learnable weight vector\n\r\rd\n\rdimensionality��the\rnumber of components of the main weight vector\n\r\rm\n\rnumber\rof 1s in a sparse binary feature\rvector, or\n\r\r\u0026nbsp;\n\rdimensionality\rof a secondary vector\n\r\rV(s,w)\n\rapproximate\rvalue of state sgiven weight vector w\n\r\rVw (s)\n\ralternate\rnotation for V(s,w)\n\r\rq(s,a, w)\n\rapproximate\rvalue of state-action pair s, agiven weight vector w\n\r\rx(s)\n\rvector\rof features visible when in state s\n\r\rx(s, a)\n\rvector\rof features visible when in state staking action a\n\r\rxi(s),xi(s,a)\n\rith\rcomponent of feature vector\n\r\rx\n\rshorthand\rfor x(S) or x(S, A)\n\r\rwTx\n\rinner\rproduct of vectors, wTx J2i w%xi��e.g., V(s,w) == wTx(s)\n\r\rv,vt\n\rsecondary\rd-vector of weights, used to learn w\n\r\ret\n\rd-vector\rof eligibility traces at time t\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\rarray estimates of action-value function qn or q*\nh(s, a)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; learned\rpreference for selecting action a\u0026nbsp;\u0026nbsp; instate s\n6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\r, 6t\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; parameter\rvector of target policy (Chapter\u0026nbsp;\u0026nbsp; 12)\nne\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; policy\rcorresponding to parameter 6\nJ(n), J(6)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; performance\rmeasure for policy n or ne\n\r\rxx\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; SUMMARY\u0026nbsp; OF\u0026nbsp;\u0026nbsp; NOTATION\n\r\rChapter 1\nThe Reinforcement Learning Problem\nThe idea that we learn by interacting\rwith our environment is probably the first to occur to us when we think about the\rnature of learning. When an infant plays, waves its arms, or looks about, it\rhas no explicit teacher, but it does have a direct sensorimotor connection to\rits environment. Exercising this connection produces a wealth of information\rabout cause and effect, about the consequences of actions, and about what to do\rin order to achieve goals. Throughout our lives, such interactions are\rundoubtedly a major source of knowledge about our environment and ourselves.\rWhether we are learning to drive a car or to hold a conversation, we are\racutely aware of how our environment responds to what we do, and we seek to\rinfluence what happens through our behavior. Learning from interaction is a\rfoundational idea underlying nearly all theories of learning and intelligence.\nIn this book we explore a computational\rapproach to learning from interaction. Rather than directly theorizing about\rhow people or animals learn, we explore ide\u0026shy;alized learning situations and\revaluate the effectiveness of various learning methods. That is, we adopt the\rperspective of an artificial intelligence researcher or engineer. We explore\rdesigns for machines that are effective in solving learning problems of\rscientific or economic interest, evaluating the designs through mathematical\ranalysis or computational experiments. The approach we explore, called reinforcement learn\u0026shy;ing ,is much more focused on goal-directed\rlearning from interaction than are other approaches to machine learning.\n�ڱ����У�����̽����һ�ִ�������ѧϰ�����㷽���� ��������ֱ�������˻���������ѧϰ������̽��������ѧϰ�龳������������ѧϰ��������Ч�ԡ� Ҳ����˵�����������˹����������߻򹤳�ʦ�����㡣 ����̽����Ч������ѧ��������Ȥ��ѧϰ�����Ļ���������ͨ����ѧ����������ʵ������������ ����̽���ķ�����Ϊǿ��ѧϰ������������ѧϰ������ע���ڽ�����Ŀ�굼��ѧϰ\n1.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rReinforcement Learning\nReinforcement learning, like many\rtopics whose names end with ��ing,�� such as ma\u0026shy;chine learning and\rmountaineering, is simultaneously a problem, a class of solution methods that\rwork well on the class of problems, and the field that studies these prob\u0026shy;lems\rand their solution methods. Reinforcement learning problems involve learning\rwhat to do��how to map situations to actions��so as to maximize a numerical re\u0026shy;ward\rsignal. In an essential way these are closed-loop\rproblems because the learning system��s actions influence its later inputs.\rMoreover, the learner is not told which actions to take, as in many forms of\rmachine learning, but instead must discover which actions yield the most reward\rby trying them out. In the most interesting and challenging cases, actions may\raffect not only the immediate reward but also the next situation and, through\rthat, all subsequent rewards. These three characteristics�� being closed-loop in\ran essential way, not having direct instructions as to what actions to take,\rand where the consequences of actions, including reward signals, play out over\rextended time periods��are the three most important distinguishing features of\rthe reinforcement learning problem.\nǿ��ѧϰ�������������ԡ�ing����β������������������ѧϰ�͵�ɽ��ͬʱҲ��һ��������һ����������������������Ч�ķ������Լ�������Щ�����������������Ľ��������� ǿ��ѧϰ�����漰ѧϰ��ʲô|�������龰ӳ�䵽��Ϊ|���������޶ȵ��������ֻ�����ѧϰ�����Ǳ���ͨ��������������Щ��Ϊ���������Ļ����� ������Ȥ��������ս�Ե������£�������������Ӱ��ֱ��������������Ӱ����һ����������ͨ����ЩӰ����Ӱ�����к��������� �������ص�����һ�������ķ�ʽ�ջ��ģ�û��ֱ�ӵ�ָʾ��Ҫ��ȡʲô�������Լ������������������������ĺ�������ʱ���ڷ������ã��������ص�����������Ҫ������������\rǿ��ѧϰ������\nA full specification of the\rreinforcement learning problem in terms of the optimal control of Markov\rdecision processes (MDPs) must wait until Chapter 3, but the basic idea is\rsimply to capture the most important aspects of the real problem facing a\rlearning agent interacting with its environment to achieve a goal. Clearly,\rsuch an agent must be able to sense the state of the environment to some extent\rand must be able to take actions that affect the state. The agent also must\rhave a goal or goals relating to the state of the environment. The MDP\rformulation is intended to include just these three aspects��sensation, action,\rand goal��in their simplest possible forms without trivializing any of them. Any\rmethod that is well suited to solving such problems we consider to be a\rreinforcement learning method.\nReinforcement learning is\rdifferent from supervised learning, the kind of learning\rstudied in most current research in the field of machine learning. Supervised\rlearn\u0026shy;ing is learning from a training set of labeled examples provided by a\rknowledgable external supervisor. Each example is a description of a situation\rtogether with a specification��the label��of the correct action the system should\rtake to that situa\u0026shy;tion, which is often to identify a category to which the\rsituation belongs. The object of this kind of learning is for the system to\rextrapolate, or generalize, its responses so that it acts correctly in situations\rnot present in the training set. This is an important kind of learning, but\ralone it is not adequate for learning from interac\u0026shy;tion. In interactive\rproblems it is often impractical to obtain examples of desired behavior that\rare both correct and representative of all the situations in which the agent\rhas to act. In uncharted territory��where one would expect learning to be most\rbeneficial��an agent must be able to learn from its own experience.\nReinforcement learning is also\rdifferent from what machine learning researchers call unsupervised\rlearning, which is typically about finding structure hidden in\rcollections of unlabeled data. The terms supervised learning and unsupervised\rlearning appear to exhaustively classify machine learning paradigms, but they\rdo not. Although one might be tempted to think of reinforcement learning as a\rkind of unsupervised learn\u0026shy;ing because it does not rely on examples of correct\rbehavior, reinforcement learning is trying to maximize a reward signal instead\rof trying to find hidden structure. Un\u0026shy;covering structure in an agent��s\rexperience can certainly be useful in reinforcement learning, but by itself\rdoes not address the reinforcement learning agent��s problem of maximizing a\rreward signal. We therefore consider reinforcement learning to be a third\rmachine learning paradigm, alongside supervised learning and unsupervised\nlearning,\rand perhaps other paradigms as well.\nǿ��ѧϰҲ������ѧϰ��������֮Ϊ������ѧϰ�ķ�ʽ��ͬ������ͨ����Ѱ��������δ�������ݼ����е��ṹ��\r��������ѧϰ��������ѧϰ�ƺ�������ѧϰ�������������׵ķ�������������ȴû�С� ��Ȼ��������������ǿ��ѧϰ����һ��������ѧϰ����Ϊ������������ȷ��Ϊ�����ӣ�ǿ��ѧϰ��ͼ������һ��������������������ͼ�ҵ����ص��ṹ�� �ڴ����˵������н�ʾ�ṹ��Ȼ��������ǿ��ѧϰ�����Ǳ�����û�н���ǿ��ѧϰ���������������������������� ���ˣ�������Ϊǿ��ѧϰ������ѧϰ��������ѧϰ�ĵ���������ѧϰ��ʽѧϰ��Ҳ����������������\nOne of the challenges that\rarise in reinforcement learning, and not in other kinds of learning, is the\rtrade-off between exploration and exploitation. To obtain a lot of reward, a\rreinforcement learning agent must prefer actions that it has tried in the past\rand found to be effective in producing reward. But to discover such actions, it\rhas to try actions that it has not selected before. The agent has to exploit what it already knows in order to obtain reward, but\rit also has to explore in order to make better action\rselections in the future. The dilemma is that neither exploration nor\rexploitation can be pursued exclusively without failing at the task. The agent\rmust try a variety of actions and progressively favor\rthose that appear to be best. On a stochastic task, each action must be tried\rmany times to gain a reliable estimate of its expected reward. The exploration-exploitation\rdilemma has been intensively studied by mathematicians for many decades (see\rChapter 2). For now, we simply note that the entire issue of balancing\rexploration and exploitation does not even arise in supervised and unsupervised\rlearning, at least in their purest forms.\nAnother key feature of\rreinforcement learning is that it explicitly considers the whole\rproblem of a goal-directed agent interacting with an uncertain environment.\rThis is in contrast with many approaches that consider subproblems without\raddress\u0026shy;ing how they might fit into a larger picture. For example, we have\rmentioned that much of machine learning research is concerned with supervised\rlearning without ex\u0026shy;plicitly specifying how such an ability would finally be\ruseful. Other researchers have developed theories of planning with general\rgoals, but without considering planning��s role in real-time decision-making, or\rthe question of where the predictive models nec\u0026shy;essary for planning would come\rfrom. Although these approaches have yielded many useful results, their focus\ron isolated subproblems is a significant limitation.\nReinforcement learning takes\rthe opposite tack, starting with a complete, interac\u0026shy;tive, goal-seeking agent.\rAll reinforcement learning agents have explicit goals, can sense aspects of\rtheir environments, and can choose actions to influence their envi\u0026shy;ronments.\rMoreover, it is usually assumed from the beginning that the agent has to\roperate despite significant uncertainty about the environment it faces. When reinforcement\rlearning involves planning, it has to address the interplay between planning\rand real-time action selection, as well as the question of how environment\rmodels are acquired and improved. When reinforcement learning involves\rsupervised learning, it does so for specific reasons that determine which\rcapabilities are critical and which are not. For learning research to make\rprogress, important subproblems have to be isolated and studied, but they\rshould be subproblems that play clear roles in complete, interactive,\rgoal-seeking agents, even if all the details of the complete agent cannot yet\rbe filled in.\nNow by a complete,\rinteractive, goal-seeking agent we do not always mean some\u0026shy;thing like a\rcomplete organism or robot. These are clearly examples, but a complete,\rinteractive, goal-seeking agent can also be a component of a larger behaving\rsystem. In this case, the agent directly interacts with the rest of the larger\rsystem and indi\u0026shy;rectly interacts with the larger system��s environment. A simple\rexample is an agent that monitors the charge level of robot��s battery and sends\rcommands to the robot��s control architecture. This agent��s environment is the\rrest of the robot together with the robot��s environment. One must look beyond\rthe most obvious examples of agents and their environments to appreciate the\rgenerality of the reinforcement learning framework.\nOne of the most exciting\raspects of modern reinforcement learning is its sub\u0026shy;stantive and fruitful\rinteractions with other engineering and scientific disciplines. Reinforcement\rlearning is part of a decades-long trend within artificial intelligence and\rmachine learning toward greater integration with statistics, optimization, and\rother mathematical subjects. For example, the ability of some reinforcement\rlearning methods to learn with parameterized approximators addresses the\rclassical ��curse of dimensionality�� in operations research and control theory.\rMore distinctively, rein\u0026shy;forcement learning has also interacted strongly with\rpsychology and neuroscience, with substantial benefits going both ways. Of all\rthe forms of machine learning, reinforcement learning is the closest to the\rkind of learning that humans and other animals do, and many of the core\ralgorithms of reinforcement learning were originally inspired by biological\rlearning systems. And reinforcement learning has also given back, both through\ra psychological model of animal learning that better matches some of the\rempirical data, and through an influential model of parts of the brain��s reward\rsystem. The body of this book develops the ideas of reinforcement learning that\rpertain to engineering and artificial intelligence, with connections to\rpsychology and neuroscience summarized in Chapters 14 and 15.\nFinally, reinforcement learning is also part of a larger trend in\rartificial intelligence back toward simple general principles. Since the late\r1960��s, many artificial intel\u0026shy;ligence researchers presumed that there are no\rgeneral principles to be discovered, that intelligence is instead due to the possession\rof vast numbers of special purpose tricks, procedures, and heuristics. It was\rsometimes said that if we could just get enough relevant facts into a machine,\rsay one million, or one billion, then it would become intelligent. Methods\rbased on general principles, such as search or learning, were characterized as\r��weak methods,�� whereas those based on specific knowledge were called ��strong\rmethods.�� This view is still common today, but much less dom\u0026shy;inant. From our\rpoint of view, it was simply premature: too little effort had been put into the\rsearch for general principles to conclude that there were none. Modern AI now\rincludes much research looking for general principles of learning, search, and\rdecision-making, as well as trying to incorporate vast amounts of domain\rknowledge. It is not clear how far back the pendulum will swing, but\rreinforcement learning re\u0026shy;search is certainly part of the swing back toward\rsimpler and fewer general principles of artificial intelligence.\n1.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rExamples\nA good way to understand reinforcement learning is to consider some\rof the examples and possible applications that have guided its development.\n\u0026#8226;\u0026nbsp;\u0026nbsp;\rA master chess player makes a\rmove. The choice is informed both by planning��\n\r\ranticipating possible replies and counterreplies��and by immediate,\rintuitive judgments of the desirability of particular positions and moves.\n\u0026#8226;\u0026nbsp;\u0026nbsp;\rAn adaptive controller adjusts\rparameters of a petroleum refinery��s operation in real time. The controller\roptimizes the yield/cost/quality trade-off on the basis of specified marginal\rcosts without sticking strictly to the set points originally suggested by\rengineers.\n\u0026#8226;\u0026nbsp;\u0026nbsp;\rA gazelle calf struggles to its\rfeet minutes after being born. Half an hour later it is running at 20miles per hour.\n\u0026#8226;\u0026nbsp;\u0026nbsp;\rA mobile robot decides whether\rit should enter a new room in search of more trash to collect or start trying\rto find its way back to its battery recharging station. It makes its decision\rbased on the current charge level of its battery and how quickly and easily it\rhas been able to find the recharger in the past.\n\u0026#8226;\u0026nbsp;\u0026nbsp;\rPhil prepares his breakfast. Closely\rexamined, even this apparently mundane activity reveals a complex web of\rconditional behavior and interlocking goal- subgoal relationships: walking to\rthe cupboard, opening it, selecting a cereal box, then reaching for, grasping,\rand retrieving the box. Other complex, tuned, interactive sequences of behavior\rare required to obtain a bowl, spoon, and milk jug. Each step involves a series\rof eye movements to obtain information and to guide reaching and locomotion.\rRapid judgments are continually made about how to carry the objects or whether\rit is better to ferry some of them to the dining table before obtaining others.\rEach step is guided by goals, such as grasping a spoon or getting to the\rrefrigerator, and is in service of other goals, such as having the spoon to eat\rwith once the cereal is prepared and ultimately obtaining nourishment. Whether\rhe is aware of it or not, Phil is accessing information about the state of his\rbody that determines his nutritional needs, level of hunger, and food preferences.\nThese examples share features\rthat are so basic that they are easy to overlook. All involve interaction\rbetween an active decision-making agent and its environment, within which the\ragent seeks to achieve a goal despite uncertainty\rabout its environ\u0026shy;ment. The agent��s actions are permitted to affect the future\rstate of the environment (e.g., the next chess position, the level of\rreservoirs of the refinery, the robot��s next location and the future charge\rlevel of its battery), thereby affecting the options and opportunities\ravailable to the agent at later times. Correct choice requires taking into\raccount indirect, delayed consequences of actions, and thus may require\rforesight or planning.\nAt the same time, in all these\rexamples the effects of actions cannot be fully predicted; thus the agent must\rmonitor its environment frequently and react appro\u0026shy;priately. For example, Phil\rmust watch the milk he pours into his cereal bowl to keep it from overflowing.\rAll these examples involve goals that are explicit in the sense that the agent\rcan judge progress toward its goal based on what it can sense directly. The\rchess player knows whether or not he wins, the refinery controller knows how\rmuch petroleum is being produced, the mobile robot knows when its batteries run\rdown, and Phil knows whether or not he is enjoying his breakfast.\nIn all of these examples the agent can use its experience to improve\rits performance over time. The chess player refines the intuition he uses to\revaluate positions, thereby improving his play; the gazelle calf improves the\refficiency with which it can run; Phil learns to streamline making his\rbreakfast. The knowledge the agent brings to the task at the start��either from\rprevious experience with related tasks or built into it by design or evolution��influences\rwhat is useful or easy to learn, but interaction with the environment is\ressential for adjusting behavior to exploit specific features of the task.\n1.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rElements of Reinforcement\rLearning\nBeyond the agent and the environment,\rone can identify four main subelements of a reinforcement learning system: a policy, a reward signal, a value function, and, optionally, a model\rof the environment.\nA policy\rdefines the learning agent��s way of behaving at a given time. Roughly speaking,\ra policy is a mapping from perceived states of the environment to actions to be\rtaken when in those states. It corresponds to what in psychology would be\rcalled a set of stimulus-response rules or associations (provided that stimuli\rinclude those that can come from within the animal). In some cases the policy\rmay be a simple function or lookup table, whereas in others it may involve\rextensive computation such as a search process. The policy is the core of a\rreinforcement learning agent in the sense that it alone is sufficient to\rdetermine behavior. In general, policies may be stochastic.\nA reward\rsignal defines the goal in a reinforcement learning problem. On each\rtime step, the environment sends to the reinforcement learning agent a single\rnumber, a reward. The agent��s sole objective is to\rmaximize the total reward it receives over the long run. The reward signal thus\rdefines what are the good and bad events for the agent. In a biological system,\rwe might think of rewards as analogous to the experiences of pleasure or pain.\rThey are the immediate and defining features of the problem faced by the agent.\rAs such, the process that generates the reward signal must be unalterable by\rthe agent. The agent can alter the signal that the process produces directly by\rits actions and indirectly by changing its environment��s state�� since the\rreward signal depends on these��but it cannot change the function that generates\rthe signal. In other words, the agent cannot simply change the problem it is\rfacing into another one. The reward signal is the primary basis for altering\rthe policy. If an action selected by the policy is followed by low reward, then\rthe policy may be changed to select some other action in that situation in the\rfuture. In general, reward signals may be stochastic functions of the state of\rthe environment and the actions taken. In Chapter 3 we explain how the idea of\ra reward function being unalterable by the agent is consistent with what we see\rin biology where reward signals are generated within an animal��s brain.\nWhereas the reward signal indicates what is good\rin an immediate sense, a value function specifies what\ris good in the long run. Roughly speaking, the value of\ra state is the total amount of reward an agent can expect to accumulate over\rthe future, starting from that state. Whereas rewards determine the immediate,\rintrin\u0026shy;sic desirability of environmental states, values indicate the long-term desirability of states after taking into account the\rstates that are likely to follow, and the rewards available in those states.\rFor example, a state might always yield a low immediate reward but still have a\rhigh value because it is regularly followed by other states that yield high\rrewards. Or the reverse could be true. To make a human analogy, rewards are\rsomewhat like pleasure (if high) and pain (if low), whereas values correspond\rto a more refined and farsighted judgment of how pleased or displeased we are\rthat our environment is in a particular state. Expressed this way, we hope it\ris clear that value functions formalize a basic and familiar idea.\nRewards are in a sense primary, whereas values,\ras predictions of rewards, are secondary. Without rewards there could be no\rvalues, and the only purpose of es\u0026shy;timating values is to achieve more reward.\rNevertheless, it is values with which we are most concerned when making and\revaluating decisions. Action choices are made based on value judgments. We seek\ractions that bring about states of highest value, not highest reward, because\rthese actions obtain the greatest amount of reward for us over the long run.\rUnfortunately, it is much harder to determine values than it is to determine\rrewards. Rewards are basically given directly by the environment, but values\rmust be estimated and re-estimated from the sequences of observations an agent\rmakes over its entire lifetime. In fact, the most important component of almost\rall reinforcement learning algorithms we consider is a method for efficiently\restimat\u0026shy;ing values. The central role of value estimation is arguably the most\rimportant thing we have learned about reinforcement learning over the last few\rdecades.\nThe fourth and final element of some reinforcement learning systems\ris a model of the environment. This is something that\rmimics the behavior of the environment, or more generally, that allows\rinferences to be made about how the environment will behave. For example, given\ra state and action, the model might predict the resultant next state and next\rreward. Models are used for planning, by which we mean\rany way of deciding on a course of action by considering possible future\rsituations before they are actually experienced. Methods for solving\rreinforcement learning problems that use models and planning are called model-based methods, as opposed to simpler model-free\rmethods that are explicitly trial-and-error learners��viewed as almost the opposite of planning. In Chapter 8we explore\rreinforcement learning systems that simultaneously learn by trial and error,\rlearn a model of the environment, and use the model for planning. Modern\rreinforcement learning spans the spectrum from low-level, trial-and-error\rlearning to high-level, deliberative planning.\n1.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rLimitations and Scope\nMost of the reinforcement learning methods we\rconsider in this book are struc\u0026shy;tured around estimating value functions, but it\ris not strictly necessary to do this to solve reinforcement learning problems.\rFor example, methods such as genetic algo\u0026shy;rithms, genetic programming,\rsimulated annealing, and other optimization methods have been used to approach\rreinforcement learning problems without ever appealing to value functions.\rThese methods evaluate the ��lifetime�� behavior of many non\u0026shy;learning agents,\reach using a different policy for interacting with its environment, and select\rthose that are able to obtain the most reward. We call these evolution\u0026shy;ary\rmethods because their operation is analogous to the way biological evolution\rproduces organisms with skilled behavior even when they do not learn during\rtheir individual lifetimes. If the space of policies is sufficiently small, or\rcan be structured so that good policies are common or easy to find��or if a lot\rof time is available for the search��then evolutionary methods can be effective.\rIn addition, evolutionary methods have advantages on problems in which the\rlearning agent cannot accurately sense the state of its environment.\nOur focus is on reinforcement\rlearning methods that involve learning while inter\u0026shy;acting with the environment,\rwhich evolutionary methods do not do (unless they evolve learning algorithms,\ras in some of the approaches that have been studied). It is our belief that\rmethods able to take advantage of the details of individual be\u0026shy;havioral\rinteractions can be much more efficient than evolutionary methods in many\rcases. Evolutionary methods ignore much of the useful structure of the\rreinforce\u0026shy;ment learning problem: they do not use the fact that the policy they\rare searching for is a function from states to actions; they do not notice\rwhich states an individual passes through during its lifetime, or which actions\rit selects. In some cases this information can be misleading (e.g., when states\rare misperceived), but more often it should enable more efficient search.\rAlthough evolution and learning share many features and naturally work\rtogether, we do not consider evolutionary methods by themselves to be\respecially well suited to reinforcement learning problems. For sim\u0026shy;plicity, in\rthis book when we use the term ��reinforcement learning method�� we do not\rinclude evolutionary methods.\nHowever, we do include some\rmethods that, like evolutionary methods, do not appeal to value functions.\rThese methods search in spaces of policies defined by a collection of numerical\rparameters. They estimate the directions the parameters should be adjusted in\rorder to most rapidly improve a policy��s performance. Un\u0026shy;like evolutionary\rmethods, however, they produce these estimates while the agent is interacting\rwith its environment and so can take advantage of the details of individ\u0026shy;ual\rbehavioral interactions. Methods like this, called policy\rgradient methods, have proven useful in many problems, and some of the\rsimplest reinforcement learning methods fall into this category. In fact, some\rof these methods take advantage of value function estimates to improve their\rgradient estimates. Overall, the distinc\u0026shy;tion between policy gradient methods\rand other methods we include as reinforcement learning methods is not sharply\rdefined.\nReinforcement learning��s\rconnection to optimization methods deserves some ad\u0026shy;ditional comment because it\ris a source of a common misunderstanding. When we say that a reinforcement\rlearning agent��s goal is to maximize a numerical reward signal, we of course\rare not insisting that the agent has to actually achieve the goal of maximum\rreward. Trying to maximize a quantity does not mean that\rthat quan\u0026shy;tity is ever maximized. The point is that a reinforcement learning\ragent is always trying to increase the amount of reward it receives. Many\rfactors can prevent it from achieving the maximum, even if one exists. In other\rwords, optimization is not the same as optimality.\nWhether or not an optimization method ever\rachieves optimality, designing arti\u0026shy;ficial intelligence systems based on\roptimization requires care because the behavior of these systems is not always\rpredictable. Reinforcement learning agents sometimes discover unexpected ways\rof making their environments deliver reward. From one perspective, this is a\rdesirable property of intelligence: it is a kind of creativity. A process based\ron variation and selection��the essence of both evolution and reinforce\u0026shy;ment\rlearning��can discover new paths to success for whatever challenges an animal\rpopulation or an artificial intelligence faces. But it raises the important\rissue of how to make sure these unexpected ��solutions�� do not have unintended\rand undesirable consequences.\nThis concern is hardly new with reinforcement\rlearning; it is a primal theme in lit\u0026shy;erature (for example, Goethe��s 1797\rpoem��The Sorcerer��s Apprentice����among many others) and\ris summed up by the trope ��Be careful what you wish for because you just might\rget it!�� Approaches to reducing the severity of this problem, such as en\u0026shy;forcing\rconstraints during optimization or by adjusting objective functions to make\roptimization sensitive to risk, are only partial solutions. Standard\rengineering prac\u0026shy;tice has long required careful examination of any result of an\roptimization process before using that result in constructing a product, a\rstructure, or any real-world system whose safe performance people will rely\rupon. This is also essential prac\u0026shy;tice for engineering uses of reinforcement\rlearning, and special care is needed if a reinforcement learning system is\rdeployed on-line in a domain in which unforeseen consequences can be\runacceptable��not just for the reinforcement learning agent, but also for the\ragent��s environment and the people in it. The fast pace of artifi\u0026shy;cial\rintelligence, especially as machine learning systems are enabling super-human\rperformance in certain domains, is bringing this concern to the fore.\nThe unpredictability of optimization is just one\raspect of the wider subject of how reinforcement learning systems can be\rdeployed responsibly in the real world. This, again, is not significantly\rdifferent from the same concern about other engineering technologies, and many\rapproaches to mitigating the risk of unwanted consequences have been developed.\rParticularly relevant are approaches to mitigating risk in ap\u0026shy;plications of\roptimal control methods, some of which have been adapted to reinforce\u0026shy;ment\rlearning. This is a large and complicated subject, with many dimensions, that\rgoes beyond what we are attempting to cover in this introductory text. However,\rwe cannot emphasize too strongly that when treated as an engineering\rmethodology�� and not just as a theory about learning and\rintelligence��reinforcement learning is subject to all the cautions that guide\rthe application of any engineering methodology.\n\r\r1.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rAn Extended Example:\rTic-Tac-Toe\nTo illustrate the general idea of\rreinforcement learning and contrast it with other approaches, we next consider\ra single example in more detail.\n\r\r\rX\n\rO\n\rO\n\r\rO\n\rX\n\rX\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\rX\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\rConsider the familiar child��s game of tic-tac-toe.\rTwo play\u0026shy;ers take turns playing on a three-by-three board. One player plays Xs\rand the other Os until one player wins by placing three marks in a row,\rhorizontally, vertically, or diagonally, as the X player has in the game shown\rto the right. If the board fills up with neither player getting three in a row,\rthe game is a draw. Because a skilled player can play so as never to lose, let\rus assume that we are playing against an imperfect\nplayer, one whose play is sometimes\rincorrect and allows us to win. For the moment, in fact, let us consider draws\rand losses to be equally bad for us. How might we construct a player that will\rfind the imperfections in its opponent��s play and learn to maximize its chances\rof winning?\nAlthough this is a simple\rproblem, it cannot readily be solved in a satisfactory way through classical\rtechniques. For example, the classical ��minimax�� solution from game theory is\rnot correct here because it assumes a particular way of playing by the\ropponent. For example, a minimax player would never reach a game state from\rwhich it could lose, even if in fact it always won from that state because of\rincorrect play by the opponent. Classical optimization methods for sequential\rdecision problems, such as dynamic programming, can compute\ran optimal solution for any opponent, but require as input a complete\rspecification of that opponent, including the probabilities with which the\ropponent makes each move in each board state. Let us assume that this\rinformation is not available a priori for this problem, as it is not for the\rvast majority of problems of practical interest. On the other hand, such\rinformation can be estimated from experience, in this case by playing many\rgames against the opponent. About the best one can do on this problem is first\rto learn a model of the opponent��s behavior, up to some level of confidence,\rand then apply dynamic programming to compute an optimal solution given the\rapproximate opponent model. In the end, this is not that different from some of\rthe reinforcement learning methods we examine later in this book.\nAn evolutionary method applied\rto this problem would directly search the space of possible policies for one\rwith a high probability of winning against the opponent. Here, a policy is a\rrule that tells the player what move to make for every state of the game��every\rpossible configuration of Xs and Os on the three-by-three board. For each\rpolicy considered, an estimate of its winning probability would be obtained by\rplaying some number of games against the opponent. This evaluation would then\rdirect which policy or policies were considered next. A typical evolutionary\rmethod would hill-climb in policy space, successively generating and evaluating\rpolicies in an attempt to obtain incremental improvements. Or, perhaps, a\rgenetic-style algorithm could be used that would maintain and evaluate a\rpopulation of policies. Literally hundreds of different optimization methods\rcould be applied.\nHere is how the tic-tac-toe problem would be\rapproached with a method making use of a value function. First we set up a\rtable of numbers, one for each possible state of the game. Each number will be\rthe latest estimate of the probability of our winning from that state. We treat\rthis estimate as the state��s value, and the whole table\ris the learned value function. State A has higher value than state B, or is\rconsidered ��better�� than state B, if the current estimate of the probability of\rour winning from A is higher than it is from B. Assuming we always play Xs,\rthen for all states with three Xs in a row the probability of winning is 1,\rbecause we have already won. Similarly, for all states with three Os in a row,\ror that are ��filled up,�� the correct probability is 0, as we cannot win from\rthem. We set the initial values of all the other states to 0.5, representing a\rguess that we have a 50% chance of winning.\nWe play many games against the opponent. To\rselect our moves we examine the states that would result from each of our\rpossible moves (one for each blank space on the board) and look up their\rcurrent values in the table. Most of the time we move greedily,\rselecting the move that leads to the state with greatest value, that is, with\rthe highest estimated probability of winning. Occasionally, however, we select\rrandomly from among the other moves instead. These are called exploratory\rmoves because they cause us to experience states that we might otherwise never\rsee. A sequence of moves made and considered during a game can be diagrammed as\rin Figure 1.1.\nWhile we are playing, we change the values of the states in which we\rfind ourselves during the game. We attempt to make them more accurate estimates\rof the proba\u0026shy;bilities of winning. To do this, we ��back up�� the value of the\rstate after each greedy move to the state before the move, as suggested by the\rarrows in Figure 1.1. More precisely, the current value of the earlier state is\radjusted to be closer to the value of the later state. This can be done by\rmoving the earlier state��s value a fraction of the way toward the value of the\rlater state. If we let s denote the state before the greedy move, and sf the state after the move, then the update to the\restimated value of s, denoted V(s),\rcan be written as\nV(s) ^ V(s) + a[v(s') - V(s^ ,\nwhere a is a small\rpositive fraction called the step-size parameter, which\rinfluences the rate of learning. This update rule is an example of a temporal-difference learning method, so called because its\rchanges are based on a difference, V(s') �� V(s), between estimates at two\rdifferent times.\nThe method described above performs quite well on\rthis task. For example, if the step-size parameter is reduced properly over\rtime (see page 35), this method converges, for any fixed opponent, to the true\rprobabilities of winning from each state given optimal play by our player.\rFurthermore, the moves then taken (except on exploratory moves) are in fact the\roptimal moves against this (imperfect) opponent. In other words, the method\rconverges to an optimal policy for playing the game against this opponent. If\rthe step-size parameter is not reduced all the way to zero over time, then this\rplayer also plays well against opponents that slowly change their\n\r\ropponent's move\n\r\r\rstarting position\n\r\r\n\r\r\r\r\rour move\nopponent's\rmove\n\r\r\r\r\rFigure 1.1: A\rsequence of tic-tac-toe moves. The solid lines represent the moves taken\rduring a game; the dashed lines represent moves that we (our reinforcement\rlearning player) considered but did not make. Our second move was an\rexploratory move, meaning that it was taken even though another sibling\rmove, the one leading to e*, was ranked higher. Exploratory moves do not\rresult in any learning, but each of our other moves does, causing backupsas suggested by the curved arrows and detailed in the text.\n\r\r\r\r\r\r\r\ropponent's move\n\r\r\r\r\r\r\r\rour move\n\r\r\r\r\r\r\r\rour move\n\r\r\r\r\r\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rway of\rplaying.\nThis example illustrates the\rdifferences between evolutionary methods and the methods that learn value\rfunctions. To evaluate a policy an evolutionary method holds the policy fixed\rand plays many games against the opponent, or simulates many games using a model\rof the opponent. The frequency of wins gives an unbiased estimate of the\rprobability of winning with that policy, and can be used to direct the next\rpolicy selection. But each policy change is made only after many games, and\ronly the final outcome of each game is used: what happens during\rthe games is ignored. For example, if the player wins, then all\rof its behavior in the game is given credit, independently of how specific\rmoves might have been critical to the win. Credit is even given to moves that\rnever occurred! Value function methods, in contrast, allow individual states to\rbe evaluated. In the end, evolutionary and value function methods both search\rthe space of policies, but learning a value function takes advantage of\rinformation available during the course of play.\nThis simple example\rillustrates some of the key features of reinforcement learning methods. First,\rthere is the emphasis on learning while interacting with an envi\u0026shy;ronment, in\rthis case with an opponent player. Second, there is a clear goal, and correct\rbehavior requires planning or foresight that takes into account delayed effects\rof one��s choices. For example, the simple reinforcement learning player would\rlearn to set up multi-move traps for a shortsighted opponent. It is a striking\rfeature ofthe reinforcement learning solution that it can achieve the effects of planning\rand lookahead without using a model of the opponent and without conducting an\rexplicit search over possible sequences of future states and actions.\nWhile this example illustrates\rsome of the key features of reinforcement learning, it is so simple that it\rmight give the impression that reinforcement learning is more limited than it\rreally is. Although tic-tac-toe is a two-person game, reinforcement learning\ralso applies in the case in which there is no external adversary, that is, in\rthe case of a ��game against nature.�� Reinforcement learning also is not\rrestricted to problems in which behavior breaks down into separate episodes,\rlike the separate games of tic-tac-toe, with reward only at the end of each\repisode. It is just as applica\u0026shy;ble when behavior continues indefinitely and\rwhen rewards of various magnitudes can be received at any time. Reinforcement\rlearning is also applicable to problems that do not even break down into\rdiscrete time steps, like the plays of tic-tac-toe. The general principles\rapply to continuous-time problems as well, although the theory gets more\rcomplicated and we omit it from this introductory treatment.\nTic-tac-toe has a relatively\rsmall, finite state set, whereas reinforcement learning can be used when the\rstate set is very large, or even infinite. For example, Gerry Tesauro (1992,\r1995) combined the algorithm described above with an artificial neu\u0026shy;ral network\rto learn to play backgammon, which has approximately 1020states. With this many states it is impossible ever to experience\rmore than a small fraction of them. Tesauro��s program learned to play far\rbetter than any previous program, and now plays at the level of the world��s\rbest human players (see Chapter 16). The neural network provides the program\rwith the ability to generalize from its experi\u0026shy;ence, so that in new states it\rselects moves based on information saved from similar states faced in the past,\ras determined by its network. How well a reinforcement learning system can work\rin problems with such large state sets is intimately tied to how appropriately\rit can generalize from past experience. It is in this role that we have the\rgreatest need for supervised learning methods with reinforcement learning.\rNeural networks and deep learning (Section 9.6) are not the only, or\rnecessarily the best, way to do this.\nIn this tic-tac-toe example,\rlearning started with no prior knowledge beyond the rules of the game, but\rreinforcement learning by no means entails a tabula rasa view of learning and\rintelligence. On the contrary, prior information can be incorporated into\rreinforcement learning in a variety of ways that can be critical for efficient\rlearning. We also had access to the true state in the tic-tac-toe example,\rwhereas reinforcement learning can also be applied when part of the state is\rhidden, or when different states appear to the learner to be the same. That\rcase, however, is substantially more difficult, and we do not cover it significantly\rin this book.\nFinally, the tic-tac-toe player was able to look ahead and know the\rstates that would result from each of its possible moves. To do this, it had to\rhave a model of the game that allowed it to ��think about�� how its environment\rwould change in response to moves that it might never make. Many problems are\rlike this, but in others even a short-term model of the effects of actions is\rlacking. Reinforcement learning can be applied in either case. No model is\rrequired, but models can easily be used if they are available or can be\rlearned.\nOn the other hand, there are reinforcement\rlearning methods that do not need any kind of environment model at all.\rModel-free systems cannot even think about how their environments will change\rin response to a single action. The tic-tac-toe player is model-free in this\rsense with respect to its opponent: it has no model of its opponent of any\rkind. Because models have to be reasonably accurate to be useful, model-free\rmethods can have advantages over more complex methods when the real bottleneck\rin solving a problem is the difficulty of constructing a sufficiently accurate\renvironment model. Model-free methods are also important building blocks for\rmodel-based methods. In this book we devote several chapters to model-free\rmethods before we discuss how they can be used as components of more complex\rmodel-based methods.\nReinforcement learning can be used at both high and low levels in a\rsystem. Al\u0026shy;though the tic-tac-toe player learned only about the basic moves of\rthe game, nothing prevents reinforcement learning from working at higher levels\rwhere each of the ��ac\u0026shy;tions�� may itself be the application of a possibly\relaborate problem-solving method. In hierarchical learning systems,\rreinforcement learning can work simultaneously on several levels.\nExercise 1.1: Self-Play\rSuppose, instead of playing against a random opponent, the reinforcement\rlearning algorithm described above played against itself, with both sides\rlearning. What do you think would happen in this case? Would it learn a\rdifferent policy for selecting moves?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 1.2: Symmetries\rMany tic-tac-toe positions appear different but are really the same because of\rsymmetries. How might we amend the learning process described above to take\radvantage of this? In what ways would this change improve the learning process?\rNow think again. Suppose the opponent did not take advantage of symmetries. In\rthat case, should we? Is it true, then, that symmetrically equivalent positions\rshould necessarily have the same value?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 1.3: Greedy\rPlay Suppose the reinforcement learning player was greedy,\rthat is, it always played the move that brought it to the position that\rit rated the best. Might it learn to play better, or worse, than a nongreedy\rplayer? What problems might occur?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 1.4: Learning\rfrom Exploration Suppose learning updates occurred after all moves, including exploratory moves. If the step-size\rparameter is appropriately reduced over time (but not the tendency to explore),\rthen the state values would converge to a set of probabilities. What are the\rtwo sets of probabilities computed when we do, and when we do not, learn from\rexploratory moves? Assuming that we do continue to make exploratory moves,\rwhich set of probabilities might be better to learn? Which would result in more\rwins?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 1.5: Other Improvements Can you think of other ways to improve the\rreinforcement learning player? Can you think of any better way to solve the\rtic-tac- toe problem as posed?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n\r\r1.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rSummary\nReinforcement learning is a computational\rapproach to understanding and automat\u0026shy;ing goal-directed learning and\rdecision-making. It is distinguished from other com\u0026shy;putational approaches by\rits emphasis on learning by an agent from direct interaction with its\renvironment, without relying on exemplary supervision or complete models of the\renvironment. In our opinion, reinforcement learning is the first field to se\u0026shy;riously\raddress the computational issues that arise when learning from interaction with\ran environment in order to achieve long-term goals.\nReinforcement learning uses a\rformal framework defining the interaction between a learning agent and its\renvironment in terms of states, actions, and rewards. This framework is\rintended to be a simple way of representing essential features of the\rartificial intelligence problem. These features include a sense of cause and\reffect, a sense of uncertainty and nondeterminism, and the existence of\rexplicit goals.\nThe concepts of value and value functions are the key features of\rmost of the reinforcement learning methods that we consider in this book. We\rtake the position that value functions are important for efficient search in\rthe space of policies. The use of value functions distinguishes reinforcement\rlearning methods from evolutionary methods that search directly in policy space\rguided by scalar evaluations of entire policies.\n1.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rEarly History of Reinforcement\rLearning\nThe history of reinforcement learning\rhas two main threads, both long and rich, that were pursued independently\rbefore intertwining in modern reinforcement learning. One thread concerns\rlearning by trial and error that started in the psychology of animal learning.\rThis thread runs through some of the earliest work in artificial intelligence\rand led to the revival of reinforcement learning in the early 1980s. The other\rthread concerns the problem of optimal control and its solution using value\rfunctions and dynamic programming. For the most part, this thread did not\rinvolve learning. Although the two threads have been largely independent, the\rexceptions revolve around a third, less distinct thread concerning\rtemporal-difference methods such as the one used in the tic-tac-toe example in\rthis chapter. All three threads came together in the late 1980s to produce the\rmodern field of reinforcement learning as we present it in this book.\nThe thread focusing on\rtrial-and-error learning is the one with which we are most familiar and about\rwhich we have the most to say in this brief history. Before doing that,\rhowever, we briefly discuss the optimal control thread.\nThe term ��optimal control��\rcame into use in the late 1950s to describe the problem of designing a\rcontroller to minimize a measure of a dynamical system��s behavior over time.\rOne of the approaches to this problem was developed in the mid-1950s by Richard\rBellman and others through extending a nineteenth century theory of Hamilton\rand Jacobi. This approach uses the concepts of a dynamical system��s state and\rof a value function, or ��optimal return function,�� to define a functional\requation, now often called the Bellman equation. The class of methods for\rsolving optimal control problems by solving this equation came to be known as\rdynamic programming (Bellman, 1957a). Bellman (1957b) also introduced the\rdiscrete stochastic version of the optimal control problem known as Markovian\rdecision processes (MDPs), and Ronald Howard (1960) devised the policy\riteration method for MDPs. All of these are essential elements underlying the\rtheory and algorithms of modern reinforcement learning.\nDynamic programming is widely\rconsidered the only feasible way of solving general stochastic optimal control\rproblems. It suffers from what Bellman called ��the curse of dimensionality,��\rmeaning that its computational requirements grow exponentially with the number\rof state variables, but it is still far more efficient and more widely\rapplicable than any other general method. Dynamic programming has been exten\u0026shy;sively\rdeveloped since the late 1950s, including extensions to partially observable\rMDPs (surveyed by Lovejoy, 1991), many applications (surveyed by White, 1985,\r1988, 1993), approximation methods (surveyed by Rust, 1996), and asynchronous\rmethods (Bertsekas, 1982, 1983). Many excellent modern treatments of dynamic\rprogramming are available (e.g., Bertsekas, 2005, 2012; Puterman, 1994; Ross,\r1983; and Whittle, 1982, 1983). Bryson (1996) provides an authoritative history\rof optimal control.\nIn this book, we consider all\rof the work in optimal control also to be, in a sense, work in reinforcement\rlearning. We define a reinforcement learning method as any ef\u0026shy;fective way of\rsolving reinforcement learning problems, and it is now clear that these\rproblems are closely related to optimal control problems, particularly\rstochastic op\u0026shy;timal control problems such as those formulated as MDPs.\rAccordingly, we must consider the solution methods of optimal control, such as\rdynamic programming, also to be reinforcement learning methods. Because almost\rall of the conventional methods require complete knowledge of the system to be\rcontrolled, it feels a little unnatural to say that they are part of\rreinforcement learning. On the other hand, many dynamic\rprogramming algorithms are incremental and iterative. Like learning methods,\rthey gradually reach the correct answer through successive approximations. As\rwe show in the rest of this book, these similarities are far more than\rsuperficial. The theories and solution methods for the cases of complete and\rincomplete knowl\u0026shy;edge are so closely related that we feel they must be considered\rtogether as part of the same subject matter.\nLet us return now to the other\rmajor thread leading to the modern field of rein\u0026shy;forcement learning, that\rcentered on the idea of trial-and-error learning. We only touch on the major\rpoints of contact here, taking up this topic in more detail in Chapter 14.\rAccording to American psychologist R. S. Woodworth the idea of trial- and-error\rlearning goes as far back as the 1850s to Alexander Bain��s discussion of\rlearning by ��groping and experiment�� and more explicitly to the British\rethologist and psychologist Conway Lloyd Morgan��s 1894 use of the term to\rdescribe his ob\u0026shy;servations of animal behavior (Woodworth, 1938). Perhaps the\rfirst to succinctly express the essence of trial-and-error learning as a\rprinciple of learning was Edward\nThorndike:\nOf several responses made to the same situation, those which are\raccom\u0026shy;panied or closely followed by satisfaction to the animal will, other\rthings being equal, be more firmly connected with the situation, so that, when\rit recurs, they will be more likely to recur; those which are accompanied or\rclosely followed by discomfort to the animal will, other things being equal,\rhave their connections with that situation weakened, so that, when it recurs,\rthey will be less likely to occur. The greater the satisfaction or discomfort,\rthe greater the strengthening or weakening of the bond. (Thorndike, 1911, p.\r244)\nThorndike called this the ��Law of Effect�� because\rit describes the effect of reinforcing events on the tendency to select actions.\rThorndike later modified the law to better account for accumulating data on\ranimal learning (such as differences between the effects of reward and\rpunishment), and the law in its various forms has generated con\u0026shy;siderable\rcontroversy among learning theorists (e.g., see Gallistel, 2005; Herrnstein,\r1970; Kimble, 1961, 1967; Mazur, 1994). Despite this, the Law of Effect��in one\rform or another��is widely regarded as a basic principle underlying much\rbehavior (e.g., Hilgard and Bower, 1975; Dennett, 1978; Campbell, 1960; Cziko,\r1995). It is the basis of the influential learning theories of Clark Hull and\rexperimental methods of B. F. Skinner (e.g., Hull, 1943; Skinner, 1938).\nThe term ��reinforcement�� in the context of animal\rlearning came into use well after Thorndike��s expression of the Law of Effect,\rto the best of our knowledge first appearing in this context in the 1927\rEnglish translation of Pavlov��s monograph on conditioned reflexes.\rReinforcement is the strengthening of a pattern of behavior as a result of an\ranimal receiving a stimulus��a reinforcer��in an appropriate temporal\rrelationship with another stimulus or with a response. Some psychologists\rextended its meaning to include the process of weakening in addition to\rstrengthening, as well applying when the omission or termination of an event\rchanges behavior. Reinforce\u0026shy;ment produces changes in behavior that persist\rafter the reinforcer is withdrawn, so that a stimulus that attracts an animal��s\rattention or that energizes its behavior without producing lasting changes is\rnot considered to be a reinforcer.\nThe idea of implementing\rtrial-and-error learning in a computer appeared among the earliest thoughts\rabout the possibility of artificial intelligence. In a 1948 report, Alan Turing\rdescribed a design for a ��pleasure-pain system�� that worked along the lines of\rthe Law of Effect:\nWhen a configuration is reached for which the action is\rundetermined, a random choice for the missing data is made and the appropriate\rentry is made in the description, tentatively, and is applied. When a pain\rstimulus occurs all tentative entries are cancelled, and when a pleasure\rstimulus occurs they are all made permanent. (Turing, 1948)\nMany ingenious electro-mechanical machines were\rconstructed that demonstrated trial-and-error learning. The earliest may have\rbeen a machine built by Thomas\nRoss (1933) that was able to\rfind its way through a simple maze and remember the path through the settings\rof switches. In 1951 W. Grey Walter, already known for his ��mechanical tortoise�� (Walter,\r1950), built a version capable of a simple form of learning (Walter, 1951). In\r1952 Claude Shannon demonstrated a maze-running mouse named Theseus that used\rtrial and error to find its way through a maze, with the maze itself\rremembering the successful directions via magnets and relays under its floor\r(Shannon, 1951, 1952). J. A. Deutsch (1954) described a maze-solving machine\rbased on his behavior theory (Deutsch, 1953) that has some properties in common\rwith model-based reinforcement learning (Chapter 8). In his\rPh.D. dis\u0026shy;sertation (Minsky, 1954), Marvin Minsky discussed computational\rmodels of rein\u0026shy;forcement learning and described his construction of an analog\rmachine composed of components he called SNARCs (Stochastic Neural-Analog\rReinforcement Calcu\u0026shy;lators) meant to resemble modifiable synaptic connections\rin the brain (Chapter 15) The fascinating web site cyberneticzoo.com contains a\rwealth of information on these and many other electro-mechanical learning\rmachines.\nBuilding electro-mechanical learning\rmachines gave way to programming digital computers to perform various types of\rlearning, some of which implemented trial- and-error learning. Farley and Clark\r(1954) described a digital simulation of a neural- network learning machine\rthat learned by trial and error. But their interests soon shifted from\rtrial-and-error learning to generalization and pattern recognition, that is,\rfrom reinforcement learning to supervised learning (Clark and Farley, 1955).\rThis began a pattern of confusion about the relationship between these types of\rlearn\u0026shy;ing. Many researchers seemed to believe that they were studying\rreinforcement learning when they were actually studying supervised learning.\rFor example, neu\u0026shy;ral network pioneers such as Rosenblatt (1962) and Widrow and\rHoff (1960) were clearly motivated by reinforcement learning��they used the\rlanguage of rewards and punishments��but the systems they studied were\rsupervised learning systems suit\u0026shy;able for pattern recognition and perceptual\rlearning. Even today, some researchers and textbooks minimize or blur the\rdistinction between these types of learning. For example, some neural-network\rtextbooks have used the term ��trial-and-error�� to describe networks that learn\rfrom training examples. This is an understandable con\u0026shy;fusion because these\rnetworks use error information to update connection weights, but this misses\rthe essential character of trial-and-error learning as selecting actions on the\rbasis of evaluative feedback that does not rely on knowledge of what the correct\raction should be.\nPartly as a result of these\rconfusions, research into genuine trial-and-error learn\u0026shy;ing became rare in the\rthe 1960s and 1970s, although there were notable exceptions. In the 1960s the\rterms ��reinforcement�� and ��reinforcement learning�� were used in the engineering\rliterature for the first time to describe engineering uses of trial- and-error\rlearning (e.g., Waltz and Fu, 1965; Mendel, 1966; Fu, 1970; Mendel and\rMcClaren, 1970). Particularly influential was Minsky��s paper ��Steps Toward Arti\u0026shy;ficial\rIntelligence�� (Minsky, 1961), which discussed several issues relevant to trial-\rand-error learning, including prediction, expectation, and what he called the basic credit-assignment problem for complex reinforcement learning\rsystems: How do you distribute credit for success among the many\rdecisions that may have been involved in producing it? All of the methods we\rdiscuss in this book are, in a sense, directed toward solving this problem.\rMinsky��s paper is well worth reading today.\nIn the next few paragraphs we discuss some of the\rother exceptions and partial exceptions to the relative neglect of\rcomputational and theoretical study of genuine trial-and-error learning in the\r1960s and 1970s.\nOne of these was the work by a New Zealand\rresearcher named John Andreae. Andreae (1963) developed a system called STeLLA\rthat learned by trial and error in interaction with its environment. This\rsystem included an internal model of the world and, later, an ��internal\rmonologue�� to deal with problems of hidden state (Andreae, 1969a). Andreae��s\rlater work (1977) placed more emphasis on learning from a teacher, but still\rincluded trial and error. Unfortunately, his pioneering research was not well\rknown, and did not greatly impact subsequent reinforcement learning research.\nMore influential was the work of Donald Michie.\rIn 1961 and 1963 he described a simple trial-and-error learning system for\rlearning how to play tic-tac-toe (or naughts and crosses) called MENACE (for\rMatchbox Educable Naughts and Crosses Engine). It consisted of a matchbox for\reach possible game position, each matchbox containing a number of colored\rbeads, a different color for each possible move from that position. By drawing\ra bead at random from the matchbox corresponding to the current game position,\rone could determine MENACE��s move. When a game was over, beads were added to or\rremoved from the boxes used during play to reinforce or punish MENACE��s\rdecisions. Michie and Chambers (1968) described another tic-tac-toe\rreinforcement learner called GLEE (Game Learning Expectimaxing Engine) and a\rreinforcement learning controller called BOXES. They applied BOXES to the task\rof learning to balance a pole hinged to a movable cart on the basis of a\rfailure signal occurring only when the pole fell or the cart reached the end of\ra track. This task was adapted from the earlier work of Widrow and Smith\r(1964), who used supervised learning methods, assuming instruction from a\rteacher already able to balance the pole. Michie and Chambers��s version of\rpole-balancing is one of the best early examples of a reinforcement learning\rtask under conditions of incomplete knowledge. It influenced much later work in\rreinforcement learning, beginning with some of our own studies (Barto, Sutton,\rand Anderson, 1983; Sutton, 1984). Michie consistently emphasized the role of\rtrial and error and learning as essential aspects of artificial intelligence\r(Michie, 1974).\nWidrow, Gupta, and Maitra (1973) modified the\rLeast-Mean-Square (LMS) al\u0026shy;gorithm of Widrow and Hoff (1960) to produce a\rreinforcement learning rule that could learn from success and failure signals\rinstead of from training examples. They called this form of learning ��selective\rbootstrap adaptation�� and described it as ��learning with a critic�� instead of\r��learning with a teacher.�� They analyzed this rule and showed how it could\rlearn to play blackjack. This was an isolated foray into reinforcement learning\rby Widrow, whose contributions to supervised learning were much more\rinfluential. Our use of the term ��critic�� is derived from Widrow, Gupta, and\rMaitra��s paper. Buchanan, Mitchell, Smith, and Johnson (1978) inde\u0026shy;pendently\rused the term critic in the context of machine learning (see also Dietterich\rand Buchanan, 1984), but for them a critic is an expert system able to do more\rthan evaluate performance.\nResearch on learning automata\rhad a more direct influence on the trial-and-error thread leading to modern\rreinforcement learning research. These are methods for solving a\rnonassociative, purely selectional learning problem known as the k-armed bandit by analogy to a slot machine, or ��one-armed\rbandit,�� except with k levers (see Chapter 2). Learning automata are simple,\rlow-memory machines for improving the probability of reward in these problems.\rLearning automata originated with work in the 1960s of the Russian\rmathematician and physicist M. L. Tsetlin and colleagues (published\rposthumously in Tsetlin, 1973) and has been extensively developed since then\rwithin engineering (see Narendra and Thathachar, 1974, 1989). These devel\u0026shy;opments\rincluded the study of stochastic learning automata, which\rare methods for updating action probabilities on the basis of reward signals.\rStochastic learning au\u0026shy;tomata were foreshadowed by earlier work in psychology,\rbeginning with William Estes�� 1950 effort toward a statistical theory of\rlearning (Estes, 1950) and further developed by others, most famously by\rpsychologist Robert Bush and statistician Frederick Mosteller (Bush and\rMosteller, 1955).\nThe statistical learning theories developed in\rpsychology were adopted by re\u0026shy;searchers in economics, leading to a thread of\rresearch in that field devoted to reinforcement learning. This work began in\r1973 with the application of Bush and Mosteller��s learning theory to a\rcollection of classical economic models (Cross, 1973). One goal of this\rresearch was to study artificial agents that act more like real peo\u0026shy;ple than do\rtraditional idealized economic agents (Arthur, 1991). This approach expanded to\rthe study of reinforcement learning in the context of game theory. Although\rreinforcement learning in economics developed largely independently of the\rearly work in artificial intelligence, reinforcement learning and game theory\ris a topic of current interest in both fields, but one that is beyond the scope\rof this book. Camerer (2003) discusses the reinforcement learning tradition in\reconomics, and Nowe et al. (2012) provide an overview of the subject from the\rpoint of view of multi-agent extensions to the approach that we introduce in\rthis book. Rein\u0026shy;forcement learning and game theory is a much different subject\rfrom reinforcement learning used in programs to play tic-tac-toe, checkers, and\rother recreational games. See, for example, Szita (2012) for an\roverview of this aspect of reinforcement learning and games.\nJohn Holland (1975) outlined a general theory of\radaptive systems based on selec\u0026shy;tional principles. His early work concerned\rtrial and error primarily in its nonasso\u0026shy;ciative form, as in evolutionary\rmethods and the k-armed bandit. In 1976 and more fully in 1986, he introduced classifier systems, true reinforcement learning systems\rincluding association and value functions. A key component of Holland��s\rclassifier systems was always a genetic algorithm, an\revolutionary method whose role was to evolve useful representations. Classifier\rsystems have been extensively developed by many researchers to form a major\rbranch of reinforcement learning research (re\u0026shy;viewed by Urbanowicz and Moore,\r2009), but genetic algorithms��which we do not consider to be reinforcement\rlearning systems by themselvesһhave received much more attention, as have other approaches to\revolutionary computation (e.g., Fogel, Owens and Walsh, 1966, and Koza, 1992).\nThe individual most\rresponsible for reviving the trial-and-error thread to rein\u0026shy;forcement learning\rwithin artificial intelligence was Harry Klopf (1972, 1975, 1982). Klopf\rrecognized that essential aspects of adaptive behavior were being lost as learn\u0026shy;ing\rresearchers came to focus almost exclusively on supervised learning. What was\rmissing, according to Klopf, were the hedonic aspects of behavior, the drive to\rachieve some result from the environment, to control the environment toward\rdesired ends and away from undesired ends. This is the essential idea of\rtrial-and-error learning. Klopf��s ideas were especially influential on the\rauthors because our assessment of them (Barto and Sutton, 1981a) led to our\rappreciation of the distinction between supervised and reinforcement learning,\rand to our eventual focus on reinforcement learning. Much of the early work\rthat we and colleagues accomplished was directed toward showing that\rreinforcement learning and supervised learning were indeed different (Barto,\rSutton, and Brouwer, 1981; Barto and Sutton, 1981b; Barto and Anandan, 1985).\rOther studies showed how reinforcement learning could address important problems\rin neural network learning, in particular, how it could produce learning\ralgorithms for multilayer networks (Barto, Anderson, and Sutton, 1982; Barto\rand Anderson, 1985; Barto and Anandan, 1985; Barto, 1985, 1986; Barto and\rJordan, 1987). We say more about reinforcement learning and neural networks in\rChapter 15.\nWe turn now to the third\rthread to the history of reinforcement learning, that concerning\rtemporal-difference learning. Temporal-difference learning methods are\rdistinctive in being driven by the difference between temporally successive\restimates of the same quantityһfor example, of the probability of winning in the tic-tac-toe\rexample. This thread is smaller and less distinct than the other two, but it\rhas played a particularly important role in the field, in part because\rtemporal-difference methods seem to be new and unique to reinforcement\rlearning.\nThe origins of\rtemporal-difference learning are in part in animal learning psychol\u0026shy;ogy, in\rparticular, in the notion of secondary reinforcers. A secondary\rreinforcer is a stimulus that has been paired with a primary reinforcer such as\rfood or pain and, as a result, has come to take on similar reinforcing\rproperties. Minsky (1954) may have been the first to realize that this\rpsychological principle could be important for artificial learning systems.\rArthur Samuel (1959) was the first to propose and implement a learning method\rthat included temporal-difference ideas, as part of his celebrated\rcheckers-playing program.\nSamuel made no reference to\rMinsky��s work or to possible connections to animal learning. His inspiration\rapparently came from Claude Shannon��s (1950) suggestion that a computer could\rbe programmed to use an evaluation function to play chess, and that it might be\rable to improve its play by modifying this function on-line. (It is possible\rthat these ideas of Shannon��s also influenced Bellman, but we know of no\revidence for this.) Minsky (1961) extensively discussed Samuel��s work in his\r��Steps�� paper, suggesting the connection to secondary reinforcement theories,\rboth natural\nand\rartificial.\nAs we have discussed, in the\rdecade following the work of Minsky and Samuel, little computational work was\rdone on trial-and-error learning, and apparently no computational work at all\rwas done on temporal-difference learning. In 1972, Klopf brought\rtrial-and-error learning together with an important component of\rtemporal-difference learning. Klopf was interested in principles that would\rscale to learning in large systems, and thus was intrigued by notions of local\rreinforcement, whereby subcomponents of an overall learning system could\rreinforce one another. He developed the idea of ��generalized reinforcement,��\rwhereby every component (nominally, every neuron) views all of its inputs in\rreinforcement terms: excitatory inputs as rewards and inhibitory inputs as\rpunishments. This is not the same idea as what we now know as\rtemporal-difference learning, and in retrospect it is farther from it than was\rSamuel��s work. On the other hand, Klopf linked the idea with trial-and-error\rlearning and related it to the massive empirical database of animal learning\rpsychology.\nSutton (1978a, 1978b, 1978c)\rdeveloped Klopf��s ideas further, particularly the links to animal learning\rtheories, describing learning rules driven by changes in tem\u0026shy;porally successive\rpredictions. He and Barto refined these ideas and developed a psychological\rmodel of classical conditioning based on temporal-difference learning (Sutton\rand Barto, 1981a; Barto and Sutton, 1982). There followed several other in\u0026shy;fluential\rpsychological models of classical conditioning based on temporal-difference\rlearning (e.g., Klopf, 1988; Moore et al., 1986; Sutton and Barto, 1987, 1990).\rSome neuroscience models developed at this time are well interpreted in terms\rof temporal- difference learning (Hawkins and Kandel, 1984; Byrne, Gingrich,\rand Baxter, 1990; Gelperin, Hopfield, and Tank, 1985; Tesauro, 1986; Friston et\ral., 1994), although in most cases there was no historical connection.\nOur early work on\rtemporal-difference learning was strongly influenced by animal learning\rtheories and by Klopf��s work. Relationships to Minsky��s ��Steps�� paper and to\rSamuel��s checkers players appear to have been recognized only afterward. By\r1981, however, we were fully aware of all the prior work mentioned above as\rpart of the temporal-difference and trial-and-error threads. At this time we\rdeveloped a method for using temporal-difference learning in trial-and-error\rlearning, known as the actor- critic architecture, and\rapplied this method to Michie and Chambers��s pole-balancing problem (Barto,\rSutton, and Anderson, 1983). This method was extensively studied in Sutton��s\r(1984) Ph.D. dissertation and extended to use backpropagation neural networks\rin Anderson��s (1986) Ph.D. dissertation. Around this time, Holland (1986)\rincorporated temporal-difference ideas explicitly into his classifier systems.\rA key step was taken by Sutton in 1988 by separating temporal-difference\rlearning from control, treating it as a general prediction method. That paper\ralso introduced the TD(A) algorithm and proved some of its convergence\rproperties.\nAs we were finalizing our work\ron the actor-critic architecture in 1981, we discov\u0026shy;ered a paper by Ian Witten\r(1977) that contains the earliest known publication of a temporal-difference\rlearning rule. He proposed the method that we now call tabular TD(0) for use as\rpart of an adaptive controller for solving MDPs. Witten��s work was a descendant\rof Andreae��s early experiments with STeLLA and other trial-and- error learning\rsystems. Thus, Witten��s 1977 paper spanned both major threads of reinforcement\rlearning research��trial-and-error learning and optimal control��while making a\rdistinct early contribution to temporal-difference learning.\nThe temporal-difference and\roptimal control threads were fully brought together in 1989 with Chris\rWatkins��s development of Q-learning. This work extended and integrated prior\rwork in all three threads of reinforcement learning research. Paul Werbos\r(1987) contributed to this integration by arguing for the convergence of trial-\rand-error learning and dynamic programming since 1977. By the time of Watkins��s\rwork there had been tremendous growth in reinforcement learning research,\rprimarily in the machine learning subfield of artificial intelligence, but also\rin neural networks and artificial intelligence more broadly. In 1992, the\rremarkable success of Gerry Tesauro��s backgammon playing program, TD-Gammon,\rbrought additional attention to the field.\nIn the time since publication\rof the first edition of this book, a flourishing subfield of neuroscience\rdeveloped that focuses on the relationship between reinforcement learning\ralgorithms and reinforcement learning in the nervous system. Most respon\u0026shy;sible\rfor this is an uncanny similarity between the behavior of temporal-difference\ralgorithms and the activity of dopamine producing neurons in the brain, as\rpointed out by a number of researchers (Friston et al., 1994; Barto, 1995a;\rHouk, Adams, and Barto, 1995; Montague, Dayan, and Sejnowski, 1996; and\rSchultz, Dayan, and Montague, 1997). Chapter 15 provides an introduction to\rthis exciting aspect of reinforcement learning.\nOther important contributions made in the recent history of\rreinforcement learning are too numerous to mention in this brief account; we\rcite many of these at the end of the individual chapters in which they arise.\nBibliographical Remarks\nFor additional general coverage of\rreinforcement learning, we refer the reader to the books by Szepesvari (2010),\rBertsekas and Tsitsiklis (1996), Kaelbling (1993a), and Masashi Sugiyama et al.\r(2013). Books that take a control or operations research perspective are those\rof Si et al. (2004), Powell (2011), Lewis and Liu (2012), and Bertsekas (2012).\rCao��s (2009) review places reinforcement learning in the context of other\rapproaches to learning and optimization of stochastic dynamic systems Three\rspecial issues of the journal Machine Learning focus on\rreinforcement learning: Sut\u0026shy;ton (1992), Kaelbling (1996), and Singh (2002).\rUseful surveys are provided by Barto (1995b); Kaelbling, Littman, and Moore\r(1996); and Keerthi and Ravindran (1997). The volume edited by Weiring and van\rOtterlo (2012) provides an excellent overview of recent developments.\nThe example of Phil��s\rbreakfast in this chapter was inspired by Agre (1988). We direct the reader to\rChapter 6for references to the kind of temporal-difference method we used in\rthe tic-tac-toe example.\n\r\rAs we shall see in Chapter 3, the\rtheory of reinforcement learning as we treat it in this book is based on\rmaximizing the expected value of the amount of reward an agent can accumulate\rover its future. This is in consistent with the classic principle of von\rNeumann and Morgenstern (1944) that rational decisions are those that maxi\u0026shy;mize\rexpected utility. However, maximizing the expected value of a random quantity\ris often not the right thing to do because it ignores the quantity��s variance,\rwhich is said to underly risk. Risk-sensitive\roptimization has been highly developed in fields where excessive risk can be\rruinous, such as in finance and optimal control. Risk is important for\rreinforcement learning as well but beyond our scope here. Heger (1994), Geibel\r(2001), Mihatsch and Neuneier (2002), and Borkar (2002) are exam\u0026shy;ples of papers\rthat consider risk in reinforcement learning, developing risk-sensitive\rversions of some of the algorithms we present here. Coraluppi and Marcus (1999)\rdis\u0026shy;cuss risk in the context of discrete-time, finite-state Markov decision\rprocesses that form the basis of our approach to reinforcement learning. We also\rwholly sidestep utility theory, which is concerned with measuring people��s\rdesires. Utility theory would be relevant if we were treating reinforcement\rlearning as a theory of human economic behavior, and some of accounts of\rreinforcement learning equate reward with utility (e.g., Russell and Norvig,\r2010). Since that is not our aim here, however, we leave connections with\rutility theory to others.\n\r\rPart I:\rTabular Solution Methods\nIn this part of the book we\rdescribe almost all the core ideas of reinforcement learning algorithms in\rtheir simplest forms: that in which the state and action spaces are small\renough for the approximate value functions to be represented as arrays, or tables. In this case, the methods can often find exact\rsolutions, that is, they can often find exactly the optimal value function and\rthe optimal policy. This contrasts with the approximate methods described in\rthe next part of the book, which only find approximate solutions, but which in\rreturn can be applied effectively to much larger problems.\nThe first chapter of this part\rof the book describes solution methods for the special case of the\rreinforcement learning problem in which there is only a single state, called bandit problems. The second chapter describes the general\rproblem formulation that we treat throughout the rest of the book��finite Markov decision processes��and its main ideas including\rBellman equations and value functions.\nThe next three chapters\rdescribe three fundamental classes of methods for solving finite Markov decision\rproblems: dynamic programming, Monte Carlo methods, and temporal-difference\rlearning. Each class of methods has its strengths and weaknesses. Dynamic\rprogramming methods are well developed mathematically, but require a complete\rand accurate model of the environment. Monte Carlo methods don��t re\u0026shy;quire a\rmodel and are conceptually simple, but are not well suited for step-by-step\rincremental computation. Finally, temporal-difference methods require no model\rand are fully incremental, but are more complex to analyze. The methods also\rdiffer in several ways with respect to their efficiency and speed of\rconvergence.\nThe remaining two chapters\rdescribe how these three classes of methods can be combined to obtain the best\rfeatures of each of them. In one chapter we describe how the strengths of Monte\rCarlo methods can be combined with the strengths of temporal-difference methods\rvia the use of eligibility traces. In the final chapter of this part of the\rbook we show how temporal-difference learning methods can be com\u0026shy;bined with\rmodel learning and planning methods (such as dynamic programming) for a\rcomplete and unified solution to the tabular reinforcement learning problem.\n\r\r26\n\r\rChapter 2\nMulti-armed Bandits\nThe most important feature distinguishing\rreinforcement learning from other types of learning is that it uses training\rinformation that evaluates the actions taken rather than\rinstructs by giving correct actions. This is what\rcreates the need for active ex\u0026shy;ploration, for an explicit trial-and-error\rsearch for good behavior. Purely evaluative feedback indicates how good the\raction taken is, but not whether it is the best or the worst action possible.\rPurely instructive feedback, on the other hand, indicates the correct action to\rtake, independently of the action actually taken. This kind of feedback is the\rbasis of supervised learning, which includes large parts of pat\u0026shy;tern classification,\rartificial neural networks, and system identification. In their pure forms,\rthese two kinds of feedback are quite distinct: evaluative feedback depends\rentirely on the action taken, whereas instructive feedback is independent of\rthe ac\u0026shy;tion taken. There are also interesting intermediate cases in which\revaluation and instruction blend together.\nIn this chapter we study the evaluative aspect of\rreinforcement learning in a sim\u0026shy;plified setting, one that does not involve\rlearning to act in more than one situation. This nonassociative\rsetting is the one in which most prior work involving evaluative feedback has\rbeen done, and it avoids much of the complexity of the full reinforce\u0026shy;ment\rlearning problem. Studying this case will enable us to see most clearly how\revaluative feedback differs from, and yet can be combined with, instructive\rfeedback.\nThe particular nonassociative, evaluative\rfeedback problem that we explore is a simple version of the k-armed bandit\rproblem. We use this problem to introduce a number of basic learning methods\rwhich we extend in later chapters to apply to the full reinforcement learning\rproblem. At the end of this chapter, we take a step closer to the full\rreinforcement learning problem by discussing what happens when the bandit\rproblem becomes associative, that is, when actions are taken in more than one\rsituation.\n2.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rA k-armed Bandit Problem\nConsider the following learning\rproblem. You are faced repeatedly with a choice among k\rdifferent options, or actions. After each choice you receive a numerical reward\rchosen from a stationary probability distribution that depends on the action\ryou selected. Your objective is to maximize the expected total reward over some\rtime period, for example, over 1000action selections,\ror time steps.\nThis is the original form of\rthe k-armed bandit problem, so named by analogy to a\rslot machine, or ��one-armed bandit,�� except that it has k\rlevers instead of one. Each action selection is like a play of one of the slot\rmachine��s levers, and the rewards are the payoffs for hitting the jackpot.\rThrough repeated action selections you are to maximize your winnings by\rconcentrating your actions on the best levers. Another analogy is that of a\rdoctor choosing between experimental treatments for a series of seriously ill\rpatients. Each action selection is a treatment selection, and each reward is\rthe survival or well-being of the patient. Today the term ��bandit problem�� is\rsometimes used for a generalization of the problem described above, but in this\rbook we use it to refer just to this simple case.\nIn our k-armed bandit problem, each of the k actions has an expected or mean reward given that that\raction is selected; let us call this the value of that\raction. We denote the action selected on time step t as At, and the corresponding reward as R. The value then of an\rarbitrary action a, denoted q^(a),\ris the expected reward given that a is selected:\nq*(a) == E[R | At = a].\nIf you knew the value of each action,\rthen it would be trivial to solve the k-armed bandit\rproblem: you would always select the action with highest value. We as\u0026shy;sume that\ryou do not know the action values with certainty, although you may have\restimates. We denote the estimated value of action a at\rtime t as Qt(a)��q^(a).\nIf you maintain estimates of\rthe action values, then at any time step there is at least one action whose\restimated value is greatest. We call these the greedy\ractions. When you select one of these actions, we say that you are exploiting your current knowledge of the values of the\ractions. If instead you select one of the nongreedy actions, then we say you\rare exploring, because this enables you to improve your\restimate of the nongreedy action��s value. Exploitation is the right thing to do\rto maximize the expected reward on the one step, but exploration may produce\rthe greater total reward in the long run. For example, suppose a greedy\raction��s value is known with certainty, while several other actions are\restimated to be nearly as good but with substantial uncertainty. The\runcertainty is such that at least one of these other actions probably is\ractually better than the greedy action, but you don��t know which one. If you have\rmany time steps ahead on which to make action selections, then it may be better\rto explore the nongreedy actions and discover which of them are better than the\rgreedy action. Reward is lower in the short run, during exploration, but higher\rin the long run because after you have discovered the better actions, you can\rexploit them many times. Because it is not possible both\rto explore \n\r\rand to exploit with any single\raction selection, one often refers to the ��conflict�� between exploration and\rexploitation.\nIn any specific case, whether it is better to\rexplore or exploit depends in a com\u0026shy;plex way on the precise values of the\restimates, uncertainties, and the number of remaining steps. There are many\rsophisticated methods for balancing exploration and exploitation for particular\rmathematical formulations of the k-armed bandit and related problems. However,\rmost of these methods make strong assumptions about stationarity and prior\rknowledge that are either violated or impossible to verify in applications and in\rthe full reinforcement learning problem that we consider in sub\u0026shy;sequent\rchapters. The guarantees of optimality or bounded loss for these methods are of\rlittle comfort when the assumptions of their theory do not apply.\nIn this book we do not worry about balancing exploration and\rexploitation in a sophisticated way; we worry only about balancing them at all.\rIn this chapter we present several simple balancing methods for the k-armed\rbandit problem and show that they work much better than methods that always\rexploit. The need to balance exploration and exploitation is a distinctive\rchallenge that arises in reinforcement learning; the simplicity of the k-armed\rbandit problem enables us to show this in a particularly clear form.\n2.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rAction-value Methods\nWe begin by looking more closely at some simple methods for\restimating the values of actions and for using the estimates to make action\rselection decisions. Recall that the true value of an action is the mean reward\rwhen that action is selected. One natural way to estimate this is by averaging\rthe rewards actually received:\nsum of rewards when a\rtaken prior to tE^-} Ri �� 1A^=a\nnumber of times a\rtaken prior to t\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ^2i�� \\1A-=a\nwhere 1predicatedenotes the random\rvariable that is 1if predicate is\rtrue and 0if it is not. If the denominator is zero, then we instead define\rQt(a) as some default value, such as Qi(a) = 0. As the\rdenominator goes to infinity, by the law of large numbers, Qi(a) converges to q*(a). We call this the sample-average\rmethod for estimating action values because each estimate is an average of the\rsample of relevant rewards. Of course this is just one way to estimate action\rvalues, and not necessarily the best one. Nevertheless, for now let us stay\rwith this simple estimation method and turn to the question of how the\restimates might be used to select actions.\nThe simplest action selection rule is to select the action (or one\rof the actions) with highest estimated action value, that is, to select at step\rt one of the greedy actions, A*, for which Qt(A*) = maxaQt(a). This greedy action selection method can be written as\n\r\r\r(2.2)\n\r\r\r\r\rAt == argmax Qt(a),\na\nwhere argmaxa denotes the value of a\rat which the expression that follows is maxi\u0026shy;mized (with ties broken\rarbitrarily). Greedy action selection always exploits current knowledge to\rmaximize immediate reward; it spends no time at all sampling appar\u0026shy;ently\rinferior actions to see if they might really be better. A simple alternative is\rto behave greedily most of the time, but every once in a while, say with small\rprobabil\u0026shy;ity ^, instead to select randomly from amongst all the actions with\requal probability independently of the action-value estimates. We call methods\rusing this near-greedy action selection rule \u0026pound;-greedy\rmethods. An advantage of these methods is that, in the limit as the number of\rsteps increases, every action will be sampled an infinite number of times, thus\rensuring that all the Qt(a) converge to ��(a). This of course implies that the probability of selecting the\roptimal action converges to greater than 1 �� \u0026pound;, that is, to near certainty.\rThese are just asymptotic guarantees, however, and say little about the\rpractical effectiveness of the methods.\nExercise 2.1 In \u0026pound;-greedy action selection, for the case of two\ractions and \u0026pound; = 0.5, what is the probability that the\rgreedy action is selected?\nExercise 2.2: Bandit\rexample Consider a multi-armed bandit problem with k\r= 4 actions, denoted 1, 2, 3, and 4. Consider applying to this problem a bandit\ralgorithm using \u0026pound;-greedy action selection, sample-average\raction-value estimates, and initial estimates of Qi(a) = 0, Va. Suppose the\rinitial sequence of actions and rewards is Ai = 1, Ri =\r1, A2= 2, R2= 1, A3= 2, R3= 2, A4= 2, R4= 2, A5= 3, R5= 0. On some of these time steps the \u0026pound; case may have occurred, causing an action to be selected at\rrandom. On which time steps did this definitely occur? On which time steps\rcould this possibly have occurred?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n2.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe 10-armed Testbed\nTo roughly assess the relative\reffectiveness of the greedy and \u0026pound;-greedy methods, we\rcompared them numerically on a suite of test problems. This was a set of 2000\rrandomly generated k-armed bandit problems with k = 10. For each bandit problem, such as that shown in Figure\r2.1, the action values, q^(a), a = 1,...,\r10, were selected according to a normal (Gaussian) distribution with mean 0 and\rvariance 1. Then, when a learning method applied to that problem selected\raction At at time t, the actual reward Rt\rwas selected from a normal distribution with mean q^(At) and variance 1. It is\rthese distributions which are shown as gray in Figure 2.1. We call this suite\rof test tasks the 10-armed testbed. For any learning\rmethod, we can measure its performance and behavior as it improves with\rexperience over 1000steps interacting with one of the bandit problem. This makes up one\rrun. Repeating this for 2000independent runs with a different bandit problem, we obtained measures of the\rlearning algorithm��s average behavior.\n\r\r\r-2\n\r\r\r\r\r\r\r\r-3\n\r\r\r\r\r\r\r\rq*(10)\n\r\r\r\r\r\r\r\r5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 6\n\r\r\r\r\r\r\r\r10\n\r\r\r\r\r\r\r\rFigure 2.1: An exemplary bandit\rproblem from the 10-armed testbed. The true value q��(a) of each of the ten\ractions was selected according to a normal distribution around zero with\runit variance, and then the actual rewards were selected around q��(a)with unit variance,\ras suggested by these gray distributions.\n\r\r\r\r\r\r\r\r1\n\r\r\r\r\r\r\r\r0\n\r\r\r\r\r\r\r\r2\n\r\r\r\r\r\r\r\r3\n\r\r\r\r\r\r\r\r4\n\r\r\r\r\r\r\r\r7\n\r\r\r\r\r\r\r\r8\n\r\r\r\r\r\r\r\r9\n\r\r\r\r\r\r\r\rReward\ndistribution\n\r\r\r\r\r\r\r\rAction\n\r\r\r\r\rFigure 2.2 compares a greedy method with two\r^-greedy methods (s = 0.01 and e\r= 0.1), as described above, on the 10-armed testbed. Both methods formed their\raction-value estimates using the sample-average technique. The upper graph\rshows the increase in expected reward with experience. The greedy method\rimproved slightly faster than the other methods at the very beginning, but then\rleveled off at a lower level. It achieved a reward per step of only about 1,\rcompared with the best possible of about 1.55 on this testbed. The greedy\rmethod performs significantly worse in the long run because it often gets stuck\rperforming suboptimal actions. The lower graph shows that the greedy method\rfound the optimal action in only approximately one-third of the tasks. In the\rother two-thirds, its initial samples of the optimal action were disappointing,\rand it never returned to it. The e-greedy methods eventually perform better\rbecause they continue to explore and to improve their chances of recognizing\rthe optimal action. The e = 0.1 method explores more, and usually finds the\roptimal action earlier, but never selects it more than 91% of the time. The e =\r0.01 method improves more slowly, but eventually would perform better than the\re = 0.1 method on both performance measures. It is also possible to reduce e over time to try to get the best of both high and low\rvalues.\nThe advantage of e-greedy over greedy methods depends on the task. For example,\rsuppose the reward variance had been larger, say 10 instead of 1. With noisier\n\r\rr = 0.01\n\r\r\u0026nbsp;\n\r\r\r\r\r��0 (greedy)\n\r\r\r\r\rAverage\n��6wa��d\n\r\r\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\r\r0\n\r\r\r\r\r500\nSteps\n\r\r\r\r\r\r\r\rSteps\n\r\r\r\r\r\r\r\r250\n\r\r\r\r\r\r\r\r1000\n\r\r\r\r\r\r\r\r750\n\r\r\r\r\r\r\r\rFigure 2.2: Average performance\rof e-greedy action-value methods on the 10-armed testbed. These data are averages over\r2000 runs with different bandit problem. All methods used sample averages\ras their action-value estimates.\n\r\r\r\r\r\r\r\r% Optimal\n\r\r\r\r\r\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rrewards it takes more exploration to find the optimal action, and\re-greedy methods should fare even better relative to the greedy method. On the\rother hand, if the reward variances were zero, then the greedy method would\rknow the true value of each action after trying it once. In this case the\rgreedy method might actually perform best because it would soon find the\roptimal action and then never explore. But even in the deterministic case,\rthere is a large advantage to exploring if we weaken some of the other\rassumptions. For example, suppose the bandit task were nonstationary, that is,\rthat the true values of the actions changed over time. In this case exploration\ris needed even in the deterministic case to make sure one of the nongreedy\ractions has not changed to become better than the greedy one. As we will see in\rthe next few chapters, effective nonstationarity is the case most commonly\rencountered in reinforcement learning. Even if the underlying task is\rstationary and deterministic, the learner faces a set of banditlike decision\rtasks each of which changes over time as learning proceeds and the agent��s\rpolicy changes. Reinforcement learning requires a balance between exploration\rand exploitation.\nExercise 2.3 In the comparison shown in Figure\r2.2, which method will perform best in the long run in terms of cumulative\rreward and cumulative probability of selecting the best action? How much better\rwill it be? Express your answer quantitatively. ��\n\r\r2.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rIncremental Implementation\nThe action-value methods we have discussed so far\rall estimate action values as sample averages of observed rewards. We now turn\rto the question of how these averages can be computed in a computationally\refficient manner, in particular, with constant memory and per-time-step\rcomputation.\nTo simplify notation we concentrate on a single\raction. Let Ri now denote the reward received after the\rith selection of this action, and let Qn\rdenote the estimate of its action value after it has been selected n �� 1times, which we can now write simply as\nRi+ R2+ ... + Rn�� 1\nQn\nn��1\nThe obvious implementation would be to maintain a\rrecord of all the rewards and then perform this computation whenever the\restimated value was needed. However, in this case the memory and computational\rrequirements would grow over time as more rewards are seen. Each additional\rreward would require more memory to store it and more computation to compute\rthe sum in the numerator.\nAs you might suspect, this is not really\rnecessary. It is easy to devise incremental formulas for updating averages with\rsmall, constant computation required to process each new reward. Given Qn and the nth reward, Rn, the new\raverage of all n rewards can be computed by\n\r\r\r\r\r1\n\r\r\r\r\r\r\r\rn\n\r\r\r\r\r\r\r\rQn+1\n\r\r\r\r\r\r\r\rRi\n\r\r\r\r\r\u0026nbsp;\n\r\r\r\r1\n\r\r\r\r\rni\n\r\r\u0026nbsp;\n\r\rn\n\r\r\r\r\rn-1\u0026nbsp;\u0026nbsp; ��\nRi\n\r\r\r\r\r\r\r\r1\n\r\r\r\r\r\r\r\r�� Rn+\r(n �� 1)һ\nnn\n\r\r\r\r\r\r\r\r1\n\r\r\r\r\r\u0026nbsp;\n\r\r\r\r\r(2.3)\n\r\r\r\r\r\r\r\r1\n\r\r\r\r\r\r\r\rn\r1\nn\nQn\r+--- \nn\n\r\r\r\r\r\r\r\r+ (n �� 1)Qnj\n+ nQn�� Qn\n\r\r\r\r\r\r\r\rRn�� Qn\n\r\r\r\r\r\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rwhich holds even for n =\r1, obtaining Q2= Ri for arbitrary Qi. This implementa\u0026shy;tion requires memory only\rfor Qn and n, and only the small\rcomputation (2.3) for each new reward. Pseudocode for a complete bandit\ralgorithm using incrementally computed sample averages and \u0026pound;-greedy\raction selection is shown in the box. The function bandit(a)\ris assumed to take an action and return a corresponding reward.\n\r\r\r(2.4)\n\r\r\r\r\rThe update rule (2.3) is of a form that occurs frequently\rthroughout this book. The general form is\nNewEstimate OldEstimate+ StepSize\rTarget�� OldEstimate\nA simple bandit algorithm\nInitialize, for a = 1to k:\nQ(a) ^ 0 N(a)\r^ 0\nRepeat forever:\narg maxa\rQ(a) with probability 1 �� e (breaking ties randomly) a random action with\rprobability e\nR ^bandit (A)\nN(A) ^ N(A)ʮ1\nQ(A) ^ Q(A)ʮnA) [r �� Q(A)]\nThe expression [Target �� OldEstimat^ is an error in the\restimate. It is reduced by taking a step toward the ��Target.�� The target is\rpresumed to indicate a desirable direction in which to move, though it may be\rnoisy. In the case above, for example, the target is the nth reward.\nNote that the step-size parameter (StepSize)\rused in the incremental method described above changes from time step to time\rstep. In processing the nth reward for action a, that\rmethod uses a step-size parameter of n. In this book we denote the step-size\rparameter by the symbol a or, more generally, by at (a).\rWe sometimes use the informal shorthand a = n to refer to this case, leaving\rthe dependence of n on the action implicit, just as we\rhave in this section.\n2.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rTracking a Nonstationary\rProblem\nThe averaging methods discussed so far are appropriate in a\rstationary environment, but not if the bandit is changing over time. As noted\rearlier, we often encounter reinforcement learning problems that are\reffectively nonstationary. In such cases it makes sense to weight recent\rrewards more heavily than long-past ones. One of the most popular ways of doing\rthis is to use a constant step-size parameter. For example, the incremental\rupdate rule (2.3) for updating an average Qn\rof the n �� 1 past rewards is modified to be\nQn+1= Qn ʮa Rn�� Qn ,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (2.5)\nwhere the step-size parameter a E (0,1][1]is constant. This results in Qn+\ri being a weighted average of past rewards and the initial estimate Q :\nQn+ 1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; =\u0026nbsp;\u0026nbsp;\u0026nbsp; Qn + a Rn\r�� Qn\n=aRn+ (1�� a)Qn\n=aRn+ (1�� a)[aRn- 1+ (1�� a)Qn- 1]\n=aRn\u0026nbsp;\u0026nbsp;\u0026nbsp; +(1�� a)aRn- 1+ (1 �� a)[2]Qn- 1\n=aRn\u0026nbsp;\u0026nbsp;\u0026nbsp; +(1�� a)aRn- 1+ (1�� a)2aRn-2+\n\u0026#8226;\u0026#8226;\u0026#8226; + (1�� a)n-1aR1+ (1�� a)nQ1\nn\n=(1�� a)nQ1+ J] a(1�� a)n-iRi.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (2.6)\ni=1\nWe call this a weighted average\rbecause the sum of the weights is (1��a)n+En=1a(1 �� a)n-i\r= 1, as you can check for yourself. Note that the weight, a(1 �� a)n-i,\rgiven to the reward Ri depends on how many rewards ago, n\r�� i, it was observed. The quantity 1��a is less than 1, and thus the weight\rgiven to Ri decreases as the number of intervening rewards increases. In fact,\rthe weight decays exponentially according to the exponent\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; on 1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\u0026nbsp;\u0026nbsp;\u0026nbsp; a.\u0026nbsp;\u0026nbsp;\u0026nbsp; (If 1 �� a = 0, then all the weight\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; goesonthe\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; very\rlast\nreward,\u0026nbsp;\u0026nbsp; Rn,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; because\rof\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; the\rconvention that 00= 1.)\rAccordingly,\u0026nbsp;\u0026nbsp; this\u0026nbsp; is\u0026nbsp;\u0026nbsp;\u0026nbsp; sometimes\ncalled an exponential,\rrecency-weighted average.\nSometimes it is convenient to vary the step-size parameter from step\rto step. Let an (a) denote the step-size parameter used to process the reward\rreceived after the nth selection of action a. As we have noted, the choice an\r(a) = n1 results in the sample-average method, which is guaranteed to converge\rto the true action values by the law of large numbers. But of course\rconvergence is not guaranteed for all choices of the sequence {an(a)}. A\rwell-known result in stochastic approximation theory gives us the conditions\rrequired to assure convergence with probability 1:\nan(a)\r= oo and\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ^\ran (a) \u0026lt; ��.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (2.7)\nn=1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; n=1\nThe first condition is required to guarantee that\rthe steps are large enough to even\u0026shy;tually overcome any initial conditions or\rrandom fluctuations. The second condition guarantees that eventually the steps\rbecome small enough to assure convergence.\nNote that both convergence conditions are met for the sample-average\rcase, an (a)= ��, but not for the case of constant step-size\rparameter, an (a) = a. In the latter case, the second condition is not met,\rindicating that the estimates never completely con\u0026shy;verge but continue to vary\rin response to the most recently received rewards. As we mentioned above, this\ris actually desirable in a nonstationary environment, and problems that are\reffectively nonstationary are the norm in reinforcement learn\u0026shy;ing. In addition,\rsequences of step-size parameters that meet the conditions (2.7) often converge\rvery slowly or need considerable tuning in order to obtain a satisfac\u0026shy;tory\rconvergence rate. Although sequences of step-size parameters that meet these\rconvergence conditions are often used in theoretical work, they are seldom used\rin applications and empirical research.\nExercise 2.4 If the step-size\rparameters, an, are not constant, then the estimate Qn\ris a weighted average of previously received rewards with a weighting different\rfrom that given by (2.6). What is the weighting on each prior reward for the\rgeneral case, analogous to (2.6), in terms of the sequence of step-size parameters?��\nExercise 2.5 (programming) Design and\rconduct an experiment to demonstrate the difficulties that sample-average\rmethods have for nonstationary problems. Use a modified version of the 10-armed testbed in which all the q^(a) start out equal and then take\rindependent random walks (say by adding a normally distributed increment with\rmean zero and standard deviation 0.01 to all the q^(a) on each step). Prepare\rplots like Figure 2.2 for an action-value method using sample averages,\rincrementally computed, and another action-value method using a constant\rstep-size parameter, a = 0.1. Use\r\u0026pound; = 0.1 and longer runs, say of 10,000 steps.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n2.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rOptimistic Initial Values\nAll the methods we have discussed so far are dependent to some\rextent on the initial action-value estimates, Qi(a). In the language of\rstatistics, these methods are biased by their initial\restimates. For the sample-average methods, the bias disappears once all actions\rhave been selected at least once, but for methods with constant a, the bias is permanent, though decreasing over time as given\rby (2.6). In practice, this kind of bias is usually not a problem and can\rsometimes be very helpful. The downside is that the initial estimates become,\rin effect, a set of parameters that must be picked by the user, if only to set\rthem all to zero. The upside is that they provide an easy way to supply some\rprior knowledge about what level of rewards can be expected.\nInitial action values can also be used as a simple way of\rencouraging exploration. Suppose that instead of setting the initial action\rvalues to zero, as we did in the 10-armed testbed, we set them all to +5.\rRecall that the q^(a) in this problem are selected from a normal distribution\rwith mean 0and variance 1. An initial estimate\rof +5 is thus wildly optimistic. But this optimism encourages action-value\rmethods to explore. Whichever actions are initially selected, the reward is\rless than the starting estimates; the learner switches to other actions, being\r��disappointed�� with the rewards it is receiving. The result is that all actions\rare tried several times before the value estimates converge. The system does a\rfair amount of exploration even if greedy actions are selected all the time.\nFigure 2.3 shows the\rperformance on the 10-armed bandit testbed of a greedy method using Qi(a) = +5,\rfor all a. For comparison, also shown is an \u0026pound;-greedy\rmethod with Qi(a) = 0. Initially,\rthe optimistic method performs worse because it explores more, but eventually\rit performs better because its exploration decreases with time. We call this\rtechnique for encouraging exploration optimistic initial val-\n\r\r\r\n\r\r\r\r\r\r%\nOptimal\naction\n\r\r\r\r\r\rSteps\n\r\r\r\r\r\rFigure 2.3: The effect of optimistic initial\raction-value estimates on the 10-armed testbed. Both\rmethods used a constant step-size parameter, a = 0.1.\n\r\r\r\r\r\u0026nbsp;\nues. We regard it as\ra simple trick that can be quite effective on stationary problems, but it is\rfar from being a generally useful approach to encouraging exploration. For\rexample, it is not well suited to nonstationary problems because its drive for\rex\u0026shy;ploration is inherently temporary. If the task changes, creating a renewed\rneed for exploration, this method cannot help. Indeed, any method that focuses\ron the initial state in any special way is unlikely to help with the general\rnonstationary case. The beginning of time occurs only once, and thus we should\rnot focus on it too much. This criticism applies as well to the sample-average\rmethods, which also treat the beginning of time as a special event, averaging\rall subsequent rewards with equal weights. Nevertheless, all of these methods\rare very simple, and one of them or some simple combination of them is often\radequate in practice. In the rest of this book we make frequent use of several\rof these simple exploration techniques.\nExercise 2.6: Mysterious Spikes The results shown in Figure 2.3 should be\rquite reliable because they are averages over 2000individual,\rrandomly chosen 10-armed bandit tasks. Why, then, are there oscillations and spikes in\rthe early part of the curve for the optimistic method? In other words, what\rmight make this method perform particularly better or worse, on average, on\rparticular early steps?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n2.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rUpper-Confidence-Bound Action\rSelection\nExploration is needed because the estimates of\rthe action values are uncertain. The greedy actions are those that look best at\rpresent, but some of the other actions may actually be better. e-greedy action\rselection forces the non-greedy actions to be tried, but indiscriminately, with\rno preference for those that are nearly greedy or particularly uncertain. It\rwould be better to select among the non-greedy actions according to their\rpotential for actually being optimal, taking into account both how close their\restimates are to being maximal and the uncertainties in those estimates.\nOne effective way of doing this is to\rselect actions as\n\r\r\rIlog t Nt(a)\n\r\r\r\r\r\r\r\r(2.8)\n\r\r\r\r\rQt(a) + c\nAt == argmax\na\nwhere log t denotes the natural logarithm of t\r(the number that e ^ 2.71828 would have to be raised to\rin order to equal t), Nt(a) denotes the number of times\rthat action a has been selected prior to time t (the denominator in (2.1)), and the number c \u0026gt; 0 controls the\rdegree of exploration. If Nt(a) = 0, then a is considered to be a maximizing action.\nThe idea of this upper confidence\rbound (UCB) action selection is that the square- root term is a measure\rof the uncertainty or variance in the estimate of a��s value. The quantity being\rmax��ed over is thus a sort of upper bound on the possible true value of action a, with the c parameter determining the\rconfidence level. Each time a is selected the\runcertainty is presumably reduced; Nt(a) is incremented\rand, as it appears in the denominator of the uncertainty term, the term is\rdecreased. On the other hand, each time an action other than a\ris selected t is increased but Nt(a) is not;\ras t appears in the numerator the uncertainty estimate is increased. The use of\rthe natural logarithm means that the increase gets smaller over time, but is\runbounded; all actions will eventually be selected, but as time goes by it will\rbe a longer wait, and thus a lower selection frequency, for actions with a\rlower value estimate or that have already been selected more times.\nResults with UCB on the\r10-armed testbed are shown in Figure 2.4. UCB will often perform well, as shown\rhere, but is more difficult than \u0026pound;-greedy to extend beyond bandits to the more\rgeneral reinforcement learning settings considered in the rest of this book.\rOne difficulty is in dealing with nonstationary problems; something more\rcomplex than the methods presented in Section 2.4 would be needed. Another\rdifficulty is dealing with large state spaces, particularly function\rapproximation as\n\r\nSteps\nFigure 2.4: Average performance of UCB action\rselection on the 10-armed testbed. As shown, UCB generally performs better\rthan \u0026pound;-greedy action selection, except in the first k steps,\rwhen it selects randomly among the as-yet-untried actions.\n\r\r\r\r\r\u0026nbsp;\n\r\u0026nbsp;\ndeveloped in Part II of this book. In these more advanced settings\rthere is currently no known practical way of utilizing the idea of UCB action\rselection.\n2.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rGradient Bandit Algorithms\nSo far in this chapter we have considered methods that estimate\raction values and use those estimates to select actions. This is often a good\rapproach, but it is not the only one possible. In this section we consider\rlearning a numerical preference Hi (a) for each action\ra. The larger the preference, the more often that action is taken, but the\rpreference has no interpretation in terms of reward. Only the relative\rpreference of one action over another is important; if we add 1000to all the preferences there is no effect on the action\rprobabilities, which are determined according to a soft-max distribution (i.e.,\rGibbs or Boltzmann distribution) as follows:\neHt(a)\n\u0026nbsp;TOC \\o \u0026quot;1-5\u0026quot; \\h \\z Pr{Ai\r= a} =\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; eH\u0026pound;(b)\r= ni(a),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (2.9)\nwhere here we have also introduced a useful new\rnotation ni(a) for the probability of taking action a at time t. Initially all\rpreferences are the same (e.g., H1(a) = 0, Va) so that all actions have an\requal probability of being selected.\nExercise 2.7 Show that\rin the case of two actions, the soft-max distribution is the same as that given\rby the logistic, or sigmoid, function often used in statistics and artificial\rneural networks.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nThere is a natural learning\ralgorithm for this setting based on the idea of stochastic gradient ascent. On\reach step, after selecting the action Ai and receiving the reward Ri, the\rpreferences are updated by:\nHi+1(Ai) = Hi(Ai)\r+ (a(Ri �� ^Ri)(1�� ni(Ai));\rand\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (��������\nHi+1(a) = Hi (a)\r�� a(Ri �� ^i)ni(a),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Va\r= Ai,\nwhere a \u0026gt; 0 is a step-size parameter, and R^ E R is the average\rof all the rewards up through and including time t, which can be computed\rincrementally as described in Section 2.3 (or Section 2.4 if the problem is\rnonstationary). The Ri term serves as a baseline with which the reward is\rcompared. If the reward is higher than the baseline, then the probability of\rtaking Ai in the future is increased, and if the reward is below baseline, then\rprobability is decreased. The non-selected actions move in the opposite\rdirection.\nFigure 2.5 shows results with the gradient bandit\ralgorithm on a variant of the 10-armed testbed in\rwhich the true expected rewards were selected according to a normal\rdistribution with a mean of +4 instead of zero (and with unit variance as\rbefore). This shifting up of all the rewards has absolutely no effect on the\rgradient bandit algorithm because of the reward baseline term, which\rinstantaneously adapts to the new level. But if the baseline were omitted (that\ris, if Ri was taken to be constant zero in (2.10)), then performance would be significantly degraded, as shown in\rthe figure.\n\r\r\ra= 0.1\n\r\r\r\r\r\r\r\ra= 0.4\n\r\r\r\r\r\r\r\r250\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 500\nSteps\n\r\r\r\r\r\r\r\r750\n\r\r\r\r\r\r\r\rFigure 2.5: Average\rperformance of the gradient bandit algorithm with and without a reward\rbaseline on the 10-armed\rtestbed when the q^(a) are chosen to be near ʮ4\rrather than near zero.\n\r\r\r\r\r\r\r\r1\n\r\r\r\r\r\r\r\r1000\n\r\r\r\r\r\r\r\r\u0026nbsp;\n\r100%\n\r\r\u0026nbsp;\n\r80%\n\r\r%\n\r60%\n\r\rOptimal\n\r\u0026nbsp;\n\r\raction\n\r40%\n\r\r\u0026nbsp;\n\r20%\n\r\r\u0026nbsp;\n\r0%\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\rThe Bandit Gradient Algorithm as\rStochastic Gradient Ascent\nOne can gain a deeper insight into the gradient\rbandit algorithm by under\u0026shy;standing it as a stochastic approximation to gradient\rascent. In exact gradient ascent, each preference Ht(a) would be incrementing proportional to the in\u0026shy;crement��s\reffect on performance:\nHt+i(a) ^\rHt(a)ʮadHR,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (2.11)\nwhere the measure of performance here is the\rexpected reward:\nE[Rt] == ^nt(b)q*(b), b\nand the measure of the increment��s effect is the partial\rderivative of this per\u0026shy;formance measure with respect to the preference.\rOf course, it is not possible to implement gradient ascent exactly in our case because\rby assumption we do not know the q^(b), but in fact the updates of our\ralgorithm (2.10) are equal to (2.11) in expected value, making the algorithm an instance of stochastic gra\u0026shy;dient ascent. The calculations showing this\rrequire only beginning calculus, but take several steps. First we take a closer\rlook at the exact performance gradient:\ndE[Rt]\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 9\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ^\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (b) (b)\nmardHt^i ?���t\n=?����\n=^ (��-��m,\nwhere Xt can be any scalar that does not\rdepend on b. We can include it here because the gradient\rsums to zero over all the actions, Eb �淺)=0.As Ht(a)\ris changed, some actions�� probabilities go up and some down, but the sum of the\rchanges must be zero because the sum of the probabilities must remain one.\n^ nt(b) (q*(b) �� Xt)\rIHH/nt(b)\nThe equation is now in the form of an expectation, summing over all\rpossible values b of the random variable At, then multiplying by the probability of taking those\rvalues. Thus:\n\r\r\rE\nE\n\r\r\r\r\r(q*(At) �� Xt) dHt(a)\r/nt(At)\n(Rt - Rt)�F����\nwhere here we have chosen Xt = Rt and substituted Rt for q^(At), which\ris permitted because E[Rt|At] = q^(At) and because the Rt\r(given At) is uncorrelated with anything else. Shortly we will establish that ����))= nt(b) (1a=b �� nt(a), where 1a=b is\rdefined to be 1 if a = b, else 0. Assuming that for now,\rwe have\n=E[(Rt �� ^Rt)nt(At)(1a=A\u0026pound; �� 7Tt(a))/nt(At)]\n=E [(Rt ��\rRt)(ia=At ��أt��)]\u0026#8226;\nRecall that our plan has been to write the performance gradient as\ran expecta\u0026shy;tion of something that we can sample on each step, as we have just\rdone, and then update on each step proportional to the sample. Substituting a\rsample of the expectation above for the performance gradient in (2.11) yields:\nHt+i(a) = Ht(a)\r+ a(Rt �� Rt) (1a=At �� ^t(a)), Va,\nwhich you will recognize as being equivalent to our original\ralgorithm (2.10).\nThus it remains only to show that ��(0) = nt(b)( 1a=b �� nt(a)), as we\rassumed. Recall the standard quotient rule for derivatives:\n\r\r\u0026nbsp;SHAPE \u0026nbsp;\\* MERGEFORMAT \r\r\r\u0026nbsp;\n\r\r\r\r\r\r\u0026nbsp;\n\r\r\r\r\rf(x)g(x)_\n\r\r\r\r\r\r\r\rd_\ndx\n\r\r\r\r\rdfg(x) �� f (x)��\ng(x)2\nUsing this, we can write\n\r\r\r\r\rdni(b)\ndHi(a)\n\r\r\r\r\r\r\r\rd\n\r\r\r\r\r\r\r\rni(b)\n\r\r\r\r\r\r\r\rdHi(a)\nd\r[ e�P dHi(a)\rVfc_1eHt(c)\n\r\r\r\r\r\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rdeHt ��Vk\u0026nbsp;\u0026nbsp; pHt(c)\r_ eHt(b) d Eh eHt(c)\ndHt(a)��c=1e\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\u0026nbsp; e ~SHt(a)~\n\r\r\u0026nbsp;\n\r\r\r\r\r2\n\r\r\r\r\r\r\r\reH t(c)\n\r\r\r\r\r(Ek=1\n\r\r\u0026nbsp;\n\r\r(by the quotient rule)\n\r\r\u0026nbsp;\n\r\r\r\r\r(because\n\r\r\r\r\r\r\r\rex)\n\r\r\r\r\r1a=6e^t(a^\rk=1eHt(c)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\u0026nbsp;\u0026nbsp; eHt(b)eHt(a)\n(Ek=1e ��Ѿ\n1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\ra=fceHt(b)eHt(b)eHt(a)\n\r\r\u0026nbsp;\n\r\r\r\r\r2\n\r\r\r\r\rEk=1eHt(c)(^k=1eHt(c)��\n\r\r\rQ.E.D.\n\r\r\r\r\r1a=bV(i(6) �� ni(b)ni(a) ni (b)( 1a=b �� W(a)).\n\r\r\u0026nbsp;\n\r\rWe have just shown that the expected update of\rthe gradient bandit algo\u0026shy;rithm is equal to the gradient of expected reward, and\rthus that the algorithm is an instance of stochastic gradient ascent. This\rassures us that the algorithm has robust convergence properties.\nNote that we did not require any properties of the reward baseline\rother than that it does not depend on the selected action. For example, we\rcould have set it to zero, or to 1000, and the algorithm\rwould still be an instance of stochastic gradient ascent. The choice of the\rbaseline does not affect the expected update of the algorithm, but it does\raffect the variance of the update and thus the rate of convergence (as shown,\re.g., in Figure 2.5). Choosing it as the average of the rewards may not be the\rvery best, but it is simple and works well in practice.\n2.9\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rAssociative\rSearch (Contextual Bandits)\nSo far in this chapter we have considered only nonassociative tasks,\rin which there is no need to associate different actions with different\rsituations. In these tasks the learner either tries to find a single best\raction when the task is stationary, or tries to track the best action as it\rchanges over time when the task is nonstationary. However, in a general\rreinforcement learning task there is more than one situation, and the goal is\rto learn a policy: a mapping from situations to the actions that are best in\rthose situations. To set the stage for the full problem, we briefly discuss the\r\n\r\rsimplest way in which nonassociative tasks extend to the associative\rsetting.\nAs an example, suppose there\rare several different k-armed bandit tasks, and that on each step you confront\rone of these chosen at random. Thus, the bandit task changes randomly from step\rto step. This would appear to you as a single, nonstationary k-armed bandit\rtask whose true action values change randomly from step to step. You could try\rusing one of the methods described in this chapter that can handle\rnonstationarity, but unless the true action values change slowly, these methods\rwill not work very well. Now suppose, however, that when a bandit task is\rselected for you, you are given some distinctive clue about its identity (but\rnot its action values). Maybe you are facing an actual slot machine that\rchanges the color of its display as it changes its action values. Now you can\rlearn a policy associating each task, signaled by the color you see, with the\rbest action to take when facing that task��for instance, if red, select arm 1;\rif green, select arm 2. With the right policy you can usually do much better\rthan you could in the absence of any information distinguishing one bandit task\rfrom another.\nThis is an example of an associative search\rtask, so called because it involves both trial-and-error learning in the form\rof search for the best actions and association\rof these actions with the situations in which they are best.[3]Associative search tasks are intermediate between the k-armed\rbandit problem and the full reinforcement learning problem. They are like the\rfull reinforcement learning problem in that they involve learning a policy, but\rlike our version of the k-armed bandit problem in that each action affects only\rthe immediate reward. If actions are allowed to affect the next\rsituation as well as the reward, then we have the full reinforcement\rlearning problem. We present this problem in the next chapter and consider its\rramifications throughout the rest of the book.\n2.10\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rSummary\nWe have presented in this chapter\rseveral simple ways of balancing exploration and exploitation. The e-greedy\rmethods choose randomly a small fraction of the time, whereas UCB methods\rchoose deterministically but achieve exploration by subtly favoring at each\rstep the actions that have so far received fewer samples. Gradient bandit\ralgorithms estimate not action values, but action preferences, and favor the\rmore preferred actions in a graded, probabilistic manner using a soft-max\rdistribu\u0026shy;tion. The simple expedient of initializing estimates optimistically\rcauses even greedy methods to explore significantly.\n\r\r\rAverage\rr6ward over first 1000 steps\n\r\r\r\r\r\r\r\rFigure 2.6: A parameter study of\rthe various bandit algorithms presented in this chapter. Each point is the\raverage reward obtained over 1000steps with a particular algorithm at a particular setting of its parameter.\n\r\r\r\r\r\r\r\r/ a / c / Q0\n\r\r\r\r\rIt is natural to ask which of these methods is best.\rAlthough this is a difficult question to answer in general, we can certainly\rrun them all on the 10-armed testbed that we have used\rthroughout this chapter and compare their performances. A complication is that\rthey all have a parameter; to get a meaningful comparison we will have to\rconsider their performance as a function of their parameter. Our graphs so far have\rshown the course of learning over time for each algorithm and parametersetting, but it would be too visually confusing to show such a learning\rcurve for each algorithm and parameter value. Instead we summarize a\rcomplete learning curve by its average value over the 1000steps;\rthis value is proportional to the area under the learning curves we have shown\rup to now. Figure 2.6 shows this measure for the various bandit algorithms from\rthis chapter, each as a function of its own parameter shown on a single scale\ron the x-axis. Note that the parameter values are varied by factors of two and\rpresented on a log scale. Note also the characteristic inverted- U shapes of\reach algorithm��s performance; all the algorithms perform best at an\rintermediate value of their parameter, neither too large nor too small. In\rassessing a method, we should attend not just to how well it does at its best\rparameter setting, but also to how sensitive it is to its parameter value. All\rof these algorithms are fairly insensitive, performing well over a range of\rparameter values varying by about an order of magnitude. Overall, on this\rproblem, UCB seems to perform best.\nDespite their simplicity, in our opinion the\rmethods presented in this chapter can fairly be considered the state of the\rart. There are more sophisticated methods, but their complexity and assumptions\rmake them impractical for the full reinforcement learning problem that is our\rreal focus. Starting in Chapter 5 we present learning methods for solving the\rfull reinforcement learning problem that use in part the simple methods\rexplored in this chapter.\nAlthough the simple methods explored in this\rchapter may be the best we can do at present, they are far from a fully\rsatisfactory solution to the problem of balancing exploration and exploitation.\nThe classical solution to balancing exploration and exploitation in\rk-armed bandit problems is to compute special functions called Gittins\rindices. These provide an optimal solution to a certain kind of bandit\rproblem more general than that con\u0026shy;sidered here but that assumes the prior\rdistribution of possible problems is known. Unfortunately, neither the theory\rnor the computational tractability of this method appear to generalize to the\rfull reinforcement learning problem that we consider in the rest of the book.\nBayesianmethods\rassume a known initial distribution over the action values and then updates the\rdistribution exactly after each step (assuming that the true action values are\rstationary). In general, the update computations can be very complex, but for\rcertain special distributions (called conjugate priors)\rthey are easy. One possibility is to then select actions at each step according\rto their posterior proba\u0026shy;bility of being the best action. This method,\rsometimes called posterior sampling or Thompson\rsampling, often performs similarly to the best of the distribution-free\rmethods we have presented in this chapter.\nIn the Bayesian setting it is even conceivable to compute the optimal balance be\u0026shy;tween exploration and exploitation.\rClearly, for any possible action we can compute the probability of each\rpossible immediate reward and the resultant posterior distri\u0026shy;butions over\raction values. This evolving distribution becomes the information\rstate of the problem. Given a horizon, say of 1000 steps, one can\rconsider all possible actions, all possible resulting rewards, all possible\rnext actions, all next rewards, and so on for all 1000 steps. Given the\rassumptions, the rewards and probabilities of each possible chain of events can\rbe determined, and one need only pick the best. But the tree of possibilities\rgrows extremely rapidly; even if there are only two ac\u0026shy;tions and two rewards,\rthe tree will have 22000leaves. It is\rgenerally not feasible to perform this immense computation exactly, but perhaps\rit could be approximated efficiently. This approach would effectively turn the\rbandit problem into an instance of the full reinforcement learning problem; it\ris beyond the current state of the art, but someday it may be possible to use\rreinforcement learning methods such as those presented in Part II of this book\rto approximate this optimal solution.\nExercise 2.8 (programming) Make a\rfigure analogous to Figure 2.6 for the non-stationary case outlined in Exercise\r2.5. Include the constant-step-size e-greedy algorithm with a\r= 0.1. Use runs of 200,000 steps and, as a performance measure for each\ralgorithm and parameter setting, use the average reward over the last 100,000 steps.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nBibliographical and Historical\rRemarks\n2.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rBandit problems have been\rstudied in statistics, engineering, and psychology. In statistics, bandit\rproblems fall under the heading ��sequential design of ex\u0026shy;periments,�� introduced\rby Thompson (1933, 1934) and Robbins (1952), and studied by Bellman (1956).\rBerry and Fristedt (1985) provide an extensive treatment of bandit problems\rfrom the perspective of statistics. Narendra and Thathachar (1989) treat bandit\rproblems from the engineering perspec\u0026shy;tive, providing a good discussion of the\rvarious theoretical traditions that have focused on them. In psychology, bandit\rproblems have played roles in statistical learning theory (e.g., Bush and\rMosteller, 1955; Estes, 1950).\nThe term greedy is often used in the\rheuristic search literature (e.g., Pearl,\n\r\r1984). The conflict between exploration and exploitation is known in\rcontrol engineering as the conflict between identification (or estimation) and\rcontrol (e.g., Witten, 1976). Feldbaum (1965) called it the dual\rcontrol problem, referring to the need to solve the two problems of\ridentification and con\u0026shy;trol simultaneously when trying to control a system\runder uncertainty. In discussing aspects of genetic algorithms, Holland (1975)\remphasized the im\u0026shy;portance of this conflict, referring to it as the conflict\rbetween the need to exploit and the need for new information.\n\r\r\r2.2\n2.3-4\n2.5\n2.6\n2.7\n2.8\n\r\r\r\r\rAction-value methods for our k-armed bandit problem\rwere first proposed by Thathachar and Sastry (1985). These are often called estimator algorithms in the learning automata literature. The\rterm action value is due to Watkins (1989). The first to\ruse \u0026pound;-greedy methods may also have been Watkins (1989, p. 187), but the idea is\rso simple that some earlier use seems likely.\nThis material falls under the general heading of stochastic\riterative algo\u0026shy;rithms, which is well covered by Bertsekas and Tsitsiklis (1996).\nOptimistic initialization was used in\rreinforcement learning by Sutton (1996).\nEarly work on using estimates of the upper confidence bound to\rselect actions was done by Lai and Robbins (1985), Kaelbling (1993b), and\rAgrawal (1995). The UCB algorithm we present here is called UCB1 in the\rliterature and was first developed by Auer, Cesa-Bianchi and Fischer (2002).\nGradient bandit algorithms are a special case of the gradient-based\rrein\u0026shy;forcement learning algorithms introduced by Williams (1992), and that later\rdeveloped into the actor-critic and policy-gradient algorithms that we treat\rlater in this book. Our development here was influenced by that by Balara- man\rRavindran. Further discussion of the choice of baseline is provided there and\rby Greensmith, Bartlett, and Baxter (2001, 2004) and Dick (2015).\nThe term softmax for the action selection\rrule (2.9) is due to Bridle (1990). This rule appears to have been first\rproposed by Luce (1959).\nThe term associative\rsearch and the corresponding problem were introduced by Barto, Sutton,\rand Brouwer (1981). The term associative reinforcement learning\rhas also been used for associative search (Barto and Anandan, 1985), but we\rprefer to reserve that term as a synonym for the full reinforcement learning\rproblem (as in Sutton, 1984). (And, as we noted, the modern litera\u0026shy;ture also\ruses the term ��contextual bandits�� for this problem.) We note that Thorndike��s\rLaw of Effect (quoted in Chapter 1) describes associative search by referring\rto the formation of associative links between situations (states) and actions.\rAccording to the terminology of operant, or instrumental, con\u0026shy;ditioning (e.g.,\rSkinner, 1938), a discriminative stimulus is a stimulus that signals the\rpresence of a particular reinforcement contingency. In our terms, different\rdiscriminative stimuli correspond to different states.\n\r\r\r2.9\n\r\r\r\r\rThe Gittins index approach is due to Gittins and Jones\r(1974). Duff (1995) showed how it is possible to learn Gittins indices for\rbandit problems through reinforcement learning. Bellman (1956) was the first to\rshow how dynamic programming could be used to compute the optimal balance\rbetween explo\u0026shy;ration and exploitation within a Bayesian formulation of the\rproblem. The survey by Kumar (1985) provides a good discussion of Bayesian and\rnon- Bayesian approaches to these problems. The term information\rstate comes from the literature on partially observable MDPs; see, e.g.,\rLovejoy (1991).\n\r\r48\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; CHAPTER2.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; MULTI-ARMED\u0026nbsp;\u0026nbsp; BANDITS\n\r\rChapter 3\nFinite Markov Decision\rProcesses\nIn this chapter we introduce the problem that we\rtry to solve in the rest of the book. This problem could be considered to define\rthe field of reinforcement learning: any method that is suited to solving this\rproblem we consider to be a reinforcement learning method.\nOur objective in this chapter is to describe the reinforcement\rlearning problem in a broad sense. We try to convey the wide range of possible\rapplications that can be framed as reinforcement learning tasks. We also\rdescribe mathematically idealized forms of the reinforcement learning problem\rfor which precise theoretical statements can be made. We introduce key elements\rof the problem��s mathematical structure, such as value functions and Bellman\requations. As in all of artificial intelligence, there is a tension between\rbreadth of applicability and mathematical tractability. In this chapter we\rintroduce this tension and discuss some of the trade-offs and challenges that\rit implies.\n3.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe\rAgent-Environment Interface\nThe reinforcement learning problem is meant to be\ra straightforward framing of the problem of learning from interaction to\rachieve a goal. The learner and decision\u0026shy;maker is called the agent.\rThe thing it interacts with, comprising everything outside the agent, is called\rthe environment. These interact continually, the agent\rselecting actions and the environment responding to those actions and\rpresenting new situa\u0026shy;tions to the agent.[4]The environment also gives rise to rewards, special numerical\rvalues that the agent tries to maximize over time. A complete specification of\ran environment, including how rewards are determined, defines a task , one instance of the reinforcement learning problem.\n\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\rr\u0026quot;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\rAgent\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\rstate\n\r\u0026nbsp;\n\rreward\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\raction\n\r\rSt\n\r\u0026nbsp;\n\rRt\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\ri . Rt+1\n\rr\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\\ t\rSt+l\n\rEnvironment\n\rM------- \n\r\u0026nbsp;\n\r\r\r\r\u0026nbsp;\nFigure 3.1: The agent-environment interaction in reinforcement\rlearning.\nMore specifically, the agent\rand environment interact at each of a sequence of discrete time steps, t = 0, 1, 2, 3, . . .. [5]At each time step t, the agent receives some representation of the\renvironment��s state, StG S, where S is the set of possible states, and on that basis selects\ran action, At G A(St), where A(St) is the set of actions available in state St. One time step later, in part as a consequence of its action, the\ragent receives a numerical reward, Rt+i G R C R, and finds itself in a new state, St+i.[6]Figure 3.1 diagrams the agent-environment interaction.\nAt each time step, the agent\rimplements a mapping from states to probabilities of selecting each possible\raction. This mapping is called the agent��s policy and is\rdenoted nt, where nt(a|s) is the probability\rthat At = a if St= s.\rReinforcement learning methods specify how the agent changes its policy as a\rresult of its experience. The agent��s goal, roughly speaking, is to maximize\rthe total amount of reward it receives over the long run.\nThis framework is abstract and\rflexible and can be applied to many different problems in many different ways.\rFor example, the time steps need not refer to fixed intervals of real time;\rthey can refer to arbitrary successive stages of decision-making and acting.\rThe actions can be low-level controls, such as the voltages applied to the\rmotors of a robot arm, or high-level decisions, such as whether or not to have\rlunch or to go to graduate school. Similarly, the states can take a wide\rvariety of forms. They can be completely determined by low-level sensations,\rsuch as direct sensor readings, or they can be more high-level and abstract,\rsuch as symbolic descriptions of objects in a room. Some of what makes up a\rstate could be based on memory of past sensations or even be entirely mental or\rsubjective. For example, an agent could be in the state of not being sure where\ran object is, or of having just been surprised in some clearly defined sense.\rSimilarly, some actions might be totally mental or computational. For example,\rsome actions might control what an agent chooses to think about, or where it\rfocuses its attention. In general, actions can be any decisions we want to\rlearn how to make, and the states can be anything we can know that might be\ruseful in making them.\nIn particular, the boundary\rbetween agent and environment is not often the same as the physical boundary of\ra robot��s or animal��s body. Usually, the boundary is drawn closer to the agent\rthan that. For example, the motors and mechanical linkages of a robot and its\rsensing hardware should usually be considered parts of the environment rather\rthan parts of the agent. Similarly, if we apply the framework to a person or\ranimal, the muscles, skeleton, and sensory organs should be considered part of\rthe environment. Rewards, too, presumably are computed inside the physical\rbodies of natural and artificial learning systems, but are considered external\rto the agent.\nThe general rule we follow is\rthat anything that cannot be changed arbitrarily by the agent is considered to\rbe outside of it and thus part of its environment. We do not assume that\reverything in the environment is unknown to the agent. For example, the agent\roften knows quite a bit about how its rewards are computed as a function of its\ractions and the states in which they are taken. But we always consider the\rreward computation to be external to the agent because it defines the task\rfacing the agent and thus must be beyond its ability to change arbitrarily. In\rfact, in some cases the agent may know everything about\rhow its environment works and still face a difficult reinforcement learning\rtask, just as we may know exactly how a puzzle like Rubik��s cube works, but\rstill be unable to solve it. The agent-environment boundary represents the\rlimit of the agent��s absolute control, not of its\rknowledge.\nThe agent-environment boundary\rcan be located at different places for different purposes. In a complicated\rrobot, many different agents may be operating at once, each with its own\rboundary. For example, one agent may make high-level decisions which form part\rof the states faced by a lower-level agent that implements the high- level\rdecisions. In practice, the agent-environment boundary is determined once one\rhas selected particular states, actions, and rewards, and thus has identified a\rspecific decision-making task of interest.\nThe reinforcement learning\rframework is a considerable abstraction of the problem of goal-directed\rlearning from interaction. It proposes that whatever the details of the\rsensory, memory, and control apparatus, and whatever objective one is trying to\rachieve, any problem of learning goal-directed behavior can be reduced to three\rsignals passing back and forth between an agent and its environment: one signal\rto represent the choices made by the agent (the actions), one signal to\rrepresent the basis on which the choices are made (the states), and one signal\rto define the agent��s goal (the rewards). This framework may not be sufficient\rto represent all decision- learning problems usefully, but it has proved to be\rwidely useful and applicable.\nOf course, the particular\rstates and actions vary greatly from task to task, and how they are represented\rcan strongly affect performance. In reinforcement learning, as in other kinds\rof learning, such representational choices are at present more art than\rscience. In this book we offer some advice and examples regarding good ways of\rrepresenting states and actions, but our primary focus is on general principles\rfor learning how to behave once the representations have been selected.\nExample 3.1: Bioreactor Suppose\rreinforcement learning is being applied to determine moment-by-moment\rtemperatures and stirring rates for a bioreactor (a large vat of nutrients and\rbacteria used to produce useful chemicals). The actions in such an application\rmight be target temperatures and target stirring rates that are passed to\rlower-level control systems that, in turn, directly activate heating elements\rand motors to attain the targets. The states are likely to be thermocouple and\rother sensory readings, perhaps filtered and delayed, plus symbolic inputs\rrepresenting the ingredients in the vat and the target chemical. The rewards\rmight be moment- by-moment measures of the rate at which the useful chemical is\rproduced by the bioreactor. Notice that here each state is a list, or vector,\rof sensor readings and symbolic inputs, and each action is a vector consisting\rof a target temperature and a stirring rate. It is typical of reinforcement\rlearning tasks to have states and actions with such structured representations.\rRewards, on the other hand, are always single numbers.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExample 3.2: Pick-and-Place Robot\rConsider using reinforcement learning to control the motion of a robot arm in a\rrepetitive pick-and-place task. If we want to learn movements that are fast and\rsmooth, the learning agent will have to control the motors directly and have\rlow-latency information about the current positions and velocities of the\rmechanical linkages. The actions in this case might be the voltages applied to\reach motor at each joint, and the states might be the latest readings of joint\rangles and velocities. The reward might be +1 for each object successfully\rpicked up and placed. To encourage smooth movements, on each time step a small,\rnegative reward can be given as a function of the moment-to-moment ��jerkiness��\rof the motion.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExample 3.3: Recycling\rRobot A mobile robot has the job of collecting empty soda cans in an office\renvironment. It has sensors for detecting cans, and an arm and gripper that can\rpick them up and place them in an onboard bin; it runs on a rechargeable\rbattery. The robot��s control system has components for interpreting sensory\rinformation, for navigating, and for controlling the arm and gripper. High-\rlevel decisions about how to search for cans are made by a reinforcement\rlearning agent based on the current charge level of the battery. This agent has\rto decide whether the robot should (1) actively search for a\rcan for a certain period of time, (2) remain stationary and wait for someone to\rbring it a can, or (3) head back to its home base to recharge its battery. This\rdecision has to be made either periodically or whenever certain events occur,\rsuch as finding an empty can. The agent therefore has three actions, and its\rstate is determined by the state of the battery. The rewards might be zero most\rof the time, but then become positive when the robot secures an empty can, or\rlarge and negative if the battery runs all the way down. In this example, the\rreinforcement learning agent is not the entire robot. The states it monitors\rdescribe conditions within the robot itself, not conditions of the robot��s\rexternal environment. The agent��s environment therefore includes the rest of\rthe robot, which might contain other complex decision-making systems, as well\ras the robot��s external environment.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n\r\rExercise 3.1\rDevise three example tasks of your own that fit into the reinforcement learning\rframework, identifying for each its states, actions, and rewards. Make the\rthree examples as different from each other as possible.\rThe framework is abstract and flexible and can be applied in many different\rways. Stretch its limits in some way in at least one of your examples.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 3.2\rIs the reinforcement learning framework adequate to usefully represent all goal-directed learning tasks? Can you think of any clear\rexceptions?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 3.3\rConsider the problem of driving. You could define the actions in terms of the\raccelerator, steering wheel, and brake, that is, where your body meets the\rmachine. Or you could define them farther out��say, where the rubber meets the\rroad, considering your actions to be tire torques. Or you could define them\rfarther in��say, where your brain meets your body, the actions being muscle\rtwitches to control your limbs. Or you could go to a really high level and say\rthat your actions are your choices of where to drive.\rWhat is the right level, the right place to draw the line between agent and\renvironment? On what basis is one location of the line to be preferred over\ranother? Is there any fundamental reason for preferring one location over\ranother, or is it a free choice?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n3.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rGoals and\rRewards\nIn reinforcement learning, the purpose\ror goal of the agent is formalized in terms of a special reward signal passing\rfrom the environment to the agent. At each time step, the reward is a simple\rnumber, Rt G R. Informally, the agent��s goal is to\rmaximize the total amount of reward it receives. This means maximizing not\rimmediate reward, but cumulative reward in the long run. We can clearly state\rthis informal idea as the reward hypothesis:\nThat all of what we mean by goals and purposes can be well thought\rof as the maximization of the expected value of the cumulative sum of a\rreceived scalar signal (called reward).\nThe use of a reward signal to\rformalize the idea of a goal is one of the most distinctive features of\rreinforcement learning.\nAlthough formulating goals in terms of reward\rsignals might at first appear limit\u0026shy;ing, in practice it has proved to be\rflexible and widely applicable. The best way to see this is to consider\rexamples of how it has been, or could be, used. For example, to make a robot\rlearn to walk, researchers have provided reward on each time step proportional\rto the robot��s forward motion. In making a robot learn how to escape from a\rmaze, the reward is often ��1for every time step\rthat passes prior to escape; this encourages the agent to escape as quickly as\rpossible. To make a robot learn to find and collect empty soda cans for\rrecycling, one might give it a reward of zero most of the time, and then a\rreward of ʮ1for each can collected. One might also want to give the robot\rnegative rewards when it bumps into things or when somebody yells at it. For an\ragent to learn to play checkers or chess, the natural rewards are +1 for\rwinning, ��1for losing, and 0for drawing and for\rall nonterminal positions.\nYou can see what is happening in all of these\rexamples. The agent always learns to maximize its reward. If we want it to do\rsomething for us, we must provide rewards to it in such a way that in\rmaximizing them the agent will also achieve our goals. It is thus critical that\rthe rewards we set up truly indicate what we want accomplished. In particular,\rthe reward signal is not the place to impart to the agent prior knowledge about\rhow to achieve what we want it to do.[7]For example, a chess-playing agent should be rewarded only for\ractually winning, not for achieving subgoals such as taking its opponent��s\rpieces or gaining control of the center of the board. If achieving these sorts\rof subgoals were rewarded, then the agent might find a way to achieve them\rwithout achieving the real goal. For example, it might find a way to take the\ropponent��s pieces even at the cost of losing the game. The reward signal is\ryour way of communicating to the robot what you want it\rto achieve, not how you want it achieved.\nNewcomers to reinforcement learning are sometimes\rsurprised that the rewards�� which define of the goal of learning��are computed\rin the environment rather than in the agent. Certainly most ultimate goals for\ranimals are recognized by computations occurring inside their bodies, for\rexample, by sensors for recognizing food, hunger, pain, and pleasure.\rNevertheless, as we discussed in the previous section, one can redraw the\ragent-environment interface in such a way that these parts of the body are\rconsidered to be outside of the agent (and thus part of the agent��s\renvironment). For example, if the goal concerns a robot��s internal energy\rreservoirs, then these are considered to be part of the environment; if the\rgoal concerns the positions of the robot��s limbs, then these too are considered\rto be part of the environment�� that is, the agent��s boundary is drawn at the\rinterface between the limbs and their control systems. These things are\rconsidered internal to the robot but external to the learning agent. For our\rpurposes, it is convenient to place the boundary of the learning agent not at\rthe limit of its physical body, but at the limit of its control.\nThe reason we do this is that the agent��s ultimate goal should be\rsomething over which it has imperfect control: it should not be able, for\rexample, to simply decree that the reward has been\rreceived in the same way that it might arbitrarily change its actions.\rTherefore, we place the reward source outside of the agent. This does not preclude\rthe agent from defining for itself a kind of internal reward, or a sequence of\rinternal rewards. Indeed, this is exactly what many reinforcement learning\rmethods do.\n3.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rReturns\nSo far we have discussed the objective of learning informally. We\rhave said that the agent��s goal is to maximize the cumulative reward it\rreceives in the long run. Howmight this be defined formally? If the sequence of rewards received after time\rstep t is denoted Rt+i, Rt+2, Rt+3, \u0026#8226; \u0026#8226; \u0026#8226;, then what precise aspect of this sequence do we wish to\rmaximize? In general, we seek to maximize the expected return,\rwhere the return Gt is defined as some specific function\rof the reward sequence. In the simplest case the return is the sum of the\rrewards:\n\r\r\r(3.1)\n\r\r\r\r\rGt=\rRt+i ʮRt+2ʮRt+3 ʮ������ʮRt,\nwhere T is a\rfinal time step. This approach makes sense in applications in which there is a\rnatural notion of final time step, that is, when the agent-environment\rinteraction breaks naturally into subsequences, which we call episodes,[8]\rsuch as plays of a game, trips through a maze, or any sort of repeated\rinteractions. Each episode ends in a special state called the terminal\rstate, followed by a reset to a standard starting state or to a sample\rfrom a standard distribution of starting states. Even if you think of episodes\ras ending in different ways, such as winning and losing a game, the next\repisode begins independently of how the previous one ended. Thus the episodes\rcan all be considered to end in the same terminal state, with different rewards\rfor the different outcomes. Tasks with episodes of this kind are called episodic tasks. In episodic tasks we sometimes need to\rdistinguish the set of all nonterminal states, denoted S, from the set of all\rstates plus the terminal state, denoted S+.\nOn the other hand, in many\rcases the agent-environment interaction does not break naturally into\ridentifiable episodes, but goes on continually without limit. For example, this\rwould be the natural way to formulate a continual process-control task, or an\rapplication to a robot with a long life span. We call these continuing\rtasks. The return formulation (3.1) is problematic for continuing tasks\rbecause the final time step would be T = oo, and the\rreturn, which is what we are trying to maximize, could itself easily be\rinfinite. (For example, suppose the agent receives a reward of ʮ1 at each time step.)\rThus, in this book we usually use a definition of return that is slightly more\rcomplex conceptually but much simpler mathematically.\nThe additional concept that we need is that of discounting.\rAccording to this approach, the agent tries to select actions so that the sum\rof the discounted rewards it receives over the future is maximized. In\rparticular, it chooses At to maximize the expected discounted return:\n\r\r\r(3.2)\n\r\r\r\r\rGtʿRt+i ʮYRt+2 ʮY2Rt+3 ʮ=����YkRt+fc+i,\nwhere 7is a\rparameter, 0\u0026lt; 7\u0026lt; 1, called the\rdiscount rate.\nThe discount rate determines the present value of future rewards: a\rreward received k time steps in the future is worth only\r7k-1times what it would be worth if it were received immediately. If 7\u0026lt; 1, the infinite sum has a finite value as long as the reward\rsequence {Rk} is bounded. If 7= 0, the agent is\r��myopic�� in being concerned only with maximizing immediate rewards: its\robjective in this case is to learn how to choose At so as to maximize only\rRt+i. If each of the agent��s actions happenedto influence only the immediate reward, not future rewards as well, then a\rmyopic agent could maximize (3.2) by separately maximizing each immediate\rreward. But in general, acting to maximize immediate reward can reduce access\rto future rewards so that the return may actually be reduced. As 7approaches 1, the objective takes\rfuture rewards into account more strongly: the agent becomes more farsighted.\nExample 3.4: Pole-Balancing Figure 3.2\rshows a task that served as an early illustration of reinforcement learning.\rThe objective here is to apply forces to a cart moving along a track so as to\rkeep a pole hinged to the cart from falling over. A failure is said to occur if\rthe pole falls past a given angle from vertical or if the cart runs off the\rtrack. The pole is reset to vertical after each failure. This task could be\rtreated as episodic, where the natural episodes are the repeated attempts to\rbalance the pole. The reward in this case could be +1 for every time step on\rwhich failure did not occur, so that the return at each time would be the\rnumber of steps until failure. Alternatively, we could treat pole-balancing as\ra continuing task, using discounting. In this case the reward would be ��1 on\reach failure and zero at all other times. The return at each time would then be\rrelated to ��7K, where K is the\rnumber of time steps before failure. In either case, the return is maximized by\rkeeping the pole balanced for as long as possible.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 3.4 Suppose you treated\rpole-balancing as an episodic task but also used discounting, with all rewards\rzero except for ��1 upon failure. What then would the return be at each time?\rHow does this return differ from that in the discounted, continuing formulation\rof this task?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 3.5 Imagine that you are\rdesigning a robot to run a maze. You decide to give it a reward of +1for escaping from the maze and a reward of zero at all other times.\rThe task seems to break down naturally into episodes��the successive runs\rthrough the maze��so you decide to treat it as an episodic task, where the goal\ris to maximize expected total reward (3.1). After running the learning agent\rfor a while, you find that it is showing no improvement in escaping from the\rmaze. What is going wrong? Have you effectively communicated to the agent what\ryou want it to achieve?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nFigure 3.2: The pole-balancing task.\n\r\rThe returns at successive times are related to each other in a way\rthat is important for the theory and algorithms of reinforcement learning:\nGt = Rt+i+ Y Rt+2 + Y2Rt+3+ Y 3Rt+4 +----- \n=Rt+i + Y(Rt+2+ YRt+3 +\ry 2Rt+4+��)\n\u0026nbsp;TOC \\o \u0026quot;1-5\u0026quot; \\h \\z =Rt+i + YGt+i\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (3.3)\nNote that this works for all time steps t \u0026lt; T,\reven if termination occurs at t + 1, if we define Gt =\r0. This often makes it easy to compute returns from reward sequences.\nExercise 3.6 Suppose y\r= 0.5 and the following sequence of rewards is received Ri\r= ��1, R2= 2, R3= 6, R4= 3, and R5= 2, with T = 5. What are Go, Gi,..., G5? Hint: Work backwards.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nNote that although the return\r(3.2) is a sum of an infinite number of terms, it is still finite if the reward\ris nonzero and constant. For example, if the reward is a constant +1, then the return is\nGt= E Y = i^ \u0026#8226;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (3.4)\nk=01\nExercise 3.7 Suppose Y = 0.9 and the\rreward sequence is Ri = 2 followed by an infinite sequence of 7s. What are Gi\rand Go?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n3.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rUnified Notation\rfor Episodic and Continuing Tasks\nIn the preceding section we described two kinds of reinforcement\rlearning tasks, one in which the agent-environment interaction naturally breaks\rdown into a sequence of separate episodes (episodic tasks), and one in which it\rdoes not (continuing tasks). The former case is mathematically easier because\reach action affects only the finite number of rewards subsequently received\rduring the episode. In this book we consider sometimes one kind of problem and\rsometimes the other, but often both. It is therefore useful to establish one\rnotation that enables us to talk precisely about both cases simultaneously.\nTo be precise about episodic\rtasks requires some additional notation. Rather than one long sequence of time\rsteps, we need to consider a series of episodes, each of which consists of a\rfinite sequence of time steps. We number the time steps of each episode\rstarting anew from zero. Therefore, we have to refer not just to St, the state\rrepresentation at time t, but to St,i, the state\rrepresentation at time t of episode i (and similarly for\rAt,i, Rt,i, nt,i, Ti, etc.). However, it turns out that,\rwhen we discuss episodic tasks we will almost never have to distinguish between\rdifferent episodes. We will almost always be considering a particular single\repisode, or stating something that is true for all episodes. Accordingly, in\rpractice we will almost always abuse notation slightly by dropping the explicit\rreference to episode number. That is, we will write St\rto refer to St,i, and so on.\nWe need one other convention to obtain a single notation that covers\rboth episodic and continuing tasks. We have defined the return as a sum over a\rfinite number of terms in one case (3.1) and as a sum over an infinite number\rof terms in the other (3.2). These can be unified by considering episode\rtermination to be the entering of a special absorbing state\rthat transitions only to itself and that generates only rewards of zero. For\rexample, consider the state transition diagram\n\r\n\r\r\r\r\r\u0026nbsp;\nHere the solid square represents the special\rabsorbing state corresponding to the end\nof an episode. Starting\rfrom So, we get the reward sequence +1, +1, +1, 0, 0, 0,_____ \nSumming these, we get the same return whether we sum over the first\rT rewards (here T = 3) or over the full infinite sequence. This remains true\reven if we introduce discounting. Thus, we can define the return, in general,\raccording to (3.2), using the convention of omitting episode numbers when they\rare not needed, and including the possibility that 7= 1if the sum remains defined (e.g., because all episodes terminate).\rAlternatively, we can also write the return as\nT-i-1\n(3.5)\nincluding the possibility that T = 00or 7= 1 (but not both). We use these con\u0026shy;ventions throughout the rest\rof the book to simplify notation and to express the close parallels between\repisodic and continuing tasks. (Later, in Chapter 10, we will introduce a\rformulation that is both continuing and undiscounted.)\n3.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\r*The Markov\rProperty\nIn the reinforcement learning framework, the agent makes its\rdecisions as a function of a signal from the environment called the\renvironment��s state. In this section we discuss what is\rrequired of the state signal, and what kind of information we should and should\rnot expect it to provide. In particular, we formally define a property of\renvironments and their state signals that is of particular interest, called the\rMarkov property.\nIn this book, by ��the state�� we mean whatever\rinformation is available to the agent. We assume that the state is given by\rsome preprocessing system that is nominally part of the environment. We do not\raddress the issues of constructing, changing, or learning the state signal in\rthis book. We take this approach not because we consider state representation\rto be unimportant, but in order to focus fully on the decision-making issues.\rIn other words, our main concern is not with designing the state signal, but\rwith deciding what action to take as a function of whatever statesignal is available. By convention, the reward signal is not part of the state,\rbut a copy of it certainly could be.\nCertainly the state signal\rshould include immediate sensations such as sensory measurements, but it can\rcontain much more than that. State representations can be highly processed\rversions of original sensations, or they can be complex structures built up\rover time from the sequence of sensations. For example, we can move our eyes\rover a scene, with only a tiny spot corresponding to the fovea visible in\rdetail at any one time, yet build up a rich and detailed representation of a\rscene. Or, more obviously, we can look at an object, then look away, and know\rthat it is still there. We can hear the word ��yes�� and consider ourselves to be\rin totally different states depending on the question that came before and\rwhich is no longer audible. At a more mundane level, a control system can\rmeasure position at two different times to produce a state representation\rincluding information about velocity. In all of these cases the state is\rconstructed and maintained on the basis of immediate sensations together with\rthe previous state or some other memory of past sensations. In this book, we do\rnot explore how that is done, but certainly it can be and has been done. There\ris no reason to restrict the state representation to immediate sensations; in\rtypical applications we should expect the state representation to be able to\rinform the agent of more than that.\nOn the other hand, the state\rsignal should not be expected to inform the agent of everything about the\renvironment, or even everything that would be useful to it in making decisions.\rIf the agent is playing blackjack, we should not expect it to know what the\rnext card in the deck is. If the agent is answering the phone, we should not\rexpect it to know in advance who the caller is. If the agent is a paramedic\rcalled to a road accident, we should not expect it to know immediately the\rinternal injuries of an unconscious victim. In all of these cases there is\rhidden state information in the environment, and that information would be\ruseful if the agent knew it, but the agent cannot know it because it has never\rreceived any relevant sensations. In short, we don��t fault an agent for not\rknowing something that matters, but only for having known something and then\rforgotten it!\nWhat we would like, ideally,\ris a state signal that summarizes past sensations compactly, yet in such a way\rthat all relevant information is retained. This normally requires more than the\rimmediate sensations, but never more than the complete history of all past\rsensations. A state signal that succeeds in retaining all relevant information\ris said to be Markov, or to have the Markov\rproperty (we define this formally below). For example, a checkers\rposition��the current configuration of all the pieces on the board��would serve\ras a Markov state because it summarizes everything important about the complete\rsequence of positions that led to it. Much of the information about the\rsequence is lost, but all that really matters for the future of the game is\rretained. Similarly, the current position and velocity of a cannonball is all\rthat matters for its future flight. It doesn��t matter how that position and\rvelocity came about. This is sometimes also referred to as an ��independence of\rpath�� property because all that matters is in the current state signal; its\rmeaning is independent of the ��path,�� or history, of signals that have led up\rto it.\nWe now formally define the Markov property for the reinforcement\rlearning prob\u0026shy;lem. To keep the mathematics simple, we assume here that there\rare a finite number of states and reward values. This enables us to work in\rterms of sums and proba\u0026shy;bilities rather than integrals and probability\rdensities, but the argument can easily be extended to include continuous states\rand rewards (or infinite discrete spaces). Consider how a general environment\rmight respond at time t+ 1 to the action taken at time t. In the most general,\rcausal case, this response may depend on every\u0026shy;thing that has happened earlier.\rIn this case the dynamics can be defined only by specifying the complete joint\rprobability distribution:\nPr{St+i = sf,\rRt+i = r | So, Ao, Ri,����St-i,At-i,\rRt, St, At},\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (3.6)\nfor all r, sf, and all possible\rvalues of the past events: So, Ao, Ri,����St_i, At-1, Rt,\rSt, At. If the state signal has the Markov property,\ron the other hand, then the environment��s response at t+1depends\ronly on the state and action representations at t, in which case the\renvironment��s dynamics can be defined by specifying only\np(s',r|s,a) == Pr{St+i = s!, Rt+i = r | St = s, At = a},\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (3.7)\nfor all r, s', s, and a. In other\rwords, a state signal has the Markov property, and is a Markov state, if and\ronly if (3.6) is equal to p(s', r|St, At) for all s', r, and histories, So, Ao,\rRi,����St_i,\rAt-i, Rt, St, At. In this case, the environment and task as a whole\rare also said to have the Markov property.\nIf an environment has the Markov\rproperty, then its one-step dynamics (3.7) enable us to predict the next state\rand expected next reward given the current state and action. One can show that,\rby iterating this equation, one can predict all future states and expected\rrewards from knowledge only of the current state as well as would be possible\rgiven the complete history up to the current time. It also follows that Markov\rstates provide the best possible basis for choosing actions. That is, the best\rpolicy for choosing actions as a function of a Markov state is just as good as\rthe best policy for choosing actions as a function of complete histories.\nEven when the state signal is\rnon-Markov, it is still appropriate to think of the state in reinforcement\rlearning as an approximation to a Markov state. In particular, we always want\rthe state to be a good basis for predicting future rewards and for selecting\ractions. In cases in which a model of the environment is learned (see Chapter 8), we also want the state to be a good basis for predicting\rsubsequent states. Markov states provide an unsurpassed basis for doing all of\rthese things. To the extent that the state approaches the ability of Markov\rstates in these ways, one will obtain better performance from reinforcement\rlearning systems. For all of these reasons, it is useful to think of the state\rat each time step as an approximation to a Markov state, although one should\rremember that it may not fully satisfy the Markov property.\nThe Markov property is important in reinforcement learning because\rdecisions and values are assumed to be a function only of the current state. In\rorder for these to be effective and informative, the state representation must\rbe informative. All of the theory presented in this book assumes Markov state\rsignals. This means that not all the theory strictly applies to cases in which\rthe Markov property does not strictly apply. However, the theory developed for\rthe Markov case still helps us to understand the behavior of the algorithms,\rand the algorithms can be successfully applied to many tasks with states that\rare not strictly Markov. A full understanding of the theory of the Markov case\ris an essential foundation for extending it to the more complex and realistic\rnon-Markov case. Finally, we note that the assumption of Markov state\rrepresentations is not unique to reinforcement learning but is also present in\rmost if not all other approaches to artificial intelligence.\nExample 3.5: Pole-Balancing State In the\rpole-balancing task introduced earlier, a state signal would be Markov if it\rspecified exactly, or made it possible to reconstruct exactly, the position and\rvelocity of the cart along the track, the angle between the cart and the pole,\rand the rate at which this angle is changing (the angular velocity). In an idealized\rcart-pole system, this information would be sufficient to exactly predict the\rfuture behavior of the cart and pole, given the actions taken by the\rcontroller. In practice, however, it is never possible to know this information\rexactly because any real sensor would introduce some distortion and delay in\rits measurements. Furthermore, in any real cart-pole system there are always\rother effects, such as the bending of the pole, the temperatures of the wheel\rand pole bearings, and various forms of backlash, that slightly affect the\rbehavior of the system. These factors would cause violations of the Markov\rproperty if the state signal were only the positions and velocities of the cart\rand the pole.\nHowever, often the positions and\rvelocities serve quite well as states. Some early studies of learning to solve\rthe pole-balancing task used a coarse state signal that divided cart positions\rinto three regions: right, left, and middle (and similar rough quantizations of\rthe other three intrinsic state variables). This distinctly non-Markov state\rwas sufficient to allow the task to be solved easily by reinforcement learning\rmethods. In fact, this coarse representation may have facilitated rapid\rlearning by forcing the learning agent to ignore fine distinctions that would\rnot have been useful in solving the task.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExample 3.6: Draw Poker In draw poker, each\rplayer is dealt a hand of five cards. There is a round of betting, in which\reach player exchanges some of his cards for new ones, and then there is a final\rround of betting. At each round, each player must match or exceed the highest\rbets of the other players, or else drop out (fold). After the second round of\rbetting, the player with the best hand who has not folded is the winner and\rcollects all the bets.\nThe state signal in draw poker is different for\reach player. Each player knows the cards in his own hand, but can only guess at\rthose in the other players�� hands. A common mistake is to think that a Markov\rstate signal should include the contents of all the players�� hands and the\rcards remaining in the deck. In a fair game, however, we assume that the\rplayers are in principle unable to determine these things from their past\robservations. If a player did know them, then she could predict some future\revents (such as the cards one could exchange for) better\rthan by remembering all past observations.\nIn addition to knowledge of one��s own cards, the\rstate in draw poker should include the bets and the numbers of cards drawn by\rthe other players. For example, if one of the other players drew three new\rcards, you may suspect he retained a pair and adjust your guess of the strength\rof his hand accordingly. The players�� bets also influence your assessment of\rtheir hands. In fact, much of your past history with these particular players\ris part of the Markov state. Does Ellen like to bluff, or does she play\rconservatively? Does her face or demeanor provide clues to the strength of her\rhand? How does Joe��s play change when it is late at night, or when he has\ralready won a lot of money?\nAlthough everything ever observed about the other\rplayers may have an effect on the probabilities that they are holding various\rkinds of hands, in practice this is far too much to remember and analyze, and\rmost of it will have no clear effect on one��s predictions and decisions. Very\rgood poker players are adept at remembering just the key clues, and at sizing\rup new players quickly, but no one remembers everything that is relevant. As a\rresult, the state representations people use to make their poker decisions are\rundoubtedly non-Markov, and the decisions themselves are presumably imperfect.\rNevertheless, people still make very good decisions in such tasks. We conclude\rthat the inability to have access to a perfect Markov\rstate representation is probably not a severe problem for a reinforcement\rlearning agent.\n��\nExercise 3.8: Broken\rVision System Imagine that you are a vision system. When you are first\rturned on for the day, an image floods into your camera. You can see lots of\rthings, but not all things. You can��t see objects that are occluded, and of\rcourse you can��t see objects that are behind you. After seeing that first\rscene, do you have access to the Markov state of the environment? Suppose your\rcamera was broken that day and you received no images at all, all day. Would\ryou have access to the Markov state then?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n3.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rMarkov Decision\rProcesses\nA reinforcement learning task that satisfies the\rMarkov property is called a Markov decision process, or MDP. If the state and action spaces are finite, then it is\rcalled a finite Markov decision process (finite MDP).\rFinite MDPs are particularly important to the theory of reinforcement learning.\rWe treat them extensively throughout this book; they are all you need to\runderstand 90% of modern reinforcement learning.\nA particular finite MDP is defined by its state and action sets and\rby the one-step dynamics of the environment. Given any state and action s and a, the probability of each possible pair of next state\rand reward, s;, r, is denoted\np(s',r|s,a) ʿPr{St+i\r= s;, Rt+i = r | St = s, At = a}.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (3.8)\nThese quantities completely specify the dynamics\rof a finite MDP. Most of the theory we present in the rest of this book\rimplicitly assumes the environment is a finite MDP.\n\r\rGiven the dynamics as specified by (3.8), one can compute anything\relse one might want to know about the environment, such as the expected rewards\rfor state-action pairs,\nr(s,a) = E[Ri+1| Si = s,\rAi = a] = [ r\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; r|s,\ra),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (3.9)\nreR s;es\nthe state-transition\rprobabilities,\np(s'|s,a) = Pr{Si+1= s' | Si = s, Ai = a} = ^^p(s��,r|s,a),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (3.10)\nreR\nand the expected rewards for\rstate-action-next-state triples,\nr(s,a,s') ^ E[Ri+1| Si = s, Ai = a,Si+1= s'] = [ep(��(^ :��a).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (3.11)\nIn the first edition of this book, the dynamics were expressed\rexclusively in terms of the latter two quantities, which were denoted PL, and R:, respectively. One weakness of that notation is that it\rstill did not fully characterize the dynamics of the rewards, giving only their\rexpectations. Another weakness is the excess of subscripts and superscripts. In\rthis edition we will predominantly use the explicit notation of (3.8), while\rsometimes referring directly to the transition probabilities (3.10).\nExercise 3.9 If the current state is\rSi, and actions are selected according to stochas\u0026shy;tic policy n, then what is\rthe expectation of Ri+1in terms of the four-argument function\rp (3.8)?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExample 3.7: Recycling Robot MDP The recycling robot (Example 3.3)\rcan be turned into a simple example of an MDP by simplifying it and providing\rsome more details. (Our aim is to produce a simple example, not a particularly\rrealistic one.) Recall that the agent makes a decision at times determined by\rexternal events (or by other parts of the robot��s control system). At each such\rtime the robot decides whether it should (1) actively\rsearch for a can, (2) remain stationary and wait for someone\rto bring it a can, or (3) go back to home base to recharge its battery. Suppose\rthe environment works as follows. The best way to find cans is to actively\rsearch for them, but this runs down the robot��s battery, whereas waiting does\rnot. Whenever the robot is searching, the possibility exists that its battery\rwill become depleted. In this case the robot must shut down and wait to be\rrescued (producing a low reward).\nThe agent makes its decisions solely as a function of the energy\rlevel of the battery. It can distinguish two levels, high and low, so that the\rstate set is S = {high, low}. Let us call the possible decisions��the agent��s\ractions��wait, search, and recharge. When the energy level is high, recharging\rwould always be foolish, so we do not include it in the action set for this\rstate. The agent��s action sets are\nA(high)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; =\u0026nbsp;\u0026nbsp;\u0026nbsp; {search,wait}\nA(low)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; =\u0026nbsp;\u0026nbsp;\u0026nbsp; {search, wait, recharge}.\nIf the energy level is high, then a period of active search can\ralways be completed without risk of depleting the battery. A period of\rsearching that begins with a high energy level leaves the energy level high\rwith probability a and reduces it to low with\rprobability 1 �� a. On the other hand, a period of searching undertaken when the\renergy level is low leaves it low with probability ¬and depletes the battery with probability 1��¬.In\rthe latter case, the robot must be rescued, and the battery is then recharged\rback to high. Each can collected by the robot counts as a unit reward, whereas\ra reward of ��3 results whenever the robot has to be rescued. Let rsearch and rWait,\rwith rsearch \u0026gt; rWait, respectively denote the expected number of\rcans the robot will collect (and hence the expected reward) while searching and\rwhile waiting. Finally, to keep things simple, suppose that no cans can be\rcollected during a run home for recharging, and that no cans can be collected\ron a step in which the battery is depleted. This system is then a finite MDP,\rand we can write down the transition probabilities and the expected rewards, as\rin Table 3.1.\ns\n\ra\n\rs!\n\rp(s\n\r|s,a)\n\rr(s, a, s7)\n\r\rhigh\n\rsearch\n\rhigh\n\ra\n\r\u0026nbsp;\n\rrsearch\n\r\rhigh\n\rsearch\n\rlow\n\r1��\n\ra\n\rrsearch\n\r\rlow\n\rsearch\n\rhigh\n\r1��\n\r13\n\r��3\n\r\rlow\n\rsearch\n\rlow\n\r¬\n\r\u0026nbsp;\n\rrsearch\n\r\rhigh\n\rwait\n\rhigh\n\r1\n\r\u0026nbsp;\n\rrwait\n\r\rhigh\n\rwait\n\rlow\n\r0\n\r\u0026nbsp;\n\rrwait\n\r\rlow\n\rwait\n\rhigh\n\r0\n\r\u0026nbsp;\n\rrwait\n\r\rlow\n\rwait\n\rlow\n\r1\n\r\u0026nbsp;\n\rrwait\n\r\rlow\n\rrecharge\n\rhigh\n\r1\n\r\u0026nbsp;\n\r0\n\r\rlow\n\rrecharge\n\rlow\n\r0\n\r\u0026nbsp;\n\r0.\n\r\r\r\r\rTable 3.1: Transition\rprobabilities and expected rewards for the finite MDP of the recycling robot\rexample. There is a row for each possible combination of current state, s,\rnext state, s7, and action possible in the current state, a G A(s).\n\r\r\r\r\r\u0026nbsp;\n\r\nFigure 3.3: Transition graph for the\rrecycling robot example.\n\r\r\r\r\r\u0026nbsp;\n��\nA transition graph is a useful way to\rsummarize the dynamics of a finite MDP. Figure 3.3 shows the transition graph\rfor the recycling robot example. There are two kinds of nodes: state\rnodes and action nodes. There is a state node for\reach possible state (a large open circle labeled by the name of the state), and\ran action node for each state-action pair (a small solid circle labeled by the\rname of the action and connected by a line to the state node). Starting in\rstate s and taking action a moves you along the line from state node s to\raction node (s, a). Then the environment responds with a transition to the next\rstate��s node via one of the arrows leaving action node (s, a). Each arrow\rcorresponds to a triple (s, s;, a), where sf\ris the next state, and we label the arrow with the transition probability, p(s;|s,\ra), and the expected reward for that transition, r(s, a, s;). Note\rthat the transition probabilities labeling the arrows leaving an action node\ralways sum to 1.\nExercise 3.10 Give a table analogous\rto to Table 3.1, but for p(s;, r|s, a). It should have columns for\rs, a, s' r, and p(s;, r|s, a), and a row for every 4-tuple for which\rp(s;, r|s, a) \u0026gt; 0.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n3.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rValue Functions\nAlmost all reinforcement learning algorithms\rinvolve estimating value functions�� functions of states\r(or of state-action pairs) that estimate how good it is\rfor the agent to be in a given state (or how good it is to perform a given\raction in a given state). The notion of ��how good�� here is defined in terms of\rfuture rewards that can be expected, or, to be precise, in terms of expected\rreturn. Of course the rewards the agent can expect to receive in the future\rdepend on what actions it will take. Accordingly, value functions are defined\rwith respect to particular policies.\nRecall that a policy, n, is a mapping from each\rstate, s G S, and action, a G A(s), to the probability n(a|s) of taking action\ra when in state s. Informally, the value of a state s\runder a policy n, denoted Vn(s), is the expected return\rwhen starting in s and following n thereafter. For MDPs, we can define v^(s)\rformally as\n\r\r\r\r\r(3.12)\n\r\r\r\r\r\r\r\rEy\u0026quot;Rt+k+i\n\r\r\r\r\r\r\r\rSt=\rs\n\r\r\r\r\r\r\r\rVn(s)\r== En[Gt | St=\rs] = En\n\r\r\r\r\r\u0026nbsp;\n\r.k=0\nwhere E^[-] denotes the expected value\rof a random variable given that the agent follows policy n, and t is any time\rstep. Note that the value of the terminal state, if any, is always zero. We\rcall the function vn the state-value\rfunction for policy n.\nSimilarly, we define the value\rof taking action a in state s under a policy n, denoted qn(s,\ra), as the expected return starting from s, taking the action a, and thereafter\rfollowing policy n:\n\r\r\r\r\r(3.13)\n\r\r\r\r\r\r\r\rEyk\rRt+k+i\n\r\r\r\r\r\r\r\rSt\r�� s, At �� a\n\r\r\r\r\r\r\r\rqn(s, a) ��\rEn[Gt | St �� s, At\r�� a] �� En\n\r\r\r\r\r\u0026nbsp;\n\r.k=0\n\r\rWe call\rqn the action-value function for\rpolicyn.\nThe value functions and can be estimated from\rexperience. For example, if an agent follows policy n and maintains an average,\rfor each state encountered, of the actual returns that have followed that\rstate, then the average will converge to the state��s value, (s), as the number\rof times that state is encountered approaches infinity. If separate averages are\rkept for each action taken in a state, then these aver\u0026shy;ages will similarly\rconverge to the action values, q^(s, a). We call estimation methods of this\rkind Monte Carlo methods because they involve averaging\rover many random samples of actual returns. These kinds of methods are\rpresented in Chapter 5. Of course, if there are very many states, then it may\rnot be practical to keep separate averages for each state individually.\rInstead, the agent would have to maintain v^ and qn as parameterized functions\r(with fewer parameters than states) and adjust the parameters to better match\rthe observed returns. This can also produce accu\u0026shy;rate estimates, although much\rdepends on the nature of the parameterized function approximator. These\rpossibilities are discussed in the second part of the book.\nA fundamental property of value functions used throughout\rreinforcement learning and dynamic programming is that they satisfy particular\rrecursive relationships. For any policy n and any state s, the following\rconsistency condition holds between the value of s and the value of its\rpossible successor states:\nV\u0026nbsp;\u0026nbsp;\u0026nbsp;\rn(s) == En[Gi | Si = s]\n\r\r\r(by (3.3)) (3.14)\n\r\r\r\r\r=En[Ri+1+ YGi+1| Si = s]\n=[أ(a|s)[[p(s',r|s,a) r + 7Enơʮ1ӽʮ1= s\na\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; s!r\n=^ n(a|s^ ^p(s',r|s,a) r + YVn(s') , Vs G S,\na\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; s,��r\nwhere it is implicit that the actions, a, are\rtaken from the set A(s), the next states, s', are taken from the set S (or from\rSʮin the case of an\repisodic problem), and the rewards, r, are taken from the set R. Note also how\rin the last equation we have merged the two sums, one over all the values of s'\rand the other over all values of r, into one sum over all possible values of\rboth. We will use this kind of merged sum often to simplify formulas. Note how\rthe final expression can be read very easily as an expected value. It is really\ra sum over all values of the three variables, a, s', and r. For each\rtriple, we compute its probability, n(a|s)p(s', r|s, a), weight the quantity in\rbrackets by that probability, then sum over all possibilities to get an expected\rvalue.\nEquation (3.14) is the Bellman\requation for Vn. It expresses a relationship between the value of a\rstate and the values of its successor states. Think of looking ahead from one\rstate to its possible successor states, as suggested by Figure 3.4 (left). Each\ropen circle represents a state and each solid circle represents a state-action\rpair. Starting from state s, the root node at the top, the agent could take any\rof some set of actions��three are shown in Figure 3.4 (left). From each of\rthese, the environment could respond with one of several next states, s',\ralong with a reward,\n\r\nFigure 3.4: Backup diagrams for v^\rand q^.\n\r\r\r\r\r\u0026nbsp;\nr. The Bellman equation (3.14) averages over all\rthe possibilities, weighting each by its probability of occurring. It states\rthat the value of the start state must equal the (discounted) value of the\rexpected next state, plus the reward expected along the way.\nThe value function Vn is the unique solution to its Bellman\requation. We show in subsequent chapters how this Bellman equation forms the\rbasis of a number of ways to compute, approximate, and learn v[ We call\rdiagrams like those shown in Figure 3.4 backup diagrams\rbecause they diagram relationships that form the basis of the update or backup operations that are at the heart of reinforcement\rlearning methods. These operations transfer value information back\rto a state (or a state�� action pair) from its successor stares (or state��action\rpairs). We use backup diagrams throughout the book to provide graphical\rsummaries of the algorithms we discuss. (Note that unlike transition graphs,\rthe state nodes of backup diagrams do not necessarily represent distinct\rrtates; for example, a state might be its own successor. We also omit explicit\rarrowheads because time always flows downward in a backup diagram.)\n\r\r\r\u0026nbsp;\n\rA\n\r\u0026nbsp;\n\rB��\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\\\n\r+5\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r+1C\n\rB)\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\rN��\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\r\r\r\r3.3\n\r8.8\n\r4.4\n\r5.3\n\r1.5\n\r\r1.5\n\r3.0\n\r2.3\n\r1.9\n\r0.5\n\r\r0.1\n\r0.7\n\r0.7\n\r0.4\n\r-0.4\n\r\r-1.0\n\r-0.4\n\r-0.4\n\r-0.6\n\r-1.2\n\r\r-1.9\n\r-1.3\n\r-1.2\n\r-1.4\n\r-2.0\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\r\r\r\rActions\n\r\r\r\r\r\r\r\rFigure 3.5: Gridworld example: exceptional reward\rdynamics (left) and state-value function for the equiprobable random policy\r(right).\n\r\r\r\r\rExample 3.8, Gridworld Figure 3.5 (left) shows a\rrectangular gridworld repre\u0026shy;sentation of a simple finite MDP. The cells of the\rgrid correspond to the states of the environment. At each cell, four actions\rare possible: north, soutli��east��and west, sehich deterministically cause the agent to move one cell fn\rthe respective di\u0026shy;rection one the grid. Actions that would take the agent off\rthe grid leave its location unchanged, but also result in a reward of ��1. Other\ractions result in ee reward of 0, except those that move the agent cut of the\rspecial states A and B. From st ate A, all four aetions yield a reward of ʮ10 and take the ageng\rto A' From state B, all actions yield a reward of ʮ5 and take the agent\rto B;.\nSuppose the agent selects all four\ractions with equal probability in all states. Figure 3.5 (right) shows the\rvalue function, Vn, for this policy, for the discounted\rreward case with Y = 0.9. This value function was computed by solving the\rsystem of linear equations (3.14). Notice the negative values near the lower\redge; these are the result of the high probability of hitting the edge of the\rgrid there under the random policy. State A is the best state to be in under\rthis policy, but its expected return is less than 10, its immediate reward,\rbecause from A the agent is taken to A', from which it is likely to run into\rthe edge of the grid. State B, on the other hand, is valued more than 5, its\rimmediate reward, because from B the agent is taken to B', which has a positive\rvalue. From B' the expected penalty (negative reward) for possibly running into\ran edge is more than compensated for by the expected gain for possibly\rstumbling onto A or B.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n\r\r\rV-\n\r\r\r\r\r\r\r\rputt\n\r\r\r\r\r\r\r\rFigure 3.6: A golf example: the\rstate-value function for putting (above) and the optimal action-value\rfunction for using the driver (below).\n\r\r\r\r\r\r\r\r-2\n\r\r\r\r\rExample 3.9: Golf To formulate playing a hole of\rgolf as a reinforcement learning task, we count a penalty (negative reward) of ��1for each stroke until we hit the ball into the hole. The state is\rthe location of the ball. The value of a state is the negative of the number of\rstrokes to the hole from that location. Our actions are how we aim and swing at\rthe ball, of course, and which club we select. Let us take\n\r\n\r\r\r\r\r\u0026nbsp;\n\r\u0026nbsp;\nthe former as given and\rconsider just the choice of club, which we assume is either a putter or a\rdriver. The upper part of Figure 3.6 shows a possible state-value function,\nV\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rputt(s), for the policy that\ralways uses the putter. The terminal state in-the-hole has\ra value of 0. From anywhere on the green we assume we can make a putt; these\rstates have value ��1. Off the green we cannot reach the hole by putting, and\rthe value is greater. If we can reach the green from a state by putting, then\rthat state must have value one less than the green��s value, that is, ��2. For\rsimplicity, let us assume we can putt very precisely and deterministically, but\rwith a limited range. This gives us the sharp contour line labeled ��2 in the\rfigure; all locations between that line and the green require exactly two\rstrokes to complete the hole. Similarly, any location within putting range of\rthe ��2 contour line must have a value of ��3, and so on to get all the contour\rlines shown in the figure. Putting doesn��t get us out of sand traps, so they\rhave a value of ��0. Overall, it takes us six strokes to\rget from the tee to the hole by putting.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 3.11 What is the Bellman\requation for action values, that is, for q^? It must give the action value\rq^(s, a) in terms of the action values, q^(s', a'), of possible successors to\rthe state-action pair (s, a). As a hint, the backup diagram corresponding to\rthis equation is given in Figure 3.4 (right). Show the sequence of equations\ranalogous to (3.14), but for action values.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 3.12 The Bellman equation\r(3.14) must hold for each state for the value function Vn shown in Figure 3.5\r(right). As an example, show numerically that this equation holds for the\rcenter state, valued at +0.7, with respect to its four neighboring states,\rvalued at +2.3, +0.4, ��0.4, and +0.7. (These numbers are accurate only to one\rdecimal place.)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 3.13 In the gridworld\rexample, rewards are positive for goals, negative for running into the edge of\rthe world, and zero the rest of the time. Are the signs of these rewards\rimportant, or only the intervals between them? Prove, using (3.2), that adding\ra constant c to all the rewards adds a constant, vc, to the values\rof all states, and thus does not affect the relative values of any states under\rany policies. What is vc in terms of c and 7?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 3.14 Now consider adding a constant c to all the rewards in\ran episodic task, such as maze running. Would this have any effect, or would it\rleave the task unchanged as in the continuing task above? Why or why not? Give\ran example. ��\nExercise 3.15 The value of a state\rdepends on the values of the actions possible in that state and on how likely\reach action is to be taken under the current policy. We can think of this in\rterms of a small backup diagram rooted at the state and considering each\rpossible action:\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\ntaken wit^\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; _\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; A\r.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ^\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; V��\nprobability ^(ajs)\n-q-K(s,a)\nGive the equation corresponding to this intuition\rand diagram for the value at the root node, Vn(s), in terms of the value at the\rexpected leaf node, q^(s, a), given\n\r\rSt = s. This equation should include an expectation conditioned on\rfollowing the policy, n. Then give a second equation in which the expected\rvalue is written out explicitly in terms of n(a|s) such that no expected value\rnotation appears in the equation.\n\r\r\rJT-------------- Qn(s,a)\n\r\r\r\r\rExercise 3.16 The value of an action, q^(s, a),\rdepends on the expected next reward and the expected sum of the remaining\rrewards. Again we can think of this in terms of a small backup diagram, this\rone rooted at an action (state-action pair) and branching to the possible next\rstates:\nexpected\n\r\r\rvأ(s)\n\r\r\r\r\rrewards-\nGive the equation corresponding to this intuition and diagram for\rthe action value, qn(s, a), in terms of the expected next reward, Riʮ1, and the expected\rnext state value, Vn (Siʮ1), given that Si = s and Ai = a. This equation should include an\rexpectation but not one conditioned conditioned on\rfollowing the policy. Then give a second equation, writing out the expected\rvalue explicitly in terms of p(s', r|s, a) defined by (3.8), such\rthat no expected value notation appears in the equation. ��\n3.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rOptimal Value Functions\nSolving a reinforcement learning task means, roughly, finding a\rpolicy that achieves a lot of reward over the long run. For finite MDPs, we can\rprecisely define an optimal policy in the following way. Value functions define\ra partial ordering over policies. A policy n is defined to be better than or\requal to a policy n' if its expected return is greater than or equal to that of\rn' for all states. In other words, n \u0026gt; n' if and only if Vn(s) \u0026gt; Vn, (s)\rfor all s G S. There is always at least one policy that is better than or equal\rto all other policies. This is an optimal policy.\rAlthough there may be more than one, we denote all the optimal policies by n^.\rThey share the same state-value function, called the optimal\rstate-value function, denoted V^, and defined as\nV^(s) == max\rVn (s),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (3.15)\nآ\nfor all s G S.\nOptimal policies also share\rthe same optimal action-value function, denoted q^, and\rdefined as\nq*(s, a) == maxq^(s, a),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (3.16)\nآ\nfor all s G S and a G A(s). For the state-action pair (s, a), this\rfunction gives the expected return for taking action a in state s and\rthereafter following an optimal policy. Thus, we can write q��in terms of Vľas follows:\n\r\r\r(3.17)\n\r\r\r\r\r��(s,a)= E[Ri+1+��*��ʮ1)\r| Si = s, Ai = a]\nExample 3.10: Optimal Value Functions for Golf\rThe lower part of Figure 3.6 shows the contours of a possible optimal\raction-value function q^(s, driver). These are the values of each state if we\rfirst play a stroke with the driver and afterward select either the driver or\rthe putter, whichever is better. The driver enables us to hit the ball farther,\rbut with less accuracy. We can reach the hole in one shot using the driver only\rif we are already very close; thus the ��1contour\rfor ��(s, driver) covers only a small portion of the green. If we have two\rstrokes, however, then we can reach the hole from much farther away, as shown\rby the ��2 contour. In this case we don��t have to drive all the way to within\rthe small ��1contour, but only to anywhere on the green; from there we can use\rthe putter. The optimal action-value function gives the values after committing\rto a particular first action, in this case, to the\rdriver, but afterward using whichever actions are best. The ��3 contour is still\rfarther out and includes the starting tee. From the tee, the best sequence of\ractions is two drives and one putt, sinking the ball in three strokes.\n��\nBecause vľis the value function for a policy, it must satisfy the\rself-consistency condition given by the Bellman equation for state values\r(3.14). Because it is the optimal value function, however, v^��s consistency\rcondition can be written in a special form without reference to any specific\rpolicy. This is the Bellman equation for v*, or the\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Bellman optimality\requation. Intuitively,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; the\u0026nbsp;\u0026nbsp; Bellmanoptimality\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; equation\nexpressesthe\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; fact\rthat the value of a state under\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; an\roptimal policymust\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; equal the\nexpected return for the best action from that state:\nv*(s) =\rmax qn (s, a)\na\u0026pound;A(s)\n��max[Gt| St �� s, At\r�� a] a\n=maxEn,[Rt+i +\rYGt+i | St = s, At = a]\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (by\r(3.3))\na\n=maxE[Rt+i +\rYv*(St+i) | St = s, At = a]\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (3.18)\na\n=max Ep(s',\rr|s, a) [r + yv*(s')] .\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (3.19)\naGA (s)\ns' ,r\nThe last two equations are two forms of the\rBellman optimality equation for v*. The Bellman optimality equation for q* is\n\r\r\r\r\rq*(s,a)\n\r\r\r\r\r\r\r\rSt\r= s, At = a\n\r\r\r\r\r\r\r\rE\n\r\r\r\r\r\r\r\rRt+i + Y maxq*(St+i, a')\na'\n\r\r\r\r\r\u0026nbsp;\n\rp(s',r|s,a)\rr + y max q*(s', a')\nL\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; a'\ns',r\nThe backup diagrams in Figure 3.7 show\rgraphically the spans of future states and actions considered in the Bellman\roptimality equations for v* and q*. These are the same as the backup diagrams\rfor v^ and q^ except that arcs have been added at the agent��s choice points to\rrepresent that the maximum over that choice is taken rather than the expected\rvalue given some policy. Figure 3.7 (left) graphically represents the Bellman\roptimality equation (3.19).\n\r\rA\n\r\r\rs\n\r\r\r\r\r\r\r\rr\n\r\r\r\r\r\r\r\r(v*)\n\r\r\r\r\r��sf\n������������#\u0026#8226;\u0026#8226; \u0026#8226;\u0026#8226;()!\nFigure\r3.7: Backup diagrams for v^ and q*\n\r\r\u0026nbsp;\n\r\rFor finite MDPs, the Bellman\roptimality equation (3.19) has a unique solution independent of the policy. The\rBellman optimality equation is actually a system of equations, one for each\rstate, so if there are N states, then there are N equations in N unknowns. If the\rdynamics of the environment are known (p(s', r|s, a)), then in principle one\rcan solve this system of equations for v* using any one of a variety of methods\rfor solving systems of nonlinear equations. One can solve a related set of\requations for q*.\nOnce one has v*, it is\rrelatively easy to determine an optimal policy. For each state s, there will be\rone or more actions at which the maximum is obtained in the Bellman optimality\requation. Any policy that assigns nonzero probability only to these actions is\ran optimal policy. You can think of this as a one-step search. If you have the\roptimal value function, v*, then the actions that appear best after a one-step\rsearch will be optimal actions. Another way of saying this is that any policy\rthat is greedy with respect to the optimal evaluation\rfunction v* is an opti\u0026shy;mal policy. The term greedy is used in computer science\rto describe any search or decision procedure that selects alternatives based\ronly on local or immediate con\u0026shy;siderations, without considering the possibility\rthat such a selection may prevent future access to even better alternatives.\rConsequently, it describes policies that select actions based only on their\rshort-term consequences. The beauty of v* is that if one uses it to evaluate the\rshort-term consequences of actions��specifically, the one-step consequences��then\ra greedy policy is actually optimal in the long-term sense in which we are\rinterested because v* already takes into account the reward consequences of all\rpossible future behavior. By means of v*, the optimal expected long-term return\ris turned into a quantity that is locally and immediately available for each\rstate. Hence, a one-step-ahead search yields the long-term optimal actions.\nHaving q* makes choosing\roptimal actions still easier. With q*, the agent does not even have to do a\rone-step-ahead search: for any state s, it can simply find any action that\rmaximizes q*(s, a). The action-value function effectively caches the results of\rall one-step-ahead searches. It provides the optimal expected long-term return\ras a value that is locally and immediately available for each state-action\rpair. Hence, at the cost of representing a function of state-action pairs,\rinstead of just of states, the optimal action-value function allows optimal\ractions to be selected without having to know anything about possible successor\rstates and their values, that is, without having to know anything about the\renvironment��s dynamics.\nExample 3.11: Bellman Optimality\rEquations for the Recycling Robot Us\u0026shy;ing (3.19), we can explicitly give the\rBellman optimality equation for the recycling robot example. To make things\rmore compact, we abbreviate the states high and low, and the actions search,\rwait, and recharge respectively by h, l, s, w, and re. Since there are only two\rstates, the Bellman optimality equation consists of two equations. The equation\rfor Vľ(h) can be written as follows:\n\r\r\r\r\rV*(h)\n\r\r\r\r\r\r\r\rP(h|h, s)[r(h, s, h) + 7V*(h)] + p(l|h, s)[r(h, s, l) + 7V*(l)], P(h|h, w)[r(h, w, h) + 7V*(h)] + p(l|h, w)[r(h, w, l) + YV*(l)]\na[rs\r+ YV*(h)] + (1 �� a)[rs + YV*(l)], 1 1[rH + YV*(h)] + 0[rH\r+ YV*(l)]\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; J\n(s + Y[aV*(h) + (1��\ra)V*(l)]^\nrH\r+ YV*(h)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; J\u0026nbsp; .\n\r\r\r\r\r\r\r\rmax\nmax\nmax\n\r\r\r\r\r\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rFollowing the same procedure for V* (l) yields the equation\n(¬(s �� 3(1 ��¬��+ Y[(1 �� ^)V*(h) + ^V*(l)]\nV\u0026nbsp;\u0026nbsp;\u0026nbsp;\r*(l) = max \u0026lt; rH + YV*(l),\n{YV*(h)\nFor any choice of (s, rw,\ra, P, and 7, with 0 \u0026lt; 7\u0026lt; 1, 0 \u0026lt; a, ^ \u0026lt; 1, there is exactly one pair of numbers, V*(h) and V*(l), that\rsimultaneously satisfy these two nonlinear equations.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n\r\r\r\u0026nbsp;\n\rA��\n\r\u0026nbsp;\n\rB��\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\\\n\r+5\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r+1C\n\rB)\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r/\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\r\r\r\r7T,\n\r\r\r\r\rExample 3.12: Solving the Gridworld Suppose we solve\rthe Bellman equa\u0026shy;tion for V* for the simple grid\rtask introduced in Example 3.8 and shown again in Figure 3.8 (left). Recall\rthat state A is followed by a reward of +10 and transition to state A', while state B is followed by a reward of +5 and transition to\rstate B'. Figure 3.8 (middle) shows the optimal value function, and Figure\r3.8 (right) shows the corresponding optimal policies. Where there are multiple\rarrows in a cell, any of the corresponding actions is optimal.\n22.C\n\r24.4\n\r22.C\n\r19.4\n\r17.5\n\r\u0026nbsp;\n\r~\u0026#9658;\n\r+\n\r4~\n\r\u0026nbsp;\n\r\u0026lt;~\n\r\r19.8\n\r22.C\n\r19.8\n\r17.8\n\r16.0\n\r\u0026nbsp;\n\rU\n\rt\n\r\u0026nbsp;\n\r4����\n\r4��\n\r\r17.8\n\r19.8\n\r17.8\n\r16.0\n\r14.4\n\r\u0026nbsp;\n\ru\n\rt\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r16.C\n\r17.8\n\r16.C\n\r14.4\n\r13.0\n\r\u0026nbsp;\n\ru\n\r��\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r14.4\n\r16.C\n\r14.4\n\r13.0\n\r11.7\n\r\u0026nbsp;\n\ru\n\rt\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\r\r\u0026nbsp;\nGridworld\nFigure 3.8: Optimal solutions to the gridworld example.\n��\nExplicitly solving the Bellman optimality equation provides one\rroute to finding an optimal policy, and thus to solving the reinforcement\rlearning problem. However,this solution is rarely directly useful. It is akin to an exhaustive search,\rlooking ahead at all possibilities, computing their probabilities of occurrence\rand their desirabili\u0026shy;ties in terms of expected rewards. This solution relies on\rat least three assumptions that are rarely true in practice: (1) we accurately know the dynamics of the envi\u0026shy;ronment; (2) we have enough computational resources to complete the computation\rof the solution; and (3) the Markov property. For the kinds of tasks in which\rwe are interested, one is generally not able to implement this solution exactly\rbecause various combinations of these assumptions are violated. For example,\ralthough the first and third assumptions present no problems for the game of\rbackgammon, the second is a major impediment. Since the game has about 1020states, it would take thousands of years on today��s fastest\rcomputers to solve the Bellman equation for v^, and the same is true for\rfinding q^. In reinforcement learning one typically has to settle for\rapproximate solutions.\nMany different decision-making methods can be viewed as ways of\rapproximately solving the Bellman optimality equation. For example, heuristic\rsearch methods can be viewed as expanding the right-hand side of (3.19) several\rtimes, up to some depth, forming a ��tree�� of possibilities, and then using a\rheuristic evaluation function to approximate vľat the ��leaf�� nodes. (Heuristic search methods such as A* are almost\ralways based on the episodic case.) The methods of dynamic programming can be\rrelated even more closely to the Bellman optimality equation. Many\rreinforcement learning methods can be clearly understood as approximately\rsolving the Bellman optimality equation, using actual experienced transitions\rin place of knowledge of the expected transitions. We consider a variety of\rsuch methods in the following chapters.\n\u0026nbsp;TOC \\o \u0026quot;1-5\u0026quot; \\h \\z Exercise\r3.17 Draw or describe the optimal state-value function for the golf ex\u0026shy;ample.��\nExercise 3.18 Draw or describe the\rcontours of the optimal action-value function for putting, q*(s,putter), for\rthe golf example.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 3.19 Give the Bellman\requation for q* for the recycling robot.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 3.20 Figure 3.8 gives the optimal value of the best state\rof the gridworld as 24.4, to one decimal place. Use your knowledge of the\roptimal policy and (3.2) to express this value symbolically, and then to\rcompute it to three decimal places. ��\nExercise 3.21 Consider the continuing MDP shown on\rto the right. The only decision to be made is that in the top state, where two\ractions are available, left\rand right. The numbers show the rewards\rthat are received deterministically after each action. There are exactly two\rdeter\u0026shy;ministic policies, nieft and bright. What policy is optimal if y �� 0? If y �� 0.9? If y �� 0.5?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 3.22\rGive an equation for v* in terms of q*.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 3.23 Give an equation for q* in terms of v* and the world��s\rdynamics,\n\r\r\u0026nbsp;TOC \\o \u0026quot;1-5\u0026quot; \\h \\z p(s',\rr|s, a).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 3.24 Give an\requation for n* in terms of q*.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 3.25 Give an\requation for n* in terms of v* and the world��s dynamics, p(s', r|s, a).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n3.9\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rOptimality and\rApproximation\nWe have defined optimal value functions and\roptimal policies. Clearly, an agent that learns an optimal policy has done very\rwell, but in practice this rarely happens. For the kinds of tasks in which we\rare interested, optimal policies can be generated only with extreme\rcomputational cost. A well-defined notion of optimality organizes the approach\rto learning we describe in this book and provides a way to understand the\rtheoretical properties of various learning algorithms, but it is an ideal that\ragents can only approximate to varying degrees. As we discussed above, even if\rwe have a complete and accurate model of the environment��s dynamics, it is\rusually not possible to simply compute an optimal policy by solving the Bellman\roptimality equation. For example, board games such as chess are a tiny fraction\rof human experience, yet large, custom-designed computers still cannot compute\rthe optimal moves. A critical aspect of the problem facing the agent is always\rthe computational power available to it, in particular, the amount of\rcomputation it can perform in a single time step.\nThe memory available is also an important constraint.\rA large amount of memory is often required\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; to\u0026nbsp;\u0026nbsp;\u0026nbsp; build up approximationsof value\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; functions,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; policies,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; and\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; models.\nIn tasks with small,finite\rstate sets, it is possible\u0026nbsp;\u0026nbsp; toform theseapproximations\rusing\narrays or tables with one entry for each state\r(or state-action pair). This we call the tabular case,\rand the corresponding methods we call tabular methods. In many cases of\rpractical interest, however, there are far more states than could possibly be\rentries in a table. In these cases the functions must be approximated, using\rsome sort of more compact parameterized function representation.\nOur framing of the reinforcement learning problem\rforces us to settle for approxi\u0026shy;mations. However, it also presents us with some\runique opportunities for achieving useful approximations. For example, in\rapproximating optimal behavior, there may be many states that the agent faces\rwith such a low probability that selecting subop- timal actions for them has\rlittle impact on the amount of reward the agent receives. Tesauro��s backgammon\rplayer, for example, plays with exceptional skill even though it might make\rvery bad decisions on board configurations that never occur in games against\rexperts. In fact, it is possible that TD-Gammon makes bad decisions for a large\rfraction of the game��s state set. The on-line nature of reinforcement learning\rmakes it possible to approximate optimal policies in ways that put more effort\rinto learning to make good decisions for frequently encountered states, at the\rexpense of less effort for infrequently encountered states. This is one key\rproperty that dis\u0026shy;tinguishes reinforcement learning from other approaches to\rapproximately solving MDPs.\n3.10\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rSummary\nLet us summarize the elements of the\rreinforcement learning problem that we have presented in this chapter.\rReinforcement learning is about learning from interaction how to behave in\rorder to achieve a goal. The reinforcement learning agent\rand its environment interact over a sequence of discrete\rtime steps. The specification of their interface defines a particular task: the\ractions are the choices made by the agent; the states are the basis for making the choices; and the rewards are the basis for evaluating the choices. Everything\rinside the agent is completely known and controllable by the agent; everything\routside is incompletely controllable but may or may not be completely known. A policy is a stochastic rule by which the agent selects actions\ras a function of states. The agent��s objective is to maximize the amount of\rreward it receives over time.\nThe return is the function\rof future rewards that the agent seeks to maximize. It has several different\rdefinitions depending upon the nature of the task and whether one wishes to discount delayed reward. The undiscounted formulation is\rappropriate for episodic tasks, in which the\ragent��environment interaction breaks naturally into episodes;\rthe discounted formulation is appropriate for continuing tasks,\rin which the interaction does not naturally break into episodes but continues\rwithout limit.\nAn environment satisfies the Markov\rproperty if its state signal compactly sum\u0026shy;marizes the past without\rdegrading the ability to predict the future. This is rarely exactly true, but\roften nearly so; the state signal should be chosen or constructed so that the\rMarkov property holds as nearly as possible. In this book we assume that this\rhas already been done and focus on the decision-making problem: how to decide\rwhat to do as a function of whatever state signal is available. If the Markov\rproperty does hold, then the environment is called a Markov\rdecision process (MDP). A finite MDP is an MDP\rwith finite state and action sets. Most of the current theory of reinforcement\rlearning is restricted to finite MDPs, but the methods and ideas apply more\rgenerally.\nA policy��s value functions\rassign to each state, or state��action pair, the expected return from that\rstate, or state��action pair, given that the agent uses the policy. The optimal value functions assign to each state, or state��action\rpair, the largest expected return achievable by any policy. A policy whose\rvalue functions are optimal is an optimal policy.\rWhereas the optimal value functions for states and state��action pairs are\runique for a given MDP, there can be many optimal policies. Any policy that is greedy with respect to the optimal value functions must be an\roptimal policy. The Bellman optimality equations are\rspecial consistency conditions that the optimal value functions must satisfy\rand that can, in principle, be solved for the optimal value functions, from\rwhich an optimal policy can be determined with relative ease.\nA reinforcement learning problem can be posed in\ra variety of different ways de\u0026shy;pending on assumptions about the level of\rknowledge initially available to the agent. In problems of complete\rknowledge, the agent has a complete and accurate model of the\renvironment��s dynamics. If the environment is an MDP, then such a model\rconsists of the one-step transition probabilities and expected rewards for all states and their allowable actions.\rIn problems of incomplete knowledge, a complete and\rperfect model of the environment is not available.\nEven if the agent has a\rcomplete and accurate environment model, the agent is typically unable to\rperform enough computation per time step to fully use it. The memory available\ris also an important constraint. Memory may be required to build up accurate\rapproximations of value functions, policies, and models. In most cases of\rpractical interest there are far more states than could possibly be entries in\ra table, and approximations must be made.\nA well-defined notion of optimality organizes the approach to\rlearning we describe in this book and provides a way to understand the\rtheoretical properties of various learning algorithms, but it is an ideal that\rreinforcement learning agents can only ap\u0026shy;proximate to varying degrees. In\rreinforcement learning we are very much concerned with cases in which optimal\rsolutions cannot be found but must be approximated in some way.\nBibliographical and Historical\rRemarks\nThe reinforcement learning problem is\rdeeply indebted to the idea of Markov decision processes (MDPs) from the field\rof optimal control. These historical influences and other major influences from\rpsychology are described in the brief history given in Chapter 1. Reinforcement\rlearning adds to MDPs a focus on approximation and incomplete information for\rrealistically large problems. MDPs and the reinforcement learning problem are\ronly weakly linked to traditional learning and decision-making problems in\rartificial intelligence. However, artificial intelligence is now vigorously\rexploring MDP formulations for planning and decision-making from a variety of\rperspectives. MDPs are more general than previous formulations used in\rartificial intelligence in that they permit more general kinds of goals and\runcertainty.\nOur presentation of the reinforcement learning problem was\rinfluenced by Watkins (1989).\n3.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe bioreactor example is based\ron the work of Ungar (1990) and Miller and Williams (1992). The recycling robot\rexample was inspired by the can- collecting robot built by Jonathan Connell\r(1989).\n3.3-4 The terminology of episodic and continuing tasks is different from that usu\u0026shy;ally used in the\rMDP literature. In that literature it is common to distinguish three types of\rtasks: (1) finite-horizon tasks, in which interaction terminates after a\rparticular fixed number of time steps; (2) indefinite-horizon tasks, in which interaction can last\rarbitrarily long but must eventually terminate; and (3) infinite-horizon tasks,\rin which interaction does not terminate. Our episodic and continuing tasks are\rsimilar to indefinite-horizon and infinite- horizon tasks, respectively, but we\rprefer to emphasize the difference in the nature of the interaction. This\rdifference seems more fundamental than the difference in the objective\rfunctions emphasized by the usual terms. Often episodic tasks use an\rindefinite-horizon objective function and continuing tasks an infinite-horizon\robjective function, but we see this as a common coincidence rather than a\rfundamental difference.\nThe pole-balancing example is from Michie and Chambers (1968) and\rBarto, Sutton, and Anderson (1983).\n3.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rFor further discussion of the\rconcept of state, see Minsky (1967).\n3.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe theory of MDPs is treated\rby, e.g., Bertsekas (2005), Ross (1983), White (1969), and Whittle (1982,\r1983). This theory is also studied under the head\u0026shy;ing of stochastic optimal\rcontrol, where adaptive optimal control methods are most\rclosely related to reinforcement learning (e.g., Kumar, 1985; Kumar and\rVaraiya, 1986).\nThe theory of MDPs evolved from efforts to understand the problem of\rmak\u0026shy;ing sequences of decisions under uncertainty, where each decision can\rdepend on the previous decisions and their outcomes. It is sometimes called the\rtheory of multistage decision processes, or sequential decision processes, and\rhas roots in the statistical literature on sequential sampling beginning with\rthe papers by Thompson (1933, 1934) and Robbins (1952) that we cited in Chapter\r2 in connection with bandit problems (which are prototypical MDPs if formulated\ras multiple-situation problems).\nThe earliest instance of which we are aware in which reinforcement\rlearning was discussed using the MDP formalism is Andreae��s (1969b) description\rof a unified view of learning machines. Witten and Corbin (1973) experimented\rwith a reinforcement learning system later analyzed by Witten (1977) using the\rMDP formalism. Although he did not explicitly mention MDPs, Werbos (1977)\rsuggested approximate solution methods for stochastic optimal control problems\rthat are related to modern reinforcement learning methods (see also Werbos,\r1982, 1987, 1988, 1989, 1992). Although Werbos��s ideas were not widely\rrecognized at the time, they were prescient in emphasizing the importance of\rapproximately solving optimal control problems in a variety of domains,\rincluding artificial intelligence. The most influential integration of\rreinforcement learning and MDPs is due to Watkins (1989). His treatment of\rreinforcement learning using the MDP formalism has been widely adopted.\nOur characterization of the dynamics of an MDP in\rterms of p(s��r|s,a) is slightly\runusual. It is more common in the MDP literature to describe the dynamics in\rterms of the state transition probabilities p(s;|s, a) and ex\u0026shy;pected\rnext rewards r(s, a). In reinforcement learning, however, we more often have to\rrefer to individual actual or sample rewards (rather than just their expected\rvalues). Our notation also makes it plainer that St and Rt are in general\rjointly determined, and thus must have the same time index. In teaching\rreinforcement learning, we have found our notation to be more straightforward\rconceptually and easier to understand.\n3.7-8 Assigning value on the basis of what is good or bad in the\rlong run has ancient roots. In control theory, mapping states to numerical\rvalues representing the long-term consequences of control decisions is a key\rpart of optimal control theory, which was developed in the 1950s by extending\rnineteenth century state-function theories of classical mechanics (see, e.g.,\rSchultz and Melsa, 1967). In describing how a computer could be programmed to\rplay chess, Shannon (1950) suggested using an evaluation function that took\rinto account the long-term advantages and disadvantages of chess positions.\nWatkins��s (1989) Q-learning algorithm for estimating q* (Chapter 6) made action-value functions an important part of reinforcement\rlearning, and con\u0026shy;sequently these functions are often called Q-functions.\rBut the idea of an action-value function is much older than this. Shannon\r(1950) suggested that a function h(P, M) could be used by a chess-playing\rprogram to decide whether a move M in position P is\rworth exploring. Michie��s (1961, 1963) MENACE system and Michie and Chambers��s\r(1968) BOXES system can be understood as estimating action-value functions. In\rclassical physics, Hamil\u0026shy;ton��s principal function is an action-value function;\rNewtonian dynamics are greedy with respect to this function (e.g., Goldstein,\r1957). Action-value functions also played a central role in Denardo��s (1967)\rtheoretical treatment of DP in terms of contraction mappings.\nWhat we call the Bellman equation for v* was first introduced by\rRichard Bellman (1957a), who called it the ��basic functional equation.�� The\rcoun\u0026shy;terpart of the Bellman optimality equation for continuous time and state\rproblems is known as the Hamilton-Jacobi-Bellman equation (or often just the\rHamilton-Jacobi equation), indicating its roots in classical physics (e.g.,\rSchultz and Melsa, 1967).\nThe golf example was suggested by\rChris Watkins.\n\r\r80\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; CHAPTER3.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; FINITE\u0026nbsp;\u0026nbsp; MARKOV\u0026nbsp;\u0026nbsp; DECISION\u0026nbsp;\u0026nbsp; PROCESSES\nChapter 4\nDynamic Programming\nThe term dynamic programming (DP) refers to a\rcollection of algorithms that can be used to compute optimal policies given a\rperfect model of the environment as a Markov decision process (MDP). Classical\rDP algorithms are of limited utility in reinforcement learning both because of\rtheir assumption of a perfect model and because of their great computational expense,\rbut they are still important theoret\u0026shy;ically. DP provides an essential\rfoundation for the understanding of the methods presented in the rest of this\rbook. In fact, all of these methods can be viewed as attempts to achieve much\rthe same effect as DP, only with less computation and without assuming a\rperfect model of the environment.\nStarting with this chapter, we usually assume\rthat the environment is a finite MDP. That is, we assume that its state,\raction, and reward sets, S, A(s), and R, for s G S, are finite, and that its\rdynamics are given by a set of probabilities p(s;, r|s, a), for all\rs G S, a G A(s), r G R, and sf G S+ (S+ is S\rplus a terminal state if the problem is episodic). Although DP ideas can be\rapplied to problems with continuous state and action spaces, exact solutions\rare possible only in special cases. A common way of obtaining approximate\rsolutions for tasks with continuous states and actions is to quantize the state\rand action spaces and then apply finite-state DP methods. The methods we\rexplore in Chapter 9 are applicable to continuous problems and are a\rsignificant extension of that approach.\nThe key idea of DP, and of\rreinforcement learning generally, is the use of value functions to organize and\rstructure the search for good policies. In this chapter we show how DP can be\rused to compute the value functions defined in Chapter 3. As discussed there,\rwe can easily obtain optimal policies once we have found the optimal value\rfunctions, v* or q*, which satisfy the Bellman optimality equations:\nv*(s)\u0026nbsp;\u0026nbsp; ��\u0026nbsp;\u0026nbsp;\u0026nbsp; maxE[Rt+i\rʮYv*(St+i) | St �� s, At �� a]\na\n��ma^7p(s\u0026#12316;r|s,a) r ʮ?v*(s;)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (4.1)\na\ns; ,r\n\r\ror\n\r\r\r\r\rq*(s,a)\n\r\r\r\r\r\r\r\rSt=\rs, At = a\n\r\r\r\r\r\r\r\rE\n\r\r\r\r\r\r\r\rRt+i\r+ Y max q*(St+i,a')\n\r\r\r\r\r\r\r\ra'\n\r\r\r\r\r\u0026nbsp;\n\r\r\r\r(4.2)\n\r\r\r\r\r^p(s', r|s, a) r + y\rm^xq*(s', a')\ns',r\nfor all s G S, a G A(s), and s' G S+. As we shall see, DP algorithms\rare obtained by turning Bellman equations such as these into assignments, that\ris, into update rules for improving approximations of the desired value\rfunctions.\n4.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rPolicy\rEvaluation\nFirst we consider how to compute the state-value function v^ for an\rarbitrary policy n. This is called policy evaluation in\rthe DP literature. We also refer to it as the prediction\rproblem. Recall from Chapter 3 that, for all s G S,\nv(s) = En[Gt | St = s]\n\r\r\r(from (3.3))\n(4.3)\n(4.4)\n\r\r\r\r\r=En[Rt+i + YGt+i | St = s]\n=En[Rt+1+ Yvn(St+i) | St = s]\n=^ أ(a|s)^p(s',r|s,a) r + yv^(s')\na\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; s',r\nwhere n(a|s) is the probability of taking action\ra in state s under policy n, and the expectations are subscripted by n to\rindicate that they are conditional on n being followed. The existence and\runiqueness of v^ are guaranteed as long as either y \u0026lt;\r1 or eventual termination is guaranteed from all states under the policy n.\n\r\r\rvfc+i(s)\n\r\r\r\r\r\r\r\r(4.5)\n\r\r\r\r\r\r\r\rEn[Rt+1+(St+i)|\rSt = s] ^^(a|s)^p(s',r|s,a) r + m(s')\n\r\r\r\r\r\r\r\rs',r\nfor all s G S. Clearly, vk = v^\ris a fixed point for this update rule because the Bellman equation for v^\rassures us of equality in this case. Indeed, the sequence {vk} can be shown\rin general to converge to v^ as k^\runder the same conditions that guarantee the existence of v^. This\ralgorithm is called iterative policy evaluation.\n\r\r\r\r\rIf the environment��s dynamics are completely known,\rthen (4.4) is a system of |S| simultaneous linear equations in |S| unknowns\r(the v^(s), s G S). In principle, its solution is a straightforward, if\rtedious, computation. For our purposes, itera\u0026shy;tive solution methods are most\rsuitable. Consider a sequence of approximate value functions vo, vi, v2,..., each mapping S+ to R (the real numbers). The initial ap\u0026shy;proximation,\rvo, is chosen arbitrarily (except that the terminal state, if any, must be\rgiven value 0), and each successive approximation is obtained by using the\rBellman equation for v^ (3.14) as an update rule:\n\r\rTo produce each successive approximation, Vkʮ1from Vk,\riterative policy evalua\u0026shy;tion applies the same operation to each state s: it\rreplaces the old value of s with a new value obtained from the old values of\rthe successor states of s, and the expected immediate rewards, along all the\rone-step transitions possible under the policy being evaluated. We call this\rkind of operation a full backup. Each iteration of\riterative policy evaluation backs up the value of every\rstate once to produce the new approxi\u0026shy;mate value function Vkʮ1. There are\rseveral different kinds of full backups, depending on whether a state (as here)\ror a state-action pair is being backed up, and depending on the precise way the\restimated values of the successor states are combined. All the backups done in\rDP algorithms are called full backups because they are\rbased on all possible next states rather than on a sample next state. The\rnature of a backup can be expressed in an equation, as above, or in a backup\rdiagram like those introduced in Chapter 3. For example, Figure 3.4 (left) is\rthe backup diagram corresponding to the full backup used in iterative policy\revaluation.\nTo write a sequential computer program to\rimplement iterative policy evaluation, as given by (4.5), you would have to use\rtwo arrays, one for the old values, Vk(s), and one for the new values, Vkʮ1(s). This\rway, the new values can be computed one by one from the old values without the\rold values being changed. Of course it is easier to use one array and update\rthe values ��in place,�� that is, with each new backed-up value immediately\roverwriting the old one. Then, depending on the order in which the states are\rbacked up, sometimes new values are used instead of old ones on the right-hand\rside of (4.5). This slightly different algorithm also converges to Vn; in fact,\rit usually converges faster than the two-array version, as you might expect,\rsince it uses new data as soon as they are available. We think of the backups\ras being done in a sweep through the state space. For\rthe in-place algorithm, the order in which states are backed up during the sweep\rhas a significant influence on the rate of convergence. We usually have the\rin-place version in mind when we think of DP algorithms.\nAnother implementation point\rconcerns the termination of the algorithm. For\u0026shy;mally, iterative policy\revaluation converges only in the limit, but in practice it must\nIterative policy evaluation\nInput n,\rthe policy to be evaluated Initialize an array V(s) = 0,\rfor all s G Sʮ Repeat A ^ 0\nFor each\rs G S:\nV\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rV(s)\nV(s)\r��Ea n(a|s) Es,��rp(s',r|s,a)\r[r + 7V(s')] A �D max(A, |v �� V(s)|)\runtil A \u0026lt; Q (a small positive number)\nOutput V Vnbe halted short of this. A typical stopping condition for iterative policy\revaluation is to test the quantity maxs^S |v^+i(s) �� v^ (s)| after each sweep\rand stop when it is sufficiently small. The box shows a complete algorithm with\rthis stopping criterion.\nExample 4.1 Consider the 4x4 gridworld shown below.\n\r\r\r\r\r\u0026nbsp;\n\r1\n\r2\n\r3\n\r\r4\n\r5\n\r6\n\r7\n\r\r8\n\r9\n\r10\n\r11\n\r\r12\n\r13\n\r14\n\r\u0026nbsp;\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\r\r\r\rR =-1\non all transitions\n\r\r\r\r\r\r\r\r\nactions\n\r\r\r\r\r\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rThe nonterminal states are S �� {1,\r2,..., 14}. There are four actions possible in each state, A\r�� {up, down, right, left}, which deterministically cause the corresponding\rstate transitions, except that actions that would take the agent off the grid\rin fact leave the state unchanged. Thus, for instance, p(6, ��1 | 5, right) �� 1, p(7, ��1 | 7, right) �� 1, and p(10, r | 5,\rright) �� 0 for all r G R. This is an undiscounted, episodic task. The reward is\r��1 on all transitions until the terminal state is reached. The terminal state\ris shaded in the figure (although it is shown in two places, it is formally one\rstate). The expected reward function is thus r(s, a, s;) �� ��1 for\rall states s, sf and actions a. Suppose the\ragent follows the equiprobable random policy (all actions equally likely). The\rleft side of Figure 4.1 shows the sequence of value functions {v^} computed by\riterative policy evaluation. The final estimate is in fact vn, which in this\rcase gives for each state the negation of the expected number of steps from\rthat state until termination.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 4.1 In Example 4.1, if n is\rthe equiprobable random policy, what is qn(11, down)? What is q^(7, down)?\u0026nbsp;\u0026nbsp; ��\nExercise 4.2 In Example 4.1, suppose a\rnew state 15 is added to the gridworld just below state 13, and its actions,\rleft, up, right, and down, take the agent to states 12, 13, 14,\rand 15, respectively. Assume that the transitions from\rthe original states are unchanged. What, then, is v^(15) for the equiprobable\rrandom policy? Now suppose the dynamics of state 13are\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; also\rchanged,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; suchthat\u0026nbsp;\u0026nbsp;\u0026nbsp; action\rdownfrom\nstate 13 takes the agent to the new\u0026nbsp;\u0026nbsp; state\u0026nbsp;\u0026nbsp; 15.\u0026nbsp;\u0026nbsp; What\u0026nbsp; is\u0026nbsp; v^(15)\u0026nbsp; forthe\requiprobable\nrandom policy in this case?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 4.3 What are\rthe equations analogous to (4.3), (4.4), and (4.5) for the action-value\rfunction q^ and its successive approximation by a sequence of functions qo,qi,q2, \u0026#8226; \u0026#8226; \u0026#8226; ?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n\r\r\r\r\rVkfor the Random Policy\n\r\r\r\r\r\r\r\rrandom\npolicy\n\r\r\r\r\r\r\r\rk =2\n\r\r\r\r\r\r\r\roptimal\npolicy\n\r\r\r\r\r\r\r\r0.0\n\r0.0\n\r0.0\n\r0.0\n\r\r0.0\n\r0.0\n\r0.0\n\r0.0\n\r\r0.0\n\r0.0\n\r0.0\n\r0.0\n\r\r0.0\n\r0.0\n\r0.0\n\r0.0\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\r\r\r\r0.0\n\r-1.0\n\r-1.0\n\r-1.0\n\r\r-1.0\n\r-1.0\n\r-1.0\n\r-1.0\n\r\r-1.0\n\r-1.0\n\r-1.0\n\r-1.0\n\r\r-1.0\n\r-1.0\n\r-1.0\n\r0.0\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\r\r\r\r0.0\n\r-1.7\n\r-2.0\n\r-2.0\n\r\r-1.7\n\r-2.0\n\r-2.0\n\r-2.0\n\r\r-2.0\n\r-2.0\n\r-2.0\n\r-1.7\n\r\r-2.0\n\r-2.0\n\r-1.7\n\r0.0\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\r\r\r\r0.0\n\r-2.4\n\r-2.9\n\r-3.0\n\r\r-2.4\n\r-2.9\n\r-3.0\n\r-2.9\n\r\r-2.9\n\r-3.0\n\r-2.9\n\r-2.4\n\r\r-3.0\n\r-2.9\n\r-2.4\n\r0.0\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\r\r\r\r0.0\n\r-6.1\n\r-8.4\n\r-9.0\n\r\r-6.1\n\r-7.7\n\r-8.4\n\r-8.4\n\r\r-8.4\n\r-8.4\n\r-7.7\n\r-6.1\n\r\r-9.0\n\r-8.4\n\r-6.1\n\r0.0\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\r\r\r\r0.0\n\r-14.\n\r-20.\n\r-22.\n\r\r-14.\n\r-18.\n\r-20.\n\r-20.\n\r\r-20.\n\r-20.\n\r-18.\n\r-14.\n\r\r-22.\n\r-20.\n\r-14.\n\r0.0\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\r\r\r\r\u0026nbsp;\n\r��\n\r��\n\r��\n\r\r��\n\r��\n\r��\n\r��\n\r\r��\n\r��\n\r��\n\r��\n\r\r��\n\r��\n\r��\n\r\u0026nbsp;\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\r\r\r\r\u0026nbsp;\n\r\u0026lt;��\n\r��\n\r��\n\r\rt\n\r��\n\r��\n\r��\n\r\r��\n\r��\n\r��\n\ri\n\r\r��\n\r��\n\r��\u0026gt;\n\r\u0026nbsp;\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\r\r\r\r\u0026nbsp;\n\r\u0026lt;��\n\r\u0026lt;��\n\r��\n\r\rt\n\r\u0026nbsp;\n\r��\n\ri\n\r\rt\n\r��\n\r��\n\r1\n\r\r��\n\r��\u0026raquo;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\r\r\r\r\u0026nbsp;\n\r\u0026lt;��\n\r\u0026lt;��\n\r\u0026nbsp;\n\r\rt\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\rt\n\ru\n\r\u0026nbsp;\n\r1\n\r\ru\n\r\u0026nbsp;\n\r��\u0026gt;\n\r\u0026nbsp;\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\r\r\r\r\u0026nbsp;\n\r\u0026lt;��\n\r\u0026lt;��\n\r\u0026nbsp;\n\r\rt\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\rt\n\ru\n\rr\n\r\u0026nbsp;\n\r\ru\n\r��\u0026gt;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\r\r\r\r\u0026nbsp;\n\r\u0026lt;��\n\r\u0026lt;��\n\r\u0026nbsp;\n\r\rt\n\r\u0026nbsp;\n\r\u0026nbsp;\n\ri\n\r\rt\n\ru\n\r��\n\r1\n\r\ru\n\r\u0026nbsp;\n\r��\u0026gt;\n\r\u0026nbsp;\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\r\r\r\rk =10\n\r\r\r\r\r\r\r\rk =3\n\r\r\r\r\r\r\r\rk =0\n\r\r\r\r\r\r\r\rGreedy Policy w.r.t. Vk\n\r\r\r\r\r\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rFigure 4.1: Convergence of iterative policy\revaluation on a small gridworld. The left column is the sequence of\rapproximations of the state-value function for the random policy (all actions\requal). The right column is the sequence of greedy policies corresponding to\rthe value function estimates (arrows are shown for all actions achieving the\rmaximum). The last policy is guaranteed only to be an improvement over the\rrandom policy, but in this case it, and all policies after the third iteration,\rare optimal.\n4.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rPolicy\rImprovement\nOur reason for computing the value function for a policy is to help\rfind better policies. Suppose we have determined the value function v^ for an\rarbitrary deterministic policy n. For some state s we would like to know\rwhether or not we should change the policy to deterministically choose an\raction a = n(s). We know how good it is to follow the current policy from\rs��that is v^(s)��but would it be better or worse to change to the new policy?\rOne way to answer this question is to consider selecting a in s and thereafter\rfollowing the existing policy, n. The value of this way of behaving is\nqn(s,a) =\rEn[Rt+i + Yv^(St+i) | St = s, At = a]\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (4.6)\n=��p(s',\rr|s, a) r + yv^(s').\ns',r\nThe key criterion is whether this is greater than\ror less than v^(s). If it is greater�� that is, if it is better to select a once\rin s and thereafter follow n than it would be to follow n all the time��then one\rwould expect it to be better still to select a every time s is encountered, and\rthat the new policy would in fact be a better one overall.\nThat this is true is a special case of a general result called the policy improvement theorem. Let n and n' be any pair of\rdeterministic policies such that, for all s G S,\n\r\r\r(4.7)\n\r\r\r\r\rqn(s,n'(s)) \u0026gt; vn(s).\nThen the policy n' must be as good as, or better than, n. That is,\rit must obtain greater or equal expected return from all states s G S:\n\r\r\r(4.8)\n\r\r\r\r\rvn'(s) \u0026gt; vn(s).\nMoreover, if there is strict inequality of (4.7)\rat any state, then there must be strict inequality of (4.8) at at least one\rstate. This result applies in particular to the two policies that we considered\rin the previous paragraph, an original deterministic policy, n, and a changed\rpolicy, n', that is identical to n except that n'(s) = a = n(s). Obviously,\r(4.7) holds at all states other than s. Thus, if q^(s, a) \u0026gt; v^(s), then the\rchanged policy is indeed better than n.\nThe idea behind the proof of the policy\rimprovement theorem is easy to under\u0026shy;stand. Starting from (4.7), we keep\rexpanding the q^ side and reapplying (4.7) until\n\r\rwe get\rvy(s):\nv(s) \u0026lt; qn (s,n;(s))\n\r\r\u0026nbsp;\n\r\rEn^[Rt+i ʮYvn(St+i) | St �� s]\n\r\r\r\u0026lt;\u0026nbsp;\n\u0026lt;\u0026nbsp;\n\r\r\r\r\rEf[Rt+i ʮYqn(St+i,n;(St+i)) I St �� s]\nEf[Rt+i ʮYEn^[Rt+2ʮYvn(St+2)] | St �� s] Ef[Rt+i ʮYRt+2ʮY2vn(St+2) 1St ��\rs]\nEn;[Rt+i ʮYRt+2ʮ72Rt+3 ʮY3vn(St+3) | St �� s]\n\r\r\u0026nbsp;\n\u0026nbsp;\n\r\r\u0026lt; Ef [Rt+i ʮYRt+2ʮY2Rt+3ʮY3Rt+4ʮ�� �� �� 1St �� s]\n��vn;(s).\nSo far we have seen how, given a policy and its value function, we\rcan easily evaluate a change in the policy at a single state to a particular\raction. It is a natural extension to consider changes at all\rstates and to all possible actions, selecting at each\rstate the action that appears best according to q^(s, a). In other words, to\rconsider the new greedy policy, n;, given by\nn;(s)\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\u0026nbsp;\u0026nbsp; argmaxq^ (s, a)\na\n\r\r\r(4.9)\n\r\r\r\r\r��argmax E[Rt+i ʮyv^(St+i) | St �� s, At\r�� a]\na\narg max ^ p(s;,\rr|s, a) r ʮyv^ (s')\na\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; s,,r\nwhere arg maxa denotes the value of a at which the expression that\rfollows is max\u0026shy;imized (with ties broken arbitrarily). The greedy policy takes\rthe action that looks best in the short term��after one step of\rlookahead��according to v^. By construc\u0026shy;tion, the greedy policy meets the\rconditions of the policy improvement theorem (4.7), so we know that it is as\rgood as, or better than, the original policy. The process of making a new\rpolicy that improves on an original policy, by making it greedy with respect to\rthe value function of the original policy, is called policy\rimprovement.\nSuppose the new greedy policy, n', is as good as, but not better\rthan, the old policy n. Then v^ �� v#, and from (4.9) it follows that for all s\rG S:\n(s)\u0026nbsp; ��\u0026nbsp;\u0026nbsp;\u0026nbsp; maxE[Rt+i ʮ(St+i) | St �� s, At �� a]\na\n��ma^\u0026gt; p(s', r|s, a) r ʮn(s')\na\ns7,r\nBut this is the same as the Bellman optimality equation (4.1), and\rtherefore, v^^ must be v*, and both n and n' must be optimal policies. Policy\rimprovement thus must give us a strictly better policy except when the original\rpolicy is already optimal.\nSo far in this section we have considered the\rspecial case of deterministic policies. In the general case, a stochastic\rpolicy n specifies probabilities, n(a|s), for takingeach action, a, in each state, s. We will not go through the details, but in\rfact all the ideas of this section extend easily to stochastic policies. In\rparticular, the policy improvement theorem carries through as stated for the\rstochastic case. In addition, if there are ties in policy improvement steps\rsuch as (4.9)��that is, if there are several actions at which the maximum is\rachieved��then in the stochastic case we need not select a single action from\ramong them. Instead, each maximizing action can be given a portion of the\rprobability of being selected in the new greedy policy. Any apportioning scheme\ris allowed as long as all submaximal actions are given zero probability.\nThe last row of Figure 4.1 shows an\rexample of policy improvement for stochastic policies. Here the original\rpolicy, n, is the equiprobable random policy, and the new policy, n', is greedy\rwith respect to v^. The value function v^ is shown in the bottom-left diagram\rand the set of possible n' is shown in the bottom-right diagram. The states\rwith multiple arrows in the n' diagram are those in which several actions\rachieve the maximum in (4.9); any apportionment of probability among these\ractions is permitted. The value function of any such policy, v^'(s), can be\rseen by inspection to be either ��1, -2, or ��3 at all states, s G S, whereas\rv^(s) is at most ��14. Thus, vآ'(s) \u0026gt; vآ(s), for all s G S,\rillustrating policy improvement. Although in this case the new policy n'\rhappens to be optimal, in general only an improvement is guaranteed.\n4.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rPolicy Iteration\nOnce a policy, n, has been improved\rusing v^ to yield a better policy, n', we can then compute v^' and improve it\ragain to yield an even better n''. We can thus obtain a sequence of\rmonotonically improving policies and value functions:\n\r\n\r\r\r\r\r\u0026nbsp;\nwhere ��^ denotes a policy evaluation\rand denotes a policy improvement. Each policy is\rguaranteed to be a strict improvement over the previous one (unless it is\ralready optimal). Because a finite MDP has only a finite number of policies,\rthis process must converge to an optimal policy and optimal value function in a\rfinite number of iterations.\nThis way of finding an optimal policy is called policy iteration. A complete al\u0026shy;gorithm is given in the box on\rthe next page.[9]Note that each policy evaluation, itself an iterative computation,\ris started with the value function for the previous policy. This typically\rresults in a great increase in the speed of convergence of policy evaluation\r(presumably because the value function changes little from one policy to the\rnext).\nPolicy iteration (using iterative policy evaluation)\n1.\u0026nbsp;\rInitialization\nV(s) G R and n(s) G A(s) arbitrarily for all s G S\n2.\u0026nbsp; Policy Evaluation Repeat\nA ^ 0\nFor each s G S: vV(s)\nV(s) �D Es��rp(s', r|s, n(s)) [r ʮyV(s')]\nA �D max(A, |v �� V(s)|) until A \u0026lt; Q (a small\rpositive number)\n3.\u0026nbsp; Policy Improvement policy-stable �D true For each s G S:\nold-action�D n(s)\nn(s) �D argmaxa Es��rP(s', r|s, a) [r ʮyV (s')]\nIf old-action �� n(s), then policy-stable �D false If policy-stable, then stop and return V c v* and n c n*; else go\rto 2\n\r\r\r\r\r\u0026quot;3\n\r\r\r\r\r\r\r\r\u0026quot;4\n\n\r\r\r\r\r\r\r\rV,\n\n\r\r\r\r\r\r\r\r\u0026quot;2\n\n\r\r\r\r\r\r\r\r��UOCBOOI JSJIM��һcc3JCC0# 0\n\r\r\r\r\r\r\r\r\r\r\r\r\r\r\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rFigure 4.2: The sequence of policies found by policy iteration on\rJack��s car rental problem, and the final state-value function. The first five diagrams\rshow, for each number of cars at each location at the end of the day, the\rnumber of cars to be moved from the first location to the second (negative\rnumbers indicate transfers from the second location to the first). Each\rsuccessive policy is a strict improvement over the previous policy, and the\rlast policy is optimal.\nbe moved from one location to the\rother in one night. We take the discount rate to be Y = 0.9 and formulate this\ras a continuing finite MDP, where the time steps are days, the state is the\rnumber of cars at each location at the end of the day, and the actions are the\rnet numbers of cars moved between the two locations overnight. Figure 4.2 shows\rthe sequence of policies found by policy iteration starting from the policy\rthat never moves any cars.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 4.4 (programming) Write a\rprogram for policy iteration and re-solve Jack��s car rental problem with the\rfollowing changes. One of Jack��s employees at the first location rides a bus\rhome each night and lives near the second location. She is happy to shuttle one\rcar to the second location for free. Each additional car still costs $2, as do\rall cars moved in the other direction. In addition, Jack has limited parking\rspace at each location. If more than 10 cars are kept overnight at a location\r(after any moving of cars), then an additional cost of $4 must be incurred to\ruse a second parking lot (independent of how many cars are kept there). These\rsorts of nonlinearities and arbitrary dynamics often occur in real problems and\rcannot easily be handled by optimization methods other than dynamic\rprogramming. To check your program, first replicate the results given for the\roriginal problem. If your computer is too slow for the full problem, cut all\rthe numbers of cars in half. ��\n\r\rExercise 4.5\rHow would policy iteration be defined for action values? Give a complete\ralgorithm for computing q*, analogous to that on page 89 for computing\nV\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\r*. Please pay special attention\rto this exercise, because the ideas involved will be used throughout the rest\rof the book.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 4.6 Suppose you are restricted to considering only policies\rthat are e-soft, meaning that the probability of\rselecting each action in each state, s, is at least e/|A(s)|. Describe\rqualitatively the changes that would be required in each of the steps 3, 2, and\r1, in that order, of the policy iteration algorithm for V* (page 89). ��\n4.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rValue Iteration\nOne drawback to policy iteration is\rthat each of its iterations involves policy eval\u0026shy;uation, which may itself be a\rprotracted iterative computation requiring multiple sweeps through the state\rset. If policy evaluation is done iteratively, then conver\u0026shy;gence exactly to Vn\roccurs only in the limit. Must we wait for exact convergence, or can we stop\rshort of that? The example in Figure 4.1 certainly suggests that it may be\rpossible to truncate policy evaluation. In that example, policy evaluation\riterations beyond the first three have no effect on the corresponding greedy\rpolicy.\nIn fact, the policy evaluation step of policy iteration can be\rtruncated in several ways without losing the convergence guarantees of policy\riteration. One important special case is when policy evaluation is stopped\rafter just one sweep (one backup of each state). This algorithm is called value iteration. It can be written as a particularly simple\rbackup operation that combines the policy improvement and truncated policy\revaluation steps:\nVkʮ1(s)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; =\rmaxE[Ri+1+ 7%(Sw) | Si = s, Ai = a]\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (4.10)\na\n=ma^7p(s', r|s,\ra) r + 7Vk(s'),\na\ns,��r\nfor all s G S. For arbitrary Vo, the\rsequence {Vk} can be shown to converge to V* under the same conditions that\rguarantee the existence of V*.\nAnother way of understanding\rvalue iteration is by reference to the Bellman op\u0026shy;timality equation (4.1). Note\rthat value iteration is obtained simply by turning the Bellman optimality\requation into an update rule. Also note how the value iteration backup is\ridentical to the policy evaluation backup (4.5) except that it requires the\rmaximum to be taken over all actions. Another way of seeing this close\rrelationship is to compare the backup diagrams for these algorithms: Figure 3.4\r(left) shows the backup diagram for policy evaluation and Figure 3.7 (left)\rshows the backup diagram for value iteration. These two are the natural backup\roperations for computing Vn and V*.\nFinally, let us consider how\rvalue iteration terminates. Like policy evaluation, value iteration formally\rrequires an infinite number of iterations to converge exactly to V*. In\rpractice, we stop once the value function changes by only a small amount\nValue iteration\nInitialize array V arbitrarily (e.g.,\rV(s) �� 0 for all s G S+)\nRepeat A �D 0\nFor each s G S: v �D V(s)\nV(s) �D maxaEsV p(s',r|s,a)\r[r ʮyV (s')]\nA �D max(A, |v �� V(s)|) until A \u0026lt; Q (a small positive number)\nOutput a deterministic policy, n c [*, such that n(s) �� argmaxa Es��rP(s', r|s, a) [r ʮyV (s')]\nin a sweep. The box shows a complete\ralgorithm with this kind of termination condition.\nValue iteration effectively combines, in each of its sweeps, one\rsweep of policy evaluation and one sweep of policy improvement. Faster\rconvergence is often achieved by interposing multiple policy evaluation sweeps\rbetween each policy improvement sweep. In general, the entire class of\rtruncated policy iteration algorithms can be thought of as sequences of sweeps,\rsome of which use policy evaluation backups and some of which use value\riteration backups. Since the max operation in (4.10) is the only difference\rbetween these backups, this just means that the max operation is added to some\rsweeps of policy evaluation. All of these algorithms converge to an optimal\rpolicy for discounted finite MDPs.\nExample 4.3: Gambler��s Problem A\rgambler has the opportunity to make bets on the outcomes of a sequence of coin\rflips. If the coin comes up heads, he wins as many dollars as he has staked on\rthat flip; if it is tails, he loses his stake. The game ends when the gambler\rwins by reaching his goal of $100, or loses by running\rout of money. On each flip, the gambler must decide what portion of his capital\rto stake, in integer numbers of dollars. This problem can be formulated as an\rundiscounted, episodic, finite MDP. The state is the gambler��s capital, s G {1,\r2,..., 99} and the actions are stakes, a G {0,1,..., min(s, 100��s)}. The reward\ris zero on all transitions except those on which the gambler reaches his goal,\rwhen it is ʮ1. The\rstate-value function then gives the probability of winning from each state. A\rpolicy is a mapping from levels of capital to stakes. The optimal policy\rmaximizes the probability of reaching the goal. Let ph denote the probability\rof the coin coming up heads. If ph is known, then the entire problem is known\rand it can be solved, for instance, by value iteration. Figure 4.3 shows the\rchange in the value function over successive sweeps of value iteration, and the\rfinal policy found, for the case of ph �� 0.4. This policy is optimal, but not\runique. In fact, there is a whole family of optimal policies, all corresponding\rto ties for the argmax action selection with respect to the optimal\n��\n\r\n\r\r\r\r\r\rvalue\rfunction. Can you guess what the entire family looks like?\ni\n\r\r\r\r\r\rValue\nestimates\n\r\r\r\r\r\rCapital\n\r\r\r\r\r\u0026nbsp;\nFinal\n3U\npolicy\r(stake) 20\n10\n1\n\r\r\r\r\r50\n\r\r\r\r\r\r\r\r25\n\r\r\r\r\r\r\r\r75\n\r\r\r\r\r\r\r\r99\n\r\r\r\r\r\r\r\r1\n\r\r\r\r\r\u0026nbsp;\n\rCapital\nFigure 4.3: The solution to the gambler��s problem for ph = 0.4. The upper graph shows the value function found by\rsuccessive sweeps of value iteration. The lower graph shows the final policy.\nExercise 4.7 Why does the optimal\rpolicy for the gambler��s problem have such a curious form? In particular, for\rcapital of 50 it bets it all on one flip, but for capital of 51 it does not.\rWhy is this a good policy?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 4.8 (programming) Implement\rvalue iteration for the gambler��s problem and solve it for ph = 0.25 and ph =\r0.55. In programming, you may find it convenient to introduce two dummy states\rcorresponding to termination with capital of 0and 100,\rgiving them values of 0 and 1 respectively. Show your results graphically, as\rin Figure 4.3. Are your results stable as Q 0?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 4.9 What is the analog of the\rvalue iteration backup (4.10) for action values, qkʮ1(s, a)?\u0026nbsp;\u0026nbsp; ��\n4.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rAsynchronous\rDynamic Programming\nA major drawback to the DP methods that we have\rdiscussed so far is that they involve operations over the entire state set of\rthe MDP, that is, they require sweeps of the state set. If the state set is\rvery large, then even a single sweep can be prohibitively expensive. For\rexample, the game of backgammon has over 1020states.\nEven if we could perform the value\riteration backup on a million states per second, it would take over a thousand\ryears to complete a single sweep.\nAsynchronousDP algorithms are in-place iterative DP algorithms that are not\rorganized in terms of systematic sweeps of the state set. These algorithms back\rup the values of states in any order whatsoever, using whatever values of other\rstates happen to be available. The values of some states may be backed up\rseveral times before the values of others are backed up once. To converge\rcorrectly, however, an asynchronous algorithm must continue to backup the\rvalues of all the states: it can��t ignore any state after some point in the\rcomputation. Asynchronous DP algorithms allow great flexibility in selecting\rstates to which backup operations are applied.\nFor example, one version of\rasynchronous value iteration backs up the value, in place, of only one state,\rsk, on each step, k, using the value iteration backup (4.10). If 0 \u0026lt; y \u0026lt; 1, asymptotic convergence to v* is guaranteed given\ronly that all states occur in the sequence {sk} an\rinfinite number of times (the sequence could even be stochastic). (In the\rundiscounted episodic case, it is possible that there are some orderings of\rbackups that do not result in convergence, but it is relatively easy to avoid\rthese.) Similarly, it is possible to intermix policy evaluation and value\riteration backups to produce a kind of asynchronous truncated policy iteration.\rAlthough the details of this and other more unusual DP algorithms are beyond\rthe scope of this book, it is clear that a few different backups form building\rblocks that can be used flexibly in a wide variety of sweepless DP algorithms.\nOf course, avoiding sweeps\rdoes not necessarily mean that we can get away with less computation. It just\rmeans that an algorithm does not need to get locked into any hopelessly long\rsweep before it can make progress improving a policy. We can try to take\radvantage of this flexibility by selecting the states to which we apply backups\rso as to improve the algorithm��s rate of progress. We can try to order the\rbackups to let value information propagate from state to state in an efficient\rway. Some states may not need their values backed up as often as others. We\rmight even try to skip backing up some states entirely if they are not relevant\rto optimal behavior. Some ideas for doing this are discussed in Chapter 8.\nAsynchronous algorithms also\rmake it easier to intermix computation with real\u0026shy;time interaction. To solve a\rgiven MDP, we can run an iterative DP algorithm at the same\rtime that an agent is actually experiencing the MDP. The agent��s\rexperience can be used to determine the states to which the DP algorithm\rapplies its backups. At the same time, the latest value and policy information\rfrom the DP algorithm can guide the agent��s decision-making. For example, we\rcan apply backups to states as the agent visits them. This makes it possible to\rfocus the DP algorithm��s backups onto parts of the state\rset that are most relevant to the agent. This kind of focusing is a repeated\rtheme in reinforcement learning.\n\r\r4.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rGeneralized\rPolicy Iteration\nPolicy iteration consists of two\rsimultaneous, interacting processes, one making the value function consistent\rwith the current policy (policy evaluation), and the other making the policy\rgreedy with respect to the current value function (policy improve\u0026shy;ment). In\rpolicy iteration, these two processes alternate, each completing before the\rother begins, but this is not really necessary. In value iteration, for\rexample, only a single iteration of policy evaluation is performed in between\reach policy improve\u0026shy;ment. In asynchronous DP methods, the evaluation and\rimprovement processes are interleaved at an even finer grain. In some cases a\rsingle state is updated in one process before returning to the other. As long\ras both processes continue to update all states, the ultimate result is\rtypically the same��convergence to the optimal value function and an optimal\rpolicy.\nWe use the term generalized policy\riteration (GPI) to refer to the general idea of letting policy\revaluation and policy im\u0026shy;provement processes interact, independent of the\rgranularity and other details of the two processes. Almost all reinforce\u0026shy;ment\rlearning methods are well described as GPI. That is, all have identifiable\rpolicies and value functions, with the pol\u0026shy;icy always being improved with\rrespect to the value function and the value function always being driven toward\rthe value function for the policy, as suggested by the diagram to the right. It\ris easy to see that if both the evaluation process and the improvement process\rstabilize, that is, no longer produce changes, then the value function and\rpolicy must be optimal.\nThe value function stabilizes only\rwhen it is consistent with the current policy, and the policy stabilizes only\rwhen it is greedy with respect to the current value function. Thus, both\rprocesses stabilize only when a policy has been found that is greedy with\rrespect to its own evaluation function. This implies that the Bellman\roptimality equation (4.1) holds, and thus that the policy and the value\rfunction are optimal.\nThe evaluation and improvement\rprocesses in GPI can be viewed as both compet\u0026shy;ing and cooperating. They compete\rin the sense that they pull in opposing directions. Making the policy greedy\rwith respect to the value function typically makes the value function incorrect\rfor the changed policy, and making the value function consistent with the\rpolicy typically causes that policy no longer to be greedy. In the long run,\rhowever, these two processes interact to find a single joint solution: the\roptimal value function and an optimal policy.\nOne might also think of the inter\u0026shy;action\rbetween the evaluation and im\u0026shy;provement processes in GPI in terms of two\rconstraints or goals��for example, as two lines in two-dimensional space as\rsuggested by the diagram to the right.\nAlthough the real geometry is much more complicated\rthan this, the diagram suggests what happens in the real case.\nEach process drives the value function\nor policy toward one of the lines representing a solution to one of\rthe two goals. The goals interact because the two lines are not orthogonal.\rDriving directly toward one goal causes some movement away from the other goal.\rInevitably, however, the joint process is brought closer to the overall goal of\roptimality. The arrows in this diagram correspond to the behavior of policy\riteration in that each takes the system all the way to achieving one of the two\rgoals completely. In GPI one could also take smaller, incomplete steps toward\reach goal. In either case, the two processes together achieve the overall goal\rof optimality even though neither is attempting to achieve it directly.\n4.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rEfficiency of\rDynamic Programming\nDP may not be practical for very large\rproblems, but compared with other methods for solving MDPs, DP methods are\ractually quite efficient. If we ignore a few tech\u0026shy;nical details, then the\r(worst case) time DP methods take to find an optimal policy is polynomial in\rthe number of states and actions. If n and k denote the\rnumber of states and actions, this means that a DP method takes a number of\rcomputational operations that is less than some polynomial function of n and k.\rA DP method is guaranteed to find an optimal policy in polynomial time even\rthough the total number of (deterministic) policies is kn. In this\rsense, DP is exponentially faster than any direct search in policy space could\rbe, because direct search would have to exhaustively examine each policy to\rprovide the same guarantee. Linear program\u0026shy;ming methods can also be used to\rsolve MDPs, and in some cases their worst-case convergence guarantees are\rbetter than those of DP methods. But linear program\u0026shy;ming methods become\rimpractical at a much smaller number of states than do DP methods (by a factor\rof about 100). For the largest problems, only DP methods are feasible.\nDP is sometimes thought to be\rof limited applicability because of the curse of dimensionality,\rthe fact that the number of states often grows exponentially with the number of\rstate variables. Large state sets do create difficulties, but these are\rinherent difficulties of the problem, not of DP as a solution method. In fact,\rDP is comparatively better suited to handling large state spaces than competing\rmethods such as direct search and linear programming.\nIn practice, DP methods can be\rused with today��s computers to solve MDPs with millions of states. Both policy\riteration and value iteration are widely used, and itis not clear which, if either, is better in general. In practice, these methods\rusually converge much faster than their theoretical worst-case run times,\rparticularly if they are started with good initial value functions or policies.\nOn problems with large state spaces, asynchronous\rDP methods are often pre\u0026shy;ferred. To complete even one sweep of a synchronous\rmethod requires computation and memory for every state. For some problems, even\rthis much memory and compu\u0026shy;tation is impractical, yet the problem is still\rpotentially solvable because relatively few states occur along optimal solution\rtrajectories. Asynchronous methods and other variations of GPI can be applied\rin such cases and may find good or optimal policies much faster than synchronous\rmethods can.\n4.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rSummary\nIn this chapter we have become familiar with the\rbasic ideas and algorithms of dynamic programming as they relate to solving\rfinite MDPs. Policy evaluation refers to the (typically)\riterative computation of the value functions for a given policy. Policy improvement refers to the computation of an improved\rpolicy given the value function for that policy. Putting these two computations\rtogether, we obtain policy iteration and value iteration, the two most popular DP methods. Either of\rthese can be used to reliably compute optimal policies and value functions for\rfinite MDPs given complete knowledge of the MDP.\nClassical DP methods operate in sweeps through\rthe state set, performing a full backup operation on\reach state. Each backup updates the value of one state based on the values of\rall possible successor states and their probabilities of occurring. Full\rbackups are closely related to Bellman equations: they are little more than\rthese equations turned into assignment statements. When the backups no longer\rresult in any changes in value, convergence has occurred to values that satisfy\rthe corre\u0026shy;sponding Bellman equation. Just as there are four primary value\rfunctions (v^, v*, qn, and q*), there are four\rcorresponding Bellman equations and four correspond\u0026shy;ing full backups. An\rintuitive view of the operation of backups is given by backup\rdiagrams.\nInsight into DP methods and, in fact, into almost\rall reinforcement learning meth\u0026shy;ods, can be gained by viewing them as generalized policy iteration (GPI). GPI is the general idea of\rtwo interacting processes revolving around an approximate policy and an\rapproximate value function. One process takes the policy as given and performs\rsome form of policy evaluation, changing the value function to be more like the\rtrue value function for the policy. The other process takes the value function\ras given and performs some form of policy improvement, changing the policy to\rmake it bet\u0026shy;ter, assuming that the value function is its value function.\rAlthough each process changes the basis for the other, overall they work\rtogether to find a joint solution: a policy and value function that are\runchanged by either process and, consequently, are optimal. In some cases, GPI\rcan be proved to converge, most notably for the classical DP methods that we\rhave presented in this chapter. In other cases conver\u0026shy;gence has not been\rproved, but still the idea of GPI improves our understanding of the methods.\nIt is not necessary to perform\rDP methods in complete sweeps through the state set. Asynchronous\rDP methods are in-place iterative methods that back up states in an\rarbitrary order, perhaps stochastically determined and using out-of-date infor\u0026shy;mation.\rMany of these methods can be viewed as fine-grained forms of GPI.\nFinally, we note one last special property of DP methods. All of\rthem update estimates of the values of states based on estimates of the values\rof successor states. That is, they update estimates on the basis of other\restimates. We call this general idea bootstrapping. Many\rreinforcement learning methods perform bootstrapping, even those that do not\rrequire, as DP requires, a complete and accurate model of the environment. In\rthe next chapter we explore reinforcement learning methods that do not require\ra model and do not bootstrap. In the chapter after that we explore methods that\rdo not require a model but do bootstrap. These key features and properties are\rseparable, yet can be mixed in interesting combinations.\nBibliographical and Historical\rRemarks\nThe term ��dynamic programming�� is due\rto Bellman (1957a), who showed how these methods could be applied to a wide\rrange of problems. Extensive treatments of DP can be found in many texts,\rincluding Bertsekas (2005, 2012), Bertsekas and Tsitsiklis (1996), Dreyfus and\rLaw (1977), Ross (1983), White (1969), and Whittle (1982, 1983). Our interest\rin DP is restricted to its use in solving MDPs, but DP also applies to other\rtypes of problems. Kumar and Kanal (1988) provide a more general look at DP.\nTo the best of our knowledge, the first connection between DP and\rreinforcement learning was made by Minsky (1961) in commenting on Samuel��s\rcheckers player. In a footnote, Minsky mentioned that it is possible to apply\rDP to problems in which Samuel��s backing-up process can be handled in closed\ranalytic form. This remark may have misled artificial intelligence researchers\rinto believing that DP was restricted to analytically tractable problems and\rtherefore largely irrelevant to arti\u0026shy;ficial intelligence. Andreae (1969b)\rmentioned DP in the context of reinforcement learning, specifically policy\riteration, although he did not make specific connections between DP and\rlearning algorithms. Werbos (1977) suggested an approach to ap\u0026shy;proximating DP\rcalled ��heuristic dynamic programming�� that emphasizes gradient- descent\rmethods for continuous-state problems (Werbos, 1982, 1987, 1988, 1989, 1992).\rThese methods are closely related to the reinforcement learning algorithms that\rwe discuss in this book. Watkins (1989) was explicit in connecting reinforce\u0026shy;ment\rlearning to DP, characterizing a class of reinforcement learning methods as\r��incremental dynamic programming.��\n4.1-4 These sections describe\rwell-established DP algorithms that are covered in any of the general DP\rreferences cited above. The policy improvement the\u0026shy;orem and the policy\riteration algorithm are due to Bellman (1957a) and\n\r\rHoward (1960). Our presentation was influenced by the local view of\rpolicy improvement taken by Watkins (1989). Our discussion of value iteration\ras a form of truncated policy iteration is based on the approach of Puterman\rand Shin (1978), who presented a class of algorithms called modified\rpolicy itera\u0026shy;tion, which includes policy iteration and value iteration\ras special cases. An analysis showing how value iteration can be made to find\ran optimal policy in finite time is given by Bertsekas (1987).\nIterative policy evaluation is an example of a classical successive\rapproxima\u0026shy;tion algorithm for solving a system of linear equations. The version\rof the algorithm that uses two arrays, one holding the old values while the\rother is updated, is often called a Jacobi-style\ralgorithm, after Jacobi��s classical use of this method. It is also sometimes\rcalled a synchronous algorithm be\u0026shy;cause it can be\rperformed in parallel, with separate processors simultaneously updating the\rvalues of individual states using input from other processors. The second array\ris needed to simulate this parallel computation sequentially. The in-place\rversion of the algorithm is often called a Gauss-Seidel-style\ralgo\u0026shy;rithm after the classical Gauss-Seidel algorithm for solving systems of\rlinear equations. In addition to iterative policy evaluation, other DP\ralgorithms can be implemented in these different versions. Bertsekas and\rTsitsiklis (1989) provide excellent coverage of these variations and their\rperformance differ\u0026shy;ences.\n4.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rAsynchronous DP algorithms are\rdue to Bertsekas (1982, 1983), who also called them distributed DP algorithms.\rThe original motivation for asyn\u0026shy;chronous DP was its implementation on a multiprocessor\rsystem with com\u0026shy;munication delays between processors and no global\rsynchronizing clock. These algorithms are extensively discussed by Bertsekas\rand Tsitsiklis (1989). Jacobi-style and Gauss-Seidel-style DP algorithms are\rspecial cases of the asynchronous version. Williams and Baird (1990) presented\rDP algorithms that are asynchronous at a finer grain than the ones we have\rdiscussed: the backup operations themselves are broken into steps that can be\rperformed asynchronously.\n4.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThis section, written with the\rhelp of Michael Littman, is based on Littman, Dean, and Kaelbling (1995). The\rphrase ��curse of dimensionality�� is due to Bellman (1957).\n100\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; CHAPTER4.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; DYNAMIC\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; PROGRAMMING\n\r\rChapter 5\nMonte Carlo Methods\nIn this chapter we consider our first learning\rmethods for estimating value functions and discovering optimal policies. Unlike\rthe previous chapter, here we do not as\u0026shy;sume complete knowledge of the\renvironment. Monte Carlo methods require only experience��sample\rsequences of states, actions, and rewards from actual or simu\u0026shy;lated interaction\rwith an environment. Learning from actual experience is\rstriking because it requires no prior knowledge of the environment��s dynamics,\ryet can still attain optimal behavior. Learning from simulated\rexperience is also powerful. Al\u0026shy;though a model is required, the model need only\rgenerate sample transitions, not the complete probability distributions of all\rpossible transitions that is required for dynamic programming (DP). In\rsurprisingly many cases it is easy to generate expe\u0026shy;rience sampled according to\rthe desired probability distributions, but infeasible to obtain the\rdistributions in explicit form.\nMonte Carlo methods are ways of solving the\rreinforcement learning problem based on averaging sample returns. To ensure\rthat well-defined returns are available, here we define Monte Carlo methods\ronly for episodic tasks. That is, we assume experience is divided into\repisodes, and that all episodes eventually terminate no matter what actions are\rselected. Only on the completion of an episode are value estimates and policies\rchanged. Monte Carlo methods can thus be incremental in an episode-by- episode\rsense, but not in a step-by-step (online) sense. The term ��Monte Carlo�� is\roften used more broadly for any estimation method whose operation involves a\rsignificant random component. Here we use it specifically for methods based on\raveraging complete returns (as opposed to methods that learn from partial\rreturns, considered in the next chapter).\nMonte Carlo methods sample and average returns for each state-action pair much like the bandit\rmethods we explored in Chapter 2 sample and average rewards\rfor each action. The main difference is that now there are multiple states,\reach acting like a different bandit problem (like an associative-search or\rcontextual bandit) and that the different bandit problems are interrelated.\rThat is, the return after taking an action in one state depends on the actions\rtaken in later states in the same episode. Because all the action selections\rare undergoing learning, the problem becomes nonstationary from the point of\rview of the earlier state.\n\r\rTo handle the\rnonstationarity, we adapt the idea of general policy iteration (GPI) developed\rin Chapter 4 for DP. Whereas there we computedvalue functions from knowledge of the MDP, here we learnvalue functions from sample returns with the MDP.\rThe value functions and corresponding policies still interact to attain\roptimality in essentially the same way (GPI). As in the DP chapter, first we\rconsider the prediction problem (the computation of Vn and q^ for a fixed\rarbitrary policy n) then policy improvement, and, finally, the control problem\rand its solution by GPI. Each of these ideas taken from DP is extended to the\rMonte Carlo case in which only sample experience is available.\n5.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rMonte Carlo Prediction\nWe begin by\rconsidering Monte Carlo methods for learning the state-value function for a\rgiven policy. Recall that the value of a state is the expected return��expected\rcumulative future discounted reward��starting from that state. An obvious way to\restimate it from experience, then, is simply to average the returns observed\rafter visits to that state. As more returns are observed, the average should\rconverge to the expected value. This idea underlies all Monte Carlo methods.\nIn particular,\rsuppose we wish to estimate Vn(s), the\rvalue of a state s under\rpolicy n, given a set of\repisodes obtained by following n and passing through s. Each occurrence of state s in an episode is called a visitto s. Of\rcourse, s may be visited multiple\rtimes in the same episode; let us call the first time it is visited in an\repisode the first visitto s. The first-visit MC methodestimates Vآ(s) as\rthe average of the returns following first visits to s, whereas the every-visit MC method averages the returns following all visits to s. These two Monte Carlo (MC) methods are very similar\rbut have slightly different theoretical properties. First-visit MC has been\rmost widely studied, dating back to the 1940s, and is the one we focus on in\rthis chapter. Every-visit MC extends more naturally to function approximation\rand eligibility traces, as discussed in Chapters 9 and 12. First-visit MC is\rshown in procedural form in the box.\nFirst-visit MC prediction,\rfor estimating V ��\nInitialize:\nn �D policy to be evaluated V �D an arbitrary state-value function\rReturns(s) �D an empty list, for all s G ��\nRepeat forever:\nGenerate\ran episode using n\nFor each\rstate s appearing in the episode:\nG�D return following the first occurrence of s Append\rG to Returns(s)\nV(s) �D average(Returns(s))\nBoth first-visit MC and\revery-visit MC converge to v^(s) as the number of visits (or first visits) to s goes to infinity. This is easy to see for the case of first-visit\rMC. In this case each return is an independent, identically distributed\restimate of vn (s) with finite variance. By the law of large numbers the sequence of\raverages of these estimates converges to their expected value. Each average is\ritself an unbiased estimate, and the standard deviation of its error falls as 1/y/n, where n is the number of returns averaged (i.e., the estimate is said to converge quadratically). Every-visit MC is less\rstraightforward, but its estimates also converge quadratically to v^(s) (Singh and Sutton,\r1996).\nThe use of Monte Carlo methods is best\rillustrated through an example.\nExample 5.1: Blackjack The object of\rthe popular casino card game of blackjack is to obtain\rcards the sum of whose numerical values is as great as possible without\rexceeding 21. All face cards count as 10, and an ace can count\reither as 1or as 11. We consider the version in which each\rplayer competes independently against the dealer. The game begins with two\rcards dealt to both dealer and player. One of the dealer��s cards is face up and\rthe other is face down. If the player has 21 immediately (an ace and a\r10-card), it is called a natural. He then wins unless\rthe dealer also has a natural, in which case the game is a draw. If the player\rdoes not have a natural, then he can request additional cards, one by one (hits), until he either stops (sticks)\ror exceeds 21 (goes bust). If he goes bust, he loses;\rif he sticks, then it becomes the dealer��s turn. The dealer hits or sticks\raccording to a fixed strategy without choice: he sticks on any sum of 17 or\rgreater, and hits otherwise. If the dealer goes bust, then the player wins;\rotherwise, the outcome��win, lose, or draw��is determined by whose final sum is\rcloser to 21.\nPlaying blackjack is naturally\rformulated as an episodic finite MDP. Each game of blackjack is an episode.\rRewards of +1, ��1, and 0 are given for winning, losing, and drawing,\rrespectively. All rewards within a game are zero, and we do not discount (Y = 1); therefore these terminal rewards are also the returns. The\rplayer��s actions are to hit or to stick. The states depend on the player��s\rcards and the dealer��s showing card. We assume that cards are dealt from an\rinfinite deck (i.e., with replacement) so that there is no advantage to keeping\rtrack of the cards already dealt. If the player holds an ace that he could\rcount as 11without going bust, then the ace is said to be usable.\rIn this case it is always counted as 11 because counting it as 1 would make the\rsum 11or less, in which case there is no decision to be made because,\robviously, the player should always hit. Thus, the player makes decisions on\rthe basis of three variables: his current sum (12-21), the dealer��s one showing card (ace-10), and\rwhether or not he holds a usable ace. This makes for a total of 200 states.\nConsider the policy that\rsticks if the player��s sum is 20 or 21, and otherwise hits. To find the\rstate-value function for this policy by a Monte Carlo approach, one simulates\rmany blackjack games using the policy and averages the returns following each\rstate. Note that in this task the same state never recurs within one episode,\rso there is no difference between first-visit and every-visit MC methods. In\rthis way, we obtained the estimates of the state-value function shown in Figure\r5.1. The estimates for states with a usable ace are less certain and less\rregular because these\n\r\r\r\n\r\r\r\r\r\rUsable\nace\n\r\r\r\r\r\r\r\rAfter 500,000 episodes\n\r\r\r\r\r\r\r\r+1\n\r\r\r\r\r\r\r\r\nTO\n\r\r\r\r\r\n\r\n\r\r\r\r\r\rNo\nusable\nace\n\r\r\r\r\r\u0026nbsp;\n\r\r\u0026nbsp;\n\u0026nbsp;\n\r\rFigure 5.1: Approximate state-value functions for the blackjack\rpolicy that sticks only on 20 or 21, computed by Monte Carlo policy evaluation.\nstates are less common. In any event, after\r500,000 games the value function is very well approximated.\nAlthough we have complete knowledge of the environment in this task,\rit would not be easy to apply DP methods to compute the value function. DP\rmethods require the distribution of next eventsһin particular, they require the quantities p(s', r|s, a)��and it is\rnot easy to determine these for blackjack. For example, suppose the player��s\rsum is 14 and he chooses to stick. What is his expected reward as a function of\rthe dealer��s showing card? All of these expected rewards and transition\rprobabilities must be computed before DP can be\rapplied, and such computations are often complex and error-prone. In contrast,\rgenerating the sample games required by Monte Carlo methods is easy. This is\rthe case surprisingly often; the ability of Monte Carlo methods to work with\rsample episodes alone can be a significant advantage even when one has complete\rknowledge of the environment��s dynamics.\nCan we generalize the idea of backup diagrams to\rMonte Carlo algorithms? The general idea of a backup diagram is to show at the\rtop the root node to be updated and to show below all the transitions and leaf\rnodes whose rewards and estimated values contribute to the update. For Monte\rCarlo estimation of v^, the root is a state node, and below it is the entire\rtrajectory of transitions along a particular single episode, ending at the\rterminal state, as in Figure 5.2. Whereas the DP diagram (Figure 3.4-left)\rshows all possible transitions, the Monte Carlo diagram shows only those\rsampled on the one episode. Whereas the DP diagram includes only one-step\rtransitions, the Monte Carlo diagram goes all the way to the end of the\repisode. These differences in the diagrams accurately reflect the fundamental\rdifferences between the algorithms.\nFigure 5.2: The backup diagram for Monte Carlo estimation of Vn.\nAn important fact about Monte Carlo methods is that the estimates\rfor each state are independent. The estimate for one state does not build upon\rthe estimate of any other state, as is the case in DP. In other words, Monte\rCarlo methods do not bootstrap as we defined it in the\rprevious chapter.\nIn particular, note that the computational expense of estimating the\rvalue of a single state is independent of the number of states. This can make\rMonte Carlo methods particularly attractive when one requires the value of only\rone or a subset of states. One can generate many sample episodes starting from\rthe states of interest, averaging returns from only these states ignoring all\rothers. This is a third advantage Monte Carlo methods can have over DP methods\r(after the ability to learn from actual experience and from simulated\rexperience).\n\r\r\r\nA bubble on a wire\rloop\n\r\r\r\r\rExample 5.2: Soap Bubble\nSuppose a wire frame forming a closed\rloop is dunked in soapy water to form a soap surface or bubble conforming at\rits edges to the wire frame. If the geometry of the wire frame is irregular but\rknown, how can you compute the shape of the surface? The shape has the property\rthat the total force on each point exerted by neighboring points is zero (or\relse the shape would change). This means that the surface��s height at any point\ris the aver\u0026shy;age of its heights at points in a small circle around that point.\rIn addition, the surface must meet at its boundaries with the wire frame. The\rusual approach to problems of this kind is to put a grid over the area covered\rby the surface and solve for its height at the grid points by an iterative\rcomputation. Grid points at the boundary areforced\u0026nbsp;\u0026nbsp;\u0026nbsp; to\u0026nbsp;\u0026nbsp;\u0026nbsp; the\u0026nbsp;\u0026nbsp;\u0026nbsp; wire\u0026nbsp; frame,and\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; all\nothers are adjusted\rtoward the average of the heightsof their\rfour nearest neighbors.\n\r\rThis process then iterates, much like\rDP��s iterative policy evaluation, and ultimately converges to a close\rapproximation to the desired surface.\nThis is similar to the kind of\rproblem for which Monte Carlo methods were origi\u0026shy;nally designed. Instead of the\riterative computation described above, imagine stand\u0026shy;ing on the surface and\rtaking a random walk, stepping randomly from grid point to neighboring grid\rpoint, with equal probability, until you reach the boundary. It turns out that\rthe expected value of the height at the boundary is a close approximation to\rthe height of the desired surface at the starting point (in fact, it is exactly\rthe value computed by the iterative method described above). Thus, one can\rclosely approximate the height of the surface at a point by simply averaging\rthe bound\u0026shy;ary heights of many walks started at the point. If one is interested\rin only the value at one point, or any fixed small set of points, then this\rMonte Carlo method can be far more efficient than the iterative method based on\rlocal consistency.\n��\nExercise 5.1 Consider the diagrams on\rthe right in Figure 5.1. Why does the estimated value function jump up for the\rlast two rows in the rear? Why does it drop off for the whole last row on the\rleft? Why are the frontmost values higher in the upper diagrams than in the\rlower?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n5.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rMonte Carlo Estimation of\rAction Values\nIf a model is not available, then it\ris particularly useful to estimate action values (the\rvalues of state-action pairs) rather than state values.\rWith a model, state values alone are sufficient to determine a policy; one\rsimply looks ahead one step and chooses whichever action leads to the best\rcombination of reward and next state, as we did in the chapter on DP. Without a\rmodel, however, state values alone are not sufficient. One must explicitly\restimate the value of each action in order for the values to be useful in\rsuggesting a policy. Thus, one of our primary goals for Monte Carlo methods is\rto estimate q*. To achieve this, we\rfirst consider the policy evaluation problem for action values.\nThe policy evaluation problem\rfor action values is to estimate q^(s, a), the expected return when starting in state s, taking action a, and thereafter\rfollowing policy n. The Monte Carlo\rmethods for this are essentially the same as just presented for state values,\rexcept now we talk about visits to a state-action pair rather than to a state.\rA state-action pair s, a is said to be\rvisited in an episode if ever the state s is visited\rand action a is taken in it. The\revery-visit MC method estimates the value of a state-action pair as the average\rof the returns that have followed all the visits to it. The first-visit MC\rmethod averages the returns following the first time in each episode that the\rstate was visited and the action was selected. These methods converge\rquadratically, as before, to the true expected values as the number of visits\rto each state-action pair approaches infinity.\nThe only complication is that many state-action\rpairs may never be visited. If n is a\rdeterministic policy, then in following n one will\robserve returns only for\n\r\r\r\r\rone of the actions from each\rstate. With no returns to average, the Monte Carlo estimates of the other\ractions will not improve with experience. This is a serious problem\rbecause the purpose of learning action values is to help in choosing among\rthe actions available in each state. To compare alternatives we need to\restimate the value of allthe actions from each state, not just the one we currently favor.\nThis is the general problem of maintaining\rexploration, as discussed in the context\rof the k-armed bandit problem in Chapter 2. For policy evaluation to work\rfor action values, we must assure continual exploration. One way to do\rthis is by specifying that the episodes start in a\rstate-action pair, and that every pair has a\rnonzero probability of being selected as the start. This guarantees that\rall state-action pairs will be visited an infinite number of times in the\rlimit of an infinite number of episodes. We call this the assumption of exploring\rstarts.\nThe assumption of exploring\rstarts is sometimes useful, but of course it cannot be relied upon in\rgeneral, particularly when learning directly from actual interaction with\ran environment. In that case the starting conditions are unlikely to be so\rhelpful. The most common alternative approach to assuring that all\rstate-action pairs are encountered is to consider only policies that are\rstochastic with a nonzero probability of selecting all actions in each\rstate. We discuss two important variants of this approach in later\rsections. For now, we retain the assumption of exploring starts and\rcomplete the presentation of a full Monte Carlo control method.\nExercise\r5.2 What is the backup diagram for Monte Carlo estimation of ����\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n\r\r\r\r\r\r\r\r5.3 Monte Carlo Control\nWe are now ready to consider\rhow Monte Carlo estimation can be used in control, that is, to approximate\roptimal poli\u0026shy;cies. The overall idea is to proceed according to the same\rpattern as in the DP chapter, that is, according to the idea of\rgeneralized policy iteration (GPI). In GPI one maintains both an approximate\rpolicy and an approximate value func\u0026shy;tion. The value function is\rrepeatedly altered to more closely approximate the value function for the\rcurrent policy, and the policy is repeatedly improved with respect to the\rcurrent value function, as suggested by the diagram to the right. These\rtwo kinds of changes work against each other to some extent, as each\rcreates a mov\u0026shy;ing target for the other, but together they cause both\rpolicy and value function to approach optimality.\nTo begin, let us consider a\rMonte Carlo version of classical policy iteration. In this method, we\rperform alternating complete steps of policy evaluation and policy\rimprovement, beginning with an arbitrary policy no and ending with the\roptimal policy and optimal action-value function:\n\r\r\r\r\r\r\r\revaluation\n\r\r\r\r\r\r\r\r^\\^^greedy(Q)/\n\r\r\r\r\r\r\r\rQ\n\r\r\r\r\r\r\r\rآ\n\r\r\r\r\r\r\r\rimprovement\n\r\r\r\r\r\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rE\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; TP\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; TP\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; TP\n\r\r\rq*,\n\r\r\r\r\r\r\r\rno\n\r\r\r\r\r-- ��qno - ��آ1��Qni- ��n2-- ��' ' '\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��n* \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n\r\rwhere ��^ denotes a\rcomplete policy evaluation and ��^ denotes a complete\rpol\u0026shy;icy improvement. Policy evaluation is done exactly as described in the\rpreceding section. Many episodes are experienced, with the approximate\raction-value func\u0026shy;tion approaching the true function asymptotically. For the\rmoment, let us assume that we do indeed observe an infinite number of episodes\rand that, in addition, the episodes are generated with exploring starts. Under\rthese assumptions, the Monte Carlo methods will compute each q^fc exactly, for arbitrary nk.\nPolicy improvement is done by making the policy greedy with respect\rto the current value function. In this case we have an action-value\rfunction, and therefore no model is needed to construct the greedy policy. For\rany action-value function q, the\rcorresponding greedy policy is the one that, for each s G S, deterministically chooses an action with maximal action-value:\nn(s) ==\rargmaxq(s, a).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (5.1)\n\r\r\u0026nbsp;\n\r\rPolicy improvement then with respect to q^fc. The آk and nkʮ1because, for\nqnfc (s,أ\u0026amp;ʮ1(s))=\n\u0026gt;\u0026nbsp;\n\u0026gt;\u0026nbsp;\ncan be done by constructing each nkʮ1as the greedy policy policy improvement theorem (Section 4.2) then\rapplies to all s G S,\nqnfc (s, argmaxq^fc (s,a))\na\nmax qnk (s, a)\na\nqnk (s,\rnk (s))\nVnfc(s).\n\r\r\u0026nbsp;\n\r\rAs we discussed in the previous\rchapter, the theorem assures us that each nkʮ1is uniformly better than nk, or just as good as nk, in which case\rthey are both optimal policies. This in turn assures us that the overall\rprocess converges to the optimal policy and optimal value function. In this way\rMonte Carlo methods can be used to find optimal policies given only sample\repisodes and no other knowledge of the environment��s dynamics.\nWe made two unlikely\rassumptions above in order to easily obtain this guarantee of convergence for\rthe Monte Carlo method. One was that the episodes have exploring starts, and\rthe other was that policy evaluation could be done with an infinite number of\repisodes. To obtain a practical algorithm we will have to remove both\rassumptions. We postpone consideration of the first assumption until later in\rthis chapter.\nFor now we focus on the\rassumption that policy evaluation operates on an infinite number of episodes.\rThis assumption is relatively easy to remove. In fact, the same issue arises\reven in classical DP methods such as iterative policy evaluation, which also\rconverge only asymptotically to the true value function. In both DP and Monte\rCarlo cases there are two ways to solve the problem. One is to hold firm to the\ridea of approximating q^fc in each policy evaluation. Measurements\rand assumptions are made to obtain bounds on the magnitude and probability of\rerror in the estimates, and then sufficient steps are taken during each policy\revaluation to assure that these bounds are sufficiently small. This approach\rcan probably be made \n\r\rcompletely satisfactory in the\rsense of guaranteeing correct convergence up to some level of approximation.\rHowever, it is also likely to require far too many episodes to be useful in\rpractice on any but the smallest problems.\nThe second approach to avoiding\rthe infinite number of episodes nominally required for policy evaluation is to\rforgo trying to complete policy evaluation before returning to policy\rimprovement. On each evaluation step we move the value function toward q^fc, but we do not expect to actually get\rclose except over many steps. We used this idea when we first introduced the\ridea of GPI in Section 4.6. One extreme form of the idea is value iteration, in\rwhich only one iteration of iterative policy evaluation is performed between\reach step of policy improvement. The in-place version of value iteration is\reven more extreme; there we alternate between improvement and evaluation steps\rfor single states.\nFor Monte Carlo policy\revaluation it is natural to alternate between evaluation and improvement on an\repisode-by-episode basis. After each episode, the observed returns are used for\rpolicy evaluation, and then the policy is improved at all the states visited in\rthe episode. A complete simple algorithm along these lines, which we call Monte Carlo ES, for Monte Carlo with Exploring Starts, is\rgiven in the box.\nIn Monte Carlo ES, all the returns for each state-action pair are\raccumulated and averaged, irrespective of what policy was in force when they\rwere observed. It is easy to see that Monte Carlo ES cannot converge to any\rsuboptimal policy. If it did, then the value function would eventually converge\rto the value function for that policy, and that in turn would cause the policy\rto change. Stability is achieved only when both the policy and the value\rfunction are optimal. Convergence to this optimal fixed point seems inevitable\ras the changes to the action-value function decrease over time, but has not yet\rbeen formally proved. In our opinion, this is one of the most fundamental open\rtheoretical questions in reinforcement learning (for a partial solution, see\rTsitsiklis, 2002).\nMonte Carlo ES (Exploring\rStarts), for estimating n n*\nInitialize, for all s G S, a G A(s):\nQ(s, a) ^ arbitrary n(s) ^ arbitrary Returns(s,\ra) ^\rempty list\nRepeat forever:\nChoose So G S and Ao G A(So) s.t. all pairs have probability \u0026gt; 0 Generate an episode\rstarting from So, Ao, following n For each pair s, a appearing in the episode:\nG^ return\rfollowing the first occurrence of s, a Append G to Returns(s, a)\nQ(s, a) ^ average(Returns(s, a))\nFor each s in the episode: n(s) ^ arg max a Q(s, a)\n\rV*\n\u0026nbsp;\n\rSTICK :\n\r21\n20\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\rUsable\nace\n\rn r\n\r19\n18\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\rHIT\n\r17\n16\n15\n14\n1 3\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r127^4/ 11 \\l\n\r\r\r\r\r\r\r\r\r\rA23456789 10\n\r\r\r\r\r\u0026nbsp;\n\r\nFigure 5.3: The optimal policy and state-value\rfunction for blackjack, found by Monte Carlo ES (Figure 5.4). The state-value\rfunction shown was computed from the action-value function found by Monte\rCarlo ES.\n\r\r\r\r\r\u0026nbsp;\nExample 5.3: Solving\rBlackjack It is straightforward to apply Monte Carlo ES to blackjack. Since the\repisodes are all simulated games, it is easy to arrange for exploring starts\rthat include all possibilities. In this case one simply picks the dealer��s\rcards, the player��s sum, and whether or not the player has a usable ace, all at\rrandom with equal probability. As the initial policy we use the policy\revaluated in the previous blackjack example, that which sticks only on 20 or\r21. The initial action-value function can be zero for all state-action pairs.\rFigure 5.3 shows the optimal policy for blackjack found by Monte Carlo ES. This\rpolicy is the same as the ��basic�� strategy of Thorp (1966) with the sole\rexception of the leftmost notch in the policy for a usable ace, which is not\rpresent in Thorp��s strategy. We are uncertain of the reason for this\rdiscrepancy, but confident that what is shown here is indeed the optimal policy\rfor the version of blackjack we have described.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n5.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rMonte Carlo Control without\rExploring Starts\nHow can we avoid the unlikely\rassumption of exploring starts��The only general way\rto ensure that all actions are selected infinitely often is for the agent to\rcontinue to select them. There are two approaches to ensuring this, resulting\rin what we call on-policy methods and off-policy\rmethods. On-policy methods attempt to evaluate or improve the policy that is\rused to make decisions, whereas off-policy methods evaluate or improve a policy\rdifferent from that used to generate the data. The Monte Carlo ES method\rdeveloped above is an example of an on-policy method. In this section we show\rhow an on-policy Monte Carlo control method can be designed that does not use\rthe unrealistic assumption of exploring starts. Off-policy methods\n\r\rare considered in the next\rsection.\nIn on-policy control methods the policy is\rgenerally soft, meaning that n(a|s) \u0026gt; 0 for all s G S and all a G A(s), but gradually shifted closer and closer to a deterministic\roptimal policy. Many of the methods discussed in Chapter 2 provide mechanisms\rfor this. The on-policy method we present in this section uses \u0026pound;-greedy policies, meaning that most of the time they choose\ran action that has maximal estimated action value, but with probability \u0026pound; they instead select an action at random. That is, all nongreedy\ractions are given the minimal probability of selection, |^)|,\rand the remaining bulk of the probability, 1�� \u0026pound; + pfsji, is given to the greedy action. The \u0026pound;-greedy policies are examples of \u0026pound;-soft\rpolicies, defined as policies for which n(a|s) \u0026gt; |a(s)|for all states\rand actions, for some \u0026pound; \u0026gt; 0. Among \u0026pound;-soft policies, \u0026pound;-greedy\rpolicies are in some sense those that are closest to greedy.\nThe overall idea of on-policy Monte Carlo control\ris still that of GPI. As in Monte Carlo ES, we use first-visit MC methods to\restimate the action-value function for the current policy. Without the\rassumption of exploring starts, however, we cannot sim\u0026shy;ply improve the policy\rby making it greedy with respect to the current value function, because that\rwould prevent further exploration of nongreedy actions. Fortunately, GPI does\rnot require that the policy be taken all the way to a greedy policy, only that\rit be moved toward a greedy policy. In our on-policy\rmethod we will move it only to an \u0026pound;-greedy\rpolicy. For any \u0026pound;-soft policy, n, any \u0026pound;-greedy policy with\rrespect to qn is guaranteed to be\rbetter than or equal to n. The complete algorithm\ris given in the box below.\nThat any \u0026pound;-greedy policy with\rrespect to q^ is an improvement\rover any \u0026pound;-soft policy n is assured by the policy improvement theorem. Let n' be the \u0026pound;-greedy policy. The\rconditions of the policy improvement theorem apply because for any\ni-policy first-visit MC\rcontrol (for \u0026pound;-soft policies), estimates n n*\nInitialize, for all s G S, a G A(s):\nQ(s, a) ^ arbitrary Returns(s, a) ^ empty list\rn(a|s) ^ an arbitrary \u0026pound;-soft\rpolicy\nRepeat forever:\n(a)Generate an episode using n\n(b)For each pair s, a appearing in\rthe episode:\n\r\r\ra\n\r\r\r\r\rG ^ return\rfollowing the first occurrence of . Append G to\rReturns(s, a)\nQ(s, a) ^ average(Returns(s, a))\n(c)For each s in the episode:\nA* ^ argmaxa Q(s, a)\nFor all a\rG A(s):\n\r\r\rif a = A* if a = A*\n\r\r\r\r\r1�� \u0026pound; + \u0026pound;/|A(s) |\n\r\rqn (s,n;(s)) = f n;(a|s)qn\r(s,a)\n\r\r\u0026nbsp;\n\r\r\rE\n\r\r\r\r\r\r\r(5.2)\n\r\r\r\r\r\r\r\r|A(s)|\n\r\r\r\r\rqn (s, a) + (1- e) max q^ (s, a)\na,\n\r\r\r\r\rqn (s,a)\n\r\r\r\r\r\r\r\r\u0026gt;\u0026nbsp;\n\r\r\r\r\r\r\r\r|A(s)|\n\r\r\r\r\r\r\r\rXX(s,a)\r+ (1- e)Yl\n\r\r\r\r\r\r\r\rn(a|s) -\n\r\r\r\r\r\u0026nbsp;\n\r(the sum is a weighted average with nonnegative weights summing to 1, and as such it must be less than or equal to the\rlargest number averaged)\n\r\r\u0026nbsp;\n\r\r\r\r\r|A(s)| vn (s).\n\r\r\r\r\r\r\r\r|A(s)|\n\r\r\r\r\r^qn (s,a) + L n(a|s)qn (s,a)\n\r\r\u0026nbsp;\n\u0026nbsp;\n\r\rThus, by the policy improvement theorem, \u0026gt; n (i.e.,(s)\u0026gt; v^(s), for all s G S). We now prove that equality can hold\ronly when both and n are optimal among the e-soft policies, that is, when they\rare better than or equal to all other e-soft policies.\nConsider a new environment that is just like the original\renvironment, except with the requirement that policies be e-soft ��moved inside��\rthe environment. The new environment has the same action and state set as the\roriginal and behaves as follows. If in state s and taking action a, then with\rprobability 1 - e the new environment behaves exactly like the old environment.\rWith probability e it repicks the action at random, with equal probabilities,\rand then behaves like the old environment with the new, random action. The best\rone can do in this new environment with general policies is the same as the\rbest one could do in the original environment with e-soft policies. Let ʯ* and c[*denote the optimal value functions for the new environment. Then a\rpolicy n is optimal among e-soft policies if and only if v^ = ^*. From the\rdefinition of Tj*we\rknow that it is the unique solution to\n\r\r\rT*(s)\n\r\r\r\r\r(1- e) max T*(s, a) +\n|A(s)|\n(1�� e) ma^ \u0026gt; p(s;, r|s, a) r + yt(s') a\u0026nbsp;\u0026nbsp; L\n\r\r\r+\n\r\r\r\r\r\u0026pound;WP(s',r|s,a) r + yt*(s;)\nS ,r\n|A(s)|\nWhen equality holds and the e-soft policy n is no longer improved,\rthen we also know, from (5.2), that\nvn(s)\u0026nbsp;\u0026nbsp; =\u0026nbsp; (1��e) max��(s,a)\r+ i J��!\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (s,a)\na\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; |A(s)|V\n=(1�� e) ma^ 7p(s',\rr|s, a) r + yv^(s')\na\n\r\r\u0026nbsp;\n\r\r\r\r\r+\n\r\r\r\r\r\r\r\r|A(s)|\n\r\r\r\r\r^^p(s',r|s,a) r + yv^(s')\na\n\r\rHowever, this equation is the same as\rthe previous one, except for the substitution of vn for vJ*. Since vJ*\ris the unique solution, it must be that v^ = ^*.\nIn essence, we have shown in the last few pages that policy\riteration works for e-soft policies. Using\rthe natural notion of greedy policy for e-soft\rpolicies, one is assured of improvement on every step, except when the best\rpolicy has been found among the e-soft\rpolicies. This analysis is independent of how the action-value functions are\rdetermined at each stage, but it does assume that they are computed exactly.\rThis brings us to roughly the same point as in the previous section. Now we\ronly achieve the best policy among the e-soft\rpolicies, but on the other hand, we have eliminated the assumption of exploring\rstarts.\n5.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rOff-policy Prediction via\rImportance Sampling\nAll learning control methods face a\rdilemma: They seek to learn action values con\u0026shy;ditional on subsequent optimal behavior, but they need to behave non-optimally in\rorder to explore all actions (to find the optimal\ractions). How can they learn about the optimal policy while behaving according\rto an exploratory policy��The on-policy approach in the preceding\rsection is actually a compromise��it learns action values not for the optimal\rpolicy, but for a near-optimal policy that still explores. A more\rstraightforward approach is to use two policies, one that is learned about and\rthat becomes the optimal policy, and one that is more exploratory and is used\rto gen\u0026shy;erate behavior. The policy being learned about is called the target policy, and the policy used to generate behavior is\rcalled the behavior policy. In this case we say that\rlearning is from data ��off�� the target policy, and the overall process is\rtermed off-policy learning.\nThroughout the rest of this\rbook we consider both on-policy and off-policy meth\u0026shy;ods. On-policy methods are\rgenerally simpler and are considered first. Off-policy methods require\radditional concepts and notation, and because the data is due to a different\rpolicy, off-policy methods are often of greater variance and are slower to\rconverge. On the other hand, off-policy methods are more powerful and general.\rThey include on-policy methods as the special case in which the target and\rbehavior policies are the same. Off-policy methods also have a variety of\radditional uses in applications. For example, they can often be applied to\rlearn from data generated by a conventional non-learning controller, or from a\rhuman expert. Off-policy learning is also seen by some as key to learning\rmulti-step predictive models of the world��s dynamics (Sutton, 2009, Sutton et\ral., 2011).\nIn this section we begin the\rstudy of off-policy methods by considering the predic\u0026shy;tion\rproblem, in which both target and behavior policies are fixed. That is, suppose\rwe wish to estimate v^ or , but all we have are episodes following another\rpolicy b, where b = n. In this case, n is the target\rpolicy, b is the behavior policy, and both policies are\rconsidered fixed and given.\nIn order to use episodes from\rb to estimate values for n, we require that every action taken under n is also\rtaken, at least occasionally, under b. That is, we require that n(a|s) \u0026gt; 0\rimplies b(a|s) \u0026gt; 0. This is called the assumption of coverage.\rIt follows from coverage that b must be stochastic in\rstates where it is not identical to n. The target policy n, on the other hand,\rmay be deterministic, and, in fact, this is a case of particular interest in\rcontrol problems. In control, the target policy is typically the deterministic\rgreedy policy with respect to the current action-value function estimate. This\rpolicy becomes a deterministic optimal policy while the behavior policy remains\rstochastic and more exploratory, for example, an \u0026pound;-greedy policy. In this\rsection, however, we consider the prediction problem, in which n is unchanging\rand given.\nAlmost all off-policy methods\rutilize importance sampling, a general technique for\restimating expected values under one distribution given samples from another.\rWe apply importance sampling to off-policy learning by weighting returns\raccording to the relative probability of their trajectories occurring under the\rtarget and behavior policies, called the importance-sampling\rratio. Given a starting state St, the prob\u0026shy;ability of the subsequent\rstate-action trajectory, At, Stʮ1, Atʮ1,...,St, occurring under any\rpolicy n is\nPr{At, Stʮ1, Atʮ1,...,St | St, At��T-1\u0026#12316;n}\n=n(At|St)p(Stʮ1|St, At)n(At+1|St+1) �� �� �� p(St|St-1, At-1)\nT -1\n=H n(Ak|Sk)P(Sk+1!Sk, Ak),\nk=t\nwhere p here is the state-transition\rprobability function defined by (3.10). Thus, the relative probability of the\rtrajectory under the target and behavior policies (the importance-sampling ratio)\ris\np= nT=t1n(Ak |Sk )p(Sw|Sk ,Ak) = T-1n(Ak |Sk)\n��nT=t1b(Ak|Sk)p(Sk+1|Sk,Ak) �� knt b(Ak|Sk)'\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 1' j\nAlthough the trajectory probabilities depend on\rthe MDP��s transition probabilities, which are generally unknown, they appear\ridentically in both the numerator and denominator, and thus cancel. The\rimportance sampling ratio ends up depending only on the two policies and the\rsequence, not on the MDP.\nNow we are ready to give a\rMonte Carlo algorithm that uses a batch of observed episodes following policy b\rto estimate Vn(s). It is convenient here to number time steps in a way that\rincreases across episode boundaries. That is, if the first episode of the batch\rends in a terminal state at time 100, then the next\repisode begins at time t = 101. This enables us to use time-step numbers to\rrefer to particular steps in particular episodes. In particular, we can define\rthe set of all time steps in which state s is visited, denoted T(s). This is\rfor an every-visit method; for a first-visit method, T(s) would only include\rtime steps that were first visits to s within their episodes. Also, let T(t)\rdenote the first time of termination following time t, and Gt denote the return\rafter t up through T(t). Then {Gt}te��(s) are the returns that pertain to state s, and {pt:T(t)_1}te��(s) are the\rcorresponding importance-sampling ratios. To estimate v^ (s), we simply scale\rthe returns by the ratios and average the results:\nv(s) = E\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��-lGt.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (5.4)\nWhen importance sampling is done as a\rsimple average in this way it is called ordinary importance\rsampling.\nAn important alternative is weighted importance\rsampling, which uses a weighted average, defined\ras\nV(s)=�������-lGt,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (5.5)\n(t)-1\nor zero if the denominator is zero. To\runderstand these two varieties of importance sampling, consider their estimates\rafter observing a single return. In the weighted- average estimate, the ratio\rpt��T(t)_i\rfor the single return cancels in the numerator and denominator, so that the\restimate is equal to the observed return independent of the ratio (assuming the\rratio is nonzero). Given that this return was the only one observed, this is a\rreasonable estimate, but its expectation is v^(s) rather than vآ(s),\rand in this statistical sense it is biased. In contrast, the simple average\r(5.4) is always v^(s) in expectation (it is unbiased), but it can be extreme.\rSuppose the ratio were ten, indicating that the trajectory observed is ten\rtimes as likely under the target policy as under the behavior policy. In this\rcase the ordinary importance- sampling estimate would be ten\rtimes the observed return. That is, it would be quite far from the\robserved return even though the episode��s trajectory is considered very\rrepresentative of the target policy.\nFormally, the difference\rbetween the two kinds of importance sampling is expressed in their biases and\rvariances. The ordinary importance-sampling estimator is unbi\u0026shy;ased whereas the\rweighted importance-sampling estimator is biased (the bias con\u0026shy;verges\rasymptotically to zero). On the other hand, the variance of the ordinary\rimportance-sampling estimator is in general unbounded because the variance of\rthe ratios can be unbounded, whereas in the weighted estimator the largest\rweight on any single return is one. In fact, assuming bounded returns, the\rvariance of the weighted importance-sampling estimator converges to zero even\rif the variance of the ratios themselves is infinite (Precup, Sutton, and\rDasgupta 2001). In practice, the weighted estimator usually has dramatically\rlower variance and is strongly pre\u0026shy;ferred. Nevertheless, we will not totally\rabandon ordinary importance sampling as it is easier to extend to the\rapproximate methods using function approximation that we explore in the second\rpart of this book.\nA complete every-visit MC\ralgorithm for off-policy policy evaluation using weighted importance sampling\ris given in the next section on page 120.\nExample 5.4: Off-policy Estimation of a Blackjack\rState Value\nWe applied both ordinary and weighted\rimportance-sampling methods to estimate the value of a single blackjack state\rfrom off-policy data. Recall that one of the\n\r\r\rMean\nsquare\nerror\n(av6��age ov6�� 100 runs)\n\r\r\r\r\r\r\r\rFigure 5.4:\rWeighted importance sampling produces lower error estimates of the value of\ra single blackjack state from off-policy episodes (see Example 5.4).\n\r\r\r\r\r\r\r\r10,000\n\r\r\r\r\r\r\r\rEpisodes (log scale)\n\r\r\r\r\radvantages of Monte Carlo methods is\rthat they can be used to evaluate a single state without forming estimates for\rany other states. In this example, we evaluated the state in which the dealer\ris showing a deuce, the sum of the player��s cards is 13, and the player has a\rusable ace (that is, the player holds an ace and a deuce, or equivalently three\races). The data was generated by starting in this state then choosing to hit or\rstick at random with equal probability (the behavior policy). The target policy\rwas to stick only on a sum of 20 or 21, as in Example 5.1. The value of this\rstate under the target policy is approximately ��0.27726 (this was determined by\rseparately generating one-hundred million episodes using the target policy and\raveraging their returns). Both off-policy methods closely approximated this\rvalue after 1000 off-policy episodes using the random policy. To make sure they\rdid this reliably, we performed 100independent runs, each starting from estimates of zero and learning\rfor 10,000 episodes. Figure 5.4 shows the resultant learning curves��the squared\rerror of the estimates of each method as a function of number of episodes,\raveraged over the 100 runs. The error approaches zero for both algorithms, but\rthe weighted importance-sampling method has much lower error at the beginning,\ras is typical in practice.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExample 5.5: Infinite Variance\n\r\r\r110100100010,000100,0001,000,00010,000,000100,000,000\nEpisodes (log scale)\n\r\r\r\r\r\r\r\rFigure 5.5: Ordinary\rimportance sampling produces surprisingly unstable estimates on the\rone-state MDP shown inset (Example 5.5). The correct estimate here is 1 (7= 1), and, even though this is the expected\rvalue of a sample return (after importance sampling), the variance of the\rsamples is infinite, and the estimates do not convergence to this value.\rThese results are for off-policy first-visit MC.\n\r\r\r\r\r\r\r\r2\nMonte-Carlo\restimate of vn(s) with ordinary\rimportance 1 sampling\r(ten runs)\n0\n\r\r\r\r\rThe estimates of ordinary importance\rsampling will typically have infinite variance, and thus unsatisfactory\rconvergence properties, whenever the scaled returns have infinite variance��and\rthis can easily happen in off-policy learning when trajecto\u0026shy;ries contain loops.\rA simple example is shown inset in Figure 5.5. There is only one nonterminal\rstate s and two actions, right and left.\rThe right action\rcauses a deterministic transition to termination, whereas the left action transitions, with probability 0.9, back to s\ror, with probability 0.1, on to termination. The rewards are +1 on the latter\rtransition and otherwise zero. Consider the target policy that always selects left. All episodes under this policy consist of some\rnumber (possibly \n\r\rzero) of transitions back to s followed by termination with a reward and\rreturn of +1.\rThus the value of s under the target policy is 1 (7= 1). Suppose we are estimating this value\rfrom off-policy data using the behavior policy that selects right and left with equal probability.\nThe lower part of Figure 5.5\rshows ten independent runs of the first-visit MC algo\u0026shy;rithm using ordinary\rimportance sampling. Even after millions of episodes, the esti\u0026shy;mates fail to\rconverge to the correct value of 1. In contrast, the weighted importance-\rsampling algorithm would give an estimate of exactly 1everafter\rthe first episode that ended with the back action. All returns not equal to\r1 (that is, ending with the end action) would be inconsistent\rwith the target policy and thus would have a Pt��T(t)-1of zero and contribute neither to the numerator nor denominator of\r(5.5). The weighted importance-sampling algorithm produces a weighted average\rof only the returns consistent with the target policy, and all of these would\rbe exactly 1.\nWe can verify that the variance of the importance-sampling-scaled\rreturns is infi\u0026shy;nite in this example by a simple calculation. The variance of\rany random variable X is the expected value of the\rdeviation from its mean ����which can be written\nVar[X ] = E [(X ����)2j = E[X2�� 2X ��+ ��2] = E[X2]����2.\nThus, if the mean is finite, as it is in our case, the variance is\rinfinite if and only if the expectation of the square of the random variable is\rinfinite. Thus, we need onlyshow that the expected square of the importance-sampling-scaled return is\rinfinite:\n2_\nn(At|St)g��\n\r\r\r����=0\n\r\r\r\r\rb(At|St)\nTo compute this expectation, we break\rit down into cases based on episode length and termination. First note that,\rfor any episode ending with the end action, the importance sampling\rratio is zero, because the target policy would never take this action; these\repisodes thus contribute nothing to the expectation (the quantity in\rparenthesis will be zero) and can be ignored. We need only consider episodes\rthat involve some number (possibly zero) of back actions that transition back to\rthe nonterminal state, followed by a back action transitioning to\rtermination. All of these episodes have a return of 1, so the G��factor can be ignored. To get the expected square we need only\rconsider each length of episode, multiplying the probability of the episode��s\roccurrence by the square of its importance-sampling ratio, and add these up:\n\r\r\r\r\r(the\rlength 1episode)\n\r\r\r\r\r\r\r\r(the\rlength 3 episode)\n\r\r\r\r\r\r\r\r1\n2\n+\n+\n+\n\r\r\r\r\r\r\r\r11 ����\r0.9 ���� 22\n\r\r\r\r\r\r\r\r2\n\r\r\r\r\r\r\r\r11 ���� 0.9 �� �� �� 0.9 22\n\r\r\r\r\r\r\r\r1,111��\n���� 0.1I --------------------- \n2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0.5 0.5 0.5\n\r\r\r\r\r\r\r\r0.1(�A�A'\n0.5 0.5\n\r\r\r\r\r\r\r\r2\n\r\r\r\r\r\r\r\r2\n\r\r\r\r\r\r\r\r(the\rlength 2episode)\n\r\r\r\r\r\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\r\r\r\r\rE\nk=0\nE\nk=0\n\r\r\r\r\r\r\r\r0.9k\n1.8k\n\r\r\r\r\r\r\r\r2k\u0026#8226; 2\n\r\r\r\r\r\r\r\r0.1\n0.2\n\r\r\r\r\r\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\r.\n��\nExercise 5.3 What is the equation\ranalogous to (5.5) for action values Q(s, a) instead of\rstate values V(s), again given returns generated using b?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 5.4 In learning curves such\ras those shown in Figure 5.4 error generally decreases with training, as indeed\rhappened for the ordinary importance-sampling method. But for the weighted\rimportance-sampling method error first increased and then decreased. Why do you\rthink this happened?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 5.5\rThe results with Example 5.5 and shown in Figure 5.5 used a first- visit MC\rmethod. Suppose that instead an every-visit MC method was used on the same\rproblem. Would the variance of the estimator still be infinite? Why or why not?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n5.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rIncremental Implementation\nMonte Carlo prediction methods can be\rimplemented incrementally, on an episode- by-episode basis, using extensions of\rthe techniques described in Chapter 2(Sec\u0026shy;tion 2.4).\rWhereas in Chapter 2 we averaged rewards, in Monte\rCarlo methods we average returns. In all other respects\rexactly the same methods as used in Chapter 2 can be used for on-policy\rMonte Carlo methods. For off-policy Monte Carlo meth\u0026shy;ods,\rwe need to separately consider those that use ordinary\rimportance sampling and those that use weighted\rimportance sampling.\nIn ordinary importance\rsampling, the returns are scaled by the importance sam\u0026shy;pling ratio pt:T(t)_i\r(5.3), then simply averaged. For these methods we can again use the incremental\rmethods of Chapter 2, but using the scaled returns in place of the rewards of\rthat chapter. This leaves the case of off-policy methods using weighted importance sampling. Here we have to form a weighted\raverage of the returns, and a slightly different incremental algorithm is\rrequired.\nSuppose we have a sequence of returns Gi, G2,...,Gn-i, all starting\rin the same state and each with a corresponding random weight Wi (e.g., Wi =\rPt:r(t)_i). We wish to form the estimate\nwfcGfczn-i wfc\nand keep it up-to-date as we obtain a\rsingle additional return Gn. In addition to keeping track of Vn, we must\rmaintain for each state the cumulative sum Cn of the weights given to the first\rn returns. The update rule for Vn is\n\r\r\u0026nbsp;\n\r\r\r\r\rVn+1==\rVn + 7^\nCn\n\r\r\r\r\r\r\r\rGn ��\n\r\r\r\r\rn \u0026gt; 1,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (5.7)\n\r\r\u0026nbsp;\n\r\rand\nCn+1= Cn + Wn+1,\nwhere Co == 0 (and Vi is arbitrary and\rthus need not be specified). The box on the next page contains a complete\repisode-by-episode incremental algorithm for Monte Carlo policy evaluation. The\ralgorithm is nominally for the off-policy case, using weighted importance\rsampling, but applies as well to the on-policy case just by choosing the target\rand behavior policies as the same (in which case (n = b), W is always 1). The\rapproximation Q converges to qn (for all encountered state-action pairs) while actions are selected\raccording to a potentially different policy, b.\nExercise 5.6 Modify the algorithm for first-visit\rMC policy evaluation (Section 5.1) to use the incremental implementation for\rsample averages described in Section 2.4.\n��\nExercise 5.7 Derive the\rweighted-average update rule (5.7) from (5.6). Follow the pattern of the\rderivation of the unweighted rule (2.3).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nOff-policy MC prediction, for estimating Q^\rq^\nInput: an arbitrary target policy n\nInitialize, for all s G S, a G A(s):\nQ(s, a) ^ arbitrary C(s, a) ^ 0\nRepeat forever:\nb^ any policy\rwith coverage of n Generate an episode using b:\nSo, Ao, R1, \u0026#8226; \u0026#8226; \u0026#8226; , ST �� 1, AT �� 1,\u0026nbsp; , ST\nG ^ 0 W ^ 1\nFor t = T �� 1, T �� 2,...downto 0:\nG ^ 7G ʮRt+i C(St,At) ^ C(St, At) + W\nQ(SUAt) ^ Q(St, At)ʮc(St,At) [G �� Q(St7 At)]\nW ��W n(At)St)\nW^ Wb(At|St)\nIf W = 0 then ExitForLoop\n5.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rOff-policy Monte Carlo Control\nWe are now ready to present an example of the\rsecond class of learning control methods we consider in this book: off-policy\rmethods. Recall that the distinguishing feature of on-policy methods is that\rthey estimate the value of a policy while using it for control. In off-policy\rmethods these two functions are separated. The policy used to generate\rbehavior, called the behavior policy, may in fact be\runrelated to the policy that is evaluated and improved, called the target policy. An advantage of this separation is that the\rtarget policy may be deterministic (e.g., greedy), while the behavior policy\rcan continue to sample all possible actions.\nOff-policy Monte Carlo control methods use one of\rthe techniques presented in the preceding two sections. They follow the\rbehavior policy while learning about and improving the target policy. These\rtechniques require that the behavior policy has a nonzero probability of\rselecting all actions that might be selected by the target policy (coverage).\rTo explore all possibilities, we require that the behavior policy be soft\r(i.e., that it select all actions in all states with nonzero probability).\nThe box on the next page shows an off-policy\rMonte Carlo method, based on GPI and weighted importance sampling, for\restimating n* and q*. The target policy n ��n* is the greedy\rpolicy with respect to Q, which is an estimate of q^. The behavior policy b can\rbe anything, but in order to assure convergence of n to the optimal policy, an\rinfinite number of returns must be obtained for each pair of state and action.\rThis can be assured by choosing b to be e-soft. The policy n converges to\roptimal at all encountered states even though actions are selected according to\ra different soft policy b, which may change between or even within episodes.\n\r\rOff-policy MC control, for\restimating n ��n*\nInitialize, for all s G S, a G A(s):\nQ(s, a) ^ arbitrary C(s, a) ^ 0\nn(s) ^ argmaxaQ(St, a)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (with\rties broken consistently)\nRepeat forever:\nb ^ any soft policy Generate an episode using b:\nS0, A0, R1, \u0026#8226; \u0026#8226; \u0026#8226; ,�� 1,\u0026nbsp; �� 1,\nG ^ 0 W ^ 1\nFor t = T �� 1, T �� 2��... downto 0:\nG ^ yG ʮRt+i\nC(St,At) ^ C(St,\rAt) + W\nAt) ^ Q(St, At)ʮc(St,At) [G �� Q(St7 At)] n(St) ^ argmaxaQ(St, a)\u0026nbsp;\u0026nbsp;\u0026nbsp; (with\rties broken consistently)\nIf At =أ(St) then ExitForLoop W\r^ W��\nA potential\rproblem is that this method learns only from the tails of episodes, when all of\rthe remaining actions in the episode are greedy. If nongreedy actions are\rcommon, then learning will be slow, particularly for states appearing in the\rearly portions of long episodes. Potentially, this could greatly slow learning.\rThere has been insufficient experience with off-policy Monte Carlo methods to assess\rhow seri\u0026shy;ous this problem is. If it is serious, the most important way to\raddress it is probably by incorporating temporal-difference learning, the\ralgorithmic idea developed in the next chapter. Alternatively, if Y is less\rthan 1, then the idea developed in the next section may also help\rsignificantly.\nExercise 5.8: Racetrack (programming) Consider driving a race car around a turn like those\rshown in Figure 5.6. You want to go as fast as possible, but not so fast as to\rrun off the track. In our simplified racetrack, the car is at one of a discrete\rset of grid positions, the cells in the diagram. The velocity is also discrete,\ra number of grid cells moved horizontally and vertically per time step. The\ractions are increments to the velocity components. Each may be changed by +1,\r��1, or 0 in one step, for a total of nine actions. Both velocity components are\rrestricted to be nonnegative and less than 5, and they cannot both be zero\rexcept at the starting line. Each episode begins in one of the randomly\rselected start states with both velocity components zero and ends when the car\rcrosses the finish line. The rewards are ��1 for each step until the car crosses\rthe finish line. If the car hits the track boundary, it is moved back to a\rrandom position on the starting line, both velocity components are reduced to\rzero, and the episode continues. Before updating the car��s location at each\rtime step, check to see if the projected path of the car intersects the track\rboundary. If it intersects the finish line, the episode ends; if it intersects\ranywhere else, the car is\n\r\r\rStarting line\n\r\r\r\r\r\r\r\rFinish\nline\n\r\r\r\r\r\r\r\rFinish\nline\n\r\r\r\r\rStarting line\nFigure 5.6: A\rcouple of right turns for the racetrack task.\nconsidered\rto have hit the track boundary and is sent back to the starting line. To make\rthe task more challenging, with probability 0.1at each time step the velocity increments are both\rzero, independently of the intended increments. Apply a Monte Carlo control\rmethod to this task to compute the optimal policy from each starting state.\rExhibit several trajectories following the optimal policy (but turn the noise\roff for these trajectories).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n5.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\r*Discounting-aware Importance\rSampling\nThe off-policy\rmethods that we have considered so far are based on forming importance-\rsampling weights for returns considered as unitary wholes, without taking into\rac\u0026shy;count the returns�� internal structures as sums of discounted rewards. We now\rbriefly consider cutting-edge research ideas for using this structure to\rsignificantly reduce the variance of off-policy estimators.\nFor example, consider the case where episodes are long and 7is significantly less than 1. For concreteness, say\rthat episodes last 100 steps and that 7= 0. The return from time 0will then be just Go = R1, but its importance sampling ratio will be\na product of 100factors,ū��ح��)ū��1)�� �� ��ū9^9:). In ordinary importance sam\u0026shy;pling, the return will\rbe scaled by the entire product, but it is really only necessary\nto scale by the\rfirst factor, by\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; .\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; The\u0026nbsp;\u0026nbsp; other 99factorsTOST �� ��\r��\nare irrelevant\rbecause after the first reward the return has already been determined. These\rlater factors are all independent of the return and of expected value 1; they\rdo not change the expected update, but they add enormously to its variance. In\rsome cases they could even make the variance infinite. Let us now consider an\ridea for avoiding this large extraneous variance.\nThe essence of the idea is to think of discounting as determining a\rprobability of\n\r\rtermination or,\requivalently, a degree\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; of partial termination. Forany\u0026nbsp;\u0026nbsp; y G[0,1),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; we\ncan think of the\rreturn G0as partly\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; terminating\rin one step, tothedegree\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 1�� y,\nproducing a return of\rjust the first reward, Ri, and\ras partly terminating after two steps, to the degree (1�� y)y,\rproducing a return of Rl + R2, and so on. The latter degree corresponds to\rterminating on the second step, 1�� y, and\rnot having already terminated on the first step, y. The\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; degree\rof termination\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; onthe\u0026nbsp;\u0026nbsp;\u0026nbsp; thirdstep\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; is\u0026nbsp;\u0026nbsp;\u0026nbsp; thus\n(1�� y)y2,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; with the y2 reflecting that\u0026nbsp; termination\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; didnot occur\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; on\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; either\u0026nbsp;\u0026nbsp; of the\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; first\ntwo steps. The partial returns\rhere are called flat partial returns:\nGt:h\r=\rRt+i + Rt+2 + �� �� �� + ��a,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0^ t \u0026lt; h \u0026lt; T,\nwhere ��flat�� denotes the absence\rof discounting, and ��partial�� denotes that these returns do not extend all the\rway to termination but instead stop at h, called the horizon(and T is the\rtime of termination of the episode). The conventional full return Gt can be viewed as a sum of flat partial returns as\rsuggested above as follows:\nGt = Rt+i + YRt+2 + Y2 Rt+3 + �� �� �� + YT-t-1Rr =(1��Y )Rt+i\n+ (1 �� y)y (Rt+i + Rt+2)\n+ (1 �� y)y 2 (Rt+i + Rt+2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; + Rt+3)\n+ (1 �� y)yt-t-2(Rt+i + Rt+2+ �� + Rt -i)\n+ YT-t-i(Rt+i + Rt+2+ �� + Rt)\n-i\n=(1��Yh-t-iGGt��h + YT-t-iGGt��T.\nh=t+i\nNow we need to scale the flat partial returns by an importance\rsampling ratio that is similarly truncated. As GGt��h only involves rewards up to a horizon h, we only need the ratio of the probabilities up to h. We define an ordinary importance-sampling estimator, analogous to\r(5.4), as\n.St�T(s)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ((1\u0026nbsp; �� Y) ES+i1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Yh-t-ipt��h-i GGt��h+\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 7T (t)-t-ipt��T (t)-iGt��T (t))\nV (s)=--------------------------------------------------------------------- ,\n(5.8)\nand a weighted importance-sampling estimator, analogous to (5.5), as\nEt�T(s)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ((1�� Y)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 7h-t-ipt��h-iGt��h\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; +\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 7T(t)-t-ipt��T(t)-iGt��T(t))\nV(s)\nEteT(s)\r((1 �� T)ES+i1Yh-t-iPt��h-i + YT(t)-t-iPt��T(t)-i)\n\r\rWe call these two estimators discounting-aware\rimportance sampling estimators. They take into account the discount rate but\rhave no effect (are the same as the off-policy estimators from Section 5.5) if Y = 1.\n5.9\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\r*Per-reward Importance Sampling\nThere is one more way in which the structure of the return as a sum\rof rewards can be taken into account in off-policy importance sampling, a way\rthat may be able to reduce variance even in the absence of discounting (that\ris, even if Y = 1). In the off-policy\restimators (5.4) and (5.5), each term of the sum in the numerator is itself a\rsum:\nPt:T _iGt = Pt��T _1 (Rt+1 + YRt+2 +------------- + YT _t-1RT)\n=Pt:T _1Rt+1 + YPt��T _1Rt+2+-------------- + YT _t-1Pt:T _1RT \u0026#8226;------------------ (5.10)\nThe off-policy estimators rely on the expected values of these\rterms; let us see if we can write them in a simpler way. Note that each\rsub-term of (5.10) is a product of a random reward and a random\rimportance-sampling ratio. For example, the first sub-term can be written,\rusing (5.3), as\nR = n(At|St) n(At+1|St+1) n(At+2|St+2)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; n(AT_1|ST_1)R\nPt:T_1t+1= b(At|St) b(At+i|St+i) b(At+2|St+2) ... b(AT_i|ST_i) t+1.\nNow notice that, of all these factors, only the first and the last\r(the reward) are correlated; all the other ratios are independent random\rvariables whose expected value is one:\nn(Ak |Sfc) b(Ak |Sk)��\nThus, because the expectation of the product of independent random\rvariables is the product of their expectations, all the ratios except the first\rdrop out in expectation, leaving just\nE[pt:T_iRt+i] = E[pt:tRtʮi].\nIf we\rrepeat this analysis for the kth term of (5.10), we get E[pt:T_iRt+k] = E[pt:t+k_iRt+k].\nIt follows then that the expectation\rof our original term (5.10) can be written\n\r\r\rG t\n\r\r\r\r\rE[pt:T _iGt] = E where\nGt = Pt:tRt+1+ YPt:t+iRt+2+ Y 2Pt:t+2Rt+3 ++ YT_t_1Pt:T _iRt .\n\r\rWe call this idea per-reward importance\rsampling. It follows immediately that there is an alternate importance-sampling\restimator, with the same unbiased expectation as the OIS estimator (5.4), using\rGGt:\nV\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\r(s)= 5iffr,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (5.n)\nwhich we might expect to sometimes be of lower variance.\nIs there a per-reward version of weighted\rimportance sampling? This is less clear. So far, all the estimators that have\rbeen proposed for this that we know of are not consistent (that is, they do not\rconverge to the true value with infinite data).\n*Exercise 5.9 Modify the algorithm for\roff-policy Monte Carlo control (page 121) to use the idea of the truncated\rweighted-average estimator (5.9). Note that you will first need to convert this\requation to action values.��\n5.10\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rOff-policy Returns\nIt will turn out that a special form of return works best, producing\rupdates of the lowest variance. One of the most insightful way of writing it is\rin terms of a special error called the temporal-difference (TD) error. The TD\rerror is defined by\n5t = Rt ʮ1+ YV (St ʮ1) �� V (St),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (5.12)\nwhere V(s) is the estimate of the value of state s on this episode.\rUsing this, the state-value off-policy return is defined by\nGp = Yk-tPt��k ֪+ V (St)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (5.13)\n\r\r\u0026nbsp;\n\r\rThe corresponding Monte Carlo algorithm would be wait until the end\rof the episode, then go back over update\nV'(s)\r��ZteT(s)\rGt\nV\u0026nbsp;\u0026nbsp;\u0026nbsp; (s)=\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; |T(s)|\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ,\nan off-line algorithm. It would the time steps performing the\n(5.14)\n\r\r\u0026nbsp;\n\r\r5.11\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rSummary\nThe Monte Carlo methods presented in\rthis chapter learn value functions and op\u0026shy;timal policies from experience in the\rform of sample episodes. This gives them at least three\rkinds of advantages over DP methods. First, they can be used to learn optimal\rbehavior directly from interaction with the environment, with no model of the\renvironment��s dynamics. Second, they can be used with simulation or sample models. For surprisingly many applications it is easy\rto simulate sample episodes\neven though it\ris difficult to construct the kind of explicit model of transition proba\u0026shy;bilities\rrequired by DP methods. Third, it is easy and efficient to focusMonte Carlo methods on a small subset of the states.\rA region of special interest can be accurately evaluated without going to the\rexpense of accurately evaluating the rest of the state set (we explore this\rfurther in Chapter 8).\nA fourth advantage of Monte Carlo methods, which we discuss later in\rthe book, is that they may be less harmed by violations of the Markov property.\rThis is because they do not update their value estimates on the basis of the\rvalue estimates of successor states. In other words, it is because they do not\rbootstrap.\nIn designing Monte Carlo control methods we have followed the\roverall schema of generalized policy iteration(GPI) introduced in Chapter 4. GPI involves\rinteracting processes of policy evaluation and policy improvement. Monte Carlo\rmethods provide an alternative policy evaluation process. Rather than use a\rmodel to compute the value of each state, they simply average many returns that\rstart in the state. Because a state��s value is the expected return, this\raverage can become a good approximation to the value. In control methods we are\rparticularly interested in approximating action-value functions, because these\rcan be used to improve the policy without requiring a model of the\renvironment��s transition dynamics. Monte Carlo methods intermix policy\revaluation and policy improvement steps on an episode-by-episode basis, and can\rbe incrementally implemented on an episode-by-episode basis.\nMaintaining sufficient explorationis an issue in Monte Carlo control methods. It is not\renough just to select the actions currently estimated to be best, because then\rno returns will be obtained for alternative actions, and it may never be\rlearned that they are actually better. One approach is to ignore this problem\rby assuming that episodes begin with state-action pairs randomly selected to\rcover all possibilities. Such exploring startscan sometimes be arranged in applications with\rsimulated episodes, but are unlikely in learning from real experience. In on-policymethods, the agent commits to always exploring and\rtries to find the best policy that still explores. In off-policymethods, the agent also explores, but learns a\rdeterministic optimal policy that may be unrelated to the policy followed.\nOff-policy predictionrefers to learning the value function of a target policyfrom data generated by a different behavior policy. Such learning methods are based on some form of importance sampling, that is, on weighting returns by the ratio of the\rprobabilities of taking the observed actions under the two policies. Ordinary im\u0026shy;portance\rsamplinguses a simple average of\rthe weighted returns, whereas weighted importance samplinguses a weighted average. Ordinary importance\rsampling pro\u0026shy;duces unbiased estimates, but has larger, possibly infinite, variance,\rwhereas weighted importance sampling always has finite variance and is\rpreferred in practice. Despite their conceptual simplicity, off-policy Monte\rCarlo methods for both prediction and control remain unsettled and are a\rsubject of ongoing research.\nThe Monte\rCarlo methods treated in this chapter differ from the DP methods treated in the\rprevious chapter in two major ways. First, they operate on sample experience,\rand thus can be used for direct learning without a model. Second, they do not\rbootstrap. That is, they do not update their value estimates on the basis of\rother value estimates. These two differences are not tightly linked, and can be\rseparated. In the next chapter we consider methods that learn from experience,\rlike Monte Carlo methods, but also bootstrap, like DP methods.\nBibliographical and Historical Remarks\nThe term\r��Monte Carlo�� dates from the 1940s, when physicists at Los Alamos de\u0026shy;vised\rgames of chance that they could study to help understand complex physical\rphenomena relating to the atom bomb. Coverage of Monte Carlo methods in this\rsense can be found in several textbooks (e.g., Kalos and Whitlock, 1986;\rRubinstein, 1981).\nAn early use of Monte Carlo methods to estimate action values in a\rreinforcement learning context was by Michie and Chambers (1968). In pole\rbalancing (Example 3.4), they used averages of episode durations to assess the\rworth (expected balancing ��life��) of each possible action in each state, and\rthen used these assessments to control action selections. Their method is\rsimilar in spirit to Monte Carlo ES with every- visit MC estimates. Narendra\rand Wheeler (1986) studied a Monte Carlo method for ergodic finite Markov\rchains that used the return accumulated between successive visits to the same\rstate as a reward for adjusting a learning automaton��s action probabilities.\nBarto and Duff (1994) discussed policy evaluation in the context of\rclassical Monte Carlo algorithms for solving systems of linear equations. They\rused the analysis of Curtiss (1954) to point out the computational advantages\rof Monte Carlo policy eval\u0026shy;uation for large problems. Singh and Sutton (1996)\rdistinguished between every-visit and first-visit MC methods and proved results\rrelating these methods to reinforce\u0026shy;ment learning algorithms.\nThe blackjack example is based on an example used by Widrow, Gupta,\rand Maitra (1973). The soap bubble example is a classical Dirichlet problem\rwhose Monte Carlo solution was first proposed by Kakutani (1945; see Hersh and\rGriego, 1969; Doyle and Snell, 1984). The racetrack exercise is adapted from\rBarto, Bradtke, and Singh (1995), and from Gardner (1973).\nMonte Carlo ES was introduced in the 1998 edition of this book. That\rmay have been the first explicit connection between Monte Carlo estimation and\rcontrol methods based on policy iteration.\nEfficient off-policy learning has become recognized as an important\rchallenge that arises in several fields. For example, it is closely related to\rthe idea of ��interventions�� and ��counterfactuals�� in probabalistic graphical\r(Bayesian) models (e.g., Pearl, 1995; Balke and Pearl, 1994). Off-policy\rmethods using importance sampling have a long history and yet still are not\rwell understood. Weighted importance sampling, which is also sometimes called\rnormalized importance sampling (e.g., Koller and Friedman, 2009), is discussed\rby Rubinstein (1981), Hesterberg (1988), Shelton (2001), and Liu (2001) among others.\nOur treatment of the idea of discounting-aware importance sampling\ris based on the analysis of Sutton, Mahmood, Precup, and van Hasselt (2014). It\rhas been worked out most fully to date by Mahmood (in preparation; Mahmood, van\rHasselt, and Sutton, 2014). Per-reward importance sampling was introduced by\rPrecup, Sutton, and Singh (2000), who called it ��per-decision�� importance sampling.\rThese works also combine off-policy learning with temporal-difference learning,\religibility traces, and approximation methods, introducing subtle issues that\rwe consider in later chapters.\nThe target policy in\roff-policy learning is sometimes referred to in the literature as the\r��estimation�� policy, as it was in the first edition of this book.\n\r\rChapter 6\nTemporal-Difference Learning\nIf one had to identify one idea as\rcentral and novel to reinforcement learning, it would undoubtedly be temporal-difference (TD) learning. TD learning is a\rcombination of Monte Carlo ideas and dynamic programming (DP) ideas. Like Monte\rCarlo methods, TD methods can learn directly from raw experience without a\rmodel of the environment��s dynamics. Like DP, TD methods update estimates based\rin part on other learned estimates, without waiting for a final outcome (they\rbootstrap). The relationship between TD, DP, and Monte Carlo methods is a\rrecurring theme in the theory of reinforcement learning; this chapter is the\rbeginning of our exploration of it. Before we are done, we will see that these\rideas and methods blend into each other and can be combined in many ways. In\rparticular, in Chapter 7 we introduce n-step algorithms, which provide a bridge\rfrom TD to Monte Carlo methods, and in Chapter 12 we introduce the TD(A)\ralgorithm, which seamlessly unifies them.\nAs usual, we start by focusing on the policy evaluation or prediction problem, that of estimating the value function v^\rfor a given policy n. For the control problem (finding\ran optimal policy), DP, TD, and Monte Carlo methods all use some variation of\rgeneralized policy iteration (GPI). The differences in the methods are\rprimarily differences in their approaches to the prediction problem.\n6.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rTD Prediction\nBoth TD and Monte Carlo methods use experience to solve the\rprediction problem. Given some experience following a policy n, both methods\rupdate their estimate V of vn for the nonterminal states St occurring in that\rexperience. Roughly speaking, Monte Carlo methods wait until the return\rfollowing the visit is known, then use that return as a target for V(St). A\rsimple every-visit Monte Carlo method suitable for nonstationary environments\ris\nV(St) ^ V(St) + a[Gt �� V(St)],\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (6.1)\nwhere Gt is the actual return following time t, and a is a constant step-size parameter (c.f., Equation 2.4). Let\rus call this method constant-a MC. Whereas Monte Carlo\rmethods must wait until the end of the episode to determine the increment to\rV(St) (only then is Gt known), TD methods need to wait only until the next time\rstep. At time t + 1they immediately form a target and make\ra useful update using the observed reward Rt+i and the estimate V(St+i). The\rsimplest TD method makes the update\nV(St) ^ V(St) + a [Rt+i + YV(St+i) ��\rV(St)]\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (6.2)\nimmediately on transition to St+i and receiving Rt+i. In effect, the\rtarget for the Monte Carlo update is Gt, whereas the target for the TD update\ris Rt+i + yV(St+i). This TD method is called TD(0), or one-step TD, because it is a special case of the TD(A) and\rn-step TD methods developed in Chapter 12 and Chapter 7. The box below\rspecifies TD(0) completely in procedural form.\n\rTabular\rTD(0) for estimating v^\nInput: the policy n to be evaluated\n\r\u0026nbsp;\n\r\rInitialize V (s) arbitrarily (e.g., V (s)\n\r=0, Vs G S+)\n\r\rRepeat (for\reach episode):\n\r\u0026nbsp;\n\r\rInitialize\rS\n\r\u0026nbsp;\n\r\rRepeat (for each step of episode):\n\r\u0026nbsp;\n\r\rA ^ action given by n for S\n\r\u0026nbsp;\n\r\rTake action\rA, observe R, S7\n\r\u0026nbsp;\n\r\rV(S) ^ V(S)ʮa[R ʮyV(S7)\n\r��V (S)]\n\r\rS ^ S7\n\r\u0026nbsp;\n\r\runtil S is\rterminal\n\r\u0026nbsp;\n\r\r\r\r\r\r\r\r\u0026nbsp;\nBecause the TD(0) bases its\rupdate in part on an existing estimate, we say that it is a bootstrapping\rmethod, like DP. We know from Chapter 3 that\nvn(s) == En[Gt | St = s]\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (6.3)\n=En[Rt+i + YGt+1| St = s]\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (from\r(3.3))\n=En[Rt+1+ Yvn(St+i) | St = s].\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (6.4)\nRoughly speaking, Monte Carlo methods use an\restimate of (6.3) as a target, whereas DP methods use an estimate of (6.4) as a\rtarget. The Monte Carlo target is an estimate because the expected value in\r(6.3) is not known; a sample return is used in place of the real expected\rreturn. The DP target is an estimate not because of the expected values, which\rare assumed to be completely provided by a model of the environment, but\rbecause v^(St+i) is not known and the current estimate, V(St+i), is used instead. The TD target is an estimate for both reasons:\rit samples the expected values in (6.4) and it uses the\rcurrent estimate V instead of the true v^. Thus, TD\rmethods combine the sampling of Monte Carlo with the bootstrapping of DP. As we\rshall see, with care and imagination this can take us a long way toward obtaining\rthe advantages of both Monte Carlo and DP methods.\n\r\r\u0026nbsp;TOC \\o \u0026quot;1-5\u0026quot; \\h \\z The\rdiagram to the right is the backup diagram for tabular TD(0). The\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Q\nvalue\restimate for the state node at the top of the backup diagram is up-\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; {\ndated\ron the basis of the one sample transition from it to the immediately\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; |\nfollowing\rstate. We refer to TD and Monte Carlo updates as sample back-\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nupsbecause they involve looking\rahead to a sample successor state (or\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; TD(0)\nstate-action\rpair), using the value of the successor and the reward along the way to compute\ra backed-up value, and then changing the value of the original state (or\rstate-action pair) accordingly. Sample backups differ\rfrom the full backups of DP methods in that they are\rbased on a single sample successor rather than on a complete distribution of\rall possible successors.\nFinally, note that the quantity in brackets in the TD(0) update is a\rsort of error, measuring the difference between the estimated value of St and\rthe better estimate Rtʮ1+ yV(Stʮ1). This quantity, called the TD error,\rarises in various forms through\u0026shy;out reinforcement learning:\n5t ==Rt\rʮ1+ YV (St ʮ1) ��V (St).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (6.5)\nNotice that the TD\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; error\rat each time is theerror in the estimate made at\u0026nbsp;\u0026nbsp; thattime.\nBecause the TD error depends on the next state and next reward, it\ris not actually available until one time step later. That is, \u0026amp; is the\rerror in V(St), available at time t + 1. Also note that if the array V does not\rchange during the episode (as it does not in Monte Carlo methods), then the\rMonte Carlo error can be written as a sum of TD errors:\nGt �� V(St) = Rt+1+ 7^+1 �� V(St) + 7V(St+1) ��7V(St+1)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (from\r(3.3))\n=^t + 7(Gt.1�� V( (St.1))\n=\u0026amp; + 7\u0026#12316;ʮ1 + 72(Gt.2�� V (Sm))\n=^t + 7\u0026amp;ʮ1+ 72\u0026amp;ʮ2\u0026nbsp; +\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; �� �� ��\u0026nbsp;\u0026nbsp; +\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 7T-t-1^T-1+ 7T-t (Gt �� V (St ))\n=^t + 7\u0026#12316;ʮ1+ 7%ʮ2\u0026nbsp; +\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ������\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; +\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 7T-t-1^T-1+ 7T-t(0�� 0)\nT -1\nH\u0026#8226;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (6.6)\nk\nThis identity is not exact if V is updated during the episode (as it\ris in TD(0)), but if the step size is small then it may still hold approximately.\rGeneralizations of this identity play an important role in the theory and\ralgorithms of temporal-difference learning.\nExercise 6.1 If V changes during the\repisode, then (6.6) only holds approximately; what would the difference be between the\rtwo sides? Let Vt denote the array of state values used at time t in the TD\rerror (6.5) and in the TD update (6.2). Redo the derivation above to determine\rthe additional amount that must be added to the sum of TD errors in order to\requal the Monte Carlo error.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExample 6.1: Driving Home Each day as you drive\rhome from work, you try to predict how long it will take to get home. When you\rleave your office, you note thetime, the day\rof week, the weather, and anything else that might be relevant. Say on this\rFriday you are leaving at exactly 6pm, and you estimate that it will take 30\rminutes to get home. As you reach your car it is 6:05, and you notice it is\rstarting to rain. Traffic is often slower in the rain, so you reestimate that\rit will take 35 minutes from then, or a total of 40 minutes. Fifteen minutes\rlater you have completed the highway portion of your journey in good time. As\ryou exit onto a secondary road you cut your estimate of total travel time to 35\rminutes. Unfortunately, at this point you get stuck behind a slow truck, and\rthe road is too narrow to pass. You end up having to follow the truck until you\rturn onto the side street where you live at 6:40. Three minutes later you are\rhome. The sequence of states, times, and predictions is thus as follows:\nElapsed Time Predicted\rPredicted State(minutes)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Time\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; to\u0026nbsp;\u0026nbsp;\u0026nbsp; Go\u0026nbsp;\u0026nbsp; Total\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Time\nleaving office, friday at 6\n\r0\n\r30\n\r30\n\r\rreach car, raining\n\r5\n\r35\n\r40\n\r\rexiting highway\n\r20\n\r15\n\r35\n\r\r2ndary road, behind truck\n\r30\n\r10\n\r40\n\r\rentering home street\n\r40\n\r3\n\r43\n\r\rarrive home\n\r43\n\r0\n\r43\n\r\r\r\u0026nbsp;\nThe rewards in\rthis example are the elapsed times on each leg of the journey.[10]\rWe are not discounting (y = 1), and thus the return for each state is the actual time to go from\rthat state. The value of each state is the expectedtime to go. The second column of numbers gives the current estimated\rvalue for each state encountered.\nA simple way to view the operation of Monte Carlo methods is to plot\rthe predicted total time (the last column) over the sequence, as in Figure 6.1\r(left). The arrows\nshow the changes in predictions\rrecommended by the constant-a MC method (6.1), for a = 1. These are exactly the\rerrors between the estimated value (predicted time to go) in each state and the\ractual return (actual time to go). For example, when you exited the highway you\rthought it would take only 15 minutes more to get home, but in fact it took 23\rminutes. Equation 6.1 applies at this point and determines an increment in the\restimate of time to go after exiting the highway. The error, Gt �� V(St), at\rthis time is eight minutes. Suppose the step-size parameter, a, is 1/2. Then the predicted time to go after exiting the highway would be\rrevised upward by four minutes as a result of this experience. This is probably\rtoo large a change in this case; the truck was probably just an unlucky break.\rIn any event, the change can only be made off-line, that is, after you have\rreached home. Only at this point do you know any of the actual returns.\nIs it necessary to wait until\rthe final outcome is known before learning can begin? Suppose on another day\ryou again estimate when leaving your office that it will take 30 minutes to\rdrive home, but then you become stuck in a massive traffic jam. Twenty-five\rminutes after leaving the office you are still bumper-to-bumper on the highway.\rYou now estimate that it will take another 25 minutes to get home, for a total\rof 50 minutes. As you wait in traffic, you already know that your initial\restimate of 30 minutes was too optimistic. Must you wait until you get home\rbefore increasing your estimate for the initial state? According to the Monte\rCarlo approach you must, because you don��t yet know the true return.\nAccording to a TD approach, on\rthe other hand, you would learn immediately, shifting your initial estimate\rfrom 30 minutes toward 50. In fact, each estimate would be shifted toward the\restimate that immediately follows it. Returning to our first day of driving,\rFigure 6.1 (right) shows the changes in the predictions recommended by the TD\rrule (6.2) (these are the changes made by the rule if a = 1). Each error is\rproportional to the change over time of the prediction, that is, to the temporal differences in predictions.\nBesides giving you something to do\rwhile waiting in traffic, there are several com\u0026shy;putational reasons why it is advantageous\rto learn based on your current predictions rather than waiting until\rtermination when you know the actual return. We briefly discuss some of these\rnext.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n6.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rAdvantages of TD Prediction\rMethods\nTD methods learn their estimates in\rpart on the basis of other estimates. They learn a guess from a guess��they bootstrap. Is this a good thing to do? What advantages do TD\rmethods have over Monte Carlo and DP methods? Developing and answering such\rquestions will take the rest of this book and more. In this section we briefly\ranticipate some of the answers.\nObviously, TD methods have an\radvantage over DP methods in that they do not require a model of the\renvironment, of its reward and next-state probability distributions.\nThe next most obvious advantage of TD methods over Monte Carlo\rmethods is that they are naturally implemented in an on-line, fully incremental\rfashion. With Monte Carlo methods one must wait until the end of an episode,\rbecause only then is the return known, whereas with TD methods one need wait\ronly one time step. Surprisingly often this turns out to be a critical\rconsideration. Some applications have very long episodes, so that delaying all\rlearning until an episode��s end is too slow. Other applications are continuing\rtasks and have no episodes at all. Finally, as we noted in the previous\rchapter, some Monte Carlo methods must ignore or discount episodes on which\rexperimental actions are taken, which can greatly slow learning. TD methods are\rmuch less susceptible to these problems because they learn from each transition\rregardless of what subsequent actions are taken.\nBut are TD methods sound? Certainly it is convenient to learn one\rguess from the next, without waiting for an actual outcome, but can we still\rguarantee convergence to the correct answer? Happily, the answer is yes. For\rany fixed policy n, TD(0) has been proved to converge to Vn, in the mean for a\rconstant step-size parameter if it is sufficiently small, and with probability 1if the step-size parameter decreases according to\rthe usual stochastic approximation conditions (2.7). Most convergence proofs\rapply only to the table-based case of the algorithm presented above (6.2), but some also apply to the case of general linear function\rapproximation. These results are discussed in a more general setting in Chapter\r9.\nIf both TD and\rMonte Carlo methods converge asymptotically to the correct pre\u0026shy;dictions, then a\rnatural next question is ��Which gets there first?�� In other words, which method\rlearns faster? Which makes the more efficient use of limited data? At the\rcurrent time this is an open question in the sense that no one has been able to\rprove mathematically that one method converges faster than the other. In fact,\rit is not even clear what is the most appropriate formal way to phrase this\rquestion! In practice, however, TD methods have usually been found to converge\rfaster than constant-a MC methods on stochastic tasks, as illustrated in\rExample 6.2.\nExercise 6.2 This is an exercise to help develop your intuition about why TD\rmethods are often more efficient than Monte Carlo methods. Consider the driving\rhome example and how it is addressed by TD and Monte Carlo methods. Can you\rimagine a scenario in which a TD update would be better on average than a Monte\rCarlo update? Give an example scenario��a description of past experience and a\rcurrent state��in which you would expect the TD update to be better. Here��s a\rhint: Suppose you have lots of experience driving home from work. Then you move\rto a new building and a new parking lot (but you still enter the highway at the\rsame place). Now you are starting to learn predictions for the new building.\rCan you see why TD updates are likely to be much better, at least initially, in\rthis case? Might the same sort of thing happen in the original task?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 6.3 From Figure 6.2 (left) it appears that the first\repisode results in a change in only V(A). What does this tell you about what happened on\rthe first episode? Why was only the estimate for this one state changed? By\rexactly how much was it changed?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExample 6.2: Random Walk In this example we empirically compare the prediction abilities of\rTD(0) and constant-a MC applied to the small Markov reward process shown in the\rupper part of the figure below. All episodes start in the center state, C, and\rproceed either left or right by one state on each step, with equal probability.\rThis behavior can be thought of as due to the combined effect of a fixed policy\rand an environment��s state-transition probabilities, but we do not care which;\rwe are concerned only with predicting returns however they are generated.\rEpisodes terminate either on the extreme left or the extreme right. When an\repisode terminates on the right, a reward of +1 occurs; all other rewards are\rzero. For example, a typical epsiode might consist of the following\rstate-and-reward sequence: C, 0, B, 0, C, 0, D, 0, E, 1. Because this task is undiscounted, the true value of each state is\rthe probability of terminating on the right if starting from that state. Thus,\rthe true value of the center state is v^(C) = 0.5. The true values of all the\rstates, A through E, are 1, |, |, |, and |. The left part of Figure 6.2 shows the\rvalues learned by TD(0) approaching the true values as more episodes are\rexperienced. Averaging over many episode sequences, the right part of the\rfigure shows the average error in the predictions found by TD(0) and constant-a\rMC, for a variety of values of a, as a function of number of episodes. In all\rcases the approximate value function was initialized to the intermediate value\rV(s) = 0.5, for all s. The TD method was consistently better than the MC method\ron this task.\nstart\n\r\n\r\r\r\r\r\u0026nbsp;\nState\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Walks\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; /\u0026nbsp; Episodes\nFigure 6.2: Results with the 5-state random walk. Above: The small\rMarkov reward process generating the episodes. Left: Results from a single run\rafter various numbers of episodes. The estimate after 100 episodes is about as\rclose as they ever get to the true values; with a constant step-size parameter\r(a = 0.1in this example), the values fluctuate indefinitely in response to\rthe outcomes of the most recent episodes. Right: Learning curves for TD(0) and\rconstant-a MC methods, for various values of a. The performance measure shown\ris the root mean-squared (RMS) error between the value function learned and the\rtrue value function, averaged over the five states. These data are averages\rover 100 different sequences of episodes.\n��\nExercise\r6.4 The specific results shown in Figure 6.2\r(right) are dependent on the value of the step-size parameter, a. Do you think\rthe conclusions about which algorithm is better would be affected if a wider\rrange of a values were used? Is there a different, fixed value of a at which\reither algorithm would have performed significantly better than shown? Why or\rwhy not?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n*Exercise 6.5 In Figure 6.2 (right)\rthe RMS error of the TD method seems to go down and then up again, particularly\rat high a��s. What could have caused this? Do you think this always occurs, or\rmight it be a function of how the approximate value function was initialized?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 6.6 Above we stated that the true values for the random walk task are 6,6,��,6, and ��,forstates A through\rE. Describe at least two different ways that these could have been\rcomputed. Which would you guess we actually used? Why?\n��\n6.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rOptimality of TD(0)\nSuppose there is available only a finite amount of experience, say\r10 episodes or 100 time steps. In this case, a common approach with incremental\rlearning methods is to present the experience repeatedly until the method\rconverges upon an answer. Given an approximate value function, V, the\rincrements specified by (6.1) or (6.2) are computed for every time step t at\rwhich a nonterminal state is visited, but the value function is changed only\ronce, by the sum of all the increments. Then all the available experience is\rprocessed again with the new value function to produce a new overall increment,\rand so on, until the value function converges. We call this batch\rupdating because updates are made only after processing each complete batch of training data.\nUnder batch updating, TD(0) converges deterministically to a single\ranswer in\u0026shy;dependent of the step-size parameter, a, as long as a is chosen to be\rsufficiently small. The constant-a MC method also converges deterministically\runder the same conditions, but to a different answer. Understanding these two\ranswers will help us understand the difference between the two methods. Under\rnormal updating the methods do not move all the way to their respective batch\ranswers, but in some sense they take steps in these directions. Before trying\rto understand the two answers in general, for all possible tasks, we first look\rat a few examples.\nExample 6.3: Random\rwalk under batch updating Batch-updating\rversions of TD(0) and constant-a MC were applied as follows to the random walk\rpredic\u0026shy;tion example (Example 6.2). After each new episode, all episodes seen so far were treated as\ra batch. They were repeatedly presented to the algorithm, either TD(0) or\rconstant-a MC, with a sufficiently small that the value function converged. The\rre\u0026shy;sulting value function was then compared with v^, and the average root\rmean-squared error across the five states (and across 100independent repetitions of the whole ex\u0026shy;periment) was plotted to obtain the\rlearning curves shown in Figure 6.3. Note that the batch TD method was\rconsistently better than the batch Monte Carlo method.\n\r\r\r\n\r\r\r\r\r\rRMS error, averaged over states\n\r\r\r\r\r\rWalks / Episodes\n\r\r\r\r\r\rFigure\r6.3: Performance of TD(0) and constant-a MC under batch training on the\rrandom walk task.\n\r\r\r\r\r\u0026nbsp;\nUnder batch training,\rconstant-a MC converges to values, V(s), that are sample averages of the actual\rreturns experienced after visiting each state s. These are optimal estimates in\rthe sense that they minimize the mean-squared error from the actual returns in\rthe training set. In this sense it is surprising that the batch TD method was\rable to perform better according to the root mean-squared error measure shown\rin Figure 6.3. How is it that batch TD was able to perform better than this\roptimal method? The answer is that the Monte Carlo method is optimal only in a\rlimited way, and that TD is optimal in a way that is more relevant to\rpredicting returns. But first let��s develop our intuitions about different\rkinds of optimality through another example. Consider Example 6.4, on the next\rpage.\nExample 6.4 illustrates a general difference\rbetween the estimates found by batch TD(0) and batch Monte Carlo methods. Batch\rMonte Carlo methods always find the estimates that minimize mean-squared error\ron the training set, whereas batch TD(0) always finds the estimates that would\rbe exactly correct for the maximum-likelihood model of the Markov process. In\rgeneral, the maximum-likelihood estimate of a parameter\ris the parameter value whose probability of generating the data is greatest. In\rthis case, the maximum-likelihood estimate is the model of the Markov process\rformed in the obvious way from the observed episodes: the estimated transition\rprobability from i to j is the\rfraction of observed transitions from i that went to j,\rand the associated expected reward is the average of the rewards observed on\rthose transitions. Given this model, we can compute the estimate of the value\rfunction that would be exactly correct if the model were exactly correct. This\ris called the certainty-equivalence estimate because it\ris equivalent to assuming that the estimate of the underlying process was known\rwith certainty rather than being approximated. In general, batch TD(0)\rconverges to the certainty-equivalence estimate.\nThis helps explain why TD methods converge more\rquickly than Monte Carlo methods. In batch form, TD(0) is faster than Monte\rCarlo methods because it com-\nExample 6.4 You are the\rPredictor\nPlace\ryourself now in the role of the predictor of returns for an unknown Markov\rreward process. Suppose you observe the following eight episodes:\n\r\r\r\r\rA,0,B,\r0\nB,1\rB, 1 B, 1\n\r\r\r\r\r\r\r\r1110\rB, B, B,\rB,\n\r\r\r\r\r\r\r\r\r\r\r\r\r\r\u0026nbsp;\n\u0026nbsp;\n\rThis means that the first episode\rstarted in state A, transitioned to B with a reward of 0, and then\rterminated from B with a reward of 0. The other seven episodes were even shorter,\rstarting from B and terminating immediately. Given this batch of data, what would\ryou say are the optimal predictions, the best values for the estimates V(A) and V(B)?\rEveryone would probably agree that the optimal value for V(B) is |,\rbecause six out of the eight times in state B the process terminated immediately\rwith a return of 1, and the other two times in B the\rprocess terminated immediately with a return of 0.\nBut what is the optimal value for the estimate V(A) given\rthis data? Here there are two reasonable answers. One is to observe that 100%\rof the times the process was in state A it traversed immediately to B (with a\rreward of 0); and since we have already decided that B has\rvalue |, therefore A must have value 4 as well. One way of viewing this answer is that it\ris based on first modeling the Markov process,\nin this case as shown to the right,\rand then computing the correct estimates given the model, which indeed in this\rcase gives V(A) = 4. This is also the answer that batch TD(0) gives.\nThe\rother reasonable answer is simply to observe that we have seen A once\rand the return that followed it was 0; we therefore estimate V(A) as 0.\rThis is the answer that batch Monte Carlo methods give. Notice that it is also\rthe answer that gives minimum squared error on the training data. In fact, it gives\rzero error on the data. But still we expect the first answer to be better. If\rthe process is Markov, we expect that the first answer will produce lower error\ron future data, even though the Monte Carlo answer is\rbetter on the existing data.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��putes the true certainty-equivalence estimate. This explains the advantage of\rTD(0) shown in the batch results on the random walk task (Figure 6.3). The\rrelationship to the certainty-equivalence estimate may also explain in part the\rspeed advantage of nonbatch TD(0) (e.g., Figure 6.2, right). Although the\rnonbatch methods do not achieve either the certainty-equivalence or the minimum\rsquared-error estimates, they can be understood as moving roughly in these\rdirections. Nonbatch TD(0) may be faster than constant-a MC because it is\rmoving toward a better estimate, even though it is not getting all the way\rthere. At the current time nothing more definite can be said about the relative\refficiency of on-line TD and Monte Carlo methods.\nFinally, it is worth noting that although the certainty-equivalence\restimate is in some sense an optimal solution, it is almost never feasible to\rcompute it directly. If N is the number of states, then\rjust forming the maximum-likelihood estimate of the process may require N2memory, and computing the corresponding value function requires on\rthe order of N3computational steps if done\rconventionally. In these terms it is indeed striking that TD methods can\rapproximate the same solution using memory no more than N and repeated\rcomputations over the training set. On tasks with large state spaces, TD\rmethods may be the only feasible way of approximating the certainty-equivalence\rsolution.\nsKExercise\r6.7 Design an off-policy version of the TD(0) update that can be used with\rarbitrary target policy n and covering behavior policy b, using at each step t\rthe importance sampling ratio pt��t(5.3).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n6.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rSarsa: On-policy TD Control\nWe turn now to the use of TD prediction methods\rfor the control problem. As usual, we follow the pattern of generalized policy\riteration (GPI), only this time using TD methods for the evaluation or\rprediction part. As with Monte Carlo methods, we face the need to trade off exploration\rand exploitation, and again approaches fall into two main classes: on-policy\rand off-policy. In this section we present an on-policy TD control method.\nThe first step is to learn an action-value function rather than a\rstate-value function. In particular, for an on-policy method we must estimate\r(s, a) for the current behavior policy n and for all states s and actions a.\rThis can be done using essentially the same TD method described above for\rlearning Vn. Recall that an episode consists of an alternating sequence of\rstates and state-action pairs:\n\r\n\r\r\r\r\r\u0026nbsp;\nIn the previous section we considered transitions\rfrom state to state and learned the values of states. Now we consider\rtransitions from state-action pair to state-action pair, and learn the values\rof state-action pairs. Formally these cases are identical: they are both Markov\rchains with a reward process. The theorems assuring the convergence of state\rvalues under TD(0) also apply to the corresponding algorithmfor action values:\n\r\r\r(6.7)\n\r\r\r\r\rQ(St, At)Q(St, At) + a\rRt+i + 7Q(St+i, At+i) �� Q(St,\rAt)\n\r\r\rSarsa\n\r\r\r\r\rThis update is done after every transition from a\rnonterminal state St. If St+i is terminal, then Q(St+i, At+i) is defined as\rzero. This rule uses every element of the quintuple of events, (St, At, Rt+i,\rSt+i, At+i), that make up a transition from one state-action pair to the next.\rThis quintuple gives rise to the name Sarsa for the\ralgorithm. The backup diagram for Sarsa is as shown to the right.\n\r\r\u0026nbsp;\n\r\rExercise 6.8Show that an action-value version of\r(6.6) holds for the action-value form of the TD error \u0026#12316;=Rt+i+ YQ(St+i,\rAt+i �� Q(St, At), again assuming that the values don��t change from step to\rstep.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nIt is straightforward to design an on-policy control algorithm based\ron the Sarsa prediction method. As in all on-policy methods, we continually\restimate for the behavior policy n, and at the same time change n toward\rgreediness with respect to qn. The general form of the Sarsa control algorithm\ris given in the box below.\nSarsa (on-policy TD control) for estimating Q ^ q*\nInitialize\rQ(s, a), Vs G S, a G A(s), arbitrarily, and Q(terminal-state,\u0026#8226;) = 0 Repeat (for each episode):\nInitialize\rS\nChoose\rA from S using policy derived from Q (e.g., e-greedy)\nRepeat\r(for each step of episode):\nTake\raction A, observe R, S7\nChoose\rA7 from S7 using policy derived from Q (e.g., e-greedy)\nQ(S,\rA) ^ Q(S,A)+ a[R + 7Q(S7, A7) �� Q(S, A)]\nS ^ S7; A ^ A7;\runtil S is terminal\nThe convergence properties of the Sarsa algorithm depend on the\rnature of the policy��s dependence on Q. For example, one could use e-greedy or\re-soft policies. According to Satinder Singh (personal communication), Sarsa\rconverges with prob\u0026shy;ability 1to an optimal policy\rand action-value function as long as all state-action pairs are visited an\rinfinite number of times and the policy converges in the limit to the greedy\rpolicy (which can be arranged, for example, with e-greedy policies by setting e\r= 1/t), but this result has not yet been published in the literature.\nExample 6.5: Windy Gridworld Shown inset in\rFigure 6.4 is a standard grid- world, with start and goal states, but with one\rdifference: there is a crosswind up\u0026shy;ward through the middle of the grid. The\ractions are the standard four��up, down, right, and left��but in the middle\rregion the resultant next states are shifted up\u0026shy;ward by a ��wind,�� the strength\rof which varies from column to column. The strength of the wind is given below\reach column, in number of cells shifted upward. For ex\u0026shy;ample, if you are one\rcell to the right of the goal, then the action left\rtakes you to\n\r\n\r\r\r\r\r\u0026nbsp;\nTime steps\n\r\r\rFigure 6.4: altered by a also shown.\n\r\r\r\r\rResults of Sarsa applied to a\rgridworld (shown inset) in which movement is location-dependent, upward ��wind.��\rA trajectory under the optimal policy is\nthe cell just above the goal. Let\rus treat this as an undiscounted episodic task, with constant rewards of ��1until the goal state is reached.\nThe graph in Figure 6.4 shows the results of applying e-greedy Sarsa\rto this task, with e= 0.1,\ra = 0.5, and the initial values Q(s, a) = 0 for all s, a. The increasing slope\rof the graph shows that the goal is reached more and more quickly over time. By\r8000 time steps, the greedy policy was long since optimal (a trajectory from it\ris shown inset); continued e-greedy exploration kept the average episode length\rat about 17 steps, two more than the minimum of 15. Note that Monte Carlo\rmethods cannot easily be used on this task because termination is not\rguaranteed for all policies. If a policy was ever found that caused the agent\rto stay in the same state, then the next episode would never end. Step-by-step\rlearning methods such as Sarsa do not have this problem because they quickly\rlearn during the episodethat\rsuch policies are poor, and switch to something else.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 6.9: Windy Gridworld with King��sMovesRe-solve the windy gridworld task assuming eight\rpossible actions, including the diagonal moves, rather than the usual four. How\rmuch better can you do with the extra actions? Can you do even better by\rincluding a ninth action that causes no movement at all other than that caused\rby the wind?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 6.10: Stochastic WindRe-solve the windy gridworld task with King��s moves,\rassuming that the effect of the wind, if there is any, is stochastic, sometimes\rvarying by 1 from the mean values given for each column. That is, a third of\rthe time you move exactly according to these values, as in the previous\rexercise, but also a third of the time you move one cell above that, and\ranother third of the time you move one cell below that. For example, if you are\rone cell to the right of the goal and you move left, then one-third of the time you move one cell above\rthe goal, one-third of the time you move two cells above the goal, and\rone-third of the time\n\u0026nbsp;TOC \\o \u0026quot;1-5\u0026quot; \\h \\z you\rmove to the goal.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n6.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rQ-learning: Off-policy TD\rControl\nOne of the early breakthroughs in reinforcement\rlearning was the development of an off-policy TD control algorithm known as Q-learning (Watkins, 1989), defined by\nQ(St, At) ^ Q(St, At) + a Rt+1+ 7maxQ(St+1,a)\r�� Q(St, At) \u0026#8226;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (6.8)\n.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; a\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; .\nIn this case, the learned action-value function, Q, directly approximates q^, the op\u0026shy;timal\raction-value function, independent of the policy being followed. This dramat\u0026shy;ically\rsimplifies the analysis of the algorithm and enabled early convergence proofs.\rThe policy still has an effect in that it determines which state-action pairs\rare visited and updated. However, all that is required for correct convergence\ris that all pairs continue to be updated. As we observed in Chapter 5, this is\ra minimal requirement in the sense that any method guaranteed to find optimal\rbehavior in the general case must require it. Under this assumption and a\rvariant of the usual stochastic approx\u0026shy;imation conditions on the sequence of\rstep-size parameters, Q has been shown to\rconverge with probability 1 to q^. The\rQ-learning algorithm is shown in procedural form in the box below.\nQ-learning (off-policy TD control) for estimating n\rnľ\nInitialize\rQ(s, a), Vs G S, a G A(s), arbitrarily, and Q(terminal-state, ��) = 0 Repeat (for each episode):\nInitialize S\nRepeat (for each step of episode):\nChoose A\rfrom S using policy derived from Q (e.g., e-greedy)\nTake\raction A, observe R, S7\nQ(S, A) ^\rQ(S, A)+ a[R + 7max0Q(S7, a) �� Q(S, A)]\nS ^ S7until S is\rterminal\nWhat is the backup diagram for Q-learning? The rule (6.8) updates a state-action pair, so the top node, the root of the\rbackup, must be a small, filled action node. The backup is also from action nodes, maximizing over all those actions possible\rin the next state. Thus the bottom nodes of the backup diagram should be all\rthese action nodes. Finally, remember that we indicate taking the maximum of\rthese ��next action�� nodes with an arc across them (Figure 3.7-right). Can you\rguess now what the diagram is? If so, please do make a guess before turning to\rthe answer in Figure 6.6on page 144.\n\r\r\rR =\n\r\r\r\r\r\r\r\rSarsa\n\r\r\r\r\r\r\r\rSum of rewards during episode\n\r\r\r\r\r\r\r\rFigure 6.5: The\rcliff-walking task. The results are from a single run, but smoothed by\raveraging the reward sums from 10successive episodes.\n\r\r\r\r\r\r\r\rEpisodes\n\r\r\r\r\rExample 6.6: Cliff Walking This gridworld example compares Sarsa and Q-\rlearning, highlighting the difference between on-policy (Sarsa) and off-policy\r(Q- learning) methods. Consider the gridworld shown in the upper part of Figure\r6.5. This is a standard undiscounted, episodic task, with start and goal\rstates, and the \n\r\rusual actions causing movement up, down, right, and left. Reward is\r��1 on all transitions except those into the region marked ��The Cliff.�� Stepping\rinto this region incurs a reward of ��100and sends the agent\rinstantly back to the start.\nThe lower part of Figure 6.5 shows the\rperformance of the Sarsa and Q-learning methods with e-greedy action selection,\re = 0.1. After an initial transient, Q-learning learns values for the optimal\rpolicy, that which travels right along the edge of the cliff. Unfortunately,\rthis results in its occasionally falling off the cliff because of the e-greedy\raction selection. Sarsa, on the other hand, takes the action selection into\raccount and learns the longer but safer path through the upper part of the\rgrid. Although Q-learning actually learns the values of the optimal policy, its\ron\u0026shy;line performance is worse than that of Sarsa, which learns the roundabout\rpolicy. Of course, if e were gradually reduced, then both methods would\rasymptotically converge to the optimal policy.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 6.11 Why is Q-learning considered an off-policy\rcontrol method? ��\nA\nQ-learning\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ExpectedSarsa\nFigure 6.6: The backup\rdiagrams for Q-learning and expected Sarsa.\n6.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rExpected Sarsa\nConsider the learning algorithm that is just like Q-learning except\rthat instead of the maximum over next state-action pairs it uses the expected\rvalue, taking into account how likely each action is under the current policy.\rThat is, consider the algorithm with the update rule\nQ(St, At)Q(St, At)+ a Rt+1+ YE[Q(St+1, At+1)| St+1] �� Q(St, At)\nQ(St,At)+ a Rt+i + 7n(a|St+i)Q(St+i, a) ��Q(St,At) , (6.9)\na\nbut that otherwise follows the schema of Q-learning. Given the next\rstate, St+i, this algorithm\rmoves deterministically in the same direction as Sarsa\rmoves in expecta\u0026shy;tion, and accordingly it is called expected Sarsa. Its backup diagram is shown on the right in\rFigure 6.6.\nExpected Sarsa is more complex computationally than Sarsa but, in\rreturn, it eliminates the variance due to the random selection of At+i. Given the same amount of experience we might expect it to\rperform slightly better than Sarsa, and indeed it generally does. Figure 6.7\rshows summary results on the cliff-walking task with Ex\u0026shy;pected Sarsa compared\rto Sarsa and Q-learning. As an on-policy method, Expected Sarsa retains the\rsignificant advantage of Sarsa over Q-learning on this problem. In addition,\rExpected Sarsa shows a significant improvement over Sarsa over a wide range of\rvalues for the step-size parameter a. In cliff walking the state transitions\rare all deterministic and all randomness comes from the policy. In such cases,\rEx\u0026shy;pected Sarsa can safely set a = 1 without suffering any degradation of\rasymptotic performance, whereas Sarsa can only perform well in the long run at\ra small value of a, at which short-term performance is poor. In this and other\rexamples there is a consistent empirical advantage of Expected Sarsa over\rSarsa.\nIn these cliff walking results\rwe have taken Expected Sarsa to be an on-policy algorithm, but in general we\rcan use a policy different from the target policy n to generate behavior, in\rwhich case Expected Sarsa becomes an off-policy algorithm. For example, suppose\rn is the greedy policy while behavior is more exploratory; then Expected Sarsa\ris exactly Q-learning. In this sense Expected Sarsa subsumes and generalizes\rQ-learning while reliably improving over Sarsa. Except for the small additional\rcomputational cost, Expected Sarsa may completely dominate both of the other\rmore-well-known TD control algorithms.\n6.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rMaximization Bias and Double\rLearning\nAll the control algorithms that we have discussed so far involve\rmaximization in the construction of their target policies. For example, in\rQ-learning the target policy is the greedy policy given the current action\rvalues, which is defined with a max, and in Sarsa the policy is often e-greedy, which also involves a maximization operation. In these\ralgorithms, a maximum over estimated values is used implicitly as an estimate\rof the maximumvalue, which can lead to a significant positive bias. To see why,\rconsider a single state s where there are many\ractions a whose true values, q(s, a), are all zero but whose estimated values, Q(s,a), are uncertain and thus\rdistributed some above and some below zero. The maximum of the true values is\rzero, but the maximum of the estimates is positive, a positive bias. We call\rthis maximization bias.\nExample 6.7: Maximization Bias Example The smaHl MDP ehown inset in\rFigure 6.8provides a simple example of how maximization bias can harm the\rperforma nce of TD control algorithm s. The MDP has two non-termin al states A and B.\rEpisodes always start in A with a choice between two actions, left and right.\rThe right net ion transitions immediately to the terminal state with a reward\rand return of zero. The left action transitions to B, also with a reward of zero, from\rwhich there are many possible actions all of which cause immediate termination\rwith a\n\r\r\r-40\n\r\r\r\r\r\r\r\r0.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0.9\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 1\nCh\n\r\r\r\r\r\r\r\rFigure\r6.7: Interim and asymptotic performance ofTDcontrol methods on the cliff-walking task as a function of a. All\ralgorithms used an e-greedy policy with e=\r0.1. Asymptotic performance is an average over 100,000episodes whereas interim performance\ris an average over the first 100 episodes. These data are averages of over\r50,000 and 10 runs for the interim and asymptotic cases respectively. The\rsolid circles mark the best interim performance of each method. Adapted\rfrom van Seijen et al. (2009).\n\r\r\r\r\r\r\r\rReward per -80 episode\n-120\n\r\r\r\r\ro\n\r\rreward drawn from a normal distribution with mean\r��0.1 and variance 1.0.\rThus, the expected return for any trajectory starting\rwith left is ��0.1, and thus taking left in state A is\ralways a mistake. Nevertheless, our control methods may favor left because\rof maximization bias making B appear to have a positive value.\rFigure 6.8 shows that Q-learning with e-greedy action selection initially\rlearns to strongly favor the left action on this example. Even at\rasymptote, Q-learning takes the left action about 5% more often than\ris optimal at our parameter settings (e = 0.1, a = 0.1, and y = 1).\n\r\r\r\r\rFigure\r6.8: Comparison of Q-learning and Double Q-learning\ron a simple episodic MDP (shown inset). Q-learning initially learns to\rtake the left action\rmuch more often than the right action,\rand always takes it significantly more often than the 5% minimum probability enforced by e-greedy\raction selection with e = 0.1. In contrast, Double Q-learning is\ressentially unaffected by maximization bias. These data are averaged over\r10,000 runs. The initial action-value estimates were zero. Any ties in\re-greedy action selection were broken randomly.\n��\n\r\r\r\r\r\r\r\rEpisodes\n\r\r\r\r\r\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rAre there algorithms that\ravoid maximization bias? To start, consider a bandit case in which we have\rnoisy estimates of the value of each of many actions, obtained as sample\raverages of the rewards received on all the plays with each action. As we\rdiscussed above, there will be a positive maximization bias if we use the\rmaximum of the estimates as an estimate of the maximum of the true values. One\rway to view the problem is that it is due to using the same samples (plays)\rboth to determine the maximizing action and to estimate its value. Suppose we\rdivided the plays in two sets and used them to learn two independent estimates,\rcall them Qi(a) and Q2(a), each an estimate of the true value\rq(a), for all a G A. We could then use one estimate,\rsay Qi, to determine the maximizing action A* = argmaxa Qi(a), and\rthe other, Q2, to provide the estimate of its value, Q2(A*)= Q2(argmaxa\rQi(a)). This estimate will then be unbiased in the sense that E[Q2(A*)] = q(A*).We\rcan also repeat the process with the role of the two estimates reversed to\ryield a second unbiased estimate Qi(argmaxaQ2(a)). This\ris the idea of doubled learning. Note that although we\rlearn two estimates, only one estimate is updated on each play; doubled\rlearning doubles the memory requirements, but is no increase at all in theamount of computation per step.\nThe idea of doubled learning extends naturally to algorithms for\rfull MDPs. For example, the doubled learning algorithm analogous to Q-learning,\rcalled Double Q- learning, divides the time steps in two, perhaps by flipping a\rcoin on each step. If the coin comes up heads, the update is\nQi(St, At) ^ Qi (St, At)+\ra Rt+i + yQ^ St+i, argmax Qi (St+i,a)) �� Qi(St, At).\na\n(6.10)\nIf the coin comes up tails, then the same update is done with Qi and\rQ2switched, so that Q2is updated. The two\rapproximate value functions are treated completely symmetrically. The behavior\rpolicy can use both action value estimates. For ex\u0026shy;ample, an e-greedy policy\rfor Double Q-learning could be based on the average (or sum) of the two action-value\restimates. A complete algorithm for Double Q-learning is given below. This is\rthe algorithm used to produce the results in Figure 6.8. In that example, doubled learning seems to eliminate the harm\rcaused by maximization bias. Of course there are also doubled versions of Sarsa\rand Expected Sarsa.\nDouble Q-learning\n\r\r\u0026nbsp;\n\r\rInitialize Qi(s, a) and Q2(s, a), Vs G S, a G A(s), arbitrarily Initialize Q1(terminal-state,\u0026#8226;) = Q2(terminal-state,\u0026#8226;) = 0\rRepeat (for each episode):\nInitialize\rS\nRepeat\r(for each step of episode):\n\r\r\rin\rQi ʮQ2)\nA))\nA))\n\r\r\r\r\rChoose A from S using policy derived from Qi and Q2(e.g., e-greedy Take action A, observe R, S7 With 0.5\rprobabilility:\nQi(S, A) ^ Qi(S, A)ʮa(ֻʮYQ2(S7, argmaxa Qi(S7, a)) �� Qi(S,\relse:\nQ2(S, A) ^ Q2(S, A)ʮa(R ʮyQi(S7,\rargmaxa Q2(S7, a)) �� Q2(S, S ^ S7 until S is terminal\n\r\r\u0026nbsp;\n\u0026nbsp;\n\r\r*Exercise 6.12 What are the update\requations for Double Expected Sarsa with an e-greedy target policy?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n6.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rGames, Afterstates, and Other\rSpecial Cases\nIn this book we try to present a uniform approach\rto a wide class of tasks, but of course there are always exceptional tasks that\rare better treated in a specialized way. For example, our general approach\rinvolves learning an action-value function, but in\rChapter 1 we presented a TD method for learning to play tic-tac-toe that\rlearned something much more like a state-value\rfunction. If we look closely at that example, it\nbecomes apparent that the function\rlearned there is neither an action-value function nor a state-value function in\rthe usual sense. A conventional state-value function evaluates states in which\rthe agent has the option of selecting an action, but the state-value function\rused in tic-tac-toe evaluates board positions after the\ragent has made its move. Let us call these afterstates,\rand value functions over these, afterstate value functions.\rAfterstates are useful when we have knowledge of an initial part of the\renvironment��s dynamics but not necessarily of the full dynamics. For example,\rin games we typically know the immediate effects of our moves. We know for each\rpossible chess move what the resulting position will be, but not how our\ropponent will reply. Afterstate value functions are a natural way to take\radvantage of this kind of knowledge and thereby produce a more efficient\rlearning method.\nThe reason it is more\refficient to design algorithms in terms of afterstates is appar\u0026shy;ent from the\rtic-tac-toe example. A conventional action-value function would map from\rpositions and moves to an estimate of the value. But\rmany position-move pairs produce the same resulting position, as in this\rexample:\n\r\r\r\r\rX\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\rO\n\r+\n\r\u0026nbsp;\n\rX\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\r\r\r\r\u0026nbsp;\n\r\u0026nbsp;\n\rX\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\rO\n\rX + ��_\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\r\u0026nbsp;\n\u0026nbsp;\n\r\r\n\r\r\r\r\r\u0026nbsp;\nIn such cases the position-move pairs are\rdifferent but produce the same ��afterpo\u0026shy;sition,�� and thus must have the same\rvalue. A conventional action-value function would have to separately assess\rboth pairs, whereas an afterstate value function would immediately assess both\requally. Any learning about the position-move pair on the left would\rimmediately transfer to the pair on the right.\nAfterstates arise in many tasks, not just games.\rFor example, in queuing tasks there are actions such as assigning customers to\rservers, rejecting customers, or discarding information. In such cases the\ractions are in fact defined in terms of their immediate effects, which are\rcompletely known.\nIt is impossible to describe\rall the possible kinds of specialized problems and cor\u0026shy;responding specialized\rlearning algorithms. However, the principles developed in this book should\rapply widely. For example, afterstate methods are still aptly de\u0026shy;scribed in\rterms of generalized policy iteration, with a policy and (afterstate) value\rfunction interacting in essentially the same way. In many cases one will still\rface the choice between on-policy and off-policy methods for managing the need\rfor persistent exploration.\nExercise 6.13 Describe\rhow the task of Jack��s Car Rental (Example 4.2) could be reformulated in terms\rof afterstates. Why, in terms of this specific task, would such a reformulation\rbe likely to speed convergence?��\n6.9\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rSummary\nIn this chapter we introduced a new kind of\rlearning method, temporal-difference (TD) learning, and showed how it can be\rapplied to the reinforcement learning prob\u0026shy;lem. As usual, we divided the\roverall problem into a prediction problem and a control problem. TD methods are\ralternatives to Monte Carlo methods for solving the pre\u0026shy;diction problem. In\rboth cases, the extension to the control problem is via the idea of generalized\rpolicy iteration (GPI) that we abstracted from dynamic programming. This is the\ridea that approximate policy and value functions should interact in such a way\rthat they both move toward their optimal values.\nOne of the two processes making up GPI drives the\rvalue function to accurately predict returns for the current policy; this is\rthe prediction problem. The other process drives the policy to improve locally\r(e.g., to be e-greedy) with respect to the current value function. When the\rfirst process is based on experience, a complication arises concerning\rmaintaining sufficient exploration. We can classify TD control methods\raccording to whether they deal with this complication by using an on- policy or\roff-policy approach. Sarsa is an on-policy method, and Q-learning is an\roff-policy method. Expected Sarsa is also an off-policy method as we present it\rhere. There is a third way in which TD methods can be extended to control which\rwe did not include in this chapter, called actor-critic methods. These method\rare covered in full in Chapter 13.\nThe methods presented in this chapter are today\rthe most widely used reinforce\u0026shy;ment learning methods. This is probably due to\rtheir great simplicity: they can be applied on-line, with a minimal amount of\rcomputation, to experience generated from interaction with an environment; they\rcan be expressed nearly completely by single equations that can be implemented\rwith small computer programs. In the next few chapters we extend these\ralgorithms, making them slightly more complicated and significantly more\rpowerful. All the new algorithms will retain the essence of those introduced\rhere: they will be able to process experience on-line, with relatively little\rcomputation, and they will be driven by TD errors. The special cases of TD\rmethods introduced in the present chapter should rightly be called one-step, tabular, model- free TD methods. In the next two\rchapters we extend them to multistep forms (a link to Monte Carlo methods) and\rforms that include a model of the environment (a link to planning and dynamic\rprogramming). Then, in the second part of the book we extend them to various\rforms of function approximation rather than tables (a link to deep learning and\rartificial neural networks).\nFinally, in this chapter we have discussed TD\rmethods entirely within the context of reinforcement learning problems, but TD\rmethods are actually more general than this. They are general methods for\rlearning to make long-term predictions about dynamical systems. For example, TD\rmethods may be relevant to predicting financial data, life spans, election\routcomes, weather patterns, animal behavior, demands on power stations, or customer\rpurchases. It was only when TD methods were analyzed as pure prediction\rmethods, independent of their use in reinforcement learning, that their\rtheoretical properties first came to be well understood. Even so, these other\npotential applications of TD learning methods have not yet been\rextensively explored.\nBibliographical and Historical Remarks\nAs we outlined in Chapter 1, the idea of TD learning has its early\rroots in ani\u0026shy;mal learning psychology and artificial intelligence, most notably\rthe work of Samuel (1959) and Klopf (1972). Samuel��s work is described as a\rcase study in Section 16.2. Also related to TD learning are Holland��s (1975,\r1976) early ideas about consistency among value predictions. These influenced\rone of the authors (Barto), who was a graduate student from 1970 to 1975 at the\rUniversity of Michigan, where Holland was teaching. Holland��s ideas led to a\rnumber of TD-related systems, including the work of Booker (1982) and the\rbucket brigade of Holland (1986), which is related to Sarsa as discussed below.\n6.1-2 Most of the specific material from these sections is from\rSutton (1988), includ\u0026shy;ing the TD(0) algorithm, the random walk example, and the\rterm ��temporal- difference learning.�� The characterization of the relationship\rto dynamic programming and Monte Carlo methods was influenced by Watkins\r(1989), Werbos (1987), and others. The use of backup diagrams here and in other\rchapters is new to this book.\nTabular TD(0) was proved to converge in the mean by Sutton (1988)\rand with probability 1 by Dayan (1992), based on the work of Watkins and Dayan\r(1992). These results were extended and strengthened by Jaakkola, Jordan, and\rSingh (1994) and Tsitsiklis (1994) by using extensions of the powerful existing\rtheory of stochastic approximation. Other extensions and general\u0026shy;izations are\rcovered in later chapters.\n6.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe optimality of the TD\ralgorithm under batch training was established by Sutton (1988). Illuminating\rthis result is Barnard��s (1993) derivation of the TD algorithm as a combination\rof one step of an incremental method for learning a model of the Markov chain\rand one step of a method for computing predictions from the model. The term certainty equivalence is from the adaptive control literature\r(e.g., Goodwin and Sin, 1984).\n6.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe Sarsa algorithm was\rintroduced by Rummery and Niranjan (1994). They explored it in conjunction with\rneural networks and called it ��Modified Con- nectionist Q-learning��. The name\r��Sarsa�� was introduced by Sutton (1996). The convergence of one-step tabular\rSarsa (the form treated in this chapter) has been proved by Satinder Singh\r(personal communication). The ��windy gridworld�� example was suggested by Tom\rKalt.\nHolland��s (1986) bucket brigade idea evolved into an algorithm\rclosely related to Sarsa. The original idea of the bucket brigade involved\rchains of rules triggering each other; it focused on passing credit back from\rthe current rule to the rules that triggered it. Over time, the bucket brigade\rcame to be more \n\r\rlike TD learning in passing\rcredit back to any temporally preceding rule, not just to the ones that\rtriggered the current rule. The modern form of the bucket brigade, when\rsimplified in various natural ways, is nearly identical to one-step Sarsa, as\rdetailed by Wilson (1994).\n6.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rQ-learning was\rintroduced by Watkins (1989), whose outline of a conver\u0026shy;gence proof was made\rrigorous by Watkins and Dayan (1992). More general convergence results were\rproved by Jaakkola, Jordan, and Singh (1994) and Tsitsiklis (1994).\n6.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rExpected Sarsa\rwas first described in an exercise in the first edition of this book, then\rfully investigated by van Seijen, van Hasselt, Whiteson, and Weir\u0026shy;ing (2009).\rThey established its convergence properties and conditions under which it will\routperform regular Sarsa and Q-learning. Our Figure 6.7 is adapted from their\rresults. Our presentation differs slightly from theirs in that they define\r��Expected Sarsa�� to be an on-policy method exclusively, whereas we use this\rname for the general algorithm in which the target and behavior policies are\rallowed to differ.\n6.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rMaximization\rbias and doubled learning were introduced and extensively in\u0026shy;vestigated by Hado\rvan Hasselt (2010, 2011). The example MDP in Figure 6.8 was adapted from that\rin his Figure 4.1 (van Hasselt, 2011).\n6.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe notion of\ran afterstate is the same as that of a ��post-decision state�� (Van Roy et al.,\r1997; Powell, 2010).\n\r\r152\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; CHAPTER6.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; TEMPORAL-DIFFERENCE\u0026nbsp;\u0026nbsp;\u0026nbsp; LEARNING\n\r\rChapter 7\nMulti-step Bootstrapping\nIn this chapter we unify the methods presented in the previous two\rchapters. Neither Monte Carlo methods nor the one-step TD methods presented in\rthe previous chapter are always the best. Multi-step TD methods generalize both\rthese methods so that one can switch from one to the other smoothly. They span\ra spectrum with Monte Carlo methods at one end and one-step TD methods at the\rother, and often the intermediate methods will perform better than either\rextreme method.\nAnother way of looking at the benefits of multi-step\rmethods is that they free you from the tyranny of the time step. With one-step\rmethods the same step determines how often the action can be changed and the\rtime interval over which bootstrapping is done. In many applications one wants\rto be able to update the action very fast to take into account anything that\rhas changed, but bootstrapping works best if it is over a length of time in\rwhich a significant and recognizable state change has occurred. With one-step\rmethods, these time intervals are the same and so a compromise must be made.\rMulti-step methods enable bootstrapping to occur over longer time intervals,\rfreeing us from the tyranny of the single time step.\nMulti-step methods are usually associated with the\ralgorithmic idea of eligibility traces, but here we will consider the\rmulti-step idea on its own, postponing the treatment of eligibility-trace\rmechanisms until later, in Chapter 12.\nAs usual, we\rfirst consider the prediction problem and then the control problem. That is, we\rfirst consider how multi-step methods can help in predicting returns as a\rfunction of state for a fixed policy (i.e., in estimating ). Then we extend the\rideas to action values and control methods.\n7.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rn-step TD Prediction\nWhat is the space of methods lying between Monte Carlo and TD\rmethods? Consider estimating from sample episodes generated using n. Monte\rCarlo methods perform a backup for each state based on the entire sequence of\robserved rewards from that state until the end of the episode. The backup of\rone-step TD methods, on the other hand, is based on just the one next reward,\rbootstrapping from the value of thestate one\rstep later as a proxy for the remaining rewards. One kind of intermediate\rmethod, then, would perform a backup based on an intermediate number of\rrewards: more than one, but less than all of them until termination. For\rexample, a two-step backup would be based on the first two rewards and the\restimated value of the state two steps later. Similarly, we could have\rthree-step backups, four-step backups, and so on. Figure 7.1 diagrams the\rspectrum of n-step backupsfor Vn, with the one-step TD backup on the left and\rthe up-until-termination Monte Carlo backup on the right.\nThe methods that use n-step backups are still TD\rmethods because they still change an earlier estimate based on how it differs\rfrom a later estimate. Now the later estimate is not one step later, but n\rsteps later. Methods in which the temporal difference extends over n steps are\rcalled n-step TD methods. The TD methods introduced in\rthe previous chapter all used one-step backups, which is why we call them\rone-step TD methods.\nMore formally, consider the backup applied to state St as a result\rof the state- reward sequence, St,��t+1, St+1,��t+2, \u0026#8226; \u0026#8226; \u0026#8226;, Rt, St (omitting the actions for simplic\u0026shy;ity). We know that\rin Monte Carlo backups the estimate of Vn (St) is updated in the direction of\rthe complete return:\nGt =��ʮ1+ Y ��ʮ2+ 72��ʮ3 + ������ + 7T-t-1RT,\n\r\r\u0026nbsp;\n\u0026nbsp;\n\r\r1 -step TD\nand TD(0)\r2-step TD 3-step TD\nM-step TD n-step TD and Monte Carlo\n\r\r\u0026nbsp;\n\u0026nbsp;\n\r\r��\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n��\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; o\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; o\n��\n��\n��\nFigure 7.1: The spectrum ranging from\rthe one-step backups of simple TD methods to the up-until-termination backups\rof Monte Carlo methods. In between are the n-step back\u0026shy;ups, based on n steps of\rreal rewards and the estimated value of the nth next state, all appropriately\rdiscounted.\nwhere T is the last time step of the episode. Let us call this\rquantity the target of the backup. Whereas in Monte\rCarlo backups the target is the return, in one-step backups the target is the\rfirst reward plus the discounted estimated value of the next state, which we\rcall the one-step return:\nGt��t+i = Rt+i + 7Vt(St+i),\nwhere Vt : S R here is an estimate at\rtime t of v^. The subscripts on Gt��t+i indicate that it is truncated return for time t using rewards up\runtil time t + 1, and the superscript s reminds us that the missing rewards are\rreplaced by an estimate at a subsequent state (shortly\rwe will introduce truncated returns using estimated values at state-action\rpairs). In the one-step return, YVt(St+i) takes the place of\nthe other terms YRt+2+ Y2Rt+3+------ + yT-t-1Rr of the full\rreturn, as we discussed\nin the previous chapter. Our point now is that this idea makes just\ras much sense after two steps as it does after one. The target for a two-step\rbackup is the two-step return:\nGt:t+2= Rt+1+ YRt+2+ 72Vt+1(St+2)\nwhere now y2Vt+i(St+2) corrects for the absence of the terms Y2Rt+3+ Y3Rt+4+ \u0026#8226; \u0026#8226; \u0026#8226; + yT-t-1Rr. Similarly,\rthe target for an arbitrary n-step backup is the n-step return:\nGt:t+n\r= Rt+1+ YRt+2+ \u0026#8226; \u0026#8226; \u0026#8226; + Y n iRt+n + 7nVt+n-1(St+n),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (7.1)\nfor all n, t such that n \u0026gt; 1 and 0 \u0026lt; t \u0026lt; T �� n. All n-step returns can be considered approximations to the full\rreturn, truncated after n steps and then corrected for the remaining missing\rterms by Vt+n-i(St+n). If t+n \u0026gt; T (if the n-step return extends to or beyond termination), then all\rthe missing terms are taken as zero, and the n-step return defined to be equal\rto the ordinary full return (Gt��t+n == Gt if t + n \u0026gt; T).\nNote that n-step returns for n \u0026gt; 1 involve future rewards and\rstates that are not available at the time of transition from t to t + 1. No\rreal algorithm can use the n-step return until after it has seen Rt+n\rand computed Vt+n-i. The first time these are available is t + n.\rThe natural algorithm state-value learning algorithm for using n-step returns\ris thus\nVt+n(St) == Vt+n-i(St) + a [Gt��t+n �� Vt+n-1(St)],\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0\u0026nbsp; \u0026lt;\u0026nbsp; t \u0026lt; T,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (7.2)\nwhile the values of all other states\rremain unchanged, Vt+n(s) = Vt+n-i(s), Vs = St. We call this algorithm n-step TD.\rNote that no changes at all are made during the first n �� 1 steps of each episode. To make up for that, an equal number of\raddition updates are made at the end of the episode, after termination and\rbefore starting the next episode. Complete pseudocode is given in the box on\rthe next page.\nExercise 7.1 In Chapter 6we noted that the Monte Carlo error can be written as the sum of TD\rerrors (6.5) if the value estimates don��t change from step to step (6.6). Show that the n-step error used in (7.2) can also be written as a\rsum TD errors (again if the value estimates don��t change) generalizing the\rearlier result. ��\nn-step TD for estimating V^ Vn\nInitialize V(s) arbitrarily, s G S\nParameters: step size a G (0,1], a positive\rinteger n\nAll store and access operations (for St and Rt) can take their index\rmod n\nRepeat (for each episode):\nInitialize\rand store So = terminal T ^\nFor t = 0,1, 2,...:\n| If t \u0026lt; T, then:\n|\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Take an\raction according to n(-|St)\n|\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Observe and\rstore the next reward asRt+i and the next\rstate as St+i\n|\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; If St+i is\rterminal, then T �� t + 1\n| t �� t �� n + 1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (t\ris the time whose state��s estimate is being updated)\n| If t \u0026gt; 0:\n|\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; G �� ^=$1+0 Yi-T-iRi\n|\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; If t + n\r\u0026lt; T, then: G �� G + YnV(Sr+n)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (Gr��r+n)\n|\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; V(St) �� V(St)+ a [G �� V(Sr)]\nUntil t = T �� 1\nExercise 7.2 (programming) With an\rn-step method, the value estimates do change from step\rto step, so an algorithm that used the sum of TD errors (see previous exercise)\rin place of the error in (7.2) would actually be a slightly different\ralgorithm. Would it be a better algorithm or a worse one? Devise and program a\rsmall experiment to answer this question empirically.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nThe n-step return uses the value function Vt+n-i to\rcorrect for the missing rewards beyond Rt+n. An important property\rof n-step returns is that their expectation is guaranteed to be a better\restimate of Vn than Vt+n-i is, in a worst-state sense. That is, the\rworst error of the expected n-step return is guaranteed to be less than or\requal to yn times the worst error under Vt+n_i:\n\r\r\rmax\ns\n\r\r\r\r\rEn[Gt��t+n|St = s] �� Vn(s)\u0026lt;\u0026nbsp;\u0026nbsp; Y\u0026nbsp; maxVt+n-i(s)�� Vn(s) ,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (7.3)\ns\nfor all n \u0026gt; 1. This is called the error\rreduction property of n-step returns. Because of the error reduction\rproperty, one can show formally that all n-step TD methods converge to the\rcorrect predictions under appropriate technical conditions. The n- step TD\rmethods thus form a family of sound methods, with one-step TD methods and Monte\rCarlo methods as extreme members.\nExample 7.1: n-step TD\rMethods on the Random Walk Consider using n-step\rTD methods on the random walk task described in Example 6.2 and shown in Figure\r6.2. Suppose the first episode progressed directly from the center state, C, to the\rright, through D and E, and then terminated on the right with a return of 1. Recall that\rthe estimated values of all the states started at an intermediatevalue, V(s) = 0,5. As a result of this experience, a one-step method would\rchange only the estimate for the last state, V(E), which\rwould be incremented toward 1, the observed return. A two-step method, on the\rother hand, would increment the values of the two states preceding termination:\rV(D) and V(E) both would be incremented toward 1. A three-step method, or any\rn-step method for n \u0026gt; 2, would increment the\rvalues of all three of the visited states toward 1, all by the\rsame amount.\nWhich value of n is better? Figure 7.2 shows the results of a simple\rempirical test for a larger random walk process, with 19 states (and with a ��1\routcome on the left, all values initialized to 0), which we\ruse as a running example in this chapter. Results are shown for n-step TD\rmethods with a range of values for n and a. The performance measure for each\rparameter setting, shown on the vertical axis, is the square-root of the\raverage squared error between the predictions at the end of the episode for the\r19 states and their true values, then averaged over the first 10 episodes and 100repetitions of the whole experiment (the same sets of walks were\rused for all parameter settings). Note that methods with an intermediate value\rof n worked best. This illustrates how the generalization of TD and Monte Carlo\rmethods to n-step methods can potentially perform better than either of the two\rextreme methods.\nExercise 7.3 Why do you think a\rlarger random walk task (19 states instead of 5) was used in the examples of\rthis chapter? Would a smaller walk have shifted the advantage to a different\rvalue of n? How about the change in left-side outcome from 0 to ��1 made in the\rlarger walk? Do you think that made any difference in the best value of n?\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n7.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rn-step Sarsa\nHow can n-step methods be used not just for prediction, but for\rcontrol? In this section we show how n-step methods can be combined with Sarsa\rin a straightforward way to produce an on-policy TD control method. The n-step\rversion of Sarsa we call n-step Sarsa, and the original version presented in\rthe previous chapter we henceforth call one-step Sarsa,\ror Sarsa(0).\nThe main idea is to simply switch states for actions (state-action\rpairs) and then use an e-greedy policy. The backup diagrams for n-step Sarsa,\rshown in Figure 7.3 are like those of n-step TD (Figure 7.1), strings of\ralternating states and actions, except that the Sarsa ones all start and end\rwith an action rather a state. We define n-step returns in terms of estimated\raction values:\nGt:t+n = Rt+1+YRt+2+ ^ \u0026#8226; \u0026#8226;ʮ7^ iRt+n +Y^Qt+n-1(St+n, At+n), n\r\u0026gt; ��,0\u0026lt; t \u0026lt; T��n,\n(7.4)\nwith Gt��t+n == Gt if t + n \u0026gt; T. The natural algorithm is then\nQt+n(St, At) == Qt+n-i(St, At) + a [Gt��t+n �� Qt+n-1(St, At)],\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0\u0026nbsp; \u0026lt;t\u0026nbsp; \u0026lt;T,\u0026nbsp; (7.5)\nwhile the values of all other states remain\runchanged, Qt+n(s, a) = Qt+n_i(s,a), for all s, a such that s = St or a = At.\rThis is the algorithm we call n-step Sarsa.\n\r\r\u0026nbsp;\n\r\rw-step Sarsa\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; n-step\nn-step Sarsa aka Monte Carlo Expected Sarsa\n\r\r\u0026nbsp;\n\r\rI ��I �˶�\n\r\n\r\r\r\r\r\u0026nbsp;\nFigure 7.3: The spectrum of n-step backups for\rstate-action values. They range from the one-step backup of Sarsa(0) to the up-until-termination\rbackup of a Monte Carlo method. In between are the n-step backups, based on n\rsteps of real rewards and the estimated value of the nth next state-action\rpair, all appropriately discounted. On the far right is the backup diagram for\rn-step Expected Sarsa.\n\r\rn-step Sarsa for estimating\rQ ^ q*, or Q ^ for a given n\nInitialize\rQ(s, a) arbitrarily, Vs G S, a G A\nInitialize\rn to be e-greedy with respect to Q, or to a fixed given policy\nParameters:\rstep size a G (0,1], small e \u0026gt; 0,\ra positive integer n\nAll store and access\roperations (for St, At, and Rt) can take their index mod n\nRepeat\r(for each episode):\nInitialize\rand store So = terminal Select and store an action Ao \u0026#12316;n(-|So)\nT �� ^\nFor t =\r0, 1, 2, . . . :\nIf t \u0026lt;\rT, then:\nTake action At\nObserve and store the next reward as Rt+i and the\rnext state as St+i If St+i is terminal, then:\nT��t+1 else:\nSelect and store an action At+i \u0026#12316;n(-|St+i) t �� t �� n + 1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (t is\rthe time whose estimate is being updated)\nIf t \u0026gt; 0:\ng��E��=[11]n+Ti+n,T)y i-T-iRi\nIf t+ n \u0026lt; T, then G �� G+ YnQ(ST+n, At\r+n)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (Gt��T +n)\nQ(St, At) �� Q(St, At) + a [G �� Q(St, A)]\nIf n is\rbeing learned, then ensure that ^(-|St) is e-greedy\rwrt Q Until t = T �� 1\n\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\rG\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\rt\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\r\rAction\rvalues increased\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Actionvalues\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; increased\nPath\rtaken\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; byone-stepSarsa\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; by10-step Sarsa\n\r\r\r\r\r\r\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\rr\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r1\n\r\u0026nbsp;\n\r\u0026nbsp;\n\rG\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r��\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\r\r\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r-\u0026#9658;\n\r-\u0026#9658;\n\r1\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r-\u0026#9658;\n\r1\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\rG\n\r\u0026nbsp;\n\r1\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\rţ\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\r\n\r\r\u0026nbsp;\n\r\rFigure 7.4: Gridworld example of the speedup of policy learning due\rto the use of n-step methods. The first panel shows the path taken by an agent\rin a single episode, ending at a location of high reward, marked by the G. In this example the values were all initially 0, and all rewards\rwere zero except for a positive reward at G. The arrows\rin the other two panels show which action values were strengthened as a result\rof this path by one-step and n-step Sarsa methods. The one-step method\rstrengthens only the last action of the sequence of actions that led to the\rhigh reward, whereas the n-step method strengthens the last n actions of the\rsequence, so that much more is learned from the one episode.\n7.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rn-step Off-policy Learning by\rImportance Sampling\nRecall that off-policy learning is learning the value function for\rone policy, n, while following\ranother policy, b. Often, n is the greedy policy for the current action- value-function\restimate, and b is a more exploratory\rpolicy, perhaps e-greedy. In order to\ruse the data from b we must take into\raccount the difference between the two policies, using their relative\rprobability of taking the actions that were taken (see Section 5.5). In n-step methods, returns are constructed over n steps, so we are interested in the relative probability of just\rthose n actions. For example, to make a simple\roff-policy version of n-step TD, the update\rfor time t (actually made at time\rt\r+ n) can simply\rbe weighted by pt:t+n_1:\nV\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rt+��St) == K+n-1(St) + aPt:t+n-1[Gt:t+n �� Vt+n-1(St)],0\u0026lt; t\u0026lt;T,(7.7)\nwhere pt:t+n_1, called the importance sampling ratio, is the relative probability under\rthe two policies of taking the n actions\rfrom At to A_t+n_1(cf. Eq. 5.3):\nmin(h,T-1)rA,Q%\nP=\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; n(Ak |Sk)\npt:h=\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; |it\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026#8226;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (7)\nFor example, if any one of the actions would never be taken by n (i.e., n(A^|S^) = 0) then the n-step return\rshould be given zero weight and be totally ignored. On the other hand, if by\rchance an action is taken that n would take\rwith much greater probability than b does, then\rthis will increase the weight that would otherwise be given to the return. This\rmakes sense because that action is characteristic of n (and therefore we want to learn about it) but is selected rarely by b and thus rarely appears in the data. To make up for this we have to\rover-weight it when it doesoccur. Note that if the two policies are actually the same (the on-policy case)\rthen the importance sampling ratio is always 1. Thus our new update (7.7)\rgeneralizes and can completely replace our earlier n-step TD update. Similarly,\rour previous n-step Sarsa update can be completely replaced by a simple\roff-policy form:\nQt+n(St,\rAt) = Qt+n-1(St, At) + apt+1��t+n-1[Gt��t+n �� Qt+n-1(St, At)] ,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (7.9)\nfor 0 \u0026lt; t \u0026lt; T. Note the importance sampling ratio here starts\rone step later than for n-step TD (above). This is because here we are updating\ra state-action pair. We do not have to care how likely we were to select the\raction; now that we have selected it we want to learn fully from what happens,\rwith importance sampling only for subsequent actions. Pseudocode for the full\ralgorithm is shown in the box.\nOff-policy n-step\rSarsa for estimating Q^ q��or Q^ for a given n\nInput: an\rarbitrary behavior policy b such that b(a|s) \u0026gt; 0, Vs G S, a G A Initialize Q(s, a) arbitrarily, Vs G S, a G A\nInitialize n to be e-greedy with respect to Q, or\ras a fixed given policy\nParameters: step size a G (0,1],\rsmall e \u0026gt; 0, a positive integer n\nAll store and access\roperations (for St, At, and Rt) can take their index mod n\nRepeat (for each episode):\nInitialize\rand store So = terminal Select and store an action Ao \u0026#12316;b(^|So)\nT ^\nFor t = 0,1, 2,...:\nIf t \u0026lt;\rT, then:\nTake action At\nObserve and store the next reward as Rt+i and the\rnext state as St+i If St+i is terminal, then:\nT �� t + 1 else:\nSelect and store an action At+i \u0026#12316;b(^|St+i) t �� t �� n + 1(t is the\rtime whose estimate is being updated)\nIf t \u0026gt; 0:\np, i-rmin(T +n-1,T-1)\rn(Ai|Si)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (p\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; )\nP���A�Ai=T+1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; b(Ai|Si)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (Pt+1��t+n-i)\ng��E��=n+T1+n,T)y\ri-T-iRi\nIf t+ n\r\u0026lt; T, then: G �� G+ YnQ(ST +n,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; At+n)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (Gt��t +n)\nQ(St, At) ��\rQ(St, At) + ap [G ��\u0026nbsp;\u0026nbsp;\u0026nbsp; Q(St,At)]\nIf n is\rbeing learned, then ensure that n(-|ST) is e-greedy wrt Q Until t = T �� 1\nThe off-policy version of n-step Expected Sarsa would use the same\rupdate as above for Sarsa except that the importance sampling ratio would have\ran additional one less factor in it. That is, the above equation would use pt+i��t+n-2instead of pt+i��t+n-i, and of course it would use the Expected Sarsa version of the n-step return (7.6). This is because in Expected Sarsa all possible\ractions are taken into account in the last state; the one actually taken has no\reffect and does not have to be corrected for.\n7.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\r*Per-reward Off-policy Methods\nThe multi-step off-policy methods presented in the previous section\rare very simple and conceptually clear, but are probably not the most\refficient. A more sophisticated approach would use per-reward importance\rsampling ideas such as were introduced in Section 5.9. To understand this\rapproach, first note that the ordinary n-step\rreturn (7.1), like all returns, can be written recursively:\nGt:h = Rt+i + lGt+i:h-\nNow consider the effect of following a behavior policy b = n that is not the same\ras the target policy n. All of the resulting\rexperience, including the first reward Rt+i and the\rnext state St+i must be weighted by the importance sampling ratio for time t, pt =ū��).Onemight be\rtempted to simply weight the righthand side of the above equation, but one can\rdo better. Suppose the action at time t would\rnever be selected by n, so that pt is zero. Then a simple weighting would result in the n-step return being zero, which could result in high variance when it\rwas used as a target. Instead, in this more sophisticated approach, one uses an\ralternate, off-policy definition of the n-step return, as\nGt��h = pt (Rt+i + lGt+i:h)\r+ (1�� pt)V(St),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; t\r\u0026lt; h \u0026lt; T, (7.10)\nwhere V(St) is some estimate of the value of St+i, and Gt��t == V(St). (We are being a little vague about exactly with time step��s\restimate is used for V(St) because the choice will depend on the practicality of specific\ralgorithms.) Now, if pt is zero, instead of\rthe target being zero and causing the estimate to shrink, the target is the\rsame as the estimate and causes no change. The importance sampling ratio being\rzero means we should ignore the sample, so leaving the estimate unchanged seems\ran appropriate outcome. Notice that the second, additional term does not change\rthe expected update; the importance sampling ratio has expected value one and\ris uncorrelated with the estimate, so the expected value of the second term is\rzero. Also note that the off-policy definition (7.10) is a strict\rgeneralization of the earlier on-policy definition of the n-step return (7.1), as the two are identical in the on-policy case,\rin which pt is always 1.\nFor a\rconventional n-step method, the learning rule to use in conjunction with\n(7.10)\u0026nbsp;\u0026nbsp;\u0026nbsp;\ris the n-step TD update (7.2), which has no explicit importance sampling\rratios other than those embedded in G. In this case, the\rapproximate value function is that at time index h �� 1= t\r+ n �� 1.\nExercise 7.4 Write the pseudocode for\rthe off-policy state-value prediction algo\u0026shy;rithm described above.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n\r\rFor action\rvalues, the off-policy definition of the n-step return corresponds to Ex\u0026shy;pected\rSarsa (there does not seem to be one that corresponds to ordinary Sarsa). It is\ra little different because the first action does not play a role in the\rimportance sampling. We are learning the value of that action and it does not\rmatter if it was unlikely or even impossible under the target policy. It has\rbeen taken and now full unit weight must be given to the reward and state that\rfollows it. Importance sam\u0026shy;pling will apply only to the actions that follow it.\rThe off-policy recursive definition of the n-step return for action values is\nGt��h = Rt+i + Y (pt+iGt+i��h + (1�� pt-i)^Qt+i) ,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (7.11)\nfor t, h such that t \u0026lt; h \u0026lt;\rT, with Gt��t == (Qt == Ean(a|St)Qt-i(St, a). A complete n- step off-policy action-value prediction method\rwould combine (7.11) and (7.5) using the estimate for time step h �� 1= t + n �� 1.\nExercise 7.5 Write the pseudocode for the off-policy action-value prediction algo\u0026shy;rithm\rdescribed immediately above.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 7.6 Show that the general (off-policy) version of the n-step return\r(7.10) can still be written exactly and compactly as the sum of state-based TD\rerrors (6.5) if the approximate state value function does not change.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 7.7 Repeat the above exercise for the action version of the off-policy\rn-step return (7.11) and the Expected Sarsa TD error (the quantity in brackets\rin Equation 6.9).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 7.8 (programming) Devise a small off-policy prediction problem and use\rit to show that the off-policy learning algorithm using (7.10) and (7.2) is\rmore data efficient than the simpler algorithm using (7.1) and (7.7).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nThe importance\rsampling that we have used in this section, the previous section, and in\rChapter 5 enables off-policy learning, but at the cost of increasing the\rvariance of the updates. The high variance forces us to use a small step-size\rparameter, resulting in slow learning. It is probably inevitable that off-policy\rtraining is slower than on-policy training��after all, the data is less relevant\rto what you are trying to learn. However, it is probably also true that the\rmethods we have presented here can be improved on. One possibility is to\rrapidly adapt the step sizes to the observed variance, as in the Autostep\rmethod (Mahmood et al, 2012).\rAnother promising approach is the invariant updates of Karampatziakis and\rLangford (2010) as extended to TD by Tian (2017). The usage technique of\rMahmood (2017; Mahmood and Sutton, 2015) is probably also part of the solution.\rIn the next section we consider an off-policy learning method that does not use\rimportance sampling.\n7.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rOff-policy Learning Without\rImportance Sampling:\nThe n-step Tree Backup Algorithm\nIs off-policy\rlearning possible without importance sampling? Q-learning and Ex\u0026shy;pected Sarsa\rfrom Chapter 6do\rthis for the one-step case, but is there a correspond\u0026shy;\n\r\ring multi-step algorithm? In this section we present\rjust such an n-step\rmethod, called the tree-backup algorithm.\n\r\r\rSuAt\nRt+i\n\r\r\r\r\rThe idea of the algorithm is suggested by the 3-step\rtree-backup diagram shown to the right. Down the central spine and labeled in\rthe figure are three sample states and rewards, and two sample actions.\n\r\r\r\nthe 3-step tree\rbackup\n\r\r\r\r\rThese are the random variables representing the\revents occurring after the initial state-action pair St, At.\rHanging off to the sides of each state are the actions that were not selected. (For the last state, all the actions are\rconsidered to have not (yet) been selected.) Because we have no sample data for\rthe unselected actions, we bootstrap and use the estimates of their values in\rforming the target for the update. This slightly extends the idea of a backup\rdiagram. So far we have always updated the estimated value of the node at the\rtop of the diagram toward a target combining the rewards along the way\r(appropriately discounted) and the estimated values of the nodes at the bottom.\rIn the tree backup, the target includes all these things plus\rthe estimated values of the dangling action nodes hanging off the sides, at all\rlevels.\nThis is why it is called a tree backup; it is a backup from the entire tree of of\restimated action values.\nMore precisely, the backup is\rfrom the estimated action values of the leaf nodes of\rthe tree. The action nodes in the interior, corresponding to the actual action\rtaken, do not participate. Each leaf node is meant to contribute to the target\rwith a weight in proportion to its probability of occurring under the target\rpolicy n. Thus a first-level action a contributes\rwith a weight of n(a|St+1), except\rthat that the action actually taken,��t+1, does not\rcontribute at all. Its probability, n(��t+1|Stʮ1), is used to weight all the second-level action values. Thus, each\rnon-selected second-level action dcontributes\rwith weight n(At+1|StʮJnCa'ISt+a). Each third-level action contributes with weight n(At+1|Stʮ1)n(At+2|St+2)n(a\u0026quot;|St+3), and so on. It is as if each arrow to an action node in the\rdiagram is weighted by its probability of that action being selected under the\rtarget policy, and that weight applies not only to that action but to the whole\rtree below it.\nWe can think of the tree\rbackup as an alternating sequence of sample transitions (from each action to\rthe subsequent state) and full backups (from each state we consider all the\rpossible actions with their probability of occuring). The sample transitions\ralso have various probabilities of occurring, but these need not be taken into\raccount because they are given the action selection and thus indepedent of the\rpolicy; they will introduce variance, but not bias.\nThe one-step return (target) of the tree-backup algorithm is the\rsame as that of Expected Sarsa. It can be written\nGt:t+1= Rt+1+ 7 I^(alSt+1)Qt(St+1, a)\na\n=^t + Qt-1(St, At),\n\r\rn-step Tree Backup for estimating Q^ q^, or Q^ q^ for a given n\nInitialize Q(s, a)\rarbitrarily, Vs G\rS, a G A\nInitialize n to be e-greedy with\rrespect to Q, or as a fixed given policy Parameters: step size a G (0,1], small e \u0026gt; 0, a positive integer n All store and access\roperations can take their index mod n\nRepeat (for each episode):\nInitialize\rand store So = terminal Select and store an action Ao \u0026#12316;n(^|So)\nStore\rQ(So, Ao) as Qo T �� ^\nFor t =\r0, 1, 2, . . . :\nIf t \u0026lt; T:\nTake\raction At\nObserve\rthe next reward R; observe and store the next state as St+i If St+i is terminal:\nT �� t + 1 Store R �� Qt as \u0026amp; else:\nStore R + yJ2a n(a|St+i)Q(St+i,\ra) �� Qt as ^t Select arbitrarily and store an action as At+i Store\rQ(St+i, At+i) as Qt+i Store n(At+i|St+i) as nt+i t �� t ��\rn + 1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (t is the\rtime whose estimate is being updated)\nIf t \u0026gt; 0: e��1 G ��\rQt\nFor k = t, ...,min(T + n �� 1, T �� 1):\nG �� G + e\r֪ e �� Yenk+i Q(St, At) ��\rQ(St, At) + a [G �� Q(St, A)]\nIf n is\rbeing learned, then ensure that n(a|ST) is e-greedy wrt Q(St, \u0026#8226;) Until t = T �� 1\n\r\rwhere ^t is a modified form of the TD error from Expected\rSarsa:\n^t =˽ʮ1+ 7��\u0026gt;(��|��ʮ1��(��ʮ1,4 �� Qt_1(St, At).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (7.12)\na\nWith these, the general n-step returns of the tree-backup algorithm can be\rdefined recursively, and then as a sum of TD errors:\nGt:t+n = Rt+1+ 7أ(a|St+1)Qt(St+1, a)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; +\r7أ(At+1(St+1)Gt+1:t+n (7.13)\n=��+ Qt-1(St, At) �� 7أ(At+1|St+1)Qt(St+1, At+1)\r+ 7أ(At+1|St+1)Gt+1:t+n =Qt-1(St, At)+ ^t +\r7n(At+1|St+1) (Gt+1:t+n ��Qt(St+1, At+1)) =Qt-1(St, At)+ ^t +\r7أ(At+1|Sft+1)Jt+1+\r72أ(At+1|St+1)n(At+2|St+2)Jt+2+\n\u0026nbsp;TOC \\o \u0026quot;1-5\u0026quot; \\h \\z min(t+n-1,T �� 1)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; k\n=Qt��1(St,At)+\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; E ��n 7n(Ai|Si),\nk=t\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; i=i+1\nunder the usual\rconvention that a product of zero factors is 1. This target is then used with\rthe usual action-value update rule from n-step\rSarsa:\nQt+n(St, At) = Qt+n��1(St, At) + a [Gt:t+n ��\rQt+n��1(St, At)],\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (7.5)\nwhile the\rvalues of all other state-action pairs remain unchanged, Qt+n(s, a)=\nQt+n��1(s,a), Vs, a such that s = St or\ra\r= At. Pseudocode for this algorithm is shown in the box\ron the previous page.\n7.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\r*A Unifying Algorithm: n-step Q(a)\nSo far in this chapter we have\rconsidered three different action-value backups, cor\u0026shy;responding to the first\rthree backup diagrams shown in Figure 7.5. n-step Sarsa has all sampled\rtransitions, the tree-backup algorithm has all state-to-action transitions\rfully branched without sampling, and the n-step Expected Sarsa backup has all\rsam\u0026shy;ple transitions except for the last state-to-action ones, which are fully\rbranched with an expected value. To what extent can these algorithms be\runified?\nOne idea for\runification is suggested by the fourth backup diagram in Figure 7.5. This is\rthe idea that one might decide on a step-by-step basis whether one wanted to\rtake the action as a sample, as in Sarsa, or consider the expectation over all\ractions instead, as in the tree backup. Then, if one chose always to sample,\rone would obtain Sarsa, whereas if one chose never to sample, one would get the\rtree-backup algorithm. Expected Sarsa would be the case where one chose to\rsample for all steps except the last one. And of course there would be many\rother possibilities, as suggested by the last diagram in the figure. To\rincrease the possibilities even further we can consider a continuous variation\rbetween sampling and expectation. Let at G [0,1] denote the degree of sampling\ron step t, with a= 1denoting full sampling and a= 0denoting a pure expectation with no sampling. The random variable at might be\rset as a\n\r\nFigure 7.5: The three kinds of n-step action-value\rbackups considered so far in this chapter (4-step case) plus a fourth kind of\rbackup that unifies them all. The ��p��s indicate half transitions on which\rimportance sampling is required in the off-policy case. The fourth kind of\rbackup unifies all the others by choosing on a state-by-state basis whether\rto sample (at = 1) or not (at = 0).\n\r\r\r\r\r\u0026nbsp;\nfunction\rof the state, action, or state-action pair at time t. We call this proposed new\ralgorithm n-step Q(a).\nNow let us develop the equations\rof n-step Q(a). First note that the n-step return of Sarsa (7.4) can be written\rin terms of its own pure-sample-based TD error:\nmin(t+n-i,T\r-1)\nGt��t+n = Qt-i(St, At)+\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Y^ t [Rk+i + lQk(Sk+i, Ak+i) ��Qfc-i(Sfc, Ak)]\nk=t\nThis\rsuggests that we may be able to cover both cases if we generalize the TD error\rto slide with at from its expectation to its sampling form:\n^t = Rt+i + 7[at+iQt(St+i,\rAt+i) + (1�� at+it+i]��Qt-i(St, At),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (7.14)\nwith\n\r\r\r(7.15)\n\r\r\r\r\rQt == ^n(a|St)Qt-i(St,a),\n\r\ras usual. Using these we can define the n-step\rreturns of Q(a) as:\nGt��t+i = Rt+i + 7[at+iQt(St+i, At+i)\r+ (1 �� ^t+i)^Qt+i]\n=A + Qt-i(St, At),\nGt��t+2 = Rt+i + 7[at+iQt(St+i, At+i)\r+ (1 �� ^t+i)^Qt+i]\n��Y (1 ��^t+i)n(At+i|St+i)Qt\r(St+i, At+i)\n+ Y(1 �� ^t+1)n(At+1|St+1)\r[Rt+2 + Y[^t+2Qt(St+2, At+2)\r+ (1 �� Ct+2)01+2]] ��7at+1Qt(St+1,\rAt+1)\n+ Y^t+1 [Rt+2 + 7[at+2Qt(St+2,\rAt+2) + (1 �� ^t+2)^Qt+2]]\n=Qt-i(St, At) + ^t\n+ Y (1��\r^t+i)n(At+i|St+i)5t+i\n+ Y^t+i^t+i\n=Qt-i(St, At) + ^t + Y[(1\r�� ^t+i)n(At+i|St+i) + ^t+i]^t+i\nmin(t+n-1,T -1)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; k\nGt��t+n = Qt-1(St,At)\r+\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ^\u0026quot;^\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ^\u0026nbsp;\u0026nbsp; IT\u0026nbsp;\u0026nbsp; Y\r[(1 �� ai)n(Ai|Si) + ] \u0026#8226; (7.16)\nk=t\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; i=t+1\nUnder on-policy training, this return is ready to\rbe used in an update such as that for n-step Sarsa (7.5). For the off-policy case we need to take ainto account in the importance sampling ratio,\rwhich we redefine more generally as\n\r\r\u0026nbsp;\n\r\rmin(h,T-1)/ r A \\q \\\n\r\r\r(7.17)\n\r\r\r\r\r\r\r\rpt��h\n\r\r\r\r\rn\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; fak\rSAM + 1 �� ak\nb(Ak|Sk)\nk=t\n\r\r\u0026nbsp;\n\r\rAfter this we can then use the\rusual general (off-policy) update for n-step Sarsa (7.9). A complete algorithm\ris given in the box on the next page.\n\r\rOff-policy n-step Q(a) for estimating Q^\rq^, or Q^ q^ for a given n\nInput: an\rarbitrary behavior policy b such that b(a|s) \u0026gt; 0, Vs G\rS, a G A Initialize Q(s, a)\rarbitrarily, Vs G\rS, a G A\nInitialize n to be e-greedy with\rrespect to Q, or as a fixed given policy Parameters: step size a G (0,1], small e \u0026gt; 0, a positive integer n All store and access\roperations can take their index mod n\nRepeat (for each episode):\nInitialize\rand store So = terminal Select and store an action Ao \u0026#12316;b(^|So)\nStore\rQ(So, Ao) as Qo T �� ^\nFor t =\r0, 1, 2, . . . :\nIf t \u0026lt; T:\nTake\raction At\nObserve\rthe next reward R; observe and store the next state as St+i If St+i is\rterminal:\nT��t+1 Store (^t ��~ R �� Qt else:\nSelect and\rstore an action At+i\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; b(^|St+i)\nSelect\rand store at+i Store Q(St+i, At+i) as Qt+i\nStore R + Yat+iQt+i + Y(1 �� at+i) Ean(a|St+i)Q(St+i, a) �� Qt as \u0026amp; Store\rn(At+i|St+i) as nt+i\nStore^1))asPt+i\nt �� t �� n + 1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (t is the time whose estimate is being updated)\nIf t \u0026gt; 0:\nP �� 1 e��1 G �� Qt\nFor k = t, ...,min(T + n �� 1, T �� 1):\nG �� G + e^k e �� Ye[(1�� ak+1)nk+1+ ak+i] p �� p(1�� ak + ak pk)\nQ(St, At) ��\rQ(St, At) + ap [G �� Q(St, At)]\nIf n is\rbeing learned, then ensure that n(a|ST) is e-greedy wrt Q(St, \u0026#8226;) Until t = T �� 1\n7.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rSummary\nIn this\rchapter we have developed a range of temporal-difference learning methods that\rlie in-between the one-step TD methods of the previous chapter and the Monte\rCarlo methods of the chapter before. Methods that involve an intermediate\ramount of bootstrapping are important because they will typically perform\rbetter than either extreme.\nOur focus in this chapter has been\ron n-step methods, which look ahead to the next n rewards, states, and actions.\nThe two 4-step\rbackup diagrams to the right together summa\u0026shy;rize most of the methods\rintroduced. The state-value backup shown is for n-step TD with importance\rsampling, and the action-value backup is for n-step Q(a), which generalizes Ex\u0026shy;pected\rSarsa and Q-learning. All n-step methods involve a delay of n time steps before\rupdating, as only then are all the required future events known. A further\rdrawback is that they involve more computation per time step than previous\rmethods. Compared to one-step methods, n-step methods also require more memory\rto record the states, actions, re\u0026shy;wards, and sometimes other variables over the\rlast n time steps. Eventually, in Chapter 12, we will see how multi-step TD\rmethods can be implemented with minimal memory and computational complexity\rusing eligibility traces, but there will always be some additional computation\rbeyond one-step methods. Such costs can be well worth paying to escape the\rtyranny of the single time step.\nAlthough n-step\rmethods are more complex than those us\u0026shy;ing eligibility traces, they have the\rgreat benefit of being conceptually clear. We have sought to take advantage of\rthis by developing two approaches to off-policy learning in the n-step case.\rOne, based on importance sampling is conceptually simple but can be of high\rvariance. If the target and behavior policies are very different it prob\u0026shy;ably\rneeds some new algorithmic ideas before it can be efficient and practical. The\rother, based on tree backups, is the natural extension of Q-learning to the\rmulti-step case with stochastic target policies. It involves no importance\rsampling but, again if the target and behavior policies are substantially\rdifferent, the bootstrapping may span only a few steps even if n is large.\nBibliographical and\rHistorical Remarks\n7.1-2 The\rnotion of n-step returns is due to Watkins (1989), who also first discussed\rtheir error reduction property. n-step algorithms were explored in the first\redition of this book, in which they were treated as of conceptual interest, but\rnot feasible in practice. The work of Cichosz (1995) and particularly van\rSeijen (2016) showed that they are actually completely practical algorithms.\rGiven this, and their conceptual clarity and simplicity, we have chosen to\rhighlight them here in the second edition. In particular, we now postpone all\rdiscussion of the backward view and of eligibility traces until Chapter 12.\nThe results in the random walk\rexamples were made for this text based on work of Sutton (1988) and Singh and\rSutton (1996). The use of backup diagrams to describe these and other\ralgorithms in this chapter is new.\n7.3-5 The\rdevelopments in these sections are based on the work of Precup, Sut\u0026shy;ton, and\rSingh (2000), Precup, Sutton, and Dasgupta (2001), and Sutton, Mahmood, Precup,\rand van Hasselt (2014).\nThe tree-backup algorithm is due\rto Precup, Sutton, and Singh (2000), but the presentation of it here is new.\n7.6 The Q(a) algorithm is new to this text, but has\rbeen explored further by De Asis, Hernandez-Garcia, Holland, and Sutton (2017).\n\r\r172\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; CHAPTER\u0026nbsp; 7.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; MULTI-STEP\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; BOOTSTRAPPING\nChapter 8\nPlanning and\rLearning with Tabular Methods\nIn this chapter we develop a unified view of reinforcement learning\rmethods that require a model of the environment, such as dynamic programming\rand heuristic search, and methods that can be used without a model, such as\rMonte Carlo and temporal-difference methods. These are respectively called model-based and model- fee\rreinforcement learning methods. Model-based methods rely on planning\ras their primary component, while model-free methods primarily rely on learning. Although there are real differences between these\rtwo kinds of methods, there are also great sim\u0026shy;ilarities. In particular, the\rheart of both kinds of methods is the computation of value functions. Moreover,\rall the methods are based on looking ahead to future events, computing a\rbacked-up value, and then using it to update an approximate value func\u0026shy;tion.\rEarlier in this book we presented Monte Carlo and temporal-difference methods\ras distinct alternatives, then showed how they can be unified by n-step methods\r(and we will do this again more thoroughly with eligibility traces in Chapter\r12). Our goal in this chapter is a similar integration of model-based and\rmodel-free methods. Having established these as distinct in earlier chapters,\rwe now explore the extent to which they can be intermixed.\n8.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rModels and Planning\nBy a model of the\renvironment we mean anything that an agent can use to predict how the\renvironment will respond to its actions. Given a state and an action, a model\rproduces a prediction of the resultant next state and next reward. If the model\ris stochastic, then there are several possible next states and next rewards,\reach with some probability of occurring. Some models produce a description of\rall possibilities and their probabilities; these we call distribution\rmodels. Other models produce just one of the possibilities, sampled\raccording to the probabilities; these we call sample models.\rFor example, consider modeling the sum of a dozen dice. A distribution model\rwould produce all possible sums and their probabilities of occurring, whereas a\rsample model would produce an individual sum drawn according to this\rprobability distribution. The kind of model assumed in dynamic programmingһestimates of the MDP��s dynamics, p(s;, r|s,\ra)һis a distribution\rmodel. The kind of model used in the blackjack example in Chapter 5 is a sample\rmodel. Distribution models are stronger than sample models in that they can\ralways be used to produce samples. However, in many applications it is much\reasier to obtain sample models than distribution models. The dozen dice are a\rsimple example of this. It would be easy to write a computer program to\rsimulate the dice rolls and return the sum, but harder and more error-prone to\rfigure out all the possible sums and their probabilities.\nModels can be used to mimic or simulate\rexperience. Given a starting state and action, a sample model produces a\rpossible transition, and a distribution model generates all possible\rtransitions weighted by their probabilities of occurring. Given a starting\rstate and a policy, a sample model could produce an entire episode, and a\rdistribution model could generate all possible episodes and their\rprobabilities. In either case, we say the model is used to simulate\rthe environment and produce simulated experience.\nThe word planning is used in several\rdifferent ways in different fields. We use the term to refer to any\rcomputational process that takes a model as input and produces or improves a\rpolicy for interacting with the modeled environment:\nmodel ���D��\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026gt; policy\nIn artificial intelligence, there are two\rdistinct approaches to planning according to our definition. State-space\rplanning, which includes the approach we take in this book, is viewed\rprimarily as a search through the state space for an optimal policy or an\roptimal path to a goal. Actions cause transitions from state to state, and\rvalue functions are computed over states. In what we call plan-space\rplanning, planning is instead a search through the space of plans.\rOperators transform one plan into another, and value functions, if any, are\rdefined over the space of plans. Plan-space planning includes evolutionary\rmethods and ��partial-order planning,�� a common kind of planning in artificial\rintelligence in which the ordering of steps is not completely determined at all\rstages of planning. Plan-space methods are difficult to apply efficiently to\rthe stochastic sequential decision problems that are the focus in reinforcement\rlearning, and we do not consider them further (but see, e.g., Russell and\rNorvig, 2010).\nThe unified view we present in this chapter is\rthat all state-space planning methods share a common structure, a structure\rthat is also present in the learning methods presented in this book. It takes\rthe rest of the chapter to develop this view, but there are two basic ideas: (1) all state-space planning methods involve computing value functions\ras a key intermediate step toward improving the policy, and (2) they compute value functions by backup operations applied to\rsimulated experience. This common structure can be diagrammed as follows:\ni ,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; simulated\rbackups\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; r\nmodel ---------- \u0026#9658;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026#8226;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ----------------------- \u0026#9658; values \u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026#9658; policy\nexperience\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; r\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; J\n\r\rDynamic programming methods clearly fit this\rstructure: they make sweeps through the space of states, generating for each\rstate the distribution of possible transitions. Each distribution is then used\rto compute a backed-up value and update the state��s estimated value. In this\rchapter we argue that various other state-space planning methods also fit this\rstructure, with individual methods differing only in the kinds of backups they\rdo, the order in which they do them, and in how long the backed-up information\ris retained.\nViewing planning methods in this way emphasizes their relationship\rto the learning methods that we have described in this book. The heart of both\rlearning and planning methods is the estimation of value functions by backup\roperations. The difference is that whereas planning uses simulated experience\rgenerated by a model, learning methods use real experience generated by the\renvironment. Of course this difference leads to a number of other differences,\rfor example, in how performance is assessed and in how flexibly experience can\rbe generated. But the common structure means that many ideas and algorithms can\rbe transferred between planning and learning. In particular, in many cases a\rlearning algorithm can be substituted for the key backup step of a planning method.\rLearning methods require only experience as input, and in many cases they can\rbe applied to simulated experience just as well as to real experience. The box\rbelow shows a simple example of a planning method based on one-step tabular\rQ-learning and on random samples from a sample model. This method, which we\rcall random-sample one-step tabular Q-planning,\rconverges to the optimal policy for the model under the same conditions that\rone-step tabular Q- learning converges to the optimal policy for the real environment\r(each state-action pair must be selected an infinite number of times in Step 1,\rand a must decrease appropriately over time).\n\r\n\r\r\r\r\r\u0026nbsp;\nIn addition to the unified\rview of planning and learning methods, a second theme in this chapter is the\rbenefits of planning in small, incremental steps. This enables planning to be\rinterrupted or redirected at any time with little wasted computation, which\rappears to be a key requirement for efficiently intermixing planning with\racting and with learning of the model. Planning in very small steps may be the\rmost efficient approach even on pure planning problems if the problem is too\rlarge to be solved exactly.\n8.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rDyna: Integrating Planning,\rActing, and Learning\nWhen planning is done on-line, while\rinteracting with the environment, a number of interesting issues arise. New\rinformation gained from the interaction may change the model and thereby\rinteract with planning. It may be desirable to customize the planning process\rin some way to the states or decisions currently under consideration, or\rexpected in the near future. If decision-making and model-learning are both\rcomputation-intensive processes, then the available computational resources may\rneed to be divided between them. To begin exploring these issues, in this section\rwe present Dyna-Q, a simple architecture integrating the major functions needed\rin an on-line planning agent. Each function appears in Dyna-Q in a simple,\ralmost trivial, form. In subsequent sections we elaborate some of the alternate\rways of achieving each function and the trade-offs between them. For now, we\rseek merely to illustrate the ideas and stimulate your intuition.\nWithin a planning agent, there\rare at least two roles for real experience: it can be used to improve the model\r(to make it more accurately match the real environment) and it can be used to\rdirectly improve the value function and policy using the kinds of reinforcement\rlearning methods we have discussed in previous chapters. The former we call model-learning, and the latter we call direct\rreinforcement learning (direct RL). The possible relationships between\rexperience, model, values, and policy are summarized in Figure 8.1. Each arrow\rshows a relationship of influence and presumed improvement. Note how experience\rcan improve value functions and policies either directly or indirectly via the\rmodel. It is the latter, which is sometimes called indirect\rreinforcement learning, that is involved in planning.\nBoth direct and indirect methods have advantages and\rdisadvantages. Indirect methods often make fuller use of a limited amount of\rexperience and thus achieve a better policy with fewer environmental\rinteractions. On the other hand, direct methods are much simpler and are not\raffected by biases in the design of the model. Some have argued that indirect\rmethods are always superior to direct ones, while others have argued that\rdirect methods are responsible for most human and animal\n\r\n\r\r\r\r\r\u0026nbsp;\nmodel\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; experience\n\r\nmodel\nlearning\nFigure 8.1: Relationships among\rlearning, planning, and acting.\n\r\r\r\r\r\u0026nbsp;\n\r\u0026nbsp;\nlearning. Related debates in psychology and\rartificial intelligence concern the relative importance of cognition as opposed\rto trial-and-error learning, and of deliberative planning as opposed to\rreactive decision-making (see Chapter 14 for discussion of some of these issues\rfrom the perspective of psychology). Our view is that the contrast between the\ralternatives in all these debates has been exaggerated, that more insight can\rbe gained by recognizing the similarities between these two sides than by opposing\rthem. For example, in this book we have emphasized the deep similarities\rbetween dynamic programming and temporal-difference methods, even though one\rwas designed for planning and the other for model-free learning.\nDyna-Q includes all of the processes shown in\rFigure 8.1��planning, acting, model- learning, and direct RL��all\roccurring continuously. The planning method is the random-sample one-step\rtabular Q-planning method given in Figure 8.1. The di\u0026shy;rect RL method is\rone-step tabular Q-learning. The model-learning method is also table-based and\rassumes the environment is deterministic. After each transition St, At Rt+1, St+1, the model records in its table entry for St, At the prediction\rthat Rt+1, St+1will deterministically follow. Thus, if the model is queried with a\rstate-action pair that has been experienced before, it simply returns the\rlast-observed next state and next reward as its prediction. During planning,\rthe Q-planning al\u0026shy;gorithm randomly samples only from state-action pairs that\rhave previously been experienced (in Step 1), so the model is never queried\rwith a pair about which it has no information.\nThe overall architecture of Dyna agents, of which the Dyna-Q\ralgorithm is one example, is shown in Figure 8.2. The central column represents\rthe basic interaction between agent and environment, giving rise to a\rtrajectory of real experience. The arrow on the left of the figure represents\rdirect reinforcement learning operating\n\r\nFigure 8.2: The general Dyna Architecture. Real\rexperience, passing back and forth between the environment and the policy,\raffects policy and value functions in much the same way as does simulated\rexperience generated by the model of the environment.\n\r\r\r\r\r\ron real experience to improve the value function\rand the policy. On the right are model-based processes. The model is learned\rfrom real experience and gives rise to simulated experience. We use the term search control to refer to the process that selects the\rstarting states and actions for the simulated experiences generated by the\rmodel. Finally, planning is achieved by applying reinforcement learning methods\rto the simulated experiences just as if they had really happened. Typically, as\rin Dyna-Q, the same reinforcement learning method is used both for learning\rfrom real experience and for planning from simulated experience. The\rreinforcement learning method is thus the ��final common path�� for both learning\rand planning. Learning and planning are deeply integrated in the sense that\rthey share almost all the same machinery, differing only in the source of their\rexperience.\nConceptually, planning, acting, model-learning, and direct RL occur\rsimultane\u0026shy;ously and in parallel in Dyna agents. For concreteness and\rimplementation on a serial computer, however, we fully specify the order in\rwhich they occur within a time step. In Dyna-Q, the acting, model-learning, and\rdirect RL processes require little computation, and we assume they consume just\ra fraction of the time. The remaining time in each step can be devoted to the\rplanning process, which is in\u0026shy;herently computation-intensive. Let us assume\rthat there is time in each step, after acting, model-learning, and direct RL,\rto complete n iterations (Steps 1-3) of the Q-planning algorithm. In the\rpseudocode algorithm for Dyna-Q in the box below, Model(s, a) denotes the\rcontents of the (predicted next state and reward) for state- action pair (s,\ra). Direct reinforcement learning, model-learning, and planning are implemented\rby steps (d), (e), and (f), respectively. If (e) and (f) were omitted, the\rremaining algorithm would be one-step tabular Q-learning.\n\r\nInitialize Q(s, a) and Model(s, a)\rfor all s G S and a G A(s)\nDo forever:\n(a)\u0026nbsp;\u0026nbsp; S �� current (nonterminal) state\n(b)\u0026nbsp;\u0026nbsp; A �� e-greedy(S, Q)\n(c)\u0026nbsp;\u0026nbsp; Execute action A; observe resultant reward, R, and state, S'\n(d)\u0026nbsp;\u0026nbsp; Q(S,A) �� Q(S,A)+ a[R ʮ7max0 Q(S,,a)\r�� Q(S,A)]\n(e)\u0026nbsp;\u0026nbsp; Model(S, A) �� R, S7 (assuming deterministic\renvironment)\n(f)\u0026nbsp;\u0026nbsp;\u0026nbsp; Repeat n times:\nS �� random previously observed state\rA �� random action previously taken in S R, S7 �� Model(S,A)\nQ(S, A) �� Q(S, A)ʮa[R ʮ7 max0\rQ(S7, a) �� Q(S, A)]\n\r\r\r\r\r\u0026nbsp;\nExample 8.1: Dyna Maze Consider the\rsimple maze shown inset in Figure 8.3. In each of the 47 states there are four\ractions, up, down, right, and left, which take the agent deterministically to\rthe corresponding neighboring states, except when movement is blocked by an\robstacle or the edge of the maze, in which case the agent remains where it is.\rReward is zero on all transitions, except those into the goal state, on which\rit is +1. After reaching the goal state (G), the agent returns to\n\r\r\r\r\r800-\n\r\r\r\r\r\r\r\r600-\n\r\r\r\r\r\r\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\rG\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\rS\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r��\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\r\r\r\ractions\n\r\r\r\r\r\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\r\r\r\r0 planning steps (direct RL\ronly)\n\r\r\r\r\r\r\r\r5 planning steps\n\r\r\r\r\r\r\r\r50 planning steps\n\r\r\r\r\r\r\r\r~i------- 1------ 1~\n10\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 20\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 30\n\r\r\r\r\r\r\r\r40\n\r\r\r\r\rSteps\nper\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 400H\nepisode\n200-\n\r\r\r50\n\r\r\r\r\r14-\n\r\r\u0026nbsp;\n\u0026nbsp;\n\r\rEpisodes\nFigure 8.3: A simple maze (inset) and the average learning curves\rfor Dyna-Q agents varying in their number of planning steps (n) per real step.\rThe task is to travel from S to G as quickly as possible.\nthe start state (S) to\rbegin a new episode. This is a discounted, episodic task with\nY\u0026nbsp; = 0.95.\nThe main part of Figure 8.3 shows\raverage learning curves from an experiment in which Dyna-Q agents were applied\rto the maze task. The initial action values were zero, the step-size parameter\rwas a= 0.1, and\rthe exploration parameter was e = 0.1. When selecting greedily among actions,\rties were broken randomly. The agents varied in the number of planning steps,\rn, they performed per real step. For each n, the curves show the number of\rsteps taken by the agent to reach the goal in each episode, averaged over 30\rrepetitions of the experiment. In each repetition, the initial seed for the\rrandom number generator was held constant across algorithms. Because of this,\rthe first episode was exactly the same (about 1700 steps) for all values of n,\rand its data are not shown in the figure. After the first episode, performance\rimproved for all values of n, but much more rapidly for larger values. Recall\rthat the n = 0 agent is a nonplanning agent, using only direct reinforcement\rlearning (one- step tabular Q-learning). This was by far the slowest agent on\rthis problem, despite the fact that the parameter values (a and e)were optimized for it. The nonplanning agent took\rabout 25 episodes to reach (e-)optimal performance, whereas the n = 5 agent\rtook about five episodes, and the n = 50 agent took only three episodes.\nFigure 8.4 shows why the planning\ragents found the solution so much faster than the nonplanning agent. Shown are\rthe policies found by the n = 0 and n = 50 agents halfway through the second\repisode. Without planning (n = 0), each episode adds only one additional step\rto the policy, and so only one step (the last) has been\n180 CHAPTER 8. PLANNING AND LEARNING WITH TABULAR\rMETHODS WITHOUT PLANNING (n=0)WITH PLANNING (n=50)\n\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r��\n\r\u0026nbsp;\n\r\u0026nbsp;\n\rG\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\rţ\n\r\rS\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\r\r\u0026nbsp;\nFigure 8.4: Policies found by\rplanning and nonplanning Dyna-Q agents halfway through the second episode. The\rarrows indicate the greedy action in each state; if no arrow is shown for a\rstate, then all of its action values were equal. The black square indicates the\rlocation of the agent.\nlearned so far. With\rplanning, again only one step is learned during the first episode, but here\rduring the second episode an extensive policy has been developed that by the\repisode��s end will reach almost back to the start state. This policy is built\rby the planning process while the agent is still wandering near the start\rstate. By the end of the third episode a complete optimal policy will have been\rfound and perfect performance attained.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nIn Dyna-Q,\rlearning and planning are accomplished by exactly the same algorithm, operating\ron real experience for learning and on simulated experience for planning.\rBecause planning proceeds incrementally, it is trivial to intermix planning and\ract\u0026shy;ing. Both proceed as fast as they can. The agent is always reactive and\ralways deliberative, responding instantly to the latest sensory information and\ryet always planning in the background. Also ongoing in the background is the\rmodel-learning process. As new information is gained, the model is updated to\rbetter match real\u0026shy;ity. As the model changes, the ongoing planning process will\rgradually compute a different way of behaving to match the new model.\nExercise 8.1 The\rnonplanning method looks particularly poor in Figure 8.4 because it is a\rone-step method; a method using multi-step bootstrapping would do better. Do\ryou think one of the multi-step bootstrapping methods from Chapter 7 could do\ras well as the Dyna method? Explain why or why not.\n8.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rWhen the Model Is Wrong\nIn the maze\rexample presented in the previous section, the changes in the model were\rrelatively modest. The model started out empty, and was then filled only with\rexactly correct information. In general, we cannot expect to be so fortunate.\rModels may be incorrect because the environment is stochastic and only a\rlimited number of samples have been observed, or because the model was learned\rusing function approximation that has generalized imperfectly, or simply\rbecause the environment has changed and its new behavior has not yet been\robserved. When the model is incorrect, the planning process is likely to\rcompute a suboptimal policy.\n\r\rIn some cases, the suboptimal\rpolicy computed by planning quickly leads to the discovery and correction of\rthe modeling error. This tends to happen when the model is optimistic in the\rsense of predicting greater reward or better state transitions than are\ractually possible. The planned policy attempts to exploit these opportunities\rand in doing so discovers that they do not exist.\n\r\r\rwandering around behind the barrier. the new opening\rand the new optimal\n\r\r\r\r\rExample 8.2: Blocking\rMaze A maze example illustrating this relatively\rminor kind of modeling error and recovery from it is shown in Figure 8.5.\rInitially, there is a short path from start to goal, to the right of the\rbarrier, as shown in the upper left of the figure. After 1000 time steps, the\rshort path is ��blocked,�� and a longer path is opened up along the left-hand\rside of the barrier, as shown in upper right of the figure. The graph shows\raverage cumulative reward for a Dyna-Q agent and an enhanced Dyna-Q+ agent to\rbe described shortly. The first part of the graph shows that both Dyna agents\rfound the short path within 1000 steps. When the environment changed, the\rgraphs become flat, indicating a period during which the agents obtained no\rreward because they were After a while, however, they were able to find\rbehavior.\n\r\r\r\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\rG\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\rS\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\r\r\r\rFigure\r8.5: Average performance of Dyna agents on a blocking task. The left\renvironment was used for the first 1000 steps, the right environment for\rthe rest. Dyna-Q+ is Dyna-Q with an exploration bonus that encourages\rexploration.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n\r\r\r\r\r\r\r\rTime steps\n\r\r\r\r\r\r\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\rG\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r��\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\rS\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\r\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rGreater difficulties arise when the environment\rchanges to become better than it was before, and yet the\rformerly correct policy does not reveal the improvement. In these cases the\rmodeling error may not be detected for a long time, if ever, as we see in the\rnext example.\nExample 8.3: Shortcut Maze The problem caused by this kind of environmental change is\rillustrated by the maze example shown in Figure 8.6. Initially, the optimal path is to go around the left side of the\rbarrier (upper left). After 3000 steps, however, a shorter path is opened up\ralong the right side, without disturbing the longer path (upper right). The\rgraph shows that the regular Dyna-Q agent never switched to the shortcut. In\rfact, it never realized that it existed. Its model said that there was no\rshortcut, so the more it planned, the less likely it was to step to the right\rand discover it. Even with an e-greedy policy, it is very unlikely that an\ragent will take so many exploratory actions as to discover the shortcut.\n\r\nTime steps\nFigure 8.6: Average performance of Dyna agents on a shortcut task. The left\renvironment was used for the first 3000 steps, the right environment for the\rrest.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n\r\r\r\r\r\u0026nbsp;\nThe general problem here is\ranother version of the conflict between exploration and exploitation. In a\rplanning context, exploration means trying actions that improve the model,\rwhereas exploitation means behaving in the optimal way given the current model.\rWe want the agent to explore to find changes in the environment, but not so\rmuch that performance is greatly degraded. As in the earlier\rexploration/exploitation conflict, there probably is no solution that is both\rperfect and practical, but simple heuristics are often effective.\nThe Dyna-Q+ agent that did solve the shortcut maze uses one such\rheuristic. This agent keeps track for each state-action pair of how many time\rsteps have elapsed since the pair was last tried in a real interaction with the\renvironment. The more time that has elapsed, the greater (we might presume) the\rchance that the dynamics of this pair has changed and that the model of it is\rincorrect. To encourage behavior that tests long-untried actions, a special\r��bonus reward�� is given on simulated experiences involving these actions. In\rparticular, if the modeled reward for a transition is r, and the transition has\rnot been tried in t time steps, then planning backups are done as if that\rtransition produced a reward of r + k^/T, for some small\rk. This encourages the agent to keep testing all accessible state transitions\rand even to find long sequences of actions in order to carry out such tests.[12] Of course\rall this testing has its cost, but in many cases, as in the shortcut maze, this\rkind of computational curiosity is well worth the extra exploration.\nExercise 8.2 Why did the Dyna agent with exploration bonus, Dyna-Q+, perform\rbetter in the first phase as well as in the second phase of the blocking and\rshortcut experiments?\nExercise 8.3 Careful inspection of Figure 8.6reveals\rthat the difference between Dyna-Q+ and Dyna-Q narrowed slightly over the first\rpart of the experiment. What is the reason for this?\nExercise 8.4 (programming) The\rexploration bonus described above actually changes the estimated values of\rstates and actions. Is this necessary? Suppose the bonus was used not in\rbackups, but solely in action selection. That is, suppose the action selected\rwas always that for which Q(St, a) +\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (St,\ra) was\nmaximal. Carry out a gridworld experiment that tests and illustrates\rthe strengths and weaknesses of this alternate approach.\n8.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rPrioritized Sweeping\nIn the Dyna agents presented in the preceding sections, simulated\rtransitions are started in state-action pairs selected uniformly at random from\rall previously ex\u0026shy;perienced pairs. But a uniform selection is usually not the\rbest; planning can be much more efficient if simulated transitions and backups\rare focused on particular state-action pairs. For example, consider what\rhappens during the second episode of the first maze task (Figure 8.4). At the\rbeginning of the second episode, only the state-action pair leading directly\rinto the goal has a positive value; the values of all other pairs are still\rzero. This means that it is pointless to back up along almost all transitions,\rbecause they take the agent from one zero-valued state to another, and thus the\rbackups would have no effect. Only a backup along a transition into the state\rjust prior to the goal, or from it, will change any values. If simulated\rtransitions are generated uniformly, then many wasteful backups will be made\rbefore stumbling onto one of these useful ones. As planning progresses, the region\rof useful back\u0026shy;ups grows, but planning is still far less efficient than it\rwould be if focused where it would do the most good. In the much larger\rproblems that are our real objective, the number of states is so large that an\runfocused search would be extremely inefficient.\nThis example suggests that\rsearch might be usefully focused by working backward from\rgoal states. Of course, we do not really want to use any methods specific to\rthe idea of ��goal state.�� We want methods that work for general reward functions.\rGoal states are just a special case, convenient for stimulating intuition. In\rgeneral, we want to work back not just from goal states but from any state\rwhose value has changed. Suppose that the values are initially correct given\rthe model, as they were in the maze example prior to discovering the goal.\rSuppose now that the agent discovers a change in the environment and changes\rits estimated value of one state, either up or down. Typically, this will imply\rthat the values of many other states should also be changed, but the only\ruseful one-step backups are those of actions that lead directly into the one\rstate whose value has been changed. If the values of these actions are updated,\rthen the values of the predecessor states may change in turn. If so, then\ractions leading into them need to be backed up, and then their\rpredecessor states may have changed. In this way one can work backward from\rarbitrary states that have changed in value, either performing useful backups\ror terminating the propagation. This general idea might be termed backward focusing of planning computations.\nAs the frontier of useful backups propagates backward, it often\rgrows rapidly, producing many state-action pairs that could usefully be backed\rup. But not all of these will be equally useful. The values of some states may\rhave changed a lot, whereas others may have changed little. The predecessor\rpairs of those that have changed a lot are more likely to also change a lot. In\ra stochastic environment, variations in estimated transition probabilities also\rcontribute to variations in the sizes of changes and in the urgency with which\rpairs need to be backed up. It is natural to prioritize the backups according\rto a measure of their urgency, and perform them in order of priority. This is\rthe idea behind prioritized sweeping. A queue is\rmaintained of every state-action pair whose estimated value would change\rnontrivially if backed up, prioritized by the size of the change. When the top\rpair in the queue is backed up, the effect on each of its predecessor pairs is\rcomputed.\nPrioritized sweeping for a deterministic environment\nInitialize\rQ(s, a), Model(s, a), for all s, a, and PQueue to empty Do forever:\n(a)\u0026nbsp;\rS �� current (nonterminal) state\n(b)\u0026nbsp;\rA �� policy(S, Q)\n(c)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\r\u0026nbsp;Execute action A; observe resultant\rreward, R, and state, S'\n(d)\u0026nbsp;\rModel(S, A) �� R, Sf\n(e)\u0026nbsp;\rP�� |R + y maxa Q(S',a) �� Q(S, A)|.\n(f)\u0026nbsp;\u0026nbsp;\rif P \u0026gt; 0, then insert S, A\rinto PQueue with priority P\n(g)\u0026nbsp;\rRepeat n times, while PQueue is\rnot empty:\nS, A ��\rfirst(PQueue)\nR,S' ��\rModel (S, A)\nQ(S, A) ��\rQ(S, A) + a[R + 7maxa Q(S ',a) �� Q(S, A)] Repeat, for all S A predicted to lead to\rS:\nR ��\rpredicted reward for S, A, S\nP �� |R +\rYmaxa Q(S, a) �� Q(��,Z)|.\nif P \u0026gt;\r0 then insert S, jA into PQueue with priority P\n\r\rIf the effect is greater than\rsome small threshold, then the pair is inserted in the queue with the new\rpriority (if there is a previous entry of the pair in the queue, then insertion\rresults in only the higher priority entry remaining in the queue). In this way\rthe effects of changes are efficiently propagated backward until quiescence.\rThe full algorithm for the case of deterministic environments is given in the\rbox.\nExample 8.4:\rPrioritized Sweeping on Mazes Prioritized sweeping has been found to\rdramatically increase the speed at which optimal solutions are found in maze\rtasks, often by a factor of 5 to 10. A typical example is shown in Figure 8.7.\rThese data are for a sequence of maze tasks of exactly the same structure as\rthe one shown in Figure 8.3, except that they vary in the grid resolution.\rPrioritized sweeping maintained a decisive advantage over unprioritized Dyna-Q.\rBoth systems made at most n = 5 backups per environmental interaction.\n\r\r\r\r\rFigure 8.7:\rPrioritized sweeping significantly shortens learning time on the Dyna maze\rtask for a wide range of grid resolutions. Reprinted from Peng and\rWilliams (1993).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n\r\r\r\r\r\r\r\rGridworld size (#states)\n\r\r\r\r\r\r\r\rBackups\nuntil\noptimal\nsolution\n\r\r\r\r\r\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rExample 8.5: Rod\rManeuvering The objective in this task is to maneuver a rod around some\rawkwardly placed obstacles within a limited rectangular work space to a goal\rposition in the fewest number of steps (see Figure 8.8). The rod can be translated along its long axis or perpendicular to\rthat axis, or it can be rotated in either direction around its center. The\rdistance of each movement is approximately 1/20 of the work space, and the\rrotation increment is 10 degrees. Translations are deterministic and quantized\rto one of 20 x 20 positions. The figure shows the obstacles and the shortest\rsolution from start to goal, found by prioritized sweeping. This problem is\rstill deterministic, but has four actions and 14,400 potential states (some of\rthese are unreachable because of the obstacles). This problem is probably too\rlarge to be solved with unprioritized methods.\n\r\nFigure 8.8: A rod-maneuvering task and its solution by prioritized sweeping.\rReprinted from Moore and Atkeson (1993).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n\r\r\r\r\r\u0026nbsp;\nExtensions of prioritized sweeping to stochastic\renvironments are straightforward. The model is maintained by keeping counts of\rthe number of times each state-action pair has been experienced and of what the\rnext states were. It is natural then to backup each pair not with a sample\rbackup, as we have been using so far, but with a full backup, taking into\raccount all possible next states and their probabilities of occurring.\nPrioritized sweeping is just\rone way of distributing computations to improve plan\u0026shy;ning efficiency, and probably\rnot the best way. One of prioritized sweeping��s limita\u0026shy;tions is that it uses full backups, which in stochastic environments may waste lots\rof computation on low-probability transitions. As we show in the following\rsection, sample backups can in many cases get closer to the true value function\rwith less computation despite the variance introduced by sampling. Sample\rbackups can win because they break the overall backing-up computation into\rsmaller pieces��those corresponding to individual transitions��which then enables\rit to be focused more narrowly on the pieces that will have the largest impact.\rThis idea was taken to what may be its logical limit in the ��small backups��\rintroduced by van Seijen and Sutton (2013). These are backups along a single\rtransition, like a sample backup, but based on the probability of the\rtransition without sampling, as in a full backup. By selecting the order in\rwhich small backups are done it is possible to greatly improve planning\refficiency beyond that possible with prioritized sweeping.\nWe have suggested in this chapter that all kinds of state-space\rplanning can be viewed as sequences of backups, varying only in the type of\rbackup, full or sample, large or small, and in the order in which the backups\rare done. In this section we have emphasized backward focusing, but this is\rjust one strategy. For example, another would be to focus on states according\rto how easily they can be reached from the states that are visited frequently\runder the current policy, which might be called forward focusing.\rPeng and Williams (1993) and Barto, Bradtke and Singh (1995) have explored\rversions of forward focusing, and the methods introduced in the next few\rsections take it to an extreme form.\n8.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rFull vs. Sample Backups\nThe examples in the previous sections give some\ridea of the range of possibilities for combining methods of learning and\rplanning. In the rest of this chapter, we analyze some of the component ideas\rinvolved, starting with the relative advantages of full and sample backups.\nMuch of this book has been\rabout different kinds of backups, and we have con\u0026shy;sidered a great many\rvarieties. Focusing for the moment on one-step backups, they vary primarily\ralong three binary dimensions. The first two dimensions are whether they back\rup state values or action values and whether they estimate the value for the\roptimal policy or for an arbitrary given policy. These two dimensions give rise\rto four classes of backups for approximating the four value functions, q^, v^,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ,\rand .\nThe other binary dimension is whether the backups\rare full backups, considering all possible events that\rmight happen, or sample backups, considering a single\rsample of what might happen. These three binary dimensions give rise to eight\rcases, seven of which correspond to specific algorithms, as shown in Figure\r8.9. (The eighth case does not seem to correspond to any useful backup.) Any of\rthese one-step backups can be used in planning methods. The Dyna-Q agents\rdiscussed earlier use q^ sample backups, but they could just as well use ��full backups, or\reither full or sample �� backups. The Dyna-AC system uses v^\rsample backups together with a learning policy structure. For stochastic\rproblems, prioritized sweeping is always done using one of the full backups.\nWhen we introduced one-step sample backups in\rChapter 6, we presented them as substitutes for full backups. In the absence\rof a distribution model, full backups are not possible, but sample backups can\rbe done using sample transitions from the environment or a sample model.\rImplicit in that point of view is that full backups, if possible, are\rpreferable to sample backups. But are they? Full backups certainly yield a\rbetter estimate because they are uncorrupted by sampling error, but they also\rrequire more computation, and computation is often the limiting resource in\rplanning. To properly assess the relative merits of full and sample backups for\rplanning we must control for their different computational requirements.\nFor concreteness, consider the full and sample backups for\rapproximating q^, and the special case of discrete states and actions, a\rtable-lookup representation of the approximate value function, Q, and a model\rin the form of estimated dynamics, p(s��r|s, a). The full backup for a state-action pair, s, a, is:\nQ(s, a)p(s\u0026#12316;r|s,a) r + 7m^xQ(s;, a;) .\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (8.1)\n\r\r6U|UJB9|-D\n\r\r\rU0IJBJ9J!\r0n|BA-D\n\n\r\r\r\r\rP\\J\n\r\r\r{s^fb\n\r\r\r\r\rɽ\nSO\nN\nv's\n\r\r\u0026nbsp;\n\u0026nbsp;\n\r\r\r\r\ruo!ien|BA9\rAo||od-o\n\n\r\r\r\r\rBSJBS\n\r\r\r(yv)ub\n\r\r\r\r\r,yt\n^0 i\nv��s\n\r\r\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\r\r\r\r\rU0��1BJ@1! 9n|BA\n\n\r\r\r\r\r\r\r\ruo!jen|EAe\rAonod\n\r\r\r\r\r(d\n(s)uA\n\r\r\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\r\r(Cl �Adajs-auo)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (da)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; pajBiujisa\nsdn\u0026gt;)0Bq a|diues\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; sdnqoeq\r||nj\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 9n|e��\nSQ0H13H yvmavi H1IM DMIMUV31\rQMV DMIMMVld f8 midVHD88T\n\r\rThe corresponding sample backup for s, a, given a sample next state\rand reward, S' and R (from the model), is the Q-learning-like update:\nQ(s, a) ��\rQ(s, a) + a R + 7maxQ(S', a') �� Q(s, a) ,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (8.2)\nL\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; a'\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nwhere a is the usual positive\rstep-size parameter.\nThe difference between these full and sample backups is significant\rto the extent that the environment is stochastic, specifically, to the extent\rthat, given a state and action, many possible next states may occur with\rvarious probabilities.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; If\u0026nbsp; only one\nnext state is possible, then the full and sample backups given\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; above\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; are\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; identical\n(taking a = 1). If there are many\rpossible next states, then there may be significant differences. In favor of\rthe full backup is that it is an exact computation, resulting in a new Q(s, a)\rwhose correctness is limited only by the correctness of the Q(s', a') at\rsuccessor states. The sample backup is in addition affected by sampling error.\rOn the other hand, the sample backup is cheaper computationally because it\rconsiders only one next state, not all possible next states. In practice, the\rcomputation required by backup operations is usually dominated by the number of\rstate-action pairs at which Q is evaluated. For a particular starting pair, s,\ra, let b be the branching factor (i.e., the number of\rpossible next states, s', for which p(s'|s,a) \u0026gt; 0). Then a full backup of\rthis pair requires roughly b times as much computation as a sample backup.\nIf there is enough time to complete\ra full backup, then the resulting estimate is generally better than that of b\rsample backups because of the absence of sampling error. But if there is\rinsufficient time to complete a full backup, then sample backups are always\rpreferable because they at least make some improvement in the value estimate\rwith fewer than b backups. In a large problem with many state-action pairs, we\rare often in the latter situation. With so many state-action pairs, full\rbackups of all of them would take a very long time. Before that we may be much\rbetter off with a few sample backups at many state-action pairs than with full\rbackups at a few pairs. Given a unit of computational effort, is it better\rdevoted to a few full backups or to b times as many sample backups?\nFigure 8.10 shows the results\rof an analysis that suggests an answer to this ques\u0026shy;tion. It shows the\restimation error as a function of computation time for full and sample backups\rfor a variety of branching factors, b. The case considered is that in which all\rb successor states are equally likely and in which the error in the initial\restimate is 1. The values at the next states are assumed correct, so the full\rbackup reduces the error to zero upon its completion. In this case, sample\rbackups reduce\nthe error according to��/ where t is the number of sample\rbackups that have been\n~bT\nperformed (assuming sample averages,\ri.e., a = 1/t). The key observation is that for moderately large b the error\rfalls dramatically with a tiny fraction of b backups. For these cases, many\rstate-action pairs could have their values improved dramatically, to within a\rfew percent of the effect of a full backup, in the same time that one\rstate-action pair could be backed up fully.\n\r\r\rFigure 8.10: Comparison of efficiency of full and sample\rbackups.\n\r\r\r\r\r\r\r\rNumber of max Q(sf, a!) computations\na0\n\r\r\r\r\rThe advantage of sample backups shown in Figure 8.10\ris probably an underesti\u0026shy;mate of the real effect. In a real problem, the values\rof the successor states wouldthemselves be\restimates updated by backups. By causing estimates to be more accu\u0026shy;rate sooner,\rsample backups will have a second advantage in that the values backed up from\rthe successor states will be more accurate. These results suggest that sample\rbackups are likely to be superior to full backups on problems with large\rstochastic branching factors and too many states to be solved exactly.\n8.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rTrajectory Sampling\nIn this\rsection we compare two ways of distributing backups. The classical approach,\rfrom dynamic programming, is to perform sweeps through the entire state (or\rstate- action) space, backing up each state (or state-action pair) once per\rsweep. This is problematic on large tasks because there may not be time to\rcomplete even one sweep. In many tasks the vast majority of the states are irrelevant\rbecause they are visited only under very poor policies or with very low\rprobability. Exhaustive sweeps implicitly devote equal time to all parts of the\rstate space rather than focusing where it is needed. As we discussed in Chapter\r4, exhaustive sweeps and the equal treatment of all states that they imply are\rnot necessary properties of dynamic programming. In principle, backups can be\rdistributed any way one likes (to assure convergence, all states or\rstate-action pairs must be visited in the limit an infinite number of times;\ralthough an exception to this is discussed in Section 8.7 below), but in\rpractice exhaustive sweeps are often used.\nThe second approach is to sample from the state or state-action\rspace according to some distribution. One could sample uniformly, as in the\rDyna-Q agent, but this would suffer from some of the same problems as\rexhaustive sweeps. More appealing is to distribute backups according to the\ron-policy distribution, that is, according to the distribution observed when following\rthe current policy. One advantage of this distribution is that it is easily\rgenerated; one simply interacts with the model, following the current policy.\rIn an episodic task, one starts in a start state (or according to the\rstarting-state distribution) and simulates until the terminal state. In a\rcontinuing task, one starts anywhere and just keeps simulating. In either case,\rsample state transitions and rewards are given by the model, and sample actions\rare given by the current policy. In other words, one simulates explicit\rindividual trajectories and performs backups at the state or state-action pairs\rencountered along the way. We call this way of generating experience and\rbackups trajectory\rsampling.\nIt is hard to imagine any\refficient way of distributing backups according to the on-policy distribution\rother than by trajectory sampling. If one had an explicit rep\u0026shy;resentation of\rthe on-policy distribution, then one could sweep through all states, weighting\rthe backup of each according to the on-policy distribution, but this leaves us\ragain with all the computational costs of exhaustive sweeps. Possibly one could\rsample and update individual state-action pairs from the distribution, but even\rif this could be done efficiently, what benefit would this provide over\rsimulating trajec\u0026shy;tories? Even knowing the on-policy distribution in an\rexplicit form is unlikely. The distribution changes whenever the policy\rchanges, and computing the distribution requires computation comparable to a\rcomplete policy evaluation. Consideration of such other possibilities makes\rtrajectory sampling seem both efficient and elegant.\nIs the on-policy distribution\rof backups a good one? Intuitively it seems like a good choice, at least better\rthan the uniform distribution. For example, if you are learning to play chess,\ryou study positions that might arise in real games, not random positions of\rchess pieces. The latter may be valid states, but to be able to accurately\rvalue them is a different skill from evaluating positions in real games. We\rwill also see in Chapter 9 that the on-policy distribution has significant\radvantages when function approximation is used. Whether or not function\rapproximation is used, one might expect on-policy focusing to significantly\rimprove the speed of planning.\nFocusing on the on-policy\rdistribution could be beneficial because it causes vast, uninteresting parts of\rthe space to be ignored, or it could be detrimental because it causes the same\rold parts of the space to be backed up over and over. We conducted a small\rexperiment to assess the effect empirically. To isolate the effect of the\rbackup distribution, we used entirely one-step full tabular backups, as defined\rby (8.1). In the uniform case, we cycled through all\rstate-action pairs, backing up each in place, and in the on-policy\rcase we simulated episodes, all starting in the same state, backing up each\rstate-action pair that occurred under the current e-greedy policy (e = 0.1). The tasks were undiscounted episodic tasks, generated randomly as\rfollows. From each of the |S| states, two actions were possible, each of which\rresulted in one of b next states, all equally likely, with a different random\rselection of b states for each state-action pair. The branching factor, b, was\rthe same for all state-action pairs. In addition, on all transitions there was\ra 0.1probability of transition to the terminal state, ending the\repisode. We used episodic tasks to get a clear measure of the quality of the\rcurrent policy. At any point in the planning process one can stop and\rexhaustively compute vn(so), the true value of the start state under the greedy\n\r\rValue of start state under greedy\rpolicy\n\r\r\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\r\rComputation time, in full backups\n\r\r\r\r\rFigure 8.11:\rRelative efficiency of backups distributed uniformly across the state\rspace versus focused on simulated on-policy trajectories, each starting in\rthe same state. Results are for randomly generated tasks of two sizes and\rvarious branching factors, b.\n\r\r\r\r\r\r\r\rComputation time, in full\rbackups\n\r\r\r\r\r\r\r\rValue of start state under\rgreedy policy\n\r\r\r\r\r\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rpolicy,����given the current action-value function Q, as an indication of how\rwell the agent would do on a new episode on which it acted greedily (all the while\rassuming the model is correct).\nThe upper part of Figure 8.11 shows results\raveraged over 200 sample tasks with 1000 states and branching factors of 1, 3,\rand 10. The quality of the policies found is plotted as a function of the\rnumber of full backups completed. In all cases, sam\u0026shy;pling according to the\ron-policy distribution resulted in faster planning initially and retarded\rplanning in the long run. The effect was stronger, and the initial period of\rfaster planning was longer, at smaller branching factors. In other experiments,\rwe found that these effects also became stronger as the number of states\rincreased. For example, the lower part of Figure 8.11 shows results for a\rbranching factor of 1 for tasks with 10,000 states. In this case the advantage\rof on-policy focusing is large and long-lasting.\nAll of these results make sense. In the short term, sampling\raccording to the on-policy distribution helps by focusing on states that are\rnear descendants of the start state. If there are many states and a small\rbranching factor, this effect will be large and long-lasting. In the long run,\rfocusing on the on-policy distribution may hurt because the commonly occurring\rstates all already have their correct values. Sampling them is useless, whereas\rsampling other states may actually perform some useful work. This presumably is\rwhy the exhaustive, unfocused approach does better in the long run, at least\rfor small problems. These results are not conclusive because they are only for\rproblems generated in a particular, random way, but they do suggest that\rsampling according to the on-policy distribution can be a great advantage for\rlarge problems, in particular for problems in which a small subset of the\rstate-action space is visited under the on-policy distribution.\n8.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rReal-time Dynamic Programming\nReal-time dynamic programming, or RTDP, is an on-policy trajectory-sampling ver\u0026shy;sion of DP��s\rvalue-iteration algorithm. Because it is closely related to conventional\rsweep-based policy iteration, RTDP illustrates in a particularly clear way some\rof the advantages that on-policy trajectory sampling can provide. RTDP backs up\rthe values of states visited in actual or simulated trajectories by means of\rfull tabu\u0026shy;lar value-iteration backups as defined by (4.10). It is basically the\ralgorithm that produced the on-policy results shown in Figure 8.11.\nThe close connection between RTDP and\rconventional DP makes it possible to derive some theoretical results by\radapting existing theory. RTDP is an example of an asynchronous\rDP algorithm as described in Section 4.5. Asynchronous DP algorithms are not\rorganized in terms of systematic sweeps of the state set; they back up state\rvalues in any order whatsoever, using whatever values of other states happen to\rbe available. In RTDP, the backup order is dictated by the order states are\rvisited in real or simulated trajectories.\nIf trajectories can start only from a designated\rset of start states, and if you areinterested in the prediction problem for a given policy, then on-policy\rtrajectory sampling allows the algorithm to completely skip states that cannot\rbe reached by the given policy from any of the start states: unreachable states\rare irrelevant to the prediction problem. For a control problem, where the goal\ris to find an optimal policy instead of evaluating a given policy, there might\rwell be states that cannot be reached by any optimal policy from any of the\rstart states, and there is no need to specify optimal actions for these\rirrelevant states. What is needed is an optimal partial policy,\rmeaning a policy that is optimal for the relevant states but can specify\rarbitrary actions, or even be undefined, for the irrelevant states (see the\rillustration below).\nBut finding such an opti\u0026shy;mal\rpartial policy with an on-\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; irrelevant\u0026nbsp; States��\npolicy trajectory-sampling\rcon-\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; unreachable from any start state\nunder\rany optimal policy\ntrol method, such as Sarsa\n(Section 6.4), in\rgeneral Start States requires visiting all state-\naction pairs��even those that will turn out to be\rirrelevant�� an infinite number of times.\nThis can be done, for exam\u0026shy;ple, by using\rexploring starts (Section 5.3). This is true for RTDP as well: for episodic\rtasks with exploring starts, RTDP is an asynchronous value-iteration algorithm\rthat converges to optimal polices for discounted finite MDPs (and for the\rundiscounted case under certain conditions). Unlike the situ\u0026shy;ation for a\rprediction problem, it is generally not possible to stop backing up any state\ror state-action pair if convergence to an optimal policy is important.\nThe most interesting result for RTDP is that for\rcertain types of problems sat\u0026shy;isfying reasonable conditions, RTDP is guaranteed\rto find a policy that is optimal on the relevant states without visiting every\rstate infinitely often, or even without visiting some states at all. Indeed, in\rsome problems, only a small fraction of the states need to be visited. This can\rbe a great advantage for problems with very large state sets, where even a\rsingle sweep may not be feasible.\nThe tasks for which this result holds are\rundiscounted episodic tasks for MDPs with absorbing goal states that generate\rzero rewards, as described in Section 3.4. At every step of a real or simulated\rtrajectory, RTDP selects a greedy action (breaking ties randomly) and applies\rthe full value-iteration backup operation to the current state. It can also\rbackup the values of an arbitrary collection of other states at each step; for\rexample, it can backup the values of states visited in a limited-horizon\rlook-ahead search from the current state.\nFor these problems, with each episode beginning\rin a state randomly chosen from the set of start states, and ending at a goal\rstate, RTDP converges (with probability one) to a policy that is optimal for\rall the relevant states[13]provided the following \n\r\rconditions are satisfied: 1) the initial value of every goal state is zero, 2) there exists at least one policy that guarantees that a goal state\rwill be reached with probability one from any start state, 3) all rewards for\rtransitions from non-goal states are strictly negative, and 4) all the initial\rvalues are equal to, or greater than, their optimal values (which can be\rsatisfied by simply setting the initial values of all states to zero). This\rresult was proved by Barto, Bradtke, and Singh (1995) by combining results for\rasynchronous DP with results about a heuristic search algorithm known as learning real-time A*due to Korf (1990).\nTasks having these properties are examples of stochastic optimal\rpath problems, which are usually\rstated in terms of cost minimization instead as reward maximiza\u0026shy;tion, as we do\rhere. Maximizing the negative returns in our version is equivalent to\rminimizing the costs of paths from a start state to a goal state. Examples of\rthis kind of task are minimum-time control tasks, where each time step required\rto reach a goal produces a reward of ��1, or problems like the Golf example in\rSection 3.7, whose objective is to hit the hole with the fewest strokes.\nExample 8.6: RTDP on the Racetrack The racetrack problem of Exercise 5.7 in\rSection 5.7 is a stochastic optimal path problem. Comparing RTDP and the con\u0026shy;ventional\rDP value iteration algorithm on an example racetrack problem illustrates some\rof the advantages of on-policy trajectory sampling.\nRecall from the exercise that an agent has to learn how to drive a\rcar around a turn like those shown in Figure 5.6 and cross the finish line as\rquickly as possible while staying on the track. Start states are all the\rzero-speed states on the starting line; the goal states are all the states that\rcan be reached in one time step by crossing the finish line from inside the\rtrack. Unlike Exercise 5.7, here there is no limit on the car��s speed, so the\rstate set is potentially infinite. However, the set of states that can be\rreached from the set of start states via any policy is finite and can be\rconsidered to be the state set of the problem. Each episode begins in a\rrandomly selected start state and ends when the car crosses the finish line.\rThe rewards are ��1 for each step until the car crosses the finish line. If the\rcar hits the track boundary, it is moved back to a random start state, and the\repisode continues.\nA racetrack similar to the small racetrack on the left of Figure 5.6\rhas 9,115 states reachable from start states by any policy, only 599 of which\rare relevant, meaning that they are reachable from some start state via some\roptimal policy. (The number of relevant states was estimated by counting the\rstates visited while executing optimal actions for 107episodes.)\nThe table below\rcompares solving this task by conventional DP and by RTDP. These results are\raverages over 25 runs, each begun with a different random number seed.\rConventional DP in this case is value iteration using exhaustive sweeps of the\rstate set, with values backed up one state at a time in place, meaning that the\rupdate for each state uses the most recent values of the other states (This is\rthe Gauss-Seidel version of value iteration, which was found to be\rapproximately twice as fast as the Jacobi version on this problem. See Section\r4.8.) No special attention was paid to\ngreedy actions.\nthe ordering of the updates;\rother orderings could have produced faster convergence. Initial values were all\rzero for each run of both methods. DP was judged to have converged when the\rmaximum change in a state value over a sweep was less than 10-4, and\rRTDP was judged to have converged when the average time to cross the finish\rline over 20episodes appeared to stabilize at an asymptotic number of steps. This version\rof RTDP backed up only the value of the current state on each step.\n\u0026nbsp;\n\rDP\n\rRTDP\n\r\rAverage\rcomputation to convergence\n\r28 sweeps\n\r4000\repisodes\n\r\rAverage\rnumber of backups to convergence\n\r252,784\n\r127,600\n\r\rAverage\rnumber of backups per episode\n\rһ\n\r31.9\n\r\r% of states\rbacked up \u0026lt; 100times\n\rһ\n\r98.45\n\r\r% of states\rbacked up \u0026lt; 10times\n\rһ\n\r80.51\n\r\r% of states\rbacked up 0times\n\rһ\n\r3.18\n\r\r\r\r\u0026nbsp;\nBoth methods produced policies averaging between 14 and 15 steps to\rcross the finish line, but RTDP required only roughly half of the backups that\rDP did. This is the result of RTDP��s on-policy trajectory sampling. Whereas the\rvalue of every state was backed up in each sweep of DP, RTDP focused backups on\rfewer states. In an average run, RTDP backed up the costs of %98.45 of the\rstates no more than 100 times and %80.51 of the states no more than 10 times;\rthe values of about 290 states were not backed up at all in an average run.\nAnother advantage of RTDP over conventional value iteration is that\rwith RTDP as the value function approaches the optimal value function, V*,\rthe policy used by the agent to generate trajectories approaches an optimal\rpolicy because it is always greedy with respect to the current value function.\rThis is in contrast to the situation in conventional value iteration. In\rpractice, value iteration terminates when the value function changes by only a\rsmall amount in a sweep, which is how we terminated it to obain the results in\rthe table above. At this point, the value function closely approximates V*, and\ra greedy policy is close to an optimal policy. However, it is possible that\rpolicies that are greedy with respect to the latest value function were\roptimal, or nearly so, long before value iteration terminates. (Recall from\rChapter 4 that optimal policies can be greedy with respect to many different\rvalue functions, not just V*.) Checking for the emergence of an optimal policy\rbefore value iteration converges is not a part of the conventional DP algorithm\rand requires a considerable amount of extra computation.\nIn the racetrack example, by running many test episodes after each\rDP sweep, with actions selected greedily according to the result of that sweep,\rit was possible to estimate the earliest point in the DP computation at which\rthe approximated optimal evaluation function was good enough so that the\rcorresponding greedy policy was nearly optimal. For this racetrack, a\rclose-to-optimal policy emerged after 15 sweeps of value iteration, or after\r136,725 value iteration backups. This is considerably less than the 252,784\rbackups DP needed to converge to V*, but sill more than the 127,600 backups\rRTDP required.\nAlthough these\rsimulations are certainly not definitive comparisons of the RTDP with conventional sweep-based\rvalue iteration, they illustrate some of advantages of on-policy trajectory\rsampling. Whereas conventional value iteration continued to back up the value\rof all the states, RTDP strongly focused on subsets of the states that were\rrelevant to the problem��s objective. This focus became increasingly narrow as\rlearning continued. Because the convergence theorem for RTDP applies to the\rsimulations, we know that RTDP eventually would have focused only on relevant\rstates, i.e., on states making up optimal paths. RTDP achieved nearly optimal\rcontrol with about 50% of the computation required by sweep-based value\riteration.\n8.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rPlanning at Decision Time\nPlanning can be used in at least two\rways. The one we have considered so far in this chapter, typified by dynamic\rprogramming and Dyna, is to use planning to gradually improve a policy or value\rfunction on the basis of simulated experience obtained from a model (either a\rsample or a distribution model). Selecting actions is then a matter of\rcomparing the current state��s action values obtained from a table in the\rtabular case we have thus far considered, or by evaluating a mathematical\rexpression in the approximate methods we consider in Part II below. Well before\ran action is selected for any current state St, planning has played a part in\rimproving the table entries, or the mathematical expression, needed to select\rthe action for many states, including St. Used this way, planning is not\rfocussed on the current state. We call planning used in this way background planning.\nThe other way to use planning\ris to begin and complete it after encountering each new\rstate St, as a computation whose output is the selection of a single action At�� on the next step planning begins anew with St+i to produce At+i, and\rso on. The simplest, and almost degenerate, example of this use of planning is\rwhen only state values are available, and an action is selected by comparing\rthe values of model- predicted next states for each action (or by comparing the\rvalues of afterstates as in the tic-tac-toe example in Chapter 1). More\rgenerally, planning used in this way can look much deeper than one-step-ahead\rand evaluate action choices leading to many different predicted state and\rreward trajectories. Unlike the first use of planning, here planning focusses\ron a particular state. We call this decision-time planning.\nThese two ways of thinking\rabout planning��using simulated experience to grad\u0026shy;ually improve a policy or\rvalue function, or using simulated experience to select an action for the\rcurrent state��can blend together in natural and interesting ways, but they have\rtended to be studied separately, and that is a good way to first understand\rthem. Let us now take a closer look at decision-time planning.\nEven when planning is only done at decision time, we can still view\rit, as we did in Section 8.1, as proceeding from simulated experience to\rbackups and values, and ultimately to a policy. It is just that now the values\rand policy are specific to the current state and the action choices available\rthere, so much so that the values and policy created by the planning process\rare typically discarded after being used to select the current action. In many\rapplications this is not a great loss because there are very many states and we\rare unlikely to return to the same state for a long time. In general, one may\rwant to do a mix of both: focus planning on the current state and\rstore the results of planning so as to be that much farther along should one\rreturn to the same state later. Decision-time planning is most useful in\rapplications in which fast responses are not required. In chess playing\rprograms, for example, one may be permitted seconds or minutes of computation\rfor each move, and strong programs may plan dozens of moves ahead within this\rtime. On the other hand, if low latency action selection is the priority, then\rone is generally better off doing planning in the background to compute a\rpolicy that can then be rapidly applied to each newly encountered state.\n8.9\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rHeuristic Search\nThe classical state-space planning\rmethods in artificial intelligence are decision-time planning methods\rcollectively known as heuristic search. In heuristic\rsearch, for each state encountered, a large tree of possible continuations is\rconsidered. The approximate value function is applied to the leaf nodes and\rthen backed up toward the current state at the root. The backing up within the\rsearch tree is just the same as in the full backups with maxes (those for vľand q^) discussed\rthroughout this book. The backing up stops at the state-action nodes for the\rcurrent state. Once the backed-up values of these nodes are computed, the best\rof them is chosen as the current action, and then all backed-up values are\rdiscarded.\nIn conventional heuristic\rsearch no effort is made to save the backed-up values by changing the\rapproximate value function. In fact, the value function is generally designed\rby people and never changed as a result of search. However, it is natural to\rconsider allowing the value function to be improved over time, using either the\rbacked-up values computed during heuristic search or any of the other methods\rpresented throughout this book. In a sense we have taken this approach all\ralong. Our greedy, e-greedy, and UCB (Section 2.7) action-selection methods are\rnot unlike heuristic search, albeit on a smaller scale. For example, to compute\rthe greedy action given a model and a state-value function, we must look ahead\rfrom each possible action to each possible next state, backup the rewards and\restimated values, and then pick the best action. Just as in conventional\rheuristic search, this process computes backed-up values of the possible actions,\rbut does not attempt to save them. Thus, heuristic search can be viewed as an\rextension of the idea of a greedy policy beyond a single step.\nThe point of searching deeper\rthan one step is to obtain better action selections. If one has a perfect model\rand an imperfect action-value function, then in fact deeper search will usually\ryield better policies.[14]Certainly, if the search is all the way to the end of the episode,\rthen the effect of the imperfect value function is eliminated, and the action\rdetermined in this way must be optimal. If the search is of sufficient \n\r\rdepth k\rsuch that ʮis very small, then the actions will be correspondingly near\roptimal. On the other hand, the deeper the search, the more computation is\rrequired, usually resulting in a slower response time. A good example is\rprovided by Tesauro��s grandmaster-level backgammon player, TD-Gammon (Section\r16.1). This system used TD learning to learn an afterstate value function\rthrough many games of self\u0026shy;play, using a form of heuristic search to make its\rmoves. As a model, TD-Gammon used a priori knowledge of the probabilities of\rdice rolls and the assumption that the opponent always selected the actions\rthat TD-Gammon rated as best for it. Tesauro found that the deeper the\rheuristic search, the better the moves made by TD-Gammon, but the longer it\rtook to make each move. Backgammon has a large branching factor, yet moves must\rbe made within a few seconds. It was only feasible to search ahead selectively\ra few steps, but even so the search resulted in significantly better action\rselections.\nWe should not overlook the\rmost obvious way in which heuristic search focuses backups: on the current\rstate. Much of the effectiveness of heuristic search is due to its search tree\rbeing tightly focused on the states and actions that might immediately follow\rthe current state. You may spend more of your life playing chess than checkers,\rbut when you play checkers, it pays to think about checkers and about your\rparticular checkers position, your likely next moves, and successor positions.\rNo matter how you select actions, it is these states and actions that are of\rhighest priority for backups and where you most urgently want your approximate\rvalue function to be accurate. Not only should your computation be\rpreferentially devoted to imminent events, but so should your limited memory\rresources. In chess, for example, there are far too many possible positions to\rstore distinct value estimates for each of them, but chess programs based on\rheuristic search can easily store distinct estimates for the millions of\rpositions they encounter looking ahead from a single position. This great\rfocusing of memory and computational resources on the current decision is\rpresumably the reason why heuristic search can be so effective.\nThe distribution of backups\rcan be altered in similar ways to focus on the current state and its likely\rsuccessors. As a limiting case we might use exactly the methods of heuristic\rsearch to construct a search tree, and then perform the individual, one-step\rbackups from bottom up, as suggested by Figure 8.12. If the backups are ordered\rin this way and a tabular representation is used, then exactly the same backup\rwould be achieved as in depth-first heuristic search. Any state-space search\rcan be viewed in this way as the piecing together of a large number of\rindividual one-step backups. Thus, the performance improvement observed with\rdeeper searches is not due to the use of multistep backups as such. Instead, it\ris due to the focus and concentration of backups on states and actions\rimmediately downstream from the current state. By devoting a large amount of\rcomputation specifically relevant to the candidate actions, decision-time\rplanning can produce better decisions than can be produced by relying on\runfocused backups.\n\r\nFigure 8.12: The deep backups of heuristic search can\rbe implemented as a sequence of one-step backups (shown here outlined). The\rordering shown is for a selective depth-first search.\n\r\r\r\r\r\u0026nbsp;\n8.10\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rRollout Algorithms\nRollout algorithms are decision-time\rplanning algorithms based on Monte Carlo con\u0026shy;trol applied to simulated\rtrajectories that all begin at the current environment state. They estimate\raction values for a given policy by averaging the returns of many simulated\rtrajectories that start with each possible action and then follow the given\rpolicy. When the action-value estimates are considered to be accurate enough,\rthe action (or one of the actions) having the highest estimated value is\rexecuted, after which the process is carried out anew from the resulting next\rstate. As explained by Tesauro and Galperin (1997), who experimented with\rrollout algorithms for playing backgammon, the term ��rollout�� comes from\restimating the value of a backgammon position by playing out, i.e., ��rolling\rout,�� the position many times to the game��s end with randomly generated\rsequences of dice rolls, where the moves of both players are made by some fixed\rpolicy.\nUnlike the Monte Carlo control\ralgorithms described in Chapter 5, the goal of a rollout algorithm is not to\restimate a complete optimal action-value function,����or a complete\raction-value function,��,fora given policy n. Instead, they produce Monte Carlo estimates of\raction values only for each current state and for a given policy usually called\rthe rollout policy. As decision-time planning\ralgorithms, rollout algorithms make immediate use of these action-value\restimates, then discard them. This makes rollout algorithms relatively simple\rto implement because there is no need to sample outcomes for every state-action\rpair, and there is no need to approximate a function over either the state\rspace or the state-action space.\nWhat then do rollout\ralgorithms accomplish? The policy improvement theorem described in Section 4.2\rtells us that given any two policies n and that are identical except that n;(s)\r= a = n(s) for some state s, if q^(s, a) \u0026gt; v^(s), then policy n! is as good as, or better, than n. Moreover, if\rthe inequality is strict, then n! is in fact\rbetter than n. This applies to rollout algorithms where s is the\rcurrent state and n is the rollout policy. Averaging the\rreturns of the simulated trajectories produces estimates of (s, a') for each action a' G A(s). Then the policy that selects an action in s that maximizes these estimates and thereafter follows n is a good candidate for a policy that improves over n. The result is like one step of the policy-iteration algorithm of\rdynamic programming discussed in Section 4.3 (though it is more like one step\rof asynchronous value iteration described in Section 4.5\rbecause it changes the action for just the current state).\nIn other words, the aim of a\rrollout algorithm is to improve upon the default policy; not to find an optimal\rpolicy. Experience has shown that rollout algorithms can be surprisingly\reffective. For example, Tesauro and Galperin (1997) were surprised by the\rdramatic improvements in backgammon playing ability produced by the rollout\rmethod. In some applications, a rollout algorithm can produce good performance\reven if the rollout policy is completely random. But clearly, the performance\rof the improved policy depends on the performance of the rollout policy and the\raccuracy of the Monte Carlo value estimates: the better the rollout policy and\rthe more accurate the value estimates, the better the policy produced by a rollout\ralgorithm is likely be.\nThis involves important\rtradeoffs because better rollout policies typically mean that more time is\rneeded to simulate enough trajectories to obtain good value esti\u0026shy;mates. As\rdecision-time planning methods, rollout algorithms usually have to meet strict\rtime constraints. The computation time needed by a rollout algorithm depends on\rthe number of actions that have to be evaluated for each decision, the number\rof time steps in the simulated trajectories needed to obtain useful sample\rreturns, the time it takes the rollout policy to make decisions, and the number\rof simulated trajectories needed to obtain good Monte Carlo action-value\restimates.\nBalancing these factors is\rimportant in any application of rollout methods, though there are several ways\rto ease the challenge. Because the Monte Carlo trials are independent of one\ranother, it is possible to run many trials in parallel on separate processors.\rAnother tact is to truncate the simulated trajectories short of complete\repisodes, correcting the truncated returns by means of a stored evaluation\rfunction (which brings into play all that we have said about truncated returns\rand backups in the preceding chapters). It is also possible, as Tesauro and\rGalperin (1997) suggest, to monitor the Monte Carlo simulations and prune away\rcandidate actions that are unlikely to turn out to be the best, or whose values\rare close enough to that of the cur\u0026shy;rent best that choosing them instead would\rmake no real difference (though Tesauro and Galperin point out that this would\rcomplicate a parallel implementation).\nWe do not ordinarily think of rollout algorithms as learning\ralgorithms because they do not maintain long-term memories of values or\rpolicies. However, these algorithms take advantage of some of the features of\rreinforcement learning that we have emphasized in this book. As instances of\rMonte Carlo control, they estimate action values by averaging the returns of a\rcollection of sample trajectories, in this case trajectories of simulated\rinteractions with a sample model of the environment. In this way they are like\rreinforcement learning algorithms in avoiding the exhaustive sweeps of dynamic\rprogramming by trajectory sampling, and in avoiding the need for distribution\rmodels by relying on sample, instead of full, backups. Finally, rollout\ralgorithms take advantage of the policy improvement property by acting greedily\rwith respect to the estimated action values.\n8.11\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rMonte Carlo Tree Search\nMonte Carlo Tree Search(MCTS) is a recent and strikingly successful example of\rdecision-time planning. At is base, MCTS is a rollout algorithm as described\rabove, but enhanced by the addition of a means for accumulating value estimates\robtained from the Monte Carlo simulations in order to successively direct\rsimulations toward more highly-rewarding trajectories. MCTS is largely\rresponsible for the improvement in computer Go from a weak amateur level in\r2005 to a grandmaster level (6dan or more) in 2015.\rMany variations of the basic algorithm have been developed, including a variant\rthat we discuss in Section 16.7 that was critical for the stunning 2016\rvictories of the program AlphaGo over an 18-time world champion Go player. MCTS\rhas proved to be effective in a wide variety of competitive settings, including\rgeneral game playing (e.g., see Finnsson \u0026amp; Bjornsson, 2008;\rGenesereth \u0026amp; Thielscher, 2014), but it is not limited to games; it can be\reffective for single-agent sequential decision problems if there is an\renvironment model simple enough for fast multistep simulation.\nMCTS is executed after encountering each new\rstate to select the agent��s action for that state; it is executed again to\rselect the action for the next state, and so on. As in a rollout algorithm,\reach execution is an iterative process that simulates many trajectories\rstarting from the current state and running to a terminal state (or until\rdiscounting makes any further reward negligible as a contribution to the\rreturn). The core idea of MCTS is to successively focus multiple simulations\rstart\u0026shy;ing at the current state by extending the initial portions of\rtrajectories that have received high evaluations from earlier simulations. MCTS\rdoes not have to retain approximate value functions or policies from one action\rselection to the next, though in many implementations it retains selected\raction values likely to be useful for its next execution.\nFor the most part, the actions in the simulated\rtrajectories are generated using a simple policy, usually called a rollout\rpolicy as it is for simpler rollout algorithms. When both the rollout policy\rand the model do not require a lot of computation, many simulated trajectories\rcan be generated in a short period of time. As in any tabular Monte Carlo\rmethod, the value of a state-action pair is estimated as the average of the\r(simulated) returns from that pair. Monte Carlo value estimates are maintained\ronly for the subset of state-action pairs that are most likely to be reached in\ra few steps, which form a tree rooted at the current state, as illustrated in\rFigure 8.13. MCTS incrementally extends the tree by adding nodes representing\rstates that look promising based on the results of the simulated trajectories.\rAny simulated trajectory will pass through the tree and then exit it at some\rleaf node. Outside the tree and at the leaf nodes the rollout policy is used\rfor action selections, \n\r\rbut at the states inside the tree something\rbetter is possible. For these states we have value estimates for of at least\rsome of the actions, so we can pick among them using an informed policy, called\rthe tree policy, that balances exploration and\rexploitation. For example, the tree policy could select actions using an\re-greedy or UCB selection rule (Chapter 2).\nIn more detail, each iteration\rof a basic version of MCTS consists of the following four steps as illustrated\rin Figure 8.13:\n1.\u0026nbsp;\u0026nbsp;\rSelection.\rStarting at the root node, a tree\rpolicy based on the action values attached to the edges of the tree\rtraverses the tree to select a leaf node.\n2.\u0026nbsp;\u0026nbsp;\rExpansion.\rOn some iterations (depending on details of the\rapplication), the tree is expanded from the selected leaf node by adding one or\rmore child nodes reached from the selected node via unexplored actions.\n3.\u0026nbsp;\u0026nbsp;\rSimulation.\rFrom the selected node, or from one of its\rnewly-added child nodes (if any), simulation of a complete episode is run with\ractions selected by the rollout policy. The result is a Monte Carlo trial with\ractions selected first by the tree policy and beyond the tree by the rollout\rpolicy.\n4.\u0026nbsp;\u0026nbsp;\rBackup.\rThe return generated by the simulated episode is\rbacked up to update, or to initialize, the action values attached to the edges\rof the tree traversed by the tree policy in this iteration of MCTS. No values\rare saved for the states and actions visited by the rollout policy beyond the\rtree. Figure 8.13 illustrates this by showing a backup from the terminal state\rof the simulated trajectory directly to the state-action node in the tree where\rthe rollout policy began (though in general, the entire return over the\rsimulated trajectory is backed up to this state-action node).\nMCTS continues executing these four steps,\rstarting each time at the tree��s root node, until no more time is left, or some\rother computational resource is exhausted. Then, finally, an action from the\rroot node (which still represents the current state of the environment) is\rselected according to some mechanism that depends on the accumulated statistics\rin the tree; for example, it may be an action having the largest action value\rof all the actions available from the root state, or perhaps the action with\rthe largest visit count to avoid selecting outliers. This is the action MCTS\ractually selects. After the environment transitions to a new state, MCTS is run\ragain, sometimes starting with a tree of a single root node representing the\rnew state, but often starting with a tree containing any descendants of this\rnode left over from the tree constructed by the previous execution of MCTS; all\rthe remaining nodes are discarded, along with the action values associated with\rthem.\nMCTS was first proposed to select moves in\rprograms playing two-person compet\u0026shy;itive games, such as Go. For game playing,\reach simulated episode is one complete play of the game in which both players\rselect actions by the tree and rollout poli\u0026shy;cies. Section 16.7 describes an\rextension of MCTS used in the AlphaGo program that combines the Monte Carlo\revaluations of MCTS with action values learned by a deep ANN via self-play\rreinforcement learning.\n\r\r\r\r\rH��\n\r\r\r\r\rRepeat while time remains\n\r\r\r\r\rSimulation\n\nRollout\nPolicy\n\r\r\r\r\r\r\r\rBackup\n\n\r\r\r\r\r\r\r\r\u0026#8226; Selection\n\n\r\r\r\r\r\r\r\rTree\nPolicy\n\r\r\r\r\r\r\r\rExpansion\n\n\r\r\r\r\r\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rFigure 8.13: Monte Carlo Tree\rSearch. When the environment changes to a new state, MCTS executes as many\riterations as possible before an action needs to be selected, incre\u0026shy;mentally\rbuilding a tree whose root node represents the current state. Each iteration\rconsists of the four operations Selection, Expansion (though possibly skipped on some iterations), Simulation, and Backup, as explained in the text and illustrated by the bold arrows in the\rtrees.\nRelating MCTS to the reinforcement learning principles we describe\rin this book provides some insight into how it achieves such impressive\rresults. At its base, MCTS is a decision-time planning algorithm based on Monte\rCarlo control applied to simulations that start from the root state; that is,\rit is a kind of rollout algorithm as described in the previous section. It\rtherefore benefits from online, incremental, sample-based value estimation and\rpolicy improvement. Beyond this, it saves action- value estimates attached to\rthe tree edges and updates them using reinforcement learning��s sample backups.\rThis has the effect of focusing the Monte Carlo trials on trajectories whose\rinitial segments are common to high-return trajectories previously simulated.\rFurther, by incrementally expanding the tree, MCTS effectively grows a lookup\rtable to store a partial action-value function, with memory allocated to the\restimated values of state-action pairs visited in the initial segments of\rhigh-yielding sample trajectories. MCTS thus avoids the problem of globally\rapproximating an action-value function while it retrains the benefit of using\rpast experience to guide exploration.\nThe striking success of decision-time planning by MCTS has deeply\rinfluenced artificial intelligence, and many researchers are studying\rmodifications and extensions of the basic procedure for use in both games and\rsingle-agent applications.\n8.12\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rSummary\nPlanning requires a model of the\renvironment. A distribution model consists of the\rprobabilities of next states and rewards for possible actions; a sample model\rproduces single transitions and rewards generated according to these\rprobabilities. Dynamic programming requires a distribution model because it\ruses full backups, which involve computing expectations\rover all the possible next states and rewards. A sample model,\ron the other hand, is what is needed to simulate interacting with the\renvironment during which sample backups, like those used\rby many reinforcement learning algorithms, can be used. Sample models are\rgenerally much easier to obtain than distribution models.\nWe have presented a\rperspective emphasizing the surprisingly close relationships between planning\roptimal behavior and learning optimal behavior. Both involve estimating the\rsame value functions, and in both cases it is natural to update the estimates\rincrementally, in a long series of small backup operations. This makes it\rstraightforward to integrate learning and planning processes simply by allowing\rboth to update the same estimated value function. In addition, any of the\rlearning meth\u0026shy;ods can be converted into planning methods simply by applying\rthem to simulated (model-generated) experience rather than to real experience.\rIn this case learning and planning become even more similar; they are possibly\ridentical algorithms op\u0026shy;erating on two different sources of experience.\nIt is straightforward to\rintegrate incremental planning methods with acting and model-learning.\rPlanning, acting, and model-learning interact in a circular fashion (Figure\r8.1), each producing what the other needs to improve; no other interaction\ramong them is either required or prohibited. The most natural approach is for\rall processes to proceed asynchronously and in parallel. If the processes must\rshare computational resources, then the division can be handled almost\rarbitrarily��by whatever organization is most convenient and efficient for the\rtask at hand.\nIn this chapter we have\rtouched upon a number of dimensions of variation among state-space planning\rmethods. One dimension is the variation in the size of backups. The smaller the\rbackups, the more incremental the planning methods can be. Among the smallest\rbackups are one-step sample backups, as in Dyna. Another important dimension is\rthe distribution of backups, that is, of the focus of search. Prioritized\rsweeping focuses backward on the predecessors of states whose values have\rrecently changed. On-policy trajectory sampling focuses on states or\rstate-action pairs that the agent is likely to encounter when controlling its\renvironment. This can allow computation to skip over parts of the state space\rthat are irrelevant to the predic\u0026shy;tion or control problem. Real-time dynamic\rprogramming, an on-policy trajectory sampling version of value iteration,\rillustrates some of the advantages this strategy has over conventional\rsweep-based policy iteration.\nPlanning can also focus\rforward from pertinent states, such as states actually encountered during an\ragent-environment interaction. The most important form of this is when planning\ris done at decision time, that is, as part of the action-selection process.\rClassical heuristic search as studied in artificial intelligence is an example\rof\nthis. Other examples are rollout algorithms and Monte Carlo Tree\rSearch that benefit\nfrom online, incremental, sample-based value estimation and policy\rimprovement.\nBibliographical and\rHistorical Remarks\n8.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe overall view of\rplanning and learning presented here has developed grad\u0026shy;ually over a number of\ryears, in part by the authors (Sutton, 1990, 1991a, 1991b; Barto, Bradtke, and\rSingh, 1991, 1995; Sutton and Pinette, 1985; Sut\u0026shy;ton and Barto, 1981b); it has\rbeen strongly influenced by Agre and Chapman (1990; Agre 1988), Bertsekas and\rTsitsiklis (1989), Singh (1993), and others. The authors were also strongly\rinfluenced by psychological studies of latent learning (Tolman, 1932) and by\rpsychological views of the nature of thought (e.g., Galanter and Gerstenhaber,\r1956; Craik, 1943; Campbell, 1960; Den\u0026shy;nett, 1978). In the Part III of the\rbook, Section 14.6 relates model-based and model-free methods to psychological\rtheories of learning and behavior, and Section 15.11 discusses ideas about how\rthe brain might implement these types of methods.\n8.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe terms direct\rand indirect, which we use to describe different kinds\rof re\u0026shy;inforcement learning, are from the adaptive control literature (e.g.,\rGoodwin and Sin, 1984), where they are used to make the same kind of\rdistinction. The term system identification is used in\radaptive control for what we call model-learning (e.g.,\rGoodwin and Sin, 1984; Ljung and Soderstrom, 1983; Young, 1984). The Dyna\rarchitecture is due to Sutton (1990), and the results in this and the next\rsection are based on results reported there. Barto and Singh (1991) consider\rsome of the issues in comparing direct and indirect reinforcement learning\rmethods.\n8.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThere have been several\rworks with model-based reinforcement learning that take the idea of exploration\rbonuses and optimistic initialization to its logical extreme, in which all\rincompletely explored choices are assumed maximally rewarding and optimal paths\rare computed to test them. The E3algorithm of\rKearns and Singh (2002) and the R-max algorithm of Brafman and Tennen- holtz\r(2003) are guaranteed to find a near-optimal solution in time polynomial in the\rnumber of states and actions. This is usually too slow for practical algorithms\rbut is probably the best that can be done in the worst case.\n8.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rPrioritized sweeping was\rdeveloped simultaneously and independently by Moore and Atkeson (1993) and Peng\rand Williams (1993). The results in Figure 8.7 are due to Peng and Williams\r(1993). The results in Figure 8.8 are due to Moore and\rAtkeson.\n8.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThis section was strongly\rinfluenced by the experiments of Singh (1993).\n\r\r8.6��7 Trajectory sampling has implicitly been a part of reinforcement\rlearning from the outset, but it was most explicitly emphasized by Barto,\rBradtke, and Singh (1995) in their introduction of RTDP. They recognized that\rKorf��s (1990) learning real-time A* (LRTA*) algorithm is\ran asynchronous DP algorithm that applies to stochastic problems as well as the\rdeterministic problems on which Korf focused. Beyond LRTA*, RTDP includes the\roption of backing up the values of many states in the time intervals between the\rexecution of actions. Barto et al. (1995) proved the convergence result de\u0026shy;scribed\rhere by combining Korf��s (1990) convergence proof for LRTA* with the result of\rBertsekas (1982) (also Bertsekas and Tsitsiklis, 1989) ensuring convergence of\rasynchronous DP for stochastic shortest path problems in the undiscounted case.\rCombining model-learning with RTDP is called Adaptive RTDP,\ralso presented by Barto et al. (1995)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; and\rdiscussed by Barto (2011).\n8.9\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rFor further reading on\rheuristic search, the\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; reader\ris encouraged to consult\ntexts and surveys such as those by Russell and Norvig (2009) and\rKorf (1988). Peng and Williams (1993) explored a forward focusing of backups\rmuch as is suggested in this section.\n8.10\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rAbramson��s (1990)\rexpected-outcome model is a rollout algorithm applied\nto two-person\rgames in which the play of\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; both\rsimulated players is ran\u0026shy;\ndom. He argued that even with random play, it is a ��powerful\rheuristic�� that is ��precise, accurate, easily estimable, efficiently\rcalculable, and domain- independent.�� Tesauro and Galperin (1997) demonstrated\rthe effectiveness of rollout algorithms for improving the play of backgammon\rprograms, adopting the term ��rollout�� from its use in evaluating backgammon\rpositions by play\u0026shy;ing out positions with different randomly generating\rsequences of dice rolls. Bertsekas, Tsitsiklis, and Wu (1997) examine rollout\ralgorithms applied to combinatorial optimization problems, and Bertsekas (2013)\rsurveys their use in discrete deterministic optimization problems, remarking\rthat they are ��of\u0026shy;ten surprisingly effective.��\n8.11\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe central ideas of MCTS\rwere introduced by Coulom (2006) and by Koc- sis and Szepesvari (2006). They\rbuilt upon previous research with Monte Carlo planning algorithms as reviewed\rby these authors. Browne, Powley, Whitehouse, Lucas, Cowling, Rohlfshagen,\rTavener, Perez, Samothrakis, and Colton (2012) survey MCTS methods and their\rapplications. This section was written with the essential help of David Silver.\nPart II: Approximate Solution Methods\nIn the second part of the book\rwe extend the tabular methods presented in Part I to apply to problems with\rarbitrarily large state spaces. In many of the tasks to which we would like to\rapply reinforcement learning the state space is combinatorial and enormous; the\rnumber of possible camera images, for example, is much larger than the number\rof atoms in the universe. In such cases we cannot expect to find an optimal\rpolicy or the optimal value function even in the limit of infinite time and\rdata; our goal instead is to find a good approximate solution using limited\rcomputational resources. In this part of the book we explore such approximate\rsolution methods.\nThe problem with large state\rspaces is not just the memory needed for large tables, but the time and data\rneeded to fill them accurately. In many of our target tasks, almost every state\rencountered will never have been seen before. To make sensible decisions in\rsuch states it is necessary to generalize from previous encounters with\rdifferent states that are in some sense similar to the current one. In other\rwords, the key issue is that of generalization. How can\rexperience with a limited subset of the state space be usefully generalized to\rproduce a good approximation over a much larger subset?\nFortunately, generalization\rfrom examples has already been extensively studied, and we do not need to\rinvent totally new methods for use in reinforcement learning. To some extent we\rneed only combine reinforcement learning methods with existing generalization\rmethods. The kind of generalization we require is often called func\u0026shy;tion\rapproximation because it takes examples from a desired function (e.g., a\rvalue function) and attempts to generalize from them to construct an\rapproximation of the entire function. Function approximation is an instance of supervised learning, the primary topic studied in machine\rlearning, artificial neural networks, pattern recog\u0026shy;nition, and statistical\rcurve fitting. In theory, any of the methods studied in these fields can be\rused in the role of function approximator within reinforcement learning\ralgorithms, although in practice some fit more easily into this role than\rothers.\nNevertheless, reinforcement\rlearning with function approximation involves a num\u0026shy;ber of new issues that do\rnot normally arise in conventional supervised learning, such as\rnonstationarity, bootstrapping, and delayed targets. We introduce these and\rother issues successively over the five chapters of this part. Initially we\rrestrict attention to on-policy training, treating in Chapter 9 the prediction\rcase, in which the policy is given and only its value function is approximated,\rand then in Chapter 10 the control case, in which an approximation to the\roptimal policy is found. Chapter 11 covers off-policy methods. In each of these\rchapters we will have to return to first principles and re-examine the\robjectives of the learning to take into account function\napproximation. Chapter 12 introduces\rand analyzes the algorithmic mechanism of eligibility traces,\rwhich dramatically improves the computational properties of multi\u0026shy;step\rreinforcement learning methods in many cases. The final chapter of this part\rexplores a different approach to control, policy-gradient\rmethods, which approximate the optimal policy directly and need never\rform an approximate value function (al\u0026shy;though they may be much more efficient\rif they do approximate a value function as well).\n\r\r210\n\r\rChapter 9\nOn-policy Prediction with Approximation\nIn this chapter, we begin our study of function approximation in\rreinforcement learn\u0026shy;ing by considering its use in estimating the state-value\rfunction from on-policy data, that is, in approximating from experience\rgenerated using a known policy n. The novelty in this chapter is that the\rapproximate value function is represented not as a table but as a parameterized\rfunctional form with weight vector w G Rd. We\rwill write v(s,w) ^ Vn(s) for the approximated value of state\rs given weight vector w. For\rexample, V might be a linear function in features of the state, with w the\rvector of feature weights. More generally, V might be the function computed by\ra multi-layer artificial neural network, with w the\rvector of connection weights in all the layers. By adjusting the weights, any\rof a wide range of different functions can be implemented by the network. Or V\rmight be the function computed by a decision tree, where w is all\rthe numbers defining the split points and leaf values of the tree. Typically,\rthe number of weights (the dimensionality of w) is\rmuch less than the number of states (d��|S|), and changing one\rweight changes the estimated value of many states. Consequently, when a single\rstate is updated, the change generalizes from that state to affect the values\rof many other states. Such generalization makes the\rlearning potentially more powerful but also potentially more difficult to\rmanage and understand.\n9.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rValue-function Approximation\nAll of the prediction methods covered\rin this book have been described as backups, that is, as updates to an\restimated value function that shift its value at particular states toward a\r��backed-up value�� for that state. Let us refer to an individual backup by the\rnotation s ^ g, where s is the state backed up and g is the backed-up value, or target, that s��s estimated value is shifted toward. For example, the Monte Carlo\rbackup for value prediction is St ^ Gt, the TD(0) backup is St ^ Rt+i+T^(St+i,wt), and the n-step TD backup is St ^ Gt:t+n. In the DP (dynamic programming) policy-evaluation backup, s ^ En[Rt+i\r+ Y\u0026amp;(St+i,wt) | St\r= s], an arbitrary state s is backed up, whereas in the other cases the state encountered in\ractual experience, St, is backed up.\nIt is natural to interpret each backup as\rspecifying an example of the desired input-output behavior of the value\rfunction. In a sense, the backup s ^ g means that the estimated value for state s should be\rmore like the number g. Up to now, the\ractual update implementing the backup has been trivial: the table entry for s��s estimated value has simply been shifted a fraction of the way\rtoward g, and the estimated values of all other states were left unchanged.\rNow we permit arbitrarily complex and sophisticated methods to implement the\rbackup, and updating at s generalizes so that\rthe estimated values of many other states are changed as well. Machine learning\rmethods that learn to mimic input-output examples in this way are called supervised learning methods, and when the outputs are numbers,\rlike g, the process is often called function\rapproximation. Function approximation methods expect to receive examples\rof the desired input-output behavior of the function they are trying to\rapproximate. We use these methods for value prediction simply by passing to\rthem the s ^ g\rof each backup as a training example. We then\rinterpret the approximate function they produce as an estimated value function.\nViewing each backup as a conventional training example in this way\renables us to use any of a wide range of existing function approximation\rmethods for value pre\u0026shy;diction. In principle, we can use any method for\rsupervised learning from examples, including artificial neural networks,\rdecision trees, and various kinds of multivariate regression. However, not all\rfunction approximation methods are equally well suited for use in reinforcement\rlearning. The most sophisticated neural network and statis\u0026shy;tical methods all\rassume a static training set over which multiple passes are made. In\rreinforcement learning, however, it is important that learning be able to occur\ron\u0026shy;line, while interacting with the environment or with a model of the\renvironment. To do this requires methods that are able to learn efficiently\rfrom incrementally acquired data. In addition, reinforcement learning generally\rrequires function approximation methods able to handle nonstationary target\rfunctions (target functions that change over time). For example, in control\rmethods based on GPI (generalized policy itera\u0026shy;tion) we often seek to learn q^ while n changes. Even if the policy remains\rthe same, the target values of training examples are nonstationary if they are\rgenerated by bootstrapping methods (DP and TD learning). Methods that cannot\reasily handle such nonstationarity are less suitable for reinforcement\rlearning.\n9.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe Prediction Objective (MSVE)\nUp to now we have not specified an explicit objective for\rprediction. In the tabular case a continuous measure of prediction quality was\rnot necessary because the learned value function could come to equal the true\rvalue function exactly. Moreover, the learned values at each state were\rdecoupled��an update at one state affected no other. But with genuine\rapproximation, an update at one state affects many others, and it is not\rpossible to get all states exactly correct. By assumption we have far \n\r\rmore states than weights, so making one state��s estimate more\raccurate invariably means making others�� less accurate. We are obligated then\rto say which states we care most about. We must specify a weighting or\rdistribution ^(s) \u0026gt; 0 representing how much we care about the error in each state\rs. By the error in a state s we mean the square of\rthe difference between the approximate value v(s,w) and the true value vn(s). Weighting this over the state space by the distribution \u0026quot;��we obtain a natural objective function, the Mean\rSquared Value Error, or MSVE:\n^~^\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��12\nMSVE(w)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ^^(s) vn(s) �� {)(s,w)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; .\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (9.1)\nThe square root of this measure, the root MSVE or\rRMSVE, gives a rough measure of how much the approximate values differ from the\rtrue values and is often used in plots. Typically one chooses ^(s) to be the fraction of time spent in s under the\rtarget policy n. This is called the on-policy distribution;\rwe focus entirely on this case in this chapter. In continuing tasks, the\ron-policy distribution is the stationary distribution under n.\nThe on-policy distribution in episodic tasks\nIn an episodic task, the on-policy distribution is a little\rdifferent in that it is not really a distribution and depends on how the\rinitial states of episodes are chosen. Let h(s) denote the probability that an\repisode begins in each state s, and let the ��distribution�� ^(s) denote the\rnumber of time steps spent, on average, in state s in a single episode. Time is\rspent in a state s if episodes start in it, or if transitions are made into it\rfrom a state s in which time is spent:\n\u0026quot;(s) =\rh(s) + ^ \u0026quot;(s) E n(a|s)p(s|s, a), Vs G\rS.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (9.2)\ns\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; a\nThis system of equations can be solved for the expected number of\rvisits \u0026quot;(s).\nThe two cases, continuing and episodic, behave\rsimilarly, but with approximation they must be treated separately in formal\ranalyses, as we will see repeatedly in this part of the book. This completes\rthe specification of the learning objective.\nIt is not completely clear that the MSVE is the\rright performance objective for reinforcement learning.[15]Remember that our ultimate purpose, the reason we are learning a\rvalue function, is to use it in finding a better policy. The best value func\u0026shy;tion\rfor this purpose is not necessarily the best for minimizing MSVE. Nevertheless,\rit is not yet clear what a more useful alternative goal for value prediction\rmight be. For now, we will focus on MSVE.\nAn ideal goal in terms of MSVE would be to find a\rglobal optimum, a weight vector w* for\rwhich MSVE(w*) \u0026lt; MSVE(w) for all possible w. Reaching this goal is some\u0026shy;times\rpossible for simple function approximators such as linear ones, but is rarely\rpossible for complex function approximators such as artificial neural networks\rand decision trees. Short of this, complex function approximators may seek to\rconverge instead to a local optimum, a weight vector w* for\rwhich MSVE(w*) \u0026lt; MSVE(w) for all w in some neighborhood of w*. Although this guarantee is only\rslightly reassur\u0026shy;ing, it is typically the best that can be said for nonlinear\rfunction approximators, and often it is enough. Still, for many cases of\rinterest in reinforcement learning there is no guaranteed of convergence to an\roptimum, or even to within a bounded distance of an optimum. Some methods may\rin fact diverge, with their MSVE approaching infinity in the limit.\nIn the last two sections we\rhave outlined a framework for combining a wide range of reinforcement learning\rmethods for value prediction with a wide range of function approximation\rmethods, using the backups of the former to generate training ex\u0026shy;amples for the\rlatter. We have also described a MSVE performance measure which these methods\rmay aspire to minimize. The range of possible function approxima\u0026shy;tion methods\ris far too large to cover all, and anyway too little is known about most of\rthem to make a reliable evaluation or recommendation. Of necessity, we consider\ronly a few possibilities. In the rest of this chapter we focus on function\rapproximation methods based on gradient principles, and on linear\rgradient-descent methods in particular. We focus on these methods in part\rbecause we consider them to be particularly promising and because they reveal\rkey theoretical issues, but also because they are simple and our space is\rlimited.\n9.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rStochastic-gradient and\rSemi-gradient Methods\nWe now develop in detail one class of learning\rmethods for function approximation in value prediction, those based on\rstochastic gradient descent (SGD). SGD methods are among the most widely used\rof all function approximation methods and are particularly well suited to\ronline reinforcement learning.\nIn gradient-descent methods,\rthe weight vector is a column vector with a fixed number of real valued\rcomponents, w == (wi, W2,...,w^)T,[16]and the approximate value function V(s,w) is a\rdifferentiable function of w\rfor all s G S. We will be updating w at each\rof a series of discrete time steps, t = 0,1, 2, 3,..., so we will need a notation wt for the\rweight vector at each step. For now, let us assume that, on each step, we\robserve a new example St ^ Vn (St) consisting of a (possibly randomly selected) state St and its true value under the policy. These states might be\rsuccessive states from an interaction with the environment, but for now we do\rnot assume so. Even though we are given the exact, correct values, Vn(St) for each St, there is still a\rdifficult problem because our function approximator has limited resources and\rthuslimited resolution. In particular, there is generally no w that gets all the\rstates, or even all the examples, exactly correct. In addition, we must\rgeneralize to all the other states that have not appeared in examples.\nWe assume that states appear in examples with the same distribution,\r\u0026quot;, over which we are trying to minimize the MSVE as given by (9.1). A good\rstrategy in this case is to try to minimize error on the observed examples. Stochastic gradient- descent (SGD) methods do this by\radjusting the weight vector after each example by a small amount in the\rdirection that would most reduce the error on that example:\n\u0026nbsp;TOC \\o \u0026quot;1-5\u0026quot; \\h \\z 1r\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 12\nwtʮ1= wt - 2aVn(St) - v(St,wt)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (9.3)\n\r\r\rWt + a\n\r\r\r\r\rVn(St) - V(St,wt) VV(St,wt),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (9.4)\nwhere a is a positive step-size parameter,\rand Vf (w), for any scalar expression f (w), denotes the\rvector of partial derivatives with respect to the components of the weight\rvector:\n(w)\u0026#8226; ( df (w) df (w)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; df\r(w)\\T\nV\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rf(w)^.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (9)\nThis derivative vector is the gradient of f\rwith respect to w. SGD methods are ��gradient descent�� methods because the\roverall step in wt is proportional to the negative gradient of the example��s\rsquared error (9.3). This is the direction in which the error falls most\rrapidly. Gradient descent methods are called ��stochastic�� when the update is\rdone, as here, on only a single example, which might have been selected\rstochastically. Over many examples, making small steps, the overall effect is\rto minimize an average performance measure such as the MSVE.\nIt may not be immediately apparent why SGD takes only a small step\rin the direction of the gradient. Could we not move all the way in this\rdirection and completely eliminate the error on the example? In many cases this\rcould be done, but usually it is not desirable. Remember that we do not seek or\rexpect to find a value function that has zero error for all states, but only an\rapproximation that balances the errors in different states. If we completely\rcorrected each example in one step, then we would not find such a balance. In\rfact, the convergence results for SGD methods assume that a\rdecreases over time. If it decreases in such a way as to satisfy the standard\rstochastic approximation conditions (2.7), then the SGD method (9.4) is\rguaranteed to converge to a local optimum.\nWe turn now to the case in which the target output, here denoted Ut\rG R, of the tth training example, St ^ Ut, is not the true value, Vn(St), but\rsome, possibly random, approximation to it. For example, Ut might be a\rnoise-corrupted version of Vn(St), or it might be one of the bootstrapping\rtargets using V mentioned in the previous section. In these cases we cannot\rperform the exact update (9.4) because Vn(St) is unknown, but we can\rapproximate it by substituting Ut in place of Vn(St). This yields the following\rgeneral SGD method for state-value prediction:\n\r\r\rwt+1= wt + a\n\r\r\r\r\rUt - V(St,wt) VV(St,wt).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (9.6)\nGradient Monte Carlo\rAlgorithm for Estimating v ��v^\nInput: the policy n to be evaluated\nInput: a differentiable function v : S\rx Rd\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; R\nInitialize\rvalue-function weights w as appropriate (e.g., w = 0) Repeat\rforever:\nGenerate\ran episode So, Ao, Ri, Si, Ai,..., Rt, St using n For t = 0,1,..., T �� 1:\nw t w + a [Gt �� v(St,w)] W(St,w)\nIf Ut is an unbiased estimate, that is, if E[Ut] = v^ (St), for each t, then wt is guaranteed to\rconverge to a local optimum under the usual stochastic approximation conditions\r(2.7) for decreasing a.\nFor example, suppose the states in the examples\rare the states generated by in\u0026shy;teraction (or simulated interaction) with the\renvironment using policy n. Because the true value of a state is the expected\rvalue of the return following it, the Monte Carlo target Ut == Gt\ris by definition an unbiased estimate of v^(St). With this choice, the general SGD method (9.6) converges to a\rlocally optimal approximation to vn(St). Thus, the gradient-descent version of Monte Carlo state-value\rprediction is guaranteed to find a locally optimal solution. Pseudocode for a\rcomplete algorithm is shown in the box.\nOne does not obtain the same guarantees if a\rbootstrapping estimate of v^ (St) is used as the target Ut in (9.6).\rBootstrapping targets such as n-step returns Gt��t+n\ror the DP target Eas,r n(a|St)p(s;, r|St, a)[r + 7v(s;,wt)] all depend on the current value of the weight vector wt, which implies that they will be biased and that they will not\rproduce a true gradient-descent method. One way to look at this is that the key\rstep from (9.3) to (9.4) relies on the target being independent of wt. This step would not be valid if a bootstrapping estimate was used\rin place of v^(St). Bootstrapping methods are not in fact instances of true gradient\rdescent (Barnard, 1993). They take into account the effect of changing the\rweight vector wt\ron the estimate, but ignore its effect on the\rtarget. They include only a part of the gradient and, accordingly, we call them\rsemi-gradient methods.\nAlthough semi-gradient (bootstrapping) methods do\rnot converge as robustly as gradient methods, they do converge reliably in\rimportant cases such as the linear case discussed in the next section.\rMoreover, they offer important advantages which makes them often clearly\rpreferred. One reason for this is that they are typically significantly faster\rto learn, as we have seen in Chapters 6and 7. Another is that\rthey enable learning to be continual and online, without waiting for the end of\ran episode. This enables them to be used on continuing problems and provides\rcomputational advantages. A prototypical semi-gradient method is semi-gradient\rTD(0), which uses Ut == Rt+i + Yv(St+i,w) as its target. Complete pseudocode for this method is given in\rthe box below.\nSemi-gradient TD(0) for estimating V Vn\nInput: the policy n to be\revaluated\nInput: a differentiable\rfunction V : S+ x Rd R such that V(terminal,-) = 0\nInitialize value-function weights w arbitrarily (e.g., w = 0)\nRepeat (for each episode):\nInitialize\rS\nRepeat\r(for each step of episode):\nChoose A \u0026#12316;n(-|S)\nTake\raction A, observe R, S' w w + a\r[R + yv(S;,w) �� V(S,w) Vf)(S,w)\nS �� S; until Sf is terminal\nExample 9.1: State Aggregation on the 1000-state Random Walk State aggregationis\ra simple form of generalizing function approximation in which states are\rgrouped together, with one estimated value (one component of the weight vector w) for each group. The value of a state is estimated as its group��s\rcomponent, and when the state is updated, that component alone is updated.\rState aggregation is a special case of SGD (9.6) in which the gradient, VV(St,wt), is 1 for St��s group��s component\rand 0for the other components.\nConsider a 1000-state version\rof the random walk task (Examples 6.2 and 7.1). The states are numbered from 1\rto 1000, left to right, and all episodes begin near the center, in state 500.\rState transitions are from the current state to one of the 100 neighboring\rstates to its left, or to one of the 100neighboring states\rto its right, all with equal probability. Of course, if the current state is\rnear an edge, then there may be fewer than 100 neighbors on that side of it. In\rthis case, all the probability that would have gone into those missing\rneighbors goes into the probability of terminating on that side (thus, state 1\rhas a 0.5 chance of terminating on the left, and state 950 has a 0.25 chance of\rterminating on the right). As usual, termination on the left produces a reward\rof ��1, and termination on the right produces a reward of +1. All other transitions have a reward of zero. We use this task as a\rrunning example throughout this section.\nFigure 9.1 shows the true\rvalue function Vn for this task. It is nearly a straight\rline, but tilted slightly toward the horizontal and curving further in this\rdirection for the last 100states at each end.\rAlso shown is the final approximate value function learned by the gradient\rMonte-Carlo algorithm with state aggregation after 100,000 episodes with a step\rsize of a = 2 x 10-5. For the state\raggregation, the 1000 states were partitioned into 10groups of 100states each (i.e., states 1-100were one group,\rstates 101-200 were another, and so on). The staircase effect shown in the\rfigure is typical of state aggregation; within each group, the approximate\rvalue is constant, and it changes abruptly from one group to the next. These\rapproximate values are\n\r\rTrue ____ ^\n\r\r\r0.0137\n\r\r\r\r\rvalue Vr-\n\r\r\r\r\r1000\n\r\r\r\r\r\r\r\rDistribution\nscale\n\r\r\r\r\r\r\r\r0.0017\n0\n\r\r\r\r\r\r\r\rState\n\r\r\r\r\r\r\r\rApproximate MC value vj\n\r\r\r\r\r\r\r\rValue\nscale\n\r\r\r\r\r\r\r\r0\n\r\r\r\r\r\r\r\rFigure 9.1: Function\rapproximation by state aggregation on the 1000-state random walk task,\rusing the gradient Monte Carlo algorithm (page 216).\n\r\r\r\r\r\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rclose to the global minimum of the\rMSVE (9.1).\nSome of the details of the approximate values are best appreciated\rby reference to the state distribution \u0026quot; for this task, shown in the lower\rportion of the figure with a right-side scale. State 500, in the center, is the\rfirst state of every episode, but it is rarely visited again. On average, about\r1.37% of the time steps are spent in the start state. The states reachable in\rone step from the start state are the second most visited, with about 0.17% of\rthe time steps being spent in each of them. From there \u0026quot; falls off almost\rlinearly, reaching about 0.0147% at the extreme states 1 and 1000. The most\rvisible effect of the distribution is on the leftmost groups, whose values are\rclearly shifted higher than the unweighted average of the true values of states\rwithin the group, and on the rightmost groups, whose values are clearly shifted\rlower. This is due to the states in these areas having the greatest asymmetry\rin their weightings by ��For example, in the leftmost group, state 99 is\rweighted more than 3 times more strongly than state 0. Thus the estimate for\rthe group is biased toward the true value of state 99, which is higher than the\rtrue value of state 0.\n9.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rLinear Methods\nOne of the most important special cases of function approximation is\rthat in which the approximate function, V(-,w),is a linear function of the weight\rvector, w. Corresponding to every state s, there is a real-valued vector of\rfeatures x(s)== (xi(s), X2(s),...,Xd(s))T,\rwith the same number of components as w. The features may be constructed\rfrom the states in many different ways; we cover a few possi\u0026shy;bilities in the\rnext sections. However the features are constructed, the approximatestate-value function is given by the inner product between w and x(s)\nd\n\r\r\r(9.7)\n\r\r\r\r\r{)(s,w) == wTx(s)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ^ WjXj(s).\nIn this case the approximate value function is\rsaid to be linear in the weights, or simply linear. The individual functions Xi : S R are called basis functions because they form a linear basis for the set\rof approximate functions of this form. Construct\u0026shy;ing n-dimensional feature\rvectors to represent states is the same as selecting a set of n\rbasis functions.\nIt is natural to use SGD updates with linear function approximation.\rThe gradient of the approximate value function with respect to w in this\rcase is\nV\u0026nbsp;\u0026nbsp;\r0(s,w) = x(s).\nThus, the general SGD update (9.6) reduces to a\rparticularly simple form in the linear case.\nBecause it is so simple, the linear SGD case is\rone of the most favorable for mathematical analysis. Almost all useful\rconvergence results for learning systems of all kinds are for linear (or\rsimpler) function approximation methods.\nIn particular, in the linear case there is only\rone optimum (or, in degenerate cases, one set of equally good optima), and thus\rany method that is guaranteed to converge to or near a local optimum is\rautomatically guaranteed to converge to or near the global optimum. For\rexample, the gradient Monte Carlo algorithm presented in the previous section\rconverges to the global optimum of the MSVE under linear function approximation\rif a is reduced over time according to the usual conditions.\nThe semi-gradient TD(0) algorithm presented in the previous section\ralso con\u0026shy;verges under linear function approximation, but this does not follow\rfrom general results on SGD; a separate theorem is necessary. The weight vector\rconverged to is also not the global optimum, but rather a point near the local\roptimum. It is useful to consider this important case in more detail,\rspecifically for the continuing case. The update at each time t is\nwt+i ==\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (9.8)\n\r\n\r\r\r\r\r\u0026nbsp;\nwhere here we have used the notational shorthand xt = x(St).\rOnce the system has reached steady state, for any given wt, the\rexpected next weight vector can be written\n\r\r\r(9.9)\n\r\r\r\r\r\r\r\rwhere\n\r\r\r\r\rE[wt+i|wt] = wt + a(b �� Awt),\nb == E[Rt+ixt] G Rdand A == E xt(xt �� ^xt+i)T G Rdx\rRd\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (9.10)\nFrom (9.9) it is clear that, if the system converges, it must\rconverge to the weight vector wtd at which\nb �� Awtd = 0\n\r\r\r(9.11)\n\r\r\r\r\rb = Awtd A-ib.\nwTD\nThis quantity is called the TD fixedpoint. In\rfact linear semi-gradient TD(0) con\u0026shy;verges to this point. Some of the theory\rproving its convergence, and the existence of the inverse above, is given in\rthe box.\nProof of Convergence of Linear TD(0)\nWhat properties assure\rconvergence of the linear TD(0) algorithm (9.8)? Some insight can be gained by\rrewriting (9.9) as\n\r\r\r(9.12)\n\r\r\r\r\rE[wt+i|wt] = (I �� aA)wt + ab.\nNote that the matrix A multiplies the weight\rvector wt and not b; only A is important to convergence. To develop intuition,\rconsider the special case in which A is a diagonal matrix. If any of the\rdiagonal elements are negative, then the corresponding diagonal element of I ��\raA will be greater than one, and the corresponding component of wt will be\ramplified, which will lead to divergence if continued. On the other hand, if\rthe diagonal elements of A are all positive, then a can be chosen smaller than\rone over the largest of them, such that I �� aA is diagonal with all diagonal\relements between 0 and 1. In this case the first term of the update tends to\rshrink wt, and stability is assured. In general case, wt will be reduced toward\rzero whenever A is positive definite, meaning yTAy \u0026gt; 0 for real vector y. Positive definiteness also ensures that\rthe inverse A-iexists.\nFor\rlinear TD(0), in the continuing case with 7\u0026lt; 1, the\rA matrix (9.10) can be written\n\r\r\u0026nbsp;SHAPE \u0026nbsp;\\* MERGEFORMAT \r\r\r\u0026nbsp;\n\r\r\r\r\r\r\u0026nbsp;\n\r\r\r\r\rA\n\r\r\r\r\rJ2\u0026quot;(s)Ylأ(a|s)Ep(r��s/|s,a)x(s)(x(s)��7x(s��))\ns\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; a\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; r, s!\n^\u0026quot;(s)^p(s/|s)x(s)(x(s) ��Yx(s/))T\n\r\r\rT\n\r\r\r\r\rs\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; sf\n[\u0026quot;(s)x(s^x(s) ��Y [ P(s/|s)x(s/)\ns1\nXtD(I �� Y P)X,\n\r\r\u0026nbsp;\n\r\r\r\r\rthe probability matrix of these\ron its diagonal,\n\r\r\r\r\rwhere \u0026quot;(s) is the stationary distribution under\rn, p(s/|s) is of transition from s to s/under\rpolicy n, P is the |S| x |S| probabilities, D is the |S| x |S| diagonal matrix\rwith the \u0026quot;(s)\nand X is the |S| x d matrix with x(s) as its\rrows. From here it is clear that\nthe inner matrix D(I - yP) is key to determining the positive definiteness of A.\nFor a key matrix of this type, positive definiteness is assured if\rall of its columns sum to a nonnegative number. This was shown by Sutton (1988,\rp. 27) based on two previously established theorems. One theorem says that any\rmatrix M is positive definite if and only if the symmetric matrix S = M + MTis positive definite (Sutton 1988, appendix). The second theorem\rsays that any symmetric real matrix S is positive definite if all of its\rdiagonal entries are positive and greater than the sum of the corresponding\roff-diagonal entries (Varga 1962, p. 23). For our key matrix, D(I - 7P), the diagonal entries are positive and the off-diagonal entries\rare negative, so all we have to show is that each row sum plus the\rcorresponding column sum is positive. The row sums are all positive because P\ris a stochastic matrix and 7\u0026lt; 1. Thus it only\rremains to show that the column sums are nonnegative. Note that the row vector\rof the column sums of any matrix M can be written as 1TM, where 1 is the column vector with all components equal to 1. Let fx denote the |S|-vector of the \u0026quot;(s), where x = PTX by\rvirtue of \u0026quot; being the stationary distribution. The column sums of our key\rmatrix, then, are:\n1tD(I\r- 7P) = xT(I - 7P)\n=XT- 7XTP\n=xT- 7XT\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (because\rx is the stationary distribution)\n=(1-7)M,\nall components of which are positive. Thus, the key matrix and its A\rmatrix are positive definite, and on-policy TD(0) is stable. (Additional\rconditions and a schedule for reducing a over time are\rneeded to prove convergence with probability one.)\nAt the TD\rfixedpoint, it has also been proven (in the continuing case) that the MSVE is\rwithin a bounded expansion of the lowest possible error:\nMSVE(wTD) ^ -^minMSVE(w).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (9.13)\n[1]- 7w\nThat is, the asymptotic error of the TD method is\rno more than times the small\u0026shy;est possible error, that attained in the limit by\rthe Monte Carlo method. Because 7is often near one, this expansion factor can be quite large, so\rthere is substantial potential loss in asymptotic performance with the TD\rmethod. On the other hand, recall that the TD methods are often of vastly\rreduced variance compared to Monte Carlo methods, and thus faster, as we saw in\rChapters 6and 7. Which method will be best depends on the nature of the\rapproximation and problem, and on how long learning contiunues.\nA bound analogous to (9.13) applies to other on-policy bootstrapping\rmethods as well. For example, linear semi-gradient DP (Eq. 9.6 with Ut == Ea n(aJSt) Es, r\np(s;, r|St, a)[r + T^s'w^)]) with backups according to the on-policy\rdistribution will also converge to the TD fixedpoint. One-step semi-gradient action-valuemethods, such as semi-gradient Sarsa(0) covered in\rthe next chapter converge to an analogous fixedpoint and an analogous bound.\rFor episodic tasks, there is a slightly different but related bound (see\rBertsekas and Tsitsiklis, 1996). There are also a few technical conditions on\rthe rewards, features, and decrease in the step-size parameter, which we have\romitted here. The full details can be found in the original paper (Tsitsiklis\rand Van Roy, 1997).\nCritical to the these convergence results is that\rstates are backed up according to the on-policy distribution. For other backup\rdistributions, bootstrapping methods using function approximation may actually\rdiverge to infinity. Examples of this and a discussion of possible solution\rmethods are given in Chapter 11.\nExample 9.2:\rBootstrapping on the 1000-state Random Walk State aggre\u0026shy;gation is a special case of linear function\rapproximation, so let's return to the 1000- state random walk to illustrate some of the\robservations made in this chapter. The left panel of Figure 9.2 shows the final\rvalue function learned by the semi-gradient TD(0) algorithm (page 217) using\rthe same state aggregation as in Example 9.1. We see that the near-asymptotic\rTD approximation is indeed farther from the true values than the Monte Carlo\rapproximation shown in Figure 9.1.\nNevertheless, TD methods retain large potential\radvantages in learning rate, and generalize MC methods, as we investigated\rfully with the multi-step TD methods of Chapter 7. The right panel of Figure\r9.2 shows results with an n-step semi\u0026shy;gradient TD method using state\raggregation and the 1000-state random walk that are strikingly similar to those\rwe obtained earlier with tabular methods and the 19-state random walk. To\robtain such quantitatively similar results we switched the state aggregation to\r20 groups of 50 states each. The 20 groups are then quantitatively close to the\r19 states of the tabular problem. In particular, the state transitionsof\rat-most 100 states to the right or left, or 50 states on average, were\rquantitively analogous to the single-state state transitions of the tabular\rsystem. To complete the match, we use here the same performance measure��an\runweighted average of the RMS error over all states and over the first 10\repisodes��rather than a MSVE objective as is otherwise more appropriate when\rusing function approximation.\nThe semi-gradient n-step TD algorithm we used in\rthis example is the natural extension of the tabular n-step TD algorithm\rpresented in Chapter 7 to semi-gradient function approximation. The key\requation, analogous to (7.2), is\nwt+n = wt+n-i + a [Gt��t+n �� v(St,wt+ra-i)] W(St,wt+ra-i),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0\u0026lt; t \u0026lt; T, (9.14)\nwhere the n-step return is generalized from (7.1)\rto\nGt:t+n\r= Rt+i + YRt+2+ �� �� �� + Yn iRt+n\r+ YnV(St+n,wt+n-i),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0\u0026nbsp;\u0026nbsp; \u0026lt;\u0026nbsp;\u0026nbsp;\u0026nbsp; t\u0026nbsp; \u0026lt;\u0026nbsp;\u0026nbsp; T\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; n-\u0026nbsp;\u0026nbsp;\u0026nbsp; (9.15)\nPseudocode for the complete algorithm is given in the box below.\nn-step\rsemi-gradient TD for estimating v ��v^\nInput: the policy n to be evaluated\nInput: a differentiable function v : S+ x RdR such that v(terminal,-) = 0\nParameters: step size a G (0,1], a positive\rinteger n\nAll store and access operations (St and Rt) can take their index\rmod n\nInitialize value-function weights w arbitrarily (e.g., w = 0)\nRepeat (for each episode):\nInitialize\rand store So = terminal T ^\nFor t = 0,1, 2,...:\n| If t \u0026lt; T, then:\n|\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Take an\raction according to n(-|St)\n|\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Observe and\rstore the next reward as Rt+i and the next\rstate as St+i\n|\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; If St+i is terminal, then T �� t + 1\n|\u0026nbsp; t �� t �� n + 1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (t is the time whose state��s estimate is being updated)\n|\u0026nbsp; If t \u0026gt; 0:\ni\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; g��yi-T-iRi\n| If t + n \u0026lt; T,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; then:\rG �� G + Ynv(Sr+n,w)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (Gr��r+n)\n| w �� w + a [G �� v(Sr,w)] W(Sr,w)\nUntil t = T �� 1\n\r\r9.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rFeature Construction for Linear\rMethods\nLinear methods are interesting because\rof their convergence guarantees, but also because in practice they can be very\refficient in terms of both data and computation. Whether or not this is so\rdepends critically on how the states are represented in terms of the features,\rwhich we investigate in this large section. Choosing features appropriate to\rthe task is an important way of adding prior domain knowledge to reinforcement\rlearning systems. Intuitively, the features should correspond to the natural\rfeatures of the task, those along which generalization is most appropriate. If\rwe are valuing geometric objects, for example, we might want to have features\rfor each possible shape, color, size, or function. If we are valuing states of\ra mobile robot, then we might want to have features for locations, degrees of\rremaining battery power, recent sonar readings, and so on.\nIn general, we also need features for combinations of these natural\rqualities. This is because the linear form prohibits the representation of\rinteractions between features, such as the presence of feature i being good only in the absence of feature j. For example, in the pole-balancing task (Example 3.4), a\rhigh angular velocity may be either good or bad depending on the angular\rposition. If the angle is high, then high angular velocity means an imminent\rdanger of falling��a bad state��whereas if the angle is low, then high angular\rvelocity means the pole is righting itself��a good state. In cases with such\rinteractions one needs to introduce features for combinations of feature values\rwhen using linear function approximation methods. In the following subsections\rwe consider a variety of general ways of doing this.\n9.5.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rPolynomials\nFor multi-dimensional continuous state\rspaces, function approximation for reinforce\u0026shy;ment learning has much in common\rwith the familiar tasks of interpolation and regression, which aim to define\rfunctions between and/or beyond given samples of function values. Various\rfamilies of polynomials commonly used for these tasks can also be used in\rreinforcement learning. Here we discuss only the most basic polyno\u0026shy;mial family.\nSuppose a reinforcement\rlearning problem��s state space is two-dimensional so that each state is a real\rvector s = (si,s2)T. You might\rchoose to represent each s with the feature\rvector (1, si, s2, sis2)Tin order to\rtake the interaction of the state variables into account by weighting the\rproduct sis2in an\rappropriate way. Or you might choose to use feature vectors like (1, si, s2, sis2, s2, s2, sys2, sis2, s2s2)Tto take more\rcomplex interactions into account. Using these features means that functions\rare approximated as multi-dimensional quadratic functions��even though the\rapproximation is still linear in the weights that have to be learned.\nThese example feature vectors\rare the result of selecting sets of polynomial basis functions, which are\rdefined for any dimension and can encompass highly-complex interactions among\rthe state variables:\nFor d state variables taking real values,\revery state s is a d-dimensional vec\u0026shy;tor (si, s2,..., sd)Tof real\rnumbers. Each d-dimensional polynomial\rbasis function Xi can be\rwritten as\nXi(s) = nd=isCi'j,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (9.16)\nwhere each Ci,j is an\rinteger in the set {0,1,����N} for an integer N \u0026gt; 0. These functions\rmake up the order-N polynomial basis,\rwhich contains (N + 1)ddifferent functions.\nHigher-order polynomial bases allow for more accurate approximations\rof more complicated functions. But because the number of functions in an\rorder-N poly\u0026shy;nomial basis grows exponentially with the state space dimension\r(for N \u0026gt; 0), it is generally necessary to select a subset of them for\rfunction approximation. This can be done using prior beliefs about the nature\rof the function to be approximated, and some automated selection methods\rdeveloped for polynomial regression can be adapted to deal with the incremental\rand nonstationary nature of reinforcement learning.\nExercise 9.1 Why does (9.16) define (N + 1)d distinct functions for\rdimension d? ��\nExercise\r9.2 Give N and the Ci,j defining the basis\rfunctions that produce feature vectors (1, si, s2, sis2, sg, s2, sis2, sis2, s2s2)T.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n9.5.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rFourier Basis\nAnother linear function approximation method is based on the\rtime-honored Fourier series, which expresses periodic functions as a weighted\rsum of sine and cosine basis functions of different frequencies. (A function f is periodic if f (x) = f (x + T) for all x and some period\rT.) The Fourier series and the more general Fourier transform are widely used\rin applied sciences because��among many other reasons��if a function to be\rapproximated is known, then the basis function weights are given by simple\rformulae and, further, with enough basis functions essentially any function can\rbe approximated as accurately as desired. In reinforcement learning, where the\rfunctions to be approximated are unknown, Fourier basis functions are of\rinterest because they are easy to use and can perform well in a range of reinforcement\rlearning problems. Konidaris, Osentoski, and Thomas (2011) presented the\rFourier basis in a simple form suitable for reinforcement learning problems\rwith multi-dimensional continuous state spaces and functions that do not have\rto be periodic.\nFirst consider the\rone-dimensional case. The usual Fourier series representation of a function of\rone dimension having period T represents the function as a linear com\u0026shy;bination\rof sine and cosine functions that are each periodic with periods that evenly\rdivide T (in other words, whose frequencies are integer multiples of a\rfundamental frequency 1/T). But if you are interested in approximating an\raperiodic function defined over a bounded interval, you can use these Fourier\rbasis functions with Tset to the length the interval. The function of interest is then just one\rperiod of the periodic linear combination of the sine and cosine basis\rfunctions.\nFurthermore, if you set T to twice the length of\rthe interval of interest and restrict attention to the approximation over the\rhalf interval [0,T/2], you can use just the cosine basis functions. This is\rpossible because you can represent any even function,\rthat is, any function that is symmetric about the origin, with just the cosine\rbasis functions. So any function over the half-period [0, T/2] can be\rapproximated as closely as desired with enough cosine basis functions. (Saying\r��any function�� is not exactly correct because the function has to be\rmathematically well-behaved, but we skip this technicality here.)\rAlternatively, it is possible to use just the sine basis functions, linear\rcombinations of which are always odd functions, that\ris functions that are anti-symmetric about the origin. But it is generally\rbetter to keep just the cosine basis functions because ��half-even�� functions\rtend to be easier to approximate than ��half-odd�� functions since the latter are\roften discontinuous at the origin.\nFollowing this logic and letting T = 2 so that\rthe functions are defined over the half-T interval [0,1], the one-dimensional\rorder-N Fourier cosine basis consists of the N + 1\rfunctions\nXi(s) = cos(ins), s G [0,1],\n\r\r\rFigure 9.3: One-dimensional\rFourier cosine basis functions\u0026nbsp;\u0026nbsp; i = 1, 2, 3, 4,\rfor approximat\u0026shy;\ning functions over the interval\r[0,1];\rxo\ris a constant function. After Konidaris et al. (2011).\nThis same reasoning applies to\rthe Fourier cosine series approximation in the multi-dimensional case as\rdescribed in the box at the top of the next page. As an example, consider\rthe d= 2case in which s = (si,s2), where each ci = (ci, c2)T. Figure 9.4 shows a selection of\rsix Fourier cosine basis functions, each labeled by the vector ci\rthat defines it (si is the horizontal axis and ci is shown as a\rrow vector with the index i omitted). Any zero in c means the function is\rconstant along that dimension. So if c = (0, 0), the function is constant\rover both dimensions; if c = (ci, 0)\rthe function is constant over the second dimension and varies over the\rfirst with frequency depending on ci; and similarly, for c = (0, c2). When c = (ci, c2) with neither cj = 0, the basis function varies along both dimensions\rand represents an interaction between the two state variables. The values\rof ci and c2determine the frequency along each dimension, and their ratio gives the\rdirection of the interaction.\n\r\r\r\r\rfor i = 0,����N. Figure 9.3 shows one-dimensional Fourier cosine basis functions Xi, for i = 1, 2, 3, 4; xo is a constant function.\nFor a state space that is the d-dimensional unit hypercube with the\rorigin in one corner, states are real vectors s = (si������s^)T, s^ G\r[0,1]. Each function in the order-N Fourier cosine basis can be written\nXi(s) = cos(nci �� s),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (9.17)\nwhere \u0026amp; = (ci,..., cld)T, with cj G {0,..., N} for j = =1,����d and i = 0,..., (N + 1)d. This\rdefines a function for each of the (N + 1)d possible integer vectors\rcl. The dot-product c%\r�� s has the effect of assigning an in\u0026shy;teger in {0,����N} to each dimension. As in the one-dimensional\rcase, this integer determines the function��s frequency along that dimension.\rThe basis functions can of course be shifted and scaled to suit the bounded\rstate space of a particular application.\nKonidaris et al. (2011) found\rthat when using Fourier cosine basis functions with a learning algorithm such\ras (9.6), semi-gradient TD(0), or semi-gradient Sarsa, it is helpful to use a\rdifferent step-size parameter for each basis function. If a is the basic\rstep-size parameter, they suggest setting the step-size parameter for basis\rfunction\n��to a\u0026lt; = a/y^(ci)2+ ... + (c^)2(except\rwhen each cj =0, in which case a\u0026lt; = a).\n\r\r\r\nFigure 9.4: A selection of six\rtwo-dimensional Fourier cosine basis functions, each labeled by the vector clthat defines it (s��i is the horizontal axis, and clis shown as a row vector with the index i omitted). After Konidaris et al. (2011).\n\r\r\r\r\rFourier cosine basis functions with Sarsa were found\rto produce good performance compared to several other collections of basis\rfunctions, including polynomial and\n\r\n\r\r\r\r\r\u0026nbsp;\n\r\u0026nbsp;\nradial basis functions, on several reinforcement learning tasks. Not\rsurprisingly, how\u0026shy;ever, Fourier basis functions have trouble with\rdiscontinuities because it is difficult to avoid ��ringing�� around points of\rdiscontinuity unless very high frequency basis functions are included.\nAs is true for polynomial approximation, the number\rof basis functions in the order-N Fourier cosine basis grows exponentially with\rthe state space dimension. This makes it necessary to select a subset of these\rfunctions if the state space has high dimension (e.g., d \u0026gt; 5). This can be\rdone using prior beliefs about the nature of the function to be approximated,\rand some automated selection methods can be adapted to deal with the\rincremental and nonstationary nature of reinforcement learning. Advantages of\rFourier basis functions in this regard are that it is easy to select functions\rby setting the ci vectors to account for suspected interactions\ramong the state variables, and by limiting the values in the cj\rvectors so that the approximation can filter out high frequency components\rconsidered to be noise.\nFigure 9.5 shows learning curves comparing the\rFourier and polynomial bases on the 1000-state random walk example. In general,\rwe do not recommend using the polynomial basis for online learning.\n\r\r\r\r\rFigure 9.5: Fourier basis vs\rpolynomials on the 1000-state random walk. Shown are learning curves for the gradient MC\rmethod with Fourier and polynomial bases of order 5, 10, and 20. The\rstep-size parameters were roughly optimized for each case: a = 0.0001 for\rthe polynomial basis and a = 0.00005 for the Fourier basis.\n\r\r\r\r\r\r\r\rEpisodes\n\r\r\r\r\r\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rExercise 9.3 Why does (9.17) define (N + 1)d distinct\rfunctions for dimension d?\n��\n9.5.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rCoarse Coding\nConsider a task in which the state set is continuous and\rtwo-dimensional. A state in this case is a point in 2-space, a vector with two real components. One kind\rof feature for this case is those corresponding to circlesin state space, as shown in Figure 9.6.\n\r\nFigure 9.6: Coarse coding.\rGeneralization from state sto state s!depends on the number of their features whose\rreceptive fields (in this case, circles) overlap. These states have one\rfeature in common, so there will be slight generalization between them.\n\r\r\r\r\r\u0026nbsp;\nIf the state is inside a circle, then the\rcorresponding feature has the value 1 and is said to be present;otherwise the feature is 0 and is said to be absent.This kind of 1-0-valued feature is called a binary\rfeature.Given a state, which\rbinary features are present indicate within which circles the state lies, and thus\rcoarsely code for its location. Representing a state with features that overlap\rin this way (although they need not be circles or binary) is known as coarse\rcoding.\n\r\r\r\nAsymmetric generalization\n\r\r\r\r\r\r\r\r\n��road generalization\n\r\r\r\r\r\r\r\r\nNarrow generalization\n\r\r\r\r\r\r\r\rFigure 9.7: Generalization in linear function approximation methods is\rdetermined by the sizes and shapes of the features�� receptive fields. All\rthree of these cases have roughly the same number and density of features.\n\r\r\r\r\rAssuming linear gradient-descent function\rapproximation, consider the effect of the size and density of the circles.\rCorresponding to each circle is a single weight (a component of w) that is affected by learning. If we train at one\rstate, a point in the space, then the weights of all circles intersecting that\rstate will be affected. Thus, by (9.7), the approximate value function will be\raffected at all states within the union of the circles, with a greater effect\rthe more circles a point has ��in common�� with the state, as shown in Figure 9.6.\rIf the circles are small, then the generalization will be over a short\rdistance, as in Figure 9.7a, whereas if they are large, it will be over a large\rdistance, as in Figure 9.7b. Moreover, the shape of the features will determinethe nature of the generalization. For example, if they are not strictly\rcircular, but are elongated in one direction, then generalization will be\rsimilarly affected, as in Figure 9.7c.\nFeatures with large receptive\rfields give broad generalization, but might also seem to limit the learned\rfunction to a coarse approximation, unable to make discrimina\u0026shy;tions much finer\rthan the width of the receptive fields. Happily, this is not the case. Initial\rgeneralization from one point to another is indeed controlled by the size and\rshape of the receptive fields, but acuity, the finest discrimination ultimately\rpossible, is controlled more by the total number of features.\nExample 9.3: Coarseness\rof Coarse Coding This example\rillustrates the effect on learning of the size of the receptive fields in\rcoarse coding. Linear function approximation based on coarse coding and (9.6)\rwas used to learn a one-dimensional square-wave function (shown at the top of\rFigure 9.8). The values of this function were used as the targets, Ut. With\rjust one dimension, the receptive fields were intervals rather than circles.\rLearning was repeated with three different sizes of the intervals: narrow,\rmedium, and broad, as shown at the bottom of the figure. All three cases had\rthe same density of features, about 50 over the extent of the function being\rlearned. Training examples were generated uniformly at random over this extent.\rThe step-size parameter was a= 0.2, where m is the number of features that were present at one\rtime. Figure 9.8 shows the functions learned in all three cases over the course\rof learning. Note that the width of the features had a strong effect early in\rlearning. With broad features, the generalization tended to be broad; with\rnarrow features, only the close neighbors of each trained point were changed,\rcausing the function learned to be more bumpy. However, the final function\rlearned was affected only slightly by the width of the features. Receptive\rfield shape tends to have a strong effect on generalization but little effect on\rasymptotic solution quality.\n\r\r\r\r\rMedium\nfeatures\n\r\r\r\r\r\r\r\rFigure 9.8: Example\rof feature width��s strong effect on initial generalization (first row) and\rweak effect on asymptotic accuracy (last row).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n\r\r\r\r\r\r\r\rNarrow\nfeatures\n\r\r\r\r\r\r\r\r10240\n\r\r\r\r\r\r\r\rfeature\nwidth\n\r\r\r\r\r\r\r\r2560\n\r\r\r\r\r\r\r\r#Examples\n10\n40\n160\n640\n\r\r\r\r\r\r\r\rBroad\nfeatures\n\r\r\r\r\r\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\r9.5.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rTile Coding\nTile coding is a form of coarse coding for\rmulti-dimensional continuous spaces that is flexible and computationally\refficient. It may be the most practical feature repre\u0026shy;sentation for modern\rsequential digital computers. Open-source software is available for many kinds\rof tile coding.\nIn tile coding the receptive\rfields of the features are grouped into partitions of the input space. Each\rsuch partition is called a tiling, and each element of the partition is called a tile. For example, the simplest tiling of a\rtwo-dimensional state space is a uniform grid such as that shown on the left\rside of Figure 9.9. The tiles or receptive field here are squares rather than\rthe circles in Figure 9.6. If just this single tiling were used, then the state\rindicated by the white spot would be represented by the single feature whose\rtile it falls within; generalization would be complete to all states within the\rsame tile and nonexistent to states outside it. With just one tiling, we would\rnot have coarse coding by just a case of state aggregation.\nTo get the strengths of coarse\rcoding requires overlapping receptive fields, and by definition the tiles of a\rpartition do not overlap. To get true coarse coding with tile coding, multiple\rtilings are used, each offset by a fraction of a tile width. A simple case with\rfour tilings is shown on the right side of Figure 9.9. Every state, such as\rthat indicated by the white spot, falls in exactly one tile in each of the four\rtilings. These four tiles correspond to four features that become active when\rthe state occurs. Specifically, the feature vector x(s) has one component for each tile in each tiling.\rIn this example there are 4 x 4 x 4 = 64 components, all of which will be 0\rexcept for the four corresponding to the tiles that s falls within. Figure 9.10\rshows the advantage of multiple offset tilings (coarse coding) over a single\rtiling on the 1000-state random walk example.\nAn immediate practical advantage\rof tile coding is that, because it works with partitions, the overall number of\rfeatures that are active at one time is the same for any state. Exactly one\rfeature is present in each tiling, so the total number of features present is\ralways the same as the number of tilings. This allows the step-\n\r\r\r\r\rһTiling 1 Tiling\r2 Tiling 3�� Tiling 4\n\r\r\r\r\r\r\r\r��Point in state\rspace �� to be represented\n\r\r\r\r\r\r\r\rFour active\ntiles/features overlap the point\rand are used to represent it\n\r\r\r\r\r\r\r\rContinuous\n\r\r\r\r\r\r\r\r2\n\r\r\r\r\r\r\r\rD\rstate space\n\r\r\r\r\r\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rFigure 9.9:\rMultiple, overlapping grid-tilings on a limited two-dimensional space. These tilings\rare offset from one another by a uniform amount in each dimension.\n\r\r\r\r\rRMSVE\naveraged over 30 runs\n\r\r\r\r\r\r\r\rFigure 9.10: Why we use coarse coding. Shown are learning curves\rrandom walk example for the gradient MC algorithm with a single tiling\rtilings. The space of 1000 states was treated as a single continuous\rdimension, covered with tiles each 200 states wide. The multiple tilings\rwere offset from each other by 4 states. The step-size parameter was set\rso that the initial learning rate in the two cases was the same, a =\r0.0001 for the single tiling and a = 0.0001/50 for the 50 tilings.\n\r\r\r\r\r\r\r\ron the 1000-state and with\rmultiple\n\r\r\r\r\r\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rsize parameter, a, to be set in\ran easy, intuitive way. For example, choosing a = mm, where mis the number of tilings, results in exact\rone-trial learning. If the example s ^ V is trained on, then whatever the prior\restimate, V(s,wt), the new estimate will be V(s��wt+i) = V. Usually one wishes to change more slowly than\rthis, to allow for generalization and stochastic variation in target outputs.\rFor example, one might choose a=\u0026nbsp;\u0026nbsp;\u0026nbsp; ,\rin which case the estimate for the trained state would move one-\ntenth of the way to the target in one update, and\rneighboring states will be moved less, proportional to the number of tiles they\rhave in common.\nTile coding also gains\rcomputational advantages from its use of binary feature vectors. Because each\rcomponent is either 0 or 1, the weighted sum making up the approximate value\rfunction (9.7) is almost trivial to compute. Rather than performing n\rmultiplications and additions, one simply computes the indices of the m��n\ractive features and then adds up the m corresponding components of the weight\rvector.\nGeneralization occurs to states\rother than the one trained if those states fall within any of the same tiles,\rproportional to the number of tiles in common. Even the choice of how to offset\rthe tilings from each other affects generalization. If they are offset\runiformly in each dimension, as they were in Figure 9.9, then different states\rcan generalize in qualitatively different ways, as shown below in the upper\rhalf of Figure 9.11. Each of the eight subfigures show the pattern of\rgeneralization from a trained state to nearby points. In this example their are\reight tilings, thus 64subregions within a tile that generalize distinctly, but all\raccording to one of these eight patterns. Note how uniform offsets result in a\rstrong effect along the diagonal in many patterns. These artifacts can be\ravoided if the tilings are offset asymmetrically, as shown in the lower half of\rthe figure. These lower generalization patterns are better because they are all\rwell centered on the trained state with no\nPossible\rgeneralizations for uniformly offset tilings\nPossible generalizations for asymmetrically\noffset tilings\nFigure 9.11: Why tile asymmetrical offsets are\rpreferred in tile coding. Shown is the strength of generalization from a\rtrained state, indicated by the small black plus, to nearby states, for the\rcase of eight tilings. If the tilings are uniformly offset (above), then there\rare diagonal artifacts and substantial variations in the generalization,\rwhereas with asymmetrically offset tilings the generalization is more spherical\rand homogeneous.\nobvious asymmetries.\nTilings in all cases are offset from each other by a\rfraction of a tile width in each dimension. If wdenotes the tile width and kthe number of tilings, then | is a fundamental unit. Within small squares Won a side, all states activate the same tiles, have\rthe same feature representation, and the same approximated value. If a state is\rmoved by fin\rany cartesian direction, the feature representation changes by one\rcomponent/tile. Uniformly offset tilings are offset from each other by exactly\rthis unit distance. For a two-dimensional space, we say that each tiling is\roffset by the displacement vector (1,1), meaning that it is offset from the previous\rtiling by ftimes this vector. In these terms, the asymmetrically offset tilings shown in\rthe lower part of Figure 9.11 are offset by a displacement vector of (1, 3).\nExtensive studies have been made of the effect of\rdifferent displacement vectors on the generalization of tile coding (Parks and\rMilitzer, 1991; An, 1991; An, Miller and Parks, 1991; Miller, Glanz and Carter,\r1991), assessing their homegeneity and tendency toward diagonal artifacts like\rthose seen for the (1, 1) displacement vectors.\nBased on this work, Miller and Glanz (1996)\rrecommend using displacement vectors consisting of the first odd integers. In\rparticular, for a continuous space of dimension d, a good choice is to use the\rfirst odd integers (1, 3, 5, 7,..., 2d �� 1), with k(the number of tilings) set to an integer power of\r2 greater than or equal to 4d. This is what we have done to produce the tilings\rin the lower half of Figure 9.11, in which d = 2, k = 23\u0026gt; 4d, and the displacement vector is (1, 3). In\ra three-dimensional case, the first four tilings would be offset in total from\ra base position by (0, 0, 0), (1, 3, 5), (2, 6,10), and (3, 9,15). Open-source software that can\refficiently make tilings like this for any d is readily available.\nIn choosing a tiling strategy,\rone has to pick the number of the tilings and the shape of the tiles. The\rnumber of tilings, along with the size of the tiles, determines the resolution\ror fineness of the asymptotic approximation, as in general coarse coding and\rillustrated in Figure 9.8. The shape of the tiles will determine the nature of\rgeneralization as in Figure 9.7. Square tiles will generalize roughly equally\rin each dimension as indicated in Figure 9.11 (lower). Tiles that are elongated\ralong one dimension, such as the stripe tilings in Figure 9.12b, will promote\rgeneralization along that dimension. The tilings in Figure 9.12b are also\rdenser and thinner on the left, promoting discrimination along the horizonal\rdimension at lower values along that dimension. The diagonal stripe tiling in\rFigure 9.12c will promote generalization along one diagonal. In higher\rdimensions, axis-aligned stripes correspond to ignoring some of the dimensions\rin some of the tilings, that is, to hyperplanar slices. Irregular tilings such\ras shown in Figure 9.12a are also possible, though rare in practice and beyond\rthe standard software.\nIn practice, it is often desirable\rto use different shaped tiles in different tilings. For example, one might use\rsome vertical stripe tilings and some horizontal stripe tilings. This would\rencourage generalization along either dimension. However, with stripe tilings\ralone it is not possible to learn that a particular conjunction of horizontal\rand vertical coordinates has a distinctive value (whatever is learned for it\rwill bleed into states with the same horizontal and vertical coordinates). For\rthis one needs the conjunctive rectangular tiles such as originally shown in\rFigure 9.9. With multiple tilings��some horizontal, same vertical, and some\rconjunctive��one can get every\u0026shy;thing: a preference for generalizing along each\rdimension, yet the ability to learn\n\r\n\r\r\r\r\r\u0026nbsp;\nLog stripes\nFigure 9.12: Tilings need not be grids. They can be\rarbitrarily shaped and non-uniform, while still in many cases being\rcomputationally efficient to compute.\n\r\rspecific values for conjunctions (see Section 16.3 for a case study using this). The choice of tilings determines\rgeneralization, and until this choice can be effectively automated, it is\rimportant that tile coding enables the choice to be made flexibly and in a way\rthat makes sense to people.\nAnother useful trick for reducing\rmemory requirements is hashing��a consistent pseudo-random collapsing of a large\rtiling into a much smaller set of tiles. Hashing produces tiles consisting of\rnoncontiguous, disjoint regions randomly spread throughout the state space, but\rthat still form an exhaustive partition. For example, one tile might consist of\rthe four subtiles shown to the right. Through hashing, memory re\u0026shy;quirements are\roften reduced by large factors with little loss of performance. This is\rpossible because high resolution is needed in only a small fraction of the\rstate space. Hashing\nfrees us from the curse of dimensionality in the sense that memory\rrequirements need not be exponential in the number of dimensions, but need\rmerely match the real demands of the task. Good open-source implementations of\rtile coding, including hashing, are widely available.\nExercise 9.4 Suppose we believe that one of two\rstate dimensions is more likely to have an effect on the value function than is\rthe other, that generalization should be primarily across this dimension rather\rthan along it. What kind of tilings could be used to take advantage of this\rprior knowledge?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n9.5.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rRadial Basis Functions\nRadial basis functions (RBFs) are the natural generalization of\rcoarse coding to continuous-valued features. Rather than each feature being\reither 0 or 1, it can be anything in the interval [0,1], reflecting various degreesto which the feature is present. A typical RBF\rfeature, i, has a Gaussian (bell-shaped) response Xi(s) dependent only on the\rdistance between the state, s, and the feature��s prototypical or center state,\rci, and relative to the feature��s width, %:\nXi(s) ^ exp (-l|s2^2��)\u0026#8226;\nThe norm or distance metric of course can be chosen in whatever way\rseems most appropriate to the states and task at hand. Figure 9.13 shows a\rone-dimensional\n\r\nFigure 9.13: One-dimensional radial\rbasis functions.\n\r\r\r\r\r\u0026nbsp;\n\r\u0026nbsp;\nexample with a Euclidean distance metric.\nThe primary advantage of RBFs\rover binary features is that they produce approxi\u0026shy;mate functions that vary\rsmoothly and are differentiable. Although this is appealing, in most cases it\rhas no practical significance. Nevertheless, extensive studies have been made\rof graded response functions such as RBFs in the context of tile coding (An,\r1991; Miller et al., 1991; An, Miller and Parks, 1991; Lane, Handelman and\rGelfand, 1992). All of these methods require substantial additional\rcomputational complexity (over tile coding) and often reduce performance when\rthere are more than two state dimensions. In high dimensions the edges of tiles\rare much more important, and it has proven difficult to obtain well controlled\rgraded tile activations near the edges.\nAn RBF networkis a linear function approximator using RBFs for its\rfeatures. Learning is defined by equations (9.6) and (9.7), exactly as in other\rlinear function approximators. In addition, some learning methods for RBF\rnetworks change the centers and widths of the features as well, bringing them\rinto the realm of nonlinear function approximators. Nonlinear methods may be\rable to fit target functions much more precisely. The downside to RBF networks,\rand to nonlinear RBF networks es\u0026shy;pecially, is greater computational complexity\rand, often, more manual tuning before learning is robust and efficient.\n9.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rNonlinear Function\rApproximation: Artificial Neu\u0026shy;ral Networks\nArtificial neural networks (ANNs) are widely used\rfor nonlinear function approxima\u0026shy;tion. An ANN is a network of interconnected\runits that have some of the properties of neurons, main component of nervous\rsystems. ANNs have a long history, with latest advances in training\rdeeply-layered ANNs being responsible for some of the most impressive abilities\rof machine learning systems, including reinforcement learn\u0026shy;ing systems. In\rChapter 16 we describe several stunning examples of reinforcement learning\rsystems that use ANN function approximation.\nFigure 9.14 shows a generic\rfeedforward ANN, meaning that there are no loops in the network, that is, there\rare no paths within the network by which a unit's output can influence its\rinput. The network in the figure has an output layer consisting of two output\runits, an input layer with four input units, and two hidden layers: layers that\rare neither input nor output layers. A real-valued weight is associated with\reach link. A weight roughly corresponds to the efficacy of a synaptic\rconnection in a real neural network (see Section 15.1). If an ANN has at least\rone loop in its connections, it is a recurrent rather than a feedforward ANN.\rAlthough both feedforward and recurrent ANNs have been used in reinforcement\rlearning, here we look only at the simpler feedforward case.\nThe units (the circles in Figure\r9.14) are typically semi-linear units, meaning that they compute a weighted sum\rof their input signals and then apply to the result a nonlinear function,\rcalled the activation function, to produce the unit's output, or\nFigure 9.14: A generic feedforward neural network with four input\runits, two output units, and two hidden layers.\nactivation. Many different activation functions are\rused, but they are typically S- shaped, or sigmoid, functions such as the\rlogistic function f(x) = 1/(1+ e-x), though sometimes the rectifier nonlinearity f\r(x) = max(0, x) is used. A step function like f (x) = 1 if x \u0026gt; 0��and 0\rotherwise, results in a binary unit with threshold 0. It is often useful for\runits in different layers to use different activation functions.\nThe activation of each output\runit of a feedforward ANN is a nonlinear function of the activation patterns over\rthe network��s input units. The functions are param\u0026shy;eterized by the network��s\rconnection weights. An ANN with no hidden layers can represent only a very\rsmall fraction of the possible input-output functions. However an ANN with a\rsingle hidden layer having a large enough finite number of sigmoid units can\rapproximate any continuous function on a compact region of the network��s input\rspace to any degree of accuracy (Cybenko, 1989). This is also true for other\rnonlinear activation functions that satisfy mild conditions, but nonlinearity\ris essen\u0026shy;tial: if all the units in a multi-layer feedforward ANN have linear\ractivation functions, the entire network is equivalent to a network with no\rhidden layers (because linear functions of linear functions are themselves\rlinear).\nDespite this ��universal\rapproximation�� property of one-hidden-layer ANNs, both experience and theory\rshow that approximating the complex functions needed for many artificial\rintelligence tasks is made easier��indeed may require��abstractions that are\rhierarchical compositions of many layers of lower-level abstractions, that is,\rabstractions produced by deep architectures such as ANNs with many hidden\rlayers. (See Bengio, 2009, for a thorough review.) The successive layers of a\rdeep ANN compute increasingly abstract representations of the network��s ��raw��\rinput, with each unit providing a feature contributing to a hierarchical\rrepresentation of the overall input-output function of the network.\nCreating these kinds of hierarchical representations\rwithout relying exclusively on hand-crafted features has been an enduring\rchallenge for artificial intelligence. This is why learning algorithms for ANNs\rwith hidden layers have received so much attention over the years. ANNs\rtypically learn by a stochastic gradient method (Section 9.3). Each weight is\radjusted in a direction aimed at improving the network��s overall performance as\rmeasured by an objective function to be either minimized or maximized. In the\rmost common supervised learning case, the objective function is the expected\rerror, or loss, over a set of labeled training examples. In reinforcement\rlearning, ANNs can use TD errors to learn value functions, or they can aim to\rmaximize expected reward as in a gradient bandit (Section 2.8) or a policy-gradient\ralgorithm (Chapter 13). In all of these cases it is necessary to estimate how a\rchange in each connection weight would influence the network��s overall\rperformance, in other words, to estimate the partial derivative of an objective\rfunction with respect to each weight, given the current values of all the\rnetwork��s weights. The gradient is the vector of these partial derivatives.\nThe most successful way to do\rthis for ANNs with hidden layers (provided the units have differentiable\ractivation functions) is the backpropagation algorithm, which con\u0026shy;sists of\ralternating forward and backward passes through the network. Each forward pass\rcomputes the activation of each unit given the current activations of the net\u0026shy;work��s\rinput units. After each forward pass, a backward pass efficiently computes a\rpartial derivative for each weight. (As in other stochastic gradient learning\ralgo\u0026shy;rithms, the vector of these partial derivatives is an estimate of the true\rgradient.) In Section 15.10 we discuss methods for training ANNs with hidden\rlayers that use reinforcement learning principles instead of backpropagation.\rThese methods are less efficient than the backpropagation algorithm, but they\rmay be closer to how real neural networks learn.\nThe backpropagation algorithm can\rproduce good results for shallow networks having 1 or 2 hidden layers, but it\rdoes not work well for deeper ANNs. In fact, training a network with k + 1hidden layers can actually result in poorer\rperformance than training a network with k hidden layers, even though the\rdeeper network can represent all the functions that the shallower network can\r(Bengio, 2009). Explaining results like these is not easy, but several factors\rare important. First, the large number of weights in a typical deep ANN makes\rit difficult to avoid the problem of overfitting, that is, the problem of\rfailing to generalize correctly to cases on which the network has not been\rtrained. Second, backpropagation does not work well for deep ANNs because the\rpartial derivatives computed by its backward passes either decay rapidly toward\rthe input side of the network, making learning by deep layers extremely slow,\ror the partial derivatives grow rapidly toward the input side of the network,\rmaking learning unstable. Methods for dealing with these problems are largely\rresponsible for many impressive results achieved by systems that use deep ANNs.\nOverfitting is a problem for any\rfunction approximation method that adjusts func\u0026shy;tions with many degrees of\rfreedom on the basis of limited training data. It is less of a problem for\ron-line reinforcement learning that does not rely on limited training sets, but\rgeneralizing effectively is still an important issue. Overfitting is a problem\n\r\rfor ANNs in general, but especially so for deep ANNs\rbecause they tend to have very large numbers of weights. Many methods have been\rdeveloped for reducing overfitting. These include stopping training when\rperformance begins to decrease on validation data different from the training\rdata (cross validation), modifying the objective function to discourage\rcomplexity of the approximation (regularization), and introducing dependencies\ramong the weights to reduce the number of degrees of freedom (e.g., weight\rsharing).\nA particularly effective method\rfor reducing overfitting by deep ANNs is the dropout method introduced by\rSrivastava, Hinton, Krizhevsky, Sutskever, and Salakhut- dinov (2014). During\rtraining, units are randomly removed from the network (dropped out) along with\rtheir connections. This can be thought of as training a large number of\r��thinned�� networks. Combining the results of these thinned networks at test\rtime is a way to improve generalization performance. The dropout method\refficiently ap\u0026shy;proximates this combination by multiplying each outgoing weight\rof a unit by the probability that that unit was retained during training.\rSrivastava et al. found that this method significantly improves generalization\rperformance. It encourages indi\u0026shy;vidual hidden units to learn features that work\rwell with random collections of other features. This increases the versatility\rof the features formed by the hidden units so that the network does not overly\rspecialize to rarely-occurring cases.\nHinton, Osindero, and Teh (2006)\rtook a major step toward solving the problem of training the deep layers of a\rdeep ANN in their work with deep belief networks, layered networks closely\rrelated to the deep ANNs discussed here. In their method, the deepest layers\rare trained one at a time using an unsupervised learning algorithm. Without\rrelying on the overall objective function, unsupervised learning can extract\rfeatures that capture statistical regularities of the input stream. The deepest\rlayer is trained first, then with input provided by this trained layer, the\rnext deepest layer is trained, and so on, until the weights in all, or many, of\rthe network's layers are set to values that now act as initial values for\rsupervised learning. The network is then fine-tuned by backpropagation with\rrespect to the overall objective function. Studies show that this approach\rgenerally works much better than backpropagation with weights initialized with\rrandom values. The better performance of networks trained with weights\rinitialized this way could be due to many factors, but one idea is that this\rmethod places the network in a region of weight space from which a\rgradient-based algorithm can make good progress.\nA type of deep ANN that has\rproven to be very successful in applications, includ\u0026shy;ing impressive\rreinforcement learning applications (Chapter 16) is the deep\rconvolu\u0026shy;tional network. This\rtype of network is specialized for processing high-dimensional data arranged in\rspatial arrays, such as images. It was inspired by how early visual processing\rworks in the brain (LeCun, Bottou, Bengio and Haffner, 1998). Because of its\rspecial architecture, a deep convolutional network can be trained by backprop-\ragation without resorting to methods like those described above to train the\rdeep layers.\nFigure 9.15 illustrates the\rarchitecture of a deep convolutional network. This in\u0026shy;stance, from LeCun et al.\r(1998), was designed to recognize hand-written characters.\n\rC3: f. maps 16@ 10x10\n\nConvolutions\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Subsampling\rConvolutions Subsampling Full connection\nFigure 9.15: Deep Convolutional\rNetwork. Republished with permission of Proceedings of the IEEE, from\rGradient-based learning applied to document recognition, LeCun, Bottou,\rBengio, and Haffner, volume 86, 1998; permission conveyed through Copyright\rClearance Center, Inc.\n\r\r\r\r\r\u0026nbsp;\nIt consists of alternating convolutional and\rsubsampling layers, followed by several fully connected final layers. Each\rconvolutional layer produces a number of feature maps. A feature map is a\rpattern of activity over an array of units, where each unit performs the same\roperation on data in its receptive field, which is the part of the data it\r��sees�� from the preceding layer (or from the external input in the case of the\rfirst convolutional layer). The units of a feature map are identical to one\ranother except that their receptive fields, which are all the same size and\rshape, are shifted to different locations on the arrays of incoming data. Units\rin the same feature map share the same weights. This means that a feature map\rdetects the same feature no matter where it is located in the input array. In\rthe network in Figure 9.15, for example, the first convolutional layer produces\r6feature\rmaps, each consisting of 28 x 28 units. Each unit in each feature map has a 5 x\r5 receptive field, and these receptive fields overlap (in this case by four\rcolumns and five rows). Consequently, each of the 6feature maps is specified by just 25 adjustable\rweights.\nThe subsampling layers of a deep\rconvolutional network reduce the spatial res\u0026shy;olution of the feature maps. Each\rfeature map in a subsampling layer consists of units that average over a\rreceptive field of units in the feature maps of the preceding convolutional\rlayer. For example, each unit in each of the 6feature maps in the first subsampling layer of the\rnetwork of Figure 9.15 averages over a 2 x 2 non-overlapping receptive fields\rof a feature map produced by the first convolutional layer, resulting in six 14\rx 14 feature maps. The subsampling layers reduce the network��s sensitivity to\rthe spatial locations of the features detected, that is, they help make the\rnetwork��s responses spatially invariant. This is useful because a feature\rdetected at one place in an image is likely to be useful at other places as\rwell.\nAdvances in the design and\rtraining of ANNs��of which we have only mentioned a few��all contribute to\rreinforcement learning. Although current reinforcement learning theory is\rmostly limited to methods using tabular or linear function approx\u0026shy;imation\rmethods, the impressive performances of notable reinforcement learning ap\u0026shy;plications\rowe much of their success to nonlinear function approximation by ANNs,\nin particular, by deep ANNs. The case studies we discuss in Chapter\r16 that use ANNs all use deep convolutional networks, which are well suited for\rthe spatial arrays that represent states in these problems. Other network\rarchitectures are appropriate for other types of problems, and one of the\rchallenges of using ANNs for function approximation is finding a network\rarchitecture that works well for the problem of interest.\n9.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rLeast-Squares TD\nIn Section 9.4 we established that TD(0) with linear function\rapproximation con\u0026shy;verges asymptotically, for appropriately decreasing step\rsizes, to the TD fixedpoint:\nwtd = A ib,\nwhere\n\r\r\rA == E\n\r\r\r\r\rxt(xt �� Yxt+i)Tand b ==E[Rt+ixt]\nWhy, we might ask, must we compute this solution iteratively? This\ris wasteful of data! Could one not do better by computing estimates of A and b, and\rthen directly computing the TD fixedpoint? The Least-Squares TD algorithm,\rcommonly known as LSTD, does exactly this. It forms the natural estimates\ntt At =����xfc(xfc�� Yxfc+i)T+ el and bt =����Rt+ixfc\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (9.18)\nk=0\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; k=0\n(where eI, for some small e \u0026gt; 0, ensures that At is\ralways invertible) and then estimates the TD fixedpoint as\n\r\r\rwt+i\n\r\r\r\r\r==Ap bt.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (9.19)\nThis algorithm is the most data efficient form of\rlinear TD(0), but it is also much more expensive computationally. Recall that\rsemi-gradient TD(0) requires memory and per-step computation that is only O(d).\nHow complex is LSTD? As it is written above the\rcomplexity seems to increase with t, but the two approximations in (9.18) could\rbe implemented incrementally using the techniques we have covered earlier\r(e.g., in Chapter 2) so that they can be done in constant time per step. Even\rso, the update for At would involve an outer product (a column vector times a\rrow vector) and thus would be a matrix update; its computational complexity\rwould be O(d2), and of course the memory required to hold the At\rmatrix would be O(d2).\nA potentially greater problem is that our final computation (9.19)\ruses the inverse of At, and the computational complexity of a general inverse\rcomputation is O(d3). Fortunately, an inverse of a matrix of our\rspecial form��a sum of outer products��canalso be updated incrementally with only O(d2) computations, as\nA;1= (A_t-i + xt(xt - 7xt+i)T)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (from\r(9.18))\n=A-_ii\r-��--ixt(xt\r- Yxt+i)TX--i,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (9.20)\n1+ (xt - 7xt+i)T^Vt-ixt\nwith A_i == ^I. Although the identity\r(9.20), known as the Sherman-Morrison formula, is\rsuperficially complicated, it involves only vector-matrix and vector-vector\rmultiplications and thus is only O(d2). Thus we can store and\rmaintain the inverse matrix A-it, and then use it in (9.19), all\rwith only O(d2) memory and per-step computation. The complete\ralgorithm is given in the box below.\nLSTD for estimating V c Vn (O(d2) version)\nInput: feature representation x(s) G Rd, Vs G S, x(terminal) == 0\nA-i^-iI\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; An\rd x d matrix\nb �� 0\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; An\rd-dimensional vector\nRepeat (for each episode):\nInitialize\rS; obtain corresponding x Repeat (for each step of episode):\nChoose A \u0026#12316;n(-|S)\nTake action A, observe R, Sf;obtain corresponding xf\nT\nv �� A-i(x �� yx;)\nA-i�� A-i�� (A-ix)vT/ (1 + vTx) b ��\rb + Rx\ne�� a-1b\nS �� S;; x �� x! until Sf is terminal\nOf course, O(d2) is still\rsignificantly more expensive than the O(d) of semi-gradient TD. Whether the\rgreater data efficiency of LSTD is worth this computational expense depends on\rhow large d is, how important it is to learn quickly, and the expense of other\rparts of the system. The fact that LSTD requires no step-size parameter is\rsometimes also touted, but the advantage of this is probably overstated. LSTD\rdoes not require a step size, but it does requires e\\\rif e is chosen too small the sequence of inverses can\rvary wildly, and if e is chosen too large then learning is slowed. In addition,\rLSTD's lack of a step size parameter means that it never forgets. This is\rsometimes desirable, but it is problematic if the target policy n changes as it\rdoes in reinforcement learning and GPI. In control applications, LSTD typically\rhas to be combined with some other mechanism to induce forgeting, mooting any\rinitial advantage of not requiring a step size parameter.\n\r\r9.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rMemory-based Function\rApproximation\nSo far we have discussed the parametricapproach to approximating value functions. In this\rapproach, a learning algorithm adjusts the parameters of a functional form\rintended to approximate the value function over a problem��s entire state space.\rEach backup, s ^ g, is a training example used by the learning algorithm to\rchange the parameters with the aim of reducing the approximation error. After\rthe update, the training example can be discarded (although it might be saved\rto be used again). When an approximate value of a state (which we will call the\rquery state)is needed, the function is simply evaluated at that state using the latest\rparameters produced by the learning algorithm.\nMemory-based function\rapproximation methods are very different. They simply save training examples in\rmemory as they arrive (or at least save a subset of the examples) without\rupdating any parameters. Then, whenever a query state��s value estimate is needed,\ra set of examples is retrieved from memory and used to compute a value estimate\rfor the query state. This approach is sometimes called lazy\rlearning because processing\rtraining examples is postponed until the system is queried to provide an\routput.\nMemory-based function\rapproximation methods are prime examples of nonpara- metricmethods. Unlike parametric methods, the\rapproximating function��s form is not limited to a fixed parameterized class of\rfunctions, such as linear functions or polynomials, but is instead determined\rby the training examples themselves, together with some means for combining\rthem to output estimated values for query states. As more training examples\raccumulate in memory, one expects nonparametric methods to produce increasingly\raccurate approximations of any target function.\nThere are many different\rmemory-based methods depending on how the stored training examples are selected\rand how they are used to respond to a query. Here, we focus on local-learningmethods that approximate a value function only\rlocally in the neighborhood of the current query state. These methods retrieve\ra set of training examples from memory whose states are judged to be the most\rrelevant to the query state, where relevance usually depends on the distance\rbetween states: the closer a training example��s state is to the query state,\rthe more relevant it is considered to be, where distance can be defined in many\rdifferent ways. After the query state is given a value, the local approximation\ris discarded.\nThe simplest example of the\rmemory-based approach is the nearest neighbormethod, which simply finds the example in memory\rwhose state is closest to the query state and returns that example��s value as\rthe approximate value of the query state. In other words, if the query state is\rs, and sf^ gis the example in memory in which s; is the closest\rstate to s, then g is returned as the approximate value of s. Slightly more\rcomplicated are weighted averagemethods that retrieve a set of nearest neigh\u0026shy;bor\rexamples and return a weighted average of their target values, where the\rweights generally decrease with increasing distance between their states and\rthe query state. Locally weighted regressionis similar, but it fits a surface to the values of\ra set of nearest states by means of a parametric approximation method that\rminimizes a\nweighted error measure like (9.1), where the weights\rdepend on distances from the query state. The value returned is the evaluation\rof the locally-fitted surface at the query state, after which the local\rapproximation surface is discarded.\nBeing nonparametric, memory-based\rmethods have the advantage over paramet\u0026shy;ric methods of not limiting\rapproximations to pre-specified functional forms. This allows accuracy to\rimprove as more data accumulates. Memory-based localapproxi\u0026shy;mation methods have other properties that\rmake them well suited for reinforcement learning. Because trajectory sampling\ris of such importance in reinforcement learn\u0026shy;ing, as discussed in Section 8.6, memory-based local methods can focus function\rapproximation on local neighborhoods of states (or state-action pairs) visited\rin real or simulated trajectories. There may be no need for global\rapproximations because many areas of the state space will never (or almost\rnever) be reached. In addition, memory-based methods allow an agent��s\rexperience to have a relatively immediate affect on value estimates in the\rneighborhood if its environment��s current state, in contrast with a parametric\rmethod��s need to incrementally adjust parameters of a global approximation.\nAvoiding global approximations is\ralso a way to address the curse of dimensionality. For example, for a state\rspace with d dimensions, a tabular method storing a global approximation\rrequires memory exponential in d. On the other hand, in storing examples for a\rmemory-based method, each example requires memory proportional to d, and the\rmemory required to store, say, n examples is linear in n. Nothing is\rexponential in d or n. Of course, the critical remaining issue is whether a\rmemory- based method can answer queries quickly enough to be useful to an\ragent. A related concern is how speed degrades as the size of the memory grows.\rFinding nearest neighbors in a large database can take too long to be practical\rin many applications.\nProponents of memory-based\rmethods have developed ways to accelerate the near\u0026shy;est neighbor search. Using\rparallel computers or special purpose hardware is one approach; another is the\ruse of special multi-dimensional data structures to store the training data. One\rdata structure studied for this application is the k-d tree (short for\rk-dimensional tree), which recursively splits a k-dimensional space into\rregions arranged as nodes of a binary tree. Depending on the amount of data and\rhow it is distributed over the state space, nearest-neighbor search using k-d\rtrees can quickly eliminate large regions of the space in the search for\rneighbors, making the searches feasible in some problems where naive searches\rwould take too long.\nLocally weighted regression\radditionally requires fast ways to do the local regres\u0026shy;sion computations which\rhave to be repeated to answer each query. Researchers have developed many ways\rto address these problems, including methods for forgetting en\u0026shy;tries in order\rto keep the size of the database within bounds. The Bibliographic and\rHistorical Comments section at the end of this chapter points to some of the\rrelevant literature, including a selection of papers describing applications of\rmemory-based learning to reinforcement learning.\n9.9\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rKernel-based Function\rApproximation\nMemory-baed methods such as the weighted average and locally\rweighted regression methods described above depend on assigning weights to\rexamples sf^ g in the database depending on the distance\rbetween sfand a query states s. The function that assigns\rthese weights is called a kernel function, or simply a kernel. In the weighted average and locally weighted\rregressions methods, for example, a kernel function k: R R assigns weights to distances between states.\rMore generally, weights do not have to depend on distances; they can depend on\rsome other measure of similarity between states. In this case, k : S x S R, so\rthat k(s, s;) is the weight given to data about sfin its influence on answering queries about s.\nViewed slightly differently, k(s, s;) is\ra measure of the strength of generalization from sحto s.\rKernel functions numerically express how relevantknowledge about any state is to any other state. As\ran example, the strengths of generalization for tile coding shown in Figure\r9.11 correspond to different kernel functions resulting from uniform and\rasymmetrical tile offsets. Although tile coding does not explicitly use a\rkernel function in its operation, it generalizes according to one. In fact, as\rwe discuss more below, the strength of generalization resulting from linear\rparametric function approximation can always be described by a kernel function.\nKernel regressionis the memory-based method that computes a kernel\rweighted average of the targets of allexamples stored in memory, assigning the result to\rthe query state. If Dis the set of stored examples, and g(s;)\rdenotes the target for state s; in a stored example, then kernel\rregression approximates the target function, in this case a value function depending\ron D, as\n��(s,D)= E k(s, s;)g(s;).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (9.21)\nThe weighted average method described above is a special case in\rwhich k(s, s;) is non-zero only when s and s; are close\rto one another so that the sum need not be computed over all of D.\nA common kernel is the Gaussian radial basis\rfunction (RBF) used in RBF function approximation as described in Section\r9.5.5. In the method described there, RBFs are features whose centers and\rwidths are either fixed from the start, with centers presumably concentrated in\rareas where many examples are expected to fall, or are adjusted in some way\rduring learning. Barring methods that adjust centers and widths, this is a\rlinear parametric method whose parameters are the weights of each RBF, which\rare typically learned by stochastic gradient, or semi-gradient, descent. The\rform of the approximation is a linear combination of the pre-determined RBFs.\rKernel regression with an RBF kernel differs from this in two ways. First, it\ris memory-based: the RBFs are centered on the states of the stored examples.\rSecond, it is nonparametric: there are no parameters to learn; the response to\ra query is given by (9.21).\nOf course, many issues have to be\raddressed for practical implementation of ker\u0026shy;nel regression, issues that are\rbeyond the scope or our brief discussion. However, it turns out that any linear\rparametric regression method like those we described in Sec\u0026shy;tion 9.4, with\rstates represented by feature vectors x(s) = (xi(s), x2(s),..., xn(s))T, can be recast as kernel\rregression where k(s, s') is the inner product of the feature vector\rrepresentations of s and s'; that is\nk(s, s') = x(s)Tx(s').\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (9.22)\nKernel regression with this kernel function produces\rthe same approximation that a linear parametric method would if it used these\rfeature vectors and learned with the same training data.\nWe skip the mathematical\rjustification for this, which can be found in any modern machine learning text,\rsuch as Bishop (2006), and simply point out an important implication. Instead\rof constructing features for linear parametric function approx\u0026shy;imators, one can\rinstead construct kernel functions directly without referring at all to feature\rvectors. Not all kernel functions can be expressed as inner products of feature\rvectors as in (9.22), but a kernel function that can be expressed like this can\roffer significant advantages over the equivalent parametric method. For many\rsets of feature vectors, (9.22) has a compact functional form that can be\revaluated without any computation taking place in the n-dimensional feature\rspace. In these cases, kernel regression is much less complex than directly\rusing a linear parametric method with states represented by these feature\rvectors. This is the so-called ��kernel trick�� that allows effectively working\rin the high-dimension of an expansive feature space while actually working only\rwith the set of stored training examples. The ker\u0026shy;nel trick is the basis of\rmany machine learning methods, and researchers have shown how it can sometimes\rbenefit reinforcement learning.\nThe Bibliographic and Historical Comments section at\rthe end of this chapter refers to a selection of publications on some of the\rmathematical details and on some of the kernel-based reinforcement learning\rmethods that have been proposed.\n9.10\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rLooking Deeper at On-policy\rLearning: Interest and Emphasis\nThe algorithms we have considered so far in this\rchapter have treated all the states encountered equally, as if they were all\requally important. In some cases, however, we are more interested in some\rstates than others. In discounted episodic problems, for example, we may be\rmore interested in accurately valuing early states in the episode than in later\rstates where discounting may have made the rewards much less important to the\rvalue of the start state. Or, if an action-value function is being learned, it\rmay be less important to accurately value poor actions whose value is much less\rthan the greedy action. Function approximation resources are always limited,\rand if they were used in a more targeted way, then performance could be\rimproved.\nOne reason we have treated all\rstates encountered equally is that then we are updating according to the\ron-policy distribution, for which stronger theoretical re\u0026shy;\n\r\rsults are\ravailable for semi-gradient methods. Recall that the on-policy distribution was\rdefined as the distribution of states encountered in an MDP while following the\rtarget policy. Now we will generalize this concept significantly. Rather than\rhaving one on-policy distribution for the MDP, we will have many. All of them will\rhave in common that they are a distribution of states encountered in\rtrajectories while following the target policy, but they will vary in how the\rtrajectories are, in a sense, initiated.\nWe now introduce some new concepts. First be introduce a non-negative\rscalar measure, a random variable It called interest, indicating the degree to which we are interested\rin accurately valuing the state (or state-action pair) at time t. If we don��t\rcare at all about the state, then the interest should be zero; if we fully\rcare, it might be one, though it��s formally allowed take any non-negative\rvalue. The interest can be set in any causal way; for example, it may depend on\rthe trajectory up to time t or the learned parameters\rat time t. The distribution ^ in the MSVE (9.1) is then defined as the\rdistribution of states encountered while following the target policy, weighted\rby the interest. Second, we introduce another non-negative scalar random\rvariable, the emphasis Mt. This scalar multiplies the learning update and thus emphasizes or\rde-emphasizes the learning done at time t. The general n-step learning rule,\rreplacing (9.14), is\nwt+n = wt+n-1+aMt [Gt��t+n - v(St,wt+ra-i)] W(St,wt+ra-i),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0\r\u0026lt;\rt \u0026lt; T, (9.23)\nwith the n-step return given by (9.15) and the emphasis determined\rrecursively from the interest by:\nMt = It + YnMt-n,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0\u0026lt; t\u0026lt;T,(9.24)\nwith Mt == 0, Vt \u0026lt; 0. These equations are taken to include the Monte Carlo case,\rfor which Gt��t+n = Gt, all the updates are taken at episode��s end, n = T - t, and Mt = It.\n9.11\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rSummary\nReinforcement learning systems must be capable of\rgeneralization if they are to be applicable to\rartificial intelligence or to large engineering applications. To achieve this,\rany of a broad range of existing methods for supervised-learning\rfunction ap\u0026shy;proximation can be used simply by treating each backup as a\rtraining example.\nPerhaps the most suitable supervised learning\rmethods are those using parameter\u0026shy;ized function approximation,\rin which the policy is parameterized by a weight vector w. Although the weight\rvector has many components, the state space is much larger still and we must\rsettle for an approximate solution. We defined MSVE(w) as a measure of the\rerror in the values (s) for a weight vector w under the on-policy\rdistribution,��The MSVE gives us a clear way to rank different value-function\rapproximations in the on-policy case.\n\r\nvn= 4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; vn = 3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; v ��=2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; vn = 1\n\r\r\r\r\r\u0026nbsp;\n\r\r\rExample of Interest and Emphasis\n\r\r\r\r\r\r\r\rTo see the potential benefits of using interest and\remphasis, consider the four- state Markov reward process shown below:\n\r\r\r\r\rEpisodes start in the leftmost state, then\rtransition one state to the right, with a reward of +1, on each step until the\rterminal state is reached. The true value of the first state is thus 4, of the\rsecond state 3, and so on as shown below each state. These are the true values;\rthe estimated values can only approximate these because they are constrained by\rthe parameterization. There are two components to the parameter vector w = (wi,\rW2)T, and the\rparameterization is as written inside each state. The estimated values of the\rfirst two states are given by wi alone and thus must be the same even though\rtheir true values are different. Similarly, the estimated values of the third\rand fourth states are given by W2alone and must be the\rsame even though their true values are different. Suppose that we are\rinterested in accurately valuing only the leftmost state; we assign it an\rinterest of 1while all the other states are assigned an interest of 0, as indicated above the states.\nFirst consider applying gradient Monte Carlo\ralgorithms to this problem. The algorithms presented earlier in this chapter\rthat do not take into account interest and emphasis (in (9.6) and the box on\rpage 216) will converge (for decreasing step sizes) to the parameter vector w^ = (3.5,1.5), which gives the first state��the only one we are\rinterested in��a value of 3.5 (i.e., intermediate between the true values of the\rfirst and second states). The methods presented in this section that do use\rinterest and emphasis, on the other hand, will learn the value of the first\rstate exactly correctly; wi will converge to 4 while w2will never\rbe updated because the emphasis is zero in all states save the leftmost.\nNow consider applying two-step semi-gradient TD\rmethods. The methods from earlier in this chapter without interest and emphasis\r(in (9.14) and (9.15) and the box on page 223) will again converge to w^ = (3.5,1.5), while the methods with interest and emphasis converge\rto w^ = (4, 2). The latter pro\u0026shy;duces the exactly correct values for the\rfirst state and for the third state (which the first state bootstraps from)\rwhile never making any updates corresponding to the second or fourth states.\nTo find a good weight vector, the\rmost popular methods are variations of stochas\u0026shy;tic gradient\rdescent(SGD).\rIn this chapter we have focused on the on-policycase with a fixed policy,also known as policy evaluation or prediction; a\rnatural learn\u0026shy;ing algorithm for this case is n-step\rsemi-gradient TD,which includes gradient MC and semi-gradient TD(0) algorithms as the\rspecial cases when n = oo and n = 1 respectively. Semi-gradient TD methods are\rnot true gradient methods. In such bootstrapping methods (including DP), the\rweight vector appears in the update tar\u0026shy;get, yet this is not taken into account\rin computing the gradientһthus they are semi-gradient methods. As such, they cannot rely on\rclassical SGD results.\nNevertheless, good results can be\robtained for semi-gradient methods in the special case of linearfunction approximation, in which the value estimates\rare sums of features times corresponding weights. The linear case is the most\rwell understood theoretically and works well in practice when provided with\rappropriate features. Choosing the features is one of the most important ways\rof adding prior domain knowledge to reinforcement learning systems. They can be\rchosen as polynomials, but this case generalizes poorly in the online learning\rsetting typically considered in reinforcement learning. Better is to choose\rfeatures according the Fourier basis, or according to some form of coarse\rcoding with sparse overlapping receptive fields. Tile coding is a form of\rcoarse coding that is particularly computationally efficient and flexible.\rRadial basis functions are useful for one- or two-dimensional tasks in which a\rsmoothly varying response is important. LSTD is the most data-efficient linear\rTD prediction method, but requires computation proportional to the square of\rthe number of weights, whereas all the other methods are of complexity linear\rin the number of weights. Nonlinear methods include artificial neural networks\rtrained by backpropagation and variations of SGD; these methods have become\rvery popular in recent years under the name deep reinforcement\rlearning.\nLinear semi-gradient n-step TD is guaranteed to\rconverge under standard condi\u0026shy;tions, for all n, to a MSVE that is within a\rbound of the optimal error. This bound is always tighter for higher n and\rapproaches zero as n o. However, in practice that choice results in very slow\rlearning, and some degree of bootstrapping (1\u0026lt; n \u0026lt; co) is usually preferrable.\nBibliographical and Historical Remarks\nGeneralization and function approximation have always been an\rintegral part of rein\u0026shy;forcement learning. Bertsekas and Tsitsiklis (1996),\rBertsekas (2012), and Sugiyama et al. (2013) present the state of the art in\rfunction approximation in reinforce\u0026shy;ment learning. Some of the early work with\rfunction approximation in reinforcement learning is discussed at the end of\rthis section.\n9.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Gradient-descent methods for minimizing mean-squared\rerror in supervised learning are well known. Widrow and Hoff (1960) introduced\rthe least-mean- square (LMS) algorithm, which is the prototypical incremental\rgradient- descent algorithm. Details of this and related algorithms are\rprovided in many texts (e.g., Widrow and Stearns, 1985; Bishop, 1995; Duda and\rHart, 1973).\nSemi-gradient TD(0) was first explored by Sutton (1984, 1988), as\rpart of the linear TD(A) algorithm that we will treat in Chapter 12. The term\r��semi-gradient�� to describe these bootstrapping methods is new to the second\redition of this book.\nThe earliest use of state aggregation in reinforcement learning may\rhave been Michie and Chambers��s BOXES system (1968). The theory of state aggre\u0026shy;gation\rin reinforcement learning has been developed by Singh, Jaakkola, and Jordan\r(1995) and Tsitsiklis and Van Roy (1996). State aggregation has been used in\rdynamic programming from its earliest days (e.g., Bellman, 1957a).\n9.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Sutton (1988) proved convergence of linear TD(0) in\rthe mean to the minimal MSVE solution for the case in which the feature\rvectors, {x(s) : s G S}, are\rlinearly independent. Convergence with probability 1 was proved by several\rresearchers at about the same time (Peng, 1993; Dayan and Sejnowski, 1994;\rTsitsiklis, 1994; Gurvits, Lin, and Hanson, 1994). In addition, Jaakkola,\rJordan, and Singh (1994) proved convergence under on-line updating. All of\rthese results assumed linearly independent feature vectors, which implies at\rleast as many components to wt as\rthere are states. Convergence for the more important case of general\r(dependent) feature vectors was first shown by Dayan (1992). A significant\rgeneralization and strengthening of Dayan��s result was proved by Tsitsiklis and\rVan Roy (1997). They proved the main result presented in this section, the\rbound on the asymptotic error of linear bootstrapping methods.\n9.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Our presentation of the range of possibilities for\rlinear function approximation is based on that by Barto (1990).\n9.5.3\u0026nbsp;\u0026nbsp;\u0026nbsp; The term coarse codingis due to Hinton (1984), and our Figure 9.6 is\rbased on one of his figures. Waltz and Fu (1965) provide an early example of\rthis type of function approximation in a reinforcement learning system.\n9.5.4\u0026nbsp;\u0026nbsp;\u0026nbsp; Tile coding, including hashing, was introduced by\rAlbus (1971, 1981). He de\u0026shy;scribed it in terms of his ��cerebellar model\rarticulator controller,�� or CMAC, as tile coding is sometimes known in the\rliterature. The term ��tile cod\u0026shy;ing�� was new to the first edition of this book,\rthough the idea of describing CMAC in these terms is taken from Watkins (1989).\rTile coding has been used in many reinforcement learning systems (e.g.,\rShewchuk and Dean, 1990; Lin and Kim, 1991; Miller, Scalera, and Kim, 1994;\rSofge and White, 1992; Tham, 1994; Sutton, 1996; Watkins, 1989) as well as in\rother types of learning control systems (e.g., Kraft and Campagna, 1990; Kraft,\rMiller, and Dietz, 1992). This section draws heavily on the work of Miller and\rGlanz (1996).\n\r\r9.5.5 Function approximation using radial basis functions\r(RBFs) has received wide attention ever since being related to neural networks\rby Broomhead and Lowe (1988). Powell (1987) reviewed earlier uses of RBFs, and\rPoggio and Girosi (1989, 1990) extensively developed and applied this approach.\n9.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; The introduction of the threshold logic unit as an\rabstract model neuron by McCulloch and Pitts (1943) was the beginning of\rartificial neural net\u0026shy;works (ANNs). The history of ANNs as learning methods for\rclassification or regression has passed through several stages: roughly, the\rPerceptron (Rosen\u0026shy;blatt, 1962) and ADALINE (ADAptive LINear Element) (Widrow\rand Hoff, 1960) stage of learning by single-layer ANNs, the\rerror-backpropagation stage (Werbos, 1974; LeCun, 1985; Parker, 1985;\rRumelhart, Hinton, and Williams, 1986) of learning by multi-layer ANNs, and the\rcurrent deep-learning stage with its emphasis on representation learning (e.g.,\rBengio, Courville, and Vincent, 2012; Goodfellow, Bengio, and Courville, 2016).\rExamples of the many books on ANNs are Haykin (1994), Bishop (1995), and Ripley\r(2007).\nANNs as function approximation for reinforcement learning goes back\rto the early neural networks of Farley and Clark (1954), who used reinforcement\u0026shy;like\rlearning to modify the weights of linear threshold functions representing\rpolicies. Widrow, Gupta, and Maitra (1973) presented a neuron-like linear\rthreshold unit implementing a learning process they called learning\rwith a criticor selective bootstrap adaptation, a reinforcement-learning variant of the ADALINE\ralgorithm. Werbos (1974, 1987, 1994) developed an approach to prediction and\rcontrol that uses ANNs trained by error backpropation to learn policies and\rvalue functions using TD-like algorithms. Barto, Sutton, and Brouwer (1981) and\rBarto and Sutton (1981b) extended the idea of an as\u0026shy;sociative memory network\r(e.g., Kohonen, 1977; Anderson, Silverstein, Ritz, and Jones, 1977) to\rreinforcement learning. Barto, Anderson, and Sutton (1982) used a two-layer ANN\rto learn a nonlinear control policy, and em\u0026shy;phasized the first layer's role of\rlearning a suitable representation. Hampson (1983, 1989) was an early proponent\rof multilayer ANNs for learning value functions. Barto, Sutton, and Anderson\r(1983) presented an actor-critic al\u0026shy;gorithm in the form of an ANN learning to\rbalance a simulated pole (see Sections 15.7 and 15.8). Barto and Anandan (1985)\rintroduced a stochastic version of Widrow, Gupta, and Maitra��s (1973) selective\rbootstrap algorithm called the associative\rreward-penalty(Ar-p) algorithm. Barto (1985, 1986) and Barto and Jordan (1987)\rdescribed multi-layer ANNs consisting of Ar-p units\rtrained with a globally-broadcast reinforcement signal to learn classi\u0026shy;fication\rrules that are not linearly separable. Barto (1985) discussed this approach to\rANNs and how this type of learning rule is related to others in the literature\rat that time. (See Section 15.10 for additional discussion of this approach to\rtraining multi-layer ANNs.) Anderson (1986, 1987, 1989) eval\u0026shy;uated numerous\rmethods for training multilayer ANNs and showed that an actor-critic algorithm\rin which both the actor and critic were implemented by two-layer ANNs trained\rby error backpropagation outperformed single\u0026shy;layer ANNs in the pole-balancing\rand tower of Hanoi tasks. Williams (1988) described several ways that\rbackpropagation and reinforcement learning can be combined for training ANNs.\rGullapalli (1990) and Williams (1992) de\u0026shy;vised reinforcement learning\ralgorithms for neuron-like units having continu\u0026shy;ous, rather than binary,\routputs. Barto, Sutton, and Watkins (1990) argued that ANNs can play\rsignificant roles for approximating functions required for solving sequential\rdecision problems. Williams (1992) related REINFORCE learning rules (Section\r13.3) to the error backpropagation method for train\u0026shy;ing multi-layer ANNs.\rSchmidhuber (2015) reviews applications of ANNs in reinforcement learning,\rincluding applications of recurrent ANNs.\n9.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; LSTD is due to Bradtke and Barto (see Bradtke, 1993,\r1994; Bradtke and Barto, 1996; Bradtke, Ydstie, and Barto, 1994), and was\rfurther developed by Boyan (1999, 2002) and Nedic and Bertsekas (2003). The\rincremental update of the inverse matrix has been known at least since 1949\r(Sherman and Morrison, 1949). An extension of least-squares methods to control\rwas introduced by Lagoudakis and Parr (2003).\n9.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Our discussion of memory-based function approximation\ris largely based on the review of locally weighted learning by Atkeson, Moore,\rand Schaal (1997). Atkeson (1992) discussed the use of locally weighted\rregression in memory- based robot learning and supplied an extensive\rbibliography covering the history of the idea. Stanfill and Waltz (1986)\rinfluentially argued for the importance of memory based methods in artificial\rintelligence, especially in light of parallel architectures then becoming\ravailable, such as the Connection Machine. Baird and Klopf (1993) introduced a\rnovel memory-based approach and used it as the function approximation method\rfor Q-learning applied to the pole-balancing task. Schaal and Atkeson (1994)\rapplied locally weighted regression to a robot juggling control problem, where it\rwas used to learn a system model. Ping (1995) used the pole-balancing task to\rexperiment with several nearest-neighbor methods for approximating value\rfunctions, poli\u0026shy;cies, and environment models. Tadepalli and Ok (1996) obtained\rpromising results with locally-weighted linear regression to learn a value\rfunction for a simulated automatic guided vehicle task. Bottou and Vapnik\r(1996) demon\u0026shy;strated surprising efficiency of several local learning algorithms\rcompared to non-local algorithms in some pattern recognition tasks, discussing\rthe impact of local learning on generalization.\nBentley (1975) introduced k-d trees and reported observing average\rrunning time of O(log n) for nearest neighbor search over nrecords. Friedman, Bent\u0026shy;ley, and Finkel (1977)\rclarified the algorithm for nearest neighbor search with k-d trees. Omohundro\r(1987) discussed efficiency gains possible with hierar\u0026shy;chical data structures\rsuch as k-d-trees. Moore, Schneider, and Deng (1997) introduced the use of k-d\rtrees for efficient locally weighted regression.\n9.9\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; The origin of kernel regression is the method\rof potential functionsof Aiz\u0026shy;erman,\rBraverman, and Rozonoer (1964). They likened the data to point electric charges\rof various signs and magnitudes distributed over space. The resulting electric\rpotential over space produced by summing the potentials of the point charges\rcorresponded to the interpolated surface. In this analogy, the kernel function\ris the potential of a point charge, which falls off as the reciprocal of the\rdistance from the charge. Connell and Utgoff (1987) applied an actor-critic\rmethod to the pole-balancing task in which the critic approxi\u0026shy;mated the value\rfunction using kernel regression with inverse distance weight\u0026shy;ing given by\r��Shepard��s function.�� Predating widespread interest in kernel regression in\rmachine learning, these authors did not use the term kernel, but referred to\r��Shepard��s method�� (Shepard, 1968). Their system could learn to balance the\rpole in about 16 episodes. Other kernel-based approaches to reinforcement\rlearning include those of Ormoneit and Sen (2002), Dietterich and Wang (2002),\rXu, Xie, Hu, Nu, and Lu (2005), Taylor and Parr (2009), Barreto, Precup, and\rPineau (2011), and Bhat, Farias, and Moallemi (2012).\nThe earliest example we know of\rin which function approximation methods were used for learning value functions\rwas Samuel��s checkers player (1959, 1967). Samuel followed Shannon��s (1950)\rsuggestion that a value function did not have to be exact to be a useful guide\rto selecting moves in a game and that it might be approximated by linear\rcombination of features. In addition to linear function approximation, Samuel\rexperimented with lookup tables and hierarchical lookup tables called signa\u0026shy;ture\rtables (Griffith, 1966, 1974; Page, 1977; Biermann, Fairfield, and Beres,\r1982).\nAt about the same time as\rSamuel��s work, Bellman and Dreyfus (1959) proposed using function approximation\rmethods with DP. (It is tempting to think that Bell\u0026shy;man and Samuel had some\rinfluence on one another, but we know of no reference to the other in the work\rof either.) There is now a fairly extensive literature on function\rapproximation methods and DP, such as multigrid methods and methods using\rsplines and orthogonal polynomials (e.g., Bellman and Dreyfus, 1959; Bellman,\rKalaba, and Kotkin, 1973; Daniel, 1976; Whitt, 1978; Reetz, 1977; Schweitzer\rand Seidmann, 1985; Chow and Tsitsiklis, 1991; Kushner and Dupuis, 1992; Rust,\r1996).\nHolland��s (1986) classifier\rsystem used a selective feature-match technique to gen\u0026shy;eralize evaluation\rinformation across state-action pairs. Each classifier matched a subset of\rstates having specified values for a subset of features, with the remaining\rfeatures having arbitrary values (��wild cards��). These subsets were then used\rin a conventional state-aggregation approach to function approximation.\rHolland��s idea was to use a genetic algorithm to evolve a set of classifiers\rthat collectively would im\u0026shy;plement a useful action-value function. Holland��s\rideas influenced the early research of the authors on reinforcement learning,\rbut we focused on different approaches to function approximation. As function\rapproximators, classifiers are limited in several ways. First, they are\rstate-aggregation methods, with concomitant limitations in scaling and in representing\rsmooth functions efficiently. In addition, the matching rules of classifiers\rcan implement only aggregation boundaries that are parallel to the feature\raxes. Perhaps the most important limitation of conventional classifier systems\ris that the classifiers are learned via the genetic algorithm, an evolutionary\rmethod. As we discussed in Chapter 1, there is available during learning much more\rdetailed information about how to learn than can be used by evolutionary\rmethods. This perspective led us to instead adapt supervised learning methods\rfor use in rein\u0026shy;forcement learning, specifically gradient-descent and neural\rnetwork methods. These differences between Holland's approach and ours are not\rsurprising because Holland's ideas were developed during a period when neural\rnetworks were generally regarded as being too weak in computational power to be\ruseful, whereas our work was at the beginning of the period that saw widespread\rquestioning of that conventional wisdom. There remain many opportunities for combining\raspects of these different approaches.\nChristensen and Korf (1986) experimented with\rregression methods for modifying coefficients of linear value function\rapproximations in the game of chess. Chapman and Kaelbling (1991) and Tan\r(1991) adapted decision-tree methods for learning value functions. Explanation-based\rlearning methods have also been adapted for learning value functions, yielding\rcompact representations (Yee, Saxena, Utgoff, and Barto, 1990; Dietterich and\rFlann, 1995).\n\r\rChapter 10\nOn-policy Control with\rApproximation\nIn this chapter we turn to the control problem with\rparametric approximation of the action-value function q(s, a, w) ^ ����s,a), where w G Rdis a finite-dimensional\rweight vector. We continue to restrict attention to the on-policy case, leaving\roff- policy methods to Chapter 11. The present chapter features the\rsemi-gradient Sarsa algorithm, the natural extension of semi-gradient TD(0)\r(last chapter) to action values and to on-policy control. In the episodic case,\rthe extension is straightforward, but in the continuing case we have to take a\rfew steps backward and re-examine how we have used discounting to define an\roptimal policy. Surprisingly, once we have genuine function approximation we\rhave to give up discounting and switch to a new ��average-reward�� formulation of\rthe control problem with new value functions.\nStarting first in the episodic case, we extend the\rfunction approximation ideas presented in the last chapter from state values to\raction values. Then we extend them to control following the general pattern of\ron-policy GPI, using e-greedy for action selection. We show results for n-step\rlinear Sarsa on the Mountain Car problem. Then we turn to the continuing case\rand repeat the development of these ideas for the average-reward case with\rdifferential values.\n10.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rEpisodic Semi-gradient Control\nThe extension of the\rsemi-gradient prediction methods of Chapter 9 to action values is\rstraightforward. In this case it is the approximate action-value function, q^\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ,\u0026nbsp; that\nis represented as a parameterized functional form with weight vector\rw. Whereas before we considered random training\rexamples of the form St ^ Ut, now we consider examples of the form St, At ^ Ut. The target Ut can be any\rapproximation of qn (St, At), including the usual backed-up values such as the\rfull Monte Carlo return, Gt, or any of the n-step Sarsa returns (7.4). The\rgeneral gradient-descent update for action-value prediction is\nwt+i = wt + aUt - q(St, At, wt) Vq(St, At, wt).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (10.1)\nFor example, the update for the one-step Sarsa method is\nwt+i =wt + a\rRt+i + 7q(St+i,\rAt+i, wt) - q(St, At, wt) Vq(St, At, wt). (10.2)\nWe call this method episodic\rsemi-gradient one-step Sarsa.For a constant policy, this method converges in the same way that TD(0) does,\rwith the same kind of error bound (9.13).\nTo form control methods, we need to couple such\raction-value prediction methods with techniques for policy improvement and\raction selection. Suitable techniques applicable to continuous actions, or to\ractions from large discrete sets, are a topic of ongoing research with as yet\rno clear resolution. On the other hand, if the action set is discrete and not\rtoo large, then we can use the techniques already developed in pre\u0026shy;vious\rchapters. That is, for each possible action aavailable in the current state St, we can compute\rq(St, a, wt) and then find the\rgreedy action Ajf = argmaxa q(St, a, wt). Policy improvement is then done (in the on-policy\rcase treated in this chapter) by changing the estimation policy to a soft\rapproximation of the greedy policy such as the e-greedy policy. Actions are\rselected according to this same policy. Pseudocode for the complete algorithm\ris given in the box.\nEpisodic\rSemi-gradient Sarsa for Estimating q^\nInput: a differentiable function q : S x Ax Rd R\nInitialize\rvalue-function weights w G Rd\rarbitrarily (e.g., w = 0) Repeat (for each episode):\nS, A �� initial\rstate and action of episode (e.g., e-greedy) Repeat (for each step of episode):\nTake action A,\robserve R, SfIf S1is terminal:\nw �� w + a [R\r- q(S, A, w)] Vq(S, A, w)\nGo to next episode Choose A1as a function of q(S;, ��, w) (e.g., e-greedy) w �� w + a[R + Yq(S;, A1, w) - q(S, A, w)]\rVq(S, A, w)\nS �� S1\nA\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; A1\nExample 10.1: Mountain\rCar Task Consider the task of\rdriving an underpow\u0026shy;ered car up a steep mountain road, as suggested by the\rdiagram in the upper left of Figure 10.1. The difficulty is that gravity is\rstronger than the car's engine, and even at full throttle the car cannot\raccelerate up the steep slope. The only solution is to first move away from the\rgoal and up the opposite slope on the left. Then, by\n\r\n\r\r\r\r\r\rFigure 10.1:\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; The\u0026nbsp; Mountain\u0026nbsp; Car\u0026nbsp; task(upper\u0026nbsp; leftpanel)\u0026nbsp; and\u0026nbsp; thecost-to-gofunction\n(-maxa q(s, a, w))\rlearned during one run.\n\r\r\r\r\r\u0026nbsp;\n\r\r\r104\n\r\r\r\r\rapplying full throttle the car can build up enough\rinertia to carry it up the steep slope even though it is slowing down the whole\rway. This is a simple example of a continuous control task where things have to\rget worse in a sense (farther from the goal) before they can get better. Many\rcontrol methodologies have great difficulties with tasks of this kind unless\rexplicitly aided by a human designer.\nThe reward in this problem is -1 on all time steps until the car\rmoves past its goal position at the top of the mountain, which ends the\repisode. There are three possible actions: full throttle forward (+1), full throttle reverse (-1), and zero throttle (0). The car moves according to a simplified physics. Its position,\rxt, and velocity, xt, are updated by\nxt+i == bound [xt + xt+i]\nXt+i == bound [xt + 0.001 ��-0.0025 cos(3xt)],\nwhere the bound operation enforces ��1.2 \u0026lt; x^+i \u0026lt; 0.5 and ��0.07\r\u0026lt; x^+i \u0026lt; 0.07. In addition, when xt+i reached the left bound,\rxt+i was reset to zero. When it reached the right bound, the goal\rwas reached and the episode was terminated. Each episode started from a random\rposition xt G [-0.6, -0.4) and zero velocity. To convert the two continuous\rstate variables to binary features, we used grid-tilings as in Figure 9.9. We\rused 8tilings, with each tile covering 1/8th of the bounded distance in\reach dimension, and asymmetrical offsets as described in Section 9.5.4.i\naIn particular,\rwe used the tile-coding software, available on the web, version 3 (Python),\rwith\nThe feature vectors x(s, a) created by tile coding were then combined linearly with the\rparameter vector to approximate the action-value function:\nq(s, a, w) == wTx(s, a) = ^^ w^ \u0026#8226;��(s, a),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (10.3)\ni\nfor each pair of state, s, and action, a.\nFigure 10.1shows what typically happens while learning to\rsolve this task with this form of function approximation.[17]Shown is the negative of the value function (the cost-to-gofunction) learned on a single run. The initial\raction values were all zero, which was optimistic (all true values are negative\rin this task), causing extensive exploration to occur even though the\rexploration parameter, e, was 0. This can be seen in the middle-top panel of\rthe figure, labeled ��Step 428��. At this time not even one episode had been\rcompleted, but the car has oscillated back and forth in the valley, following\rcircular trajectories in state space. All the states visited frequently are\rvalued worse than unexplored states, because the actual rewards have been worse\rthan what was (unrealistically) expected. This continually drives the agent\raway from wherever it has been, to explore new states, until a solution is\rfound.\nFigure 10.2shows several learning curves for semi-gradient\rSarsa on this problem, with various step sizes.\n\r\n\r\r\r\r\r\rMountain\rCar\nSteps per episode log scale\naveraged over 100 runs\n\r\r\r\r\r\rFigure 10.2: Mountain Car learning\rcurves for the semi-gradient Sarsa method with tile- coding function approximation\rand e-greedy action selection.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n\r\r\r\r\r\u0026nbsp;\nExercise 10.1 Why have we not considered Monte Carlo methods in this chapter?\n��\n\r\r10.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rn-step Semi-gradient Sarsa\nWe can obtain an n-step version of episodic semi-gradient Sarsa by\rusing an n- step return as the update target in the semi-gradient Sarsa update equation\r(10.1). The n-step return immediately generalizes from its tabular form (7.4)\rto a function approximation form:\nGt:t+n = Rt+i+7Rt+2+ ' ' '+7^ 1��t+n+'TKA+n, At+n,\rwt+n-l), n ^ ��,0^ t \u0026lt; T-n,\n(10.4)\nwith Gt��t+n == Gt if t + n \u0026gt; T, as usual. The n-step update equation is\nwt+n == wt+n-l + a [Gt��t+n _ q(St, At, wt+n-l)] Vq(St, At, wt+n-l),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0\u0026lt; t \u0026lt; T.\n(10.5)\nComplete pseudocode is given in the box below.\nEpisodic semi-gradient n-step Sarsa for estimating q ^ q^, or q ^ q^\nInput: a differentiable\rfunction q : S x A x Rd R, possibly n Initialize value-function\rweight vector w arbitrarily (e.g., w = 0)\nParameters: step size a \u0026gt; 0, small\re \u0026gt; 0, a positive integer n\nAll store and access operations (St, At, and Rt) can take their\rindex mod n\nRepeat (for each episode):\nInitialize and store So = terminal\nSelect and store an action Ao \u0026#12316;n(-|So) or e-greedy wrt q(So, ��, w)\nT �� o\nFor t = 0, 1, 2, . . . :\n| If t \u0026lt; T, then:\n|\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Take\raction At\n|Observe and store the next reward as Rt+i and\rthe next state as St+i\n|\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; If\rSt+i is terminal, then:\n|\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; T\r�� t + 1\n|\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; else:\n|\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Select\rand store At+i \u0026#12316;n(-|St+i) or e-greedy\rwrt q(St+i, ��, w)\n| t �� t - n + 1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (t is the time whose estimate is being updated)\n|\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; If\rt\r\u0026gt; 0:\ni\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; g �� srn^i+o y i-T-iRi\n|\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; If\rt\r+ n \u0026lt; T, then G �� G + 7n(?(Sr+n, A+n, w)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (Gr��r+n\n|\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; w �� w + a [G\r- q(Sr, Ar, w)] Vq(Sr, Ar, w)\nUntil t = T �� 1\nAs we have seen before, performance is best if an\rintermediate level of bootstrap\u0026shy;ping is used, corresponding to an n larger than\r1. Figure 10.3 shows how this\n\r\n\r\r\r\r\r\rMountain\rCar\nSteps per episode log scale\n\r\r\r\r\r\rEpisode\n\r\r\r\r\r\rFigure 10.3: One-step vs multi-step performance of n-step\rsemi-gradient Sarsa on the Moun\u0026shy;tain Car task. Good step sizes were used: a= 0.5/8 for n = 1and a= 0.3/8 for n = 8.\n\r\r\r\r\r\r\r\rMountain Car\nSteps per episode\naveraged over first 50 episodes and 100 runs\n\r\r\r\r\r\r\r\rFigure 10.4: Effect of the a and\rn on early performance of n-step semi-gradient Sarsa and tile-coding\rfunction approximation on the Mountain Car task. As usual, an intermediate\rlevel of bootstrapping (n = 4) performed best. These results are for\rselected a values, on a log scale, and then connected by straight lines.\rThe standard errors ranged from 0.5 (less than the line width) for n = 1 to\rabout 4 for n = 16, so the main effects are all statistically significant.\n\r\r\r\r\r\r\r\r^ x number of tilings (8)\n\r\r\r\r\r\r\r\ralgorithm tends to learn faster\rand obtain a better asymptotic performance at n = 8 than at n = 1 on the Mountain\rCar task. Figure 10.4 shows the results of a more detailed study of the\reffect of the parameters a and n on the rate of learning on this task.\nExercise 10.2Give pseudocode for\rsemi-gradient one-step ExpectedSarsa for con\u0026shy;trol.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 10.3 Why do the results shown in Figure 10.4 have higher standard errors at\rlarge n than at low n?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n\r\r\r\r\r\n\r\u0026nbsp;\n10.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rAverage Reward: A New Problem\rSetting for Con\u0026shy;tinuing Tasks\nWe now introduce a third classical settingһalongside the episodic and discounted settings��for formulating the\rgoal in Markov decision problems (MDPs). Like the discounted setting, the average reward setting applies to continuing problems, prob\u0026shy;lems\rfor which the interaction between agent and environment goes on and on for\u0026shy;ever\rwithout termination or start states. Unlike that setting, however, there is no\rdiscounting��the agent cares just as much about delayed rewards as it does about\rimmediate reward. The average-reward setting is one of the major settings con\u0026shy;sidered\rin the classical theory of dynamic programming and, though less often, in\rreinforcement learning. As we discuss in the next section, the discounted\rsetting is problematic with function approximation, and thus the average-reward\rsetting is needed to replace it.\nIn the average-reward setting, the quality of a policy n is defined\ras the average rate of reward while following that policy, which we denote as\rr(n):\n1T\nr(n)== lim t [ E[Rt 1Ao��t-i \u0026#12316;n]\n��400 �A\u0026nbsp; ^\nt=l\n=lim E[Rt |\rAo��t-i \u0026#12316;n] ,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (10.6)\nt^^\n=E \u0026#12316;(s)E n(a|s)E p(s;,\rr|s, a)r,\ns\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; a\nwhere the expectations are conditioned on the\rprior actions, Ao, Ai,..., At-i, being taken according to n, and is\rthe steady-state distribution, (s) == limt^^ Pr{St = which is assumed to exist\rand to be independent of So. This property is known as ergodicity.\rIt means that where the MDP starts or any early decision made by the agent can\rhave only a temporary effect; in the long run your expectation of being in a\rstate depends only on the policy and the MDP transition probabilities.\rErgodicity is sufficient to guarantee the existence of the limits in the\requations above.\nThere are subtle distinctions that can be drawn\rbetween different kinds of optimal\u0026shy;ity in the undiscounted continuing case.\rNevertheless, for most practical purposes it may be adequate simply to order\rpolicies according to their average reward per time step, in other words,\raccording to their r(n). This quantity is essentially the average reward under\rn, as suggested by (10.6). In particular, we consider all policies that attain\rthe maximal value of r(n) to be optimal.\nNote that the steady state distribution is the special distribution\runder which, if you select actions according to n, you remain in the same\rdistribution. That is, for which\n(s)[ n(a|s, w)p(s'|s,a)=��(s').\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (10.7)\nsa\nIn the average-reward setting, returns are defined in terms of\rdifferences betweenrewards and the average reward:\n\r\r\r(10.8)\n\r\r\r\r\rGt = Rt+i -r(n) + Rt+2-r(n) + Rt+3-r(n) +\nThis is known as the differential return, and the corresponding value functions\rare known as differential value functions. They are\rdefined in the same way and we will use the same notation for them as we have\rall along:\u0026nbsp; (s) == En[Gt|St = s] and\nqn(s, a) == En[Gt|St = s, At = a] (similarly for v* and ��).Differential value functions also have Bellman equations, just\rslightly different from those we have seen earlier. We simply remove all 7s and replace\rall rewards by the difference between the reward and the true average reward:\nV\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rn(s) = ^2n(a|s^p(s', r|s, a) r- r(n) + v^(s')\na\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; r��s.\n\r\r\u0026nbsp;SHAPE \u0026nbsp;\\* MERGEFORMAT \r\r\r\u0026nbsp;\n\r\r\r\r\r\r\u0026nbsp;\n\r\r\r\r\rr - r(n) + [\rn(a'|s')q^(s', a')\n\r\r\r\r\r\r\r\r, and\n\r\r\r\r\r\nqn (s,a) = ��p(s',r|s,a) r��s7v^(s) = ma^ 7p(s', r|s, a) r��s7q*(s,a) = ��p(s',r|s,a) r��s7(cf. Eqs. 3.14, 4.1, and 4.2).\na7\nr - r(n) + v*(s')\n-r(n) + max q*(s', a')\na7\n\r\rThere is also a differential form of the two TD\rerrors:\n\r\r\r(10.9)\n(10.10)\n\r\r\r\r\r^t == Rt+i-Rt+i + v(St+i,wt) - v(St,wt), and ^t = Rt+i-Rt+i\r+ 3(St+i, At+i, wt) - 3(St,\rAt, wt).\nwhere Rt is an estimate at time t of the average reward r(n). With these alternate\rdefinitions, most of our algorithms and many theoretical results carry through\rto the average-reward setting.\nFor example, the average reward version of semi-gradient Sarsa is\rdefined just as in (10.2) except with the differential version of the TD error.\rThat is, by\n\r\r\r(10.11)\n\r\r\r\r\rwt+i == wt + a^tVq(St, At, wt),\nwith ^t given by (10.10). The pseudocode for the\rcomplete algorithm is given on the next page.\nDifferential semi-gradient Sarsa for estimating q^\nInput: a\rdifferentiable function q : S x Ax Rd\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; R\nParameters: step sizes a, ^ \u0026gt;\r0\nInitialize value-function weights\rw G Rd arbitrarily (e.g., w = 0) Initialize average reward estimate Rarbitrarily (e.g., R = 0) Initialize state S, and action A\nRepeat (for each step):\nTake action A,\robserve R, Sح\nChoose A;as a function of q(S;, ��, w) (e.g., e-greedy)\n5R - R + q(S;, A;, w) - q(S, A, w)\nR �� R + ^5\nw �� w + a5Vq(S, A, w)\nS �� Sح ^Af\nExample 10.2: An\rAccess-Control Queuing Task This\ris a decision task involving access control to a set of kservers. Customers of four different priorities\rarrive at a single queue. If given access to a server, the customers pay a\rreward of 1, 2, 4, or 8to the server, depending on their priority, with higher priority\rcustomers paying more. In each time step, the customer at the head of the queue\ris either accepted (assigned to one of the servers) or rejected (removed from\rthe queue, with a reward of zero). In either case, on the next time step the\rnext customer in the queue is considered. The queue never empties, and the\rpriorities of the customers in the queue are equally randomly distributed. Of\rcourse a customer can not be served if there is no free server; the customer is\ralways rejected in this case. Each busy server becomes free with probability pon each time step. Although we have just described\rthem for definiteness, let us assume the statistics of arrivals and departures\rare unknown. The task is to decide on each step whether to accept or reject the\rnext customer, on the basis of his priority and the number of free servers, so\ras to maximize long-term reward without discounting.\nIn this example we consider a tabular solution to\rthis problem. Although there is no generalization between states, we can still\rconsider it in the general function approximation setting as this setting\rgeneralizes the tabular setting. Thus we have a differential action-value\restimate for each pair of state (number of free servers and priority of the\rcustomer at the head of the queue) and action (accept or reject). Figure 10.5\rshows the solution found by differential semi-gradient Sarsa for this task with\rk = 10 and p = 0.06. The algorithm parameters were a = 0.01,¬=0.01, and e\r= 0.1. The initial action values and R were zero.\n10.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\r\r\r\rPolicy\n\r\r\r\r\r\r\r\rPriority\n\r\r\r\r\r\r\r\rRFJFCT\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r1\n\r1 1 1\n\rACCEPT\n11 1 1 1 1 1\n\r\r\r\r1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 2345678910\nNumber of free servers\n\r\r\r\r\r\r\r\rFigure 10.5: The\rpolicy and value function found by differential semi-gradient one-step\rSarsa on the access-control queuing task after 2million steps. The drop on the\rright of the graph is probably due to insufficient data; many of these\rstates were never experienced. The value learned for Rwas about 2.31.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n\r\r\r\r\r\r\r\rValue\nFunction\n\r\r\r\r\r\r\r\rNumber\rof free servers\n\r\r\r\r\rDeprecating the Discounted\rSetting\nThe continuing, discounted problem formulation has\rbeen very useful in the tabular case, in which the returns from each state can\rbe separately identified and averaged. But in the approximate case it is\rquestionable whether one should ever use this problem formulation.\nTo see why, consider an infinite sequence of returns\rwith no beginning or end, and no clearly identified states. The states might be\rrepresented only by feature vectors, which may do little to distinguish the\rstates from each other. As a special case, all of the feature vectors may be\rthe same. Thus one really has only the reward sequence (and the actions), and\rperformance has to be assessed purely from these. How could it be done? One way\ris by averaging the rewards over a long interval��this is the idea of the\raverage-reward setting. How could discounting be used? Well, for each time step\rwe could measure the discounted return. Some returns would be small and some\rbig, so again we would have to average them over a sufficiently large time\rinterval. In the continuing setting there are no starts and ends, and no\rspecial time steps, so there is nothing else that could be done. However, if\ryou do this, it turns out that the average of the discounted returns is\rproportional to the average reward. In fact, for policy n, the average of the\rdiscounted returns is always r(n)/(1 - 7), \n\r\rthat is, it is essentially the average reward, r(n).\rIn particular, the orderingof all policies in the average discounted return\rsetting would be exactly the same as in the average-reward setting. The\rdiscount rate 7thus has no effect on the problem formulation. It could in fact be zeroand the ranking would be unchanged.\nThis surprising fact is proven in the box, but the\rbasic idea can be seen via a symmetry argument. Each time step is exactly the\rsame as every other. With discounting, every reward will appear exactly once in\reach position in some return. The tth reward will appear undiscounted in the t\r- 1st return, discounted once in the t - 2nd return, and discounted 999 times\rin the t - 1000th return. The weight on the tth reward is thus 1 + 7+ y2+ y3+ ������ = 1/(1 - 7). Since all states are the same, they are all\rweighted by this, and thus the average of the returns will be this times the\raverage reward, or r(n)/(1- 7).\nSo in this key case, what the discounted case was\rinvented for, discounting is not applicable. The discounted case is still\rpertinent, or at least possible, for the episodic case.\nThe Futility of Discounting in Continuing Problems\nPerhaps discounting can be saved\rby choosing an objective that sums dis\u0026shy;counted values over the distribution\rwith which states occur under the policy:\nJ(n)= I]\r(s)vj(s)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (wherev? is the discounted value function)\ns\n=E ��(s)E أ(a|s) E y^p(s', r|s, a) [r + 7#(s')] (Bellman Eq.)\n\u0026nbsp;TOC \\o \u0026quot;1-5\u0026quot; \\h \\z s\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; a\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; s7r\n=r(n) + [��(s)Ylأ(a|s) [ [ W, r|s, a)7vY(s') (from (10.6))\ns\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; a\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; s7r\n=r(n)\r+ 7Evy (s')E\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (s)\rn(a|s)p(s |s, a)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (from\r(3.10))\ns7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; sa\n=r(n) + 7[ vY(s')^n(s')\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (from\r(10.7))\ns7\n=r(n) + 7J (n)\n=r(n) + 7r(n) + 72J (n)\n=r(n) + 7r(n) + 72r(n) + 73r(n) +---------- \n\r\r\rr(n).\n\r\r\r\r\r=1\n1\u0026nbsp;\u0026nbsp;\r-7\nThe proposed\rdiscounted objective orders policies identically to the undis\u0026shy;counted (average\rreward) objective. We have failed to save discounting!\n10.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rn-step Differential\rSemi-gradient Sarsa\nIn order to generalize to n-step\rbootstrapping, we need an n-step version of the TD error. We begin by generalizing\rthe n-step return (7.4) to its differential form, with function approximation:\nGt:t+n = Rt+i --Rt+i\r+ Rt+2-��t+2+ �� �� �� + Rt+n-��t+n + ^(St+n, At+n, wt+n-i),\n(10.12)\nwhere R is an estimate of r(n), n \u0026gt; 1, and t + n \u0026lt; T. If t + n\r\u0026gt; T, then we define Gt��t+n == Gt as\rusual. The n-step TD error is then\n\r\r\r(10.13)\n\r\r\r\r\r5t\r= Gt:t+n - 3(St, At, w).\nafter which we can apply our usual semi-gradient Sarsa update\r(10.11). Pseudocode for the complete algorithm is given in the box.\nDifferential semi-gradient n-step Sarsa for estimating q ^ q^, or q ^ q^\nInput: a\rdifferentiable function q : S x A x Rm\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; R,\ra policy n\nInitialize value-function weights w G Rm\rarbitrarily (e.g., w = 0)\nInitialize\raverage-reward estimate R G R arbitrarily (e.g., R = 0) Parameters: step size\ra, ^ \u0026gt; 0, a positive integer n\nAll store and access operations (St, At, and Rt) can take their\rindex mod n\nInitialize\rand store So and Ao For t = 0,1, 2,...:\nTake action At\nObserve and store the next\rreward as Rt+i and the next state as St+i Select and store an action At+i \u0026#12316;n(-|St+i), or e-greedy wrt q(So, ��, w) t �� t - n + 1\u0026nbsp;\u0026nbsp; (t is the time whose estimate is being updated)\nIf t \u0026gt; 0:\n5��^T=+J-Vi(Ri -\rR) + q(ST +n, At+n, w) - q(Sr, At, w)\nR �� R + ^5\nw �� w +\ra5Vq(Sr, Ar, w)\n10.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rSummary\nIn this chapter we have extended the ideas of\rparameterized function approximation and semi-gradient descent, introduced in\rthe previous chapter, to control. The ex\u0026shy;tension is immediate for the episodic\rcase, but for the continuing case we have to introduce a whole new problem\rformulation based on maximizing the average reward per time step. Surprisingly, the discounted\rformulation cannot be carried over to control in the presence of\rapproximations. In the approximate case most policies cannot be represented by\ra value function. The arbitrary policies that remain need to be ranked, and the\rscalar average reward r(n) provides an effective way to do this.\nThe average reward formulation involves new differentialversions of value func\u0026shy;tions, Bellman equations,\rand TD errors, but all of these parallel the old ones, and the conceptual\rchanges are small. There is also a new parallel set of differential algorithms\rfor the average-reward case. We illustrate this by developing differential\rversions of semi-gradient n-step Sarsa.\nBibliographical and Historical Remarks\n10.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Semi-gradient Sarsa with function approximation was\rfirst explored by Rum- mery and Niranjan (1994). Linear semi-gradient Sarsa\rwith e-greedy action selection does not converge in the usual sense, but does\renter a bounded region near the best solution (Gordon, 1995). Precup and\rPerkins (2003) showed convergence in a differentiable action selection setting.\rSee also Perkins and Pendrith (2002) and Melo, Meyn, and Ribiero (2008). The\rmountain-car example is based on a similar task studied by Moore (1990), but\rthe exact form used here is from Sutton (1996).\n10.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Episodic n-step semi-gradient Sarsa is based on the\rforward Sarsa(��)algo\u0026shy;rithm\rof van Seijen (2016). The empirical results shown here are new to the second\redition of this text.\n10.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; The average-reward formulation has been described\rfor dynamic program\u0026shy;ming (e.g., Puterman, 1994) and from the point of view of\rreinforcement learning (Mahadevan, 1996; Tadepalli and Ok, 1994; Bertsekas and\rTsitiklis, 1996; Tsitsiklis and Van Roy, 1999). The algorithm described here is\rthe on- policy analog of the ��R-learning�� algorithm introduced by Schwartz\r(1993). The name R-learning was probably meant to be the alphabetic successor\rto Q-learning, but we prefer to think of it as a reference to the learning of\rdiffer\u0026shy;ential or relativevalues. The access-control queuing example was\rsuggested by the work of Carlstrom and Nordstrom (1997).\n10.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; The recognition of the limitations of discounting as\ra formulation of the rein\u0026shy;forcement learning problem with function\rapproximation became apparent to the authors shortly after the publication of\rthe first edition of this text. The second edition of this book may be the\rfirst publication of the demonstration of the futility of discounting in the\rbox on page 265.\n10.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe\rdifferential version of n-step semi-gradient Sarsa is new to this text and has\rnot been significantly studied.\n\r\rChapter 11\nOff-policy Methods with\rApproximation\nThis book has treated on-policy and off-policy\rlearning methods since Chapter 5 primarily as two alternative ways of handling\rthe conflict between exploitation and exploration inherent in learning forms of\rgeneralized policy iteration. The two chap\u0026shy;ters preceding this have treated the\ron-policy\rcase with function approximation, and in this chapter we treat the off-policy case with function approximation. The exten\u0026shy;sion\rto function approximation turns out to be significantly different and harder\rfor off-policy learning than it is for on-policy learning. The tabular\roff-policy methods developed in Chapters 6and 7 readily extend to semi-gradient algorithms,\rbut these algorithms do not converge nearly as robustly as they do under\ron-policy training. In this chapter we explore the convergence problems, take a\rcloser look at the theory of linear function approximation, introduce a notion\rof learnability, and then discuss new algorithms with stronger convergence\rguarantees for the off-policy case.\nRecall that in off-policy\rlearning we seek to learn a value function for a target\rpolicyn, given data due to a\rdifferent behavior policyb. In the prediction case, both policies are static and given, and\rwe seek to learn either state values v ��or action values q ��q^. In\rthe control case, action values are learned, and both policies typically change\rduring learning��n being the greedy policy with respect to q, and b being\rsomething more exploratory such as the e-greedy policy with respect to q.\nThe challenge of off-policy\rlearning can be divided into two parts, one that arises in the tabular case and\rone that arises only with function approximation. The first part of the\rchallenge has to do with the target of the learning update, and the second part\rhas to do with the distribution of the updates. The techniques related to\rimportance sampling and rejection sampling developed in Chapters 6and 7 deal with the first part; these may increase\rvariance but are needed in all successful algorithms, tabular and approximate.\rThe extension of these techniques to function approximation are quickly dealt\rwith in the first section of this chapter.\nSomething more is needed for the second part of the\rchallenge of off-policy learning with function approximation because the\rdistribution of updates in the off-policycase is\rnot according to the on-policy distribution. The on-policy distribution is\rimportant to the stability of semi-gradient methods. Two general approaches\rhave been explored to deal with this. One is to use importance sampling methods\ragain, this time to warp the update distribution back to the on-policy\rdistribution, so that semi-gradient methods are guaranteed to converge (in the\rlinear case). The other is to develop true gradient methods that do not rely on\rany special distribution for stability. We present methods based on both\rapproaches. This is a cutting- edge research area, and it is not clear which of\rthese approaches is most effective in practice.\n11.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rSemi-gradient Methods\nWe begin by describing how the methods\rdeveloped in earlier chapters for the off- policy case extend readily to\rfunction approximation as semi-gradient methods. These methods address the\rfirst part of the challenge of off-policy learning but not the second part.\rAlthough these methods may diverge in some cases, and in that sense are not\rsound, they are still often successfully used. Remember that these methods are guaranteed stable and asymptotically unbiased for the\rtabular case, which corresponds to a special case of function approximation. So\rit may still be possible to combine them with feature selection methods in such\ra way that the combined system could be assured stable. In any event, these\rmethods are simple and thus a good place to start.\nIn Chapter 7 we described a\rvariety of tabular off-policy algorithms. To convert them to semi-gradient\rform, we simply replace the update to the array (V or Q) to an update to the\rweight vector (w), using the approximate value function (v or q)\n\r\r\rand its gradient. Many of ratio:\n����\rn(At|St)\nPt= Pt��t=\rb(At|St).\nFor example, the one-step,\n\r\r\r\r\rthese algorithms use the per-step importance\rsampling\n(11.1)\nstate-value algorithm is semi-gradient off-policy TD(0), which is\rjust like the corresponding on-policy algorithm (page 217) except for the\raddition of pt:\nwt+i = wt + apt^tVv(St ,wt),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (11.2)\nwhere 5t is defined appropriately depending on whether\rthe problem is episodic and discounted, or continuing and undiscounted using\raverage reward:\n^t = Rt+i + 7v(St+i,wt) - v(St,wt), or\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (episodic)\n^t = Rt+i - Rt\r+ v(St+i,wt) - -0(St,wt).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (continuing)\nFor action values, the one-step algorithm is\rsemi-gradient Expected Sarsa:\n\r\r\r(11.3)\n\r\r\r\r\rwt+i = wt + a^tVq(St, At, wt), with\n^t == Rt+i + ^^n(a|St+i)q(St+i,a, wt)\r- q(St, At, wt), or\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (episodic)\na\n^t = Rt+i - Rt + ^ n(a|St+i)q(St+i,a, wt) - q(St, At, wt).\r(continuing)\na\nNote that this algorithm does not use importance\rsampling. In the tabular case it is clear that this is appropriate because the\ronly sampled action is At, and in learning its value we do not have to consider\rany other actions. With function approximation it is less clear because we\rmight want to weight different state-action pairs differently once they all\rcontribute to the same overall approximation. Proper resolution of this issue\rawaits a more thorough understanding of the theory of function approximation in\rreinforcement learning.\nIn the multi-step generalizations of these algorithms, both the\rstate-value and action-value algorithms involve importance sampling. For\rexample, the n-step ver\u0026shy;sion of semi-gradient Expected Sarsa is\nwt+n == wt+n-i + apt+i \u0026#8226; \u0026#8226; \u0026#8226; pt+n-i [Gt��t+n - q(St, At, wt+n-i)] Vq(St, At, wt+n-i)\n(11.4)\nwith\nGt:t+n\r= Rt+i + \u0026#8226; \u0026#8226; \u0026#8226; + 7n iRt+n + 7nq(St+ n, At+n, wt+n-i), or (episodic)\nGt��t+n = Rt+i - Rt +----- +\rRt+n - Rt+n-i + q(St+n, At+n, wt+n-i), (continuing)\nwhere here we are being slightly informal in our\rtreatment of episodes�� end. In the first equation, the pts for t \u0026gt; T should\rbe taken to be 1, and Gt��nshould be taken to be Gt if t + n\r\u0026gt; T.\nRecall that we also presented in Chapter 7 on off-policy algorithm\rthat does not involve importance sampling at all: the n-step tree-backup\ralgorithm. Here is its semi-gradient version:\nwt+n = wt+n-i + a [Gt��t+n �� q(St, At, wt+n-i)] Vq(St, At, wt+n-i), with (11.5)\nt+n-i\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; k\nGt��t+n ^ q(St, At, wt-i) + L ^ n 7n(Ai|Si),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (11.6)\nk=t\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; i=t+i\nwith ^t as defined on the previous page for Expected Sarsa. We also\rdefined in Chapter 7 an algorithm that unifies all action-value algorithms:\rn-step Q(a). We leave the semi-gradient form of that algorithm, and also of the\rn-step state-value algorithm, as exercises to the reader.\nExercise 11.1 Convert the equation of\rn-step off-policy TD (7.7) to semi-gradient form. Give accompanying definitions\rof the return for both the episodic and contin\u0026shy;uing cases.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n^Exercise 11.2 Convert\rthe equations of n-step Q(a) (7.9, 7.14, 7.16, and 7.17) to semi-gradient form.\rGive definitions that cover both the episodic and continuing cases.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n11.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rExamples of Off-policy\rDivergence\nIn this section we begin to discuss\rthe second part of the challenge of off-policy learning with function\rapproximation��that the distribution of updates does not match the on-policy\rdistribution. We describe some instructive counterexamples to off-policy\rlearning��cases where semi-gradient and other simple algorithms are unstable and\rdiverge.\nTo establish intuitions, it is best to consider first a very simple\rexample. Suppose, perhaps as part of a larger MDP, there are two states whose\restimated values are of the functional form w and 2w, where the parameter vector w consists of only a single component w. This occurs under linear\rfunction approximation with single\u0026shy;component feature vectors for the two states\rof 1 and 2 respectively. In the first state, there is only one action available,\rand it results deterministically in a transition to the second state with a\rreward of 0:\nSuppose initially w = 10. The\rtransition will then be from a state of estimated value 10 to a state of\restimated value 20. It will look like a good transition, and w will be\rincreased to raise the first state��s estimated value. If 7is nearly 1, then the TD error will be nearly 10, and, if a = 0.1, then w will be increased to nearly 11in trying\rto reduce the TD error. However, the second state��s estimated value will also\rbe increased, to nearly 22. If the transition occurs again, then it will be\rfrom a state of estimated value d1 to a state of estimated value f^22, for a TD error of d1��larger, not smaller than before. It will\rlook even more like the first state is undervalued, and its value will be\rincreased again, this time to d2.1. This looks bad, and in fact with further\rupdates w will diverge to infinity. To see this definitively we have to look\rmore carefully at the sequence of updates. The TD error on a transition between\rthe two states is\n5t = Rt+i\r+ Yv(St+i,wt) - v(St,wt) = 0+ 72wt - wt = (27- 1)wt, and the off-policy\rsemi-gradient TD(0) update (from (11.2)) is\nwt+i\r= wt + apt5tVv(St,wt) = wt + a �� 1�� (27- 1)wt �� 1= (1+ a(2? - 1))wt.\nNote that the importance sampling\rratio, pt, is 1 on this transition because there is only one action available\rfrom the the first state, so its probabilities of being taken under the target\rand behavior policies must both be 1. In the final update above, the new\rparameter is the old parameter times a scalar constant, 1 + a(2Y - 1). If this constant is greater than 1, then the\rsystem is unstable and w will go to positive or negative infinity depending on\rits initial value. Here this constant is greater than 1 whenever 7\u0026gt; 0.5. Note that stability does not depend on the specific step\rsize, as long as a \u0026gt; 0. Smaller or larger step sizes would affect the rate\rat which w goes to infinity, but not whether it goes there or not.\nKey to this example is that\rthe one transition occurs repeatedly without w being updated on other\rtransitions. This is possible under off-policy training because\n\r\rthe behavior policy might select actions on those\rother transitions which the target policy never would. For these transitions,\rpt would be zero and no update would be made. Under on-policy training,\rhowever, pt is always one. Each time there is a transition from the wstate to the 2w state, increasing w, there would also have to be a\rtransition out of the 2w state. That transition would reduce w, unless is was\rto a state whose value was higher (because 7\u0026lt; 1) than 2w, and then that state would have to be followed by\ra state of even higher value, or else it would be reduced. Each state can\rsupport the one before only by creating a higher expectation. Eventually the\rpiper must be paid. In the on-policy case the promise of future reward must be\rkept and the system is kept in check. But in the off-policy case, a promise can\rbe made and then, after taking an action that the target policy never would,\rforgotten and forgiven.\nThis simple example communicates\rmuch of the reason why off-policy training can lead to divergence, but it is\rnot completely convincing because it is not complete��it is just a fragment of a\rcomplete MDP. Can there really be a complete system with instability? The\rsimplest complete example of divergence is Baird's counterexample. Consider the episodic seven-state, two-action MDP\rshown in Figure 11.1. The dashed action\rtakes the system to one of the six upper states with equal probability, whereas\rthe solid\raction takes the system to the\rseventh state. The behavior policy bselects the two actions with probabilities | and \u0026#8226;,\rso that the next-state distribution under it is uniform (the same for all\rnonterminal states), which is also the starting distribution for each episode.\rThe target policy n always takes the solid action, and so the on- policy\rdistribution is concentrated in the seventh state. The reward is zero on all\rtransitions. The discount rate is 7= 0.99.\n\r\r\rFigure 11.1: Baird��s counterexample. The approximate state-value\rfunction for this Markov process is of the form shown by the linear\rexpressions inside each state. The solid action usually results in the\rseventh state, and the dashed action usually results in one of the other\rsix states, each with equal probability. The reward is always zero.\n\r\r\r\r\rConsider estimating the state-value\runder the linear parameterization indicated by the expression shown in each\rstate circle. For example, the estimated value ofthe first state is 2wi + ws, where the subscript corresponds to the component of the overall\rweight vector w G R8; this corresponds to a feature vector for the first\rstate being x(1) =\r(2, 0, 0, 0, 0, 0, 0, 1)T. The reward is zero on all transitions, so\rthe true value function is v^ (s) = 0, for all s, which can be exactly approximated if w = 0. In fact, there are many solutions, as there are more components to\rthe weight vector (8) than there are nonterminal states (7). Moreover, the set of\rfeature vectors, {x(s) : s G S}, is a linearly\rindependent set. In all these ways this task seems a favorable case for linear\rfunction approximation.\nIf we apply semi-gradient TD(0) to this problem\r(11.2), then the weights diverge to infinity, as shown in Figure 11.2 (left).\rThe instability occurs for any positive step size, no matter how small. In\rfact, it even occurs if we do a DP-style expected backup instead of a learning\rbackup, as shown in Figure 11.2 (right). That is, if the weight vector, wk, is updated in sweeps\rthrough the state space, performing a synchronous, semi-gradient backup at\revery state, s, using the DP (full backup) target:\nwk+i == wk + ��(E[Rt+i + 7v(St+i,wk) | St = s] - v(s,wk)) Vv(s,wk). (11.7)\nIn this case, there is no randomness and no\rasynchrony. Each state is updated exactly once per sweep as in a classical DP\rbackup. The method is entirely conventional except in its use of semi-gradient\rfunction approximation. Yet still the system is unstable.\nIf we alter just the distribution of\rDP backups in Baird��s counterexample, from the uniform distribution to the\ron-policy distribution (which generally requires asyn-\n\r\n\r\r\r\r\r\u0026nbsp;\nFigure 11.2: Demonstration of instability on Baird��s\rcounterexample. Shown are the evolu\u0026shy;tion of the components of the parameter\rvector w of\rthe two semi-gradient algorithms. The step size was a = 0.01, and the initial weights were w = (1,1,1,1,1,1,10,1)T.\nchronous updating), then convergence is guaranteed\rto a solution with error bounded by (9.13). This example is striking because\rthe TD and DP methods used are ar\u0026shy;guably the simplest and best-understood\rbootstrapping methods, and the linear, semi-descent method used is arguably the\rsimplest and best-understood kind of function approximation. The example shows\rthat even the simplest combination of bootstrapping and function approximation\rcan be unstable if the backups are not done according to the on-policy\rdistribution.\nThere are also counterexamples\rsimilar to Baird��s showing divergence for Q-learning. This is cause for concern\rbecause otherwise Q-learning has the best convergence guarantees of all control\rmethods. Considerable effort has gone into trying to find a remedy to this\rproblem or to obtain some weaker, but still workable, guarantee. For example,\rit may be possible to guarantee convergence of Q-learning as long as the\rbehavior policy (the policy used to select actions) is sufficiently close to\rthe esti\u0026shy;mation policy (the policy used in GPI), for example, when it is the\re-greedy policy. To the best of our knowledge, Q-learning has never been found\rto diverge in this case, but there has been no theoretical analysis. In the\rrest of this section we present several other ideas that have been explored.\nSuppose that instead of taking\rjust a step toward the expected one-step return on each iteration, as in\rBaird��s counterexample, we actually change the value function all the way to\rthe best, least-squares approximation. Would this solve the instability\rproblem? Of course it would if the feature vectors, {x(s) : sG S}, formed a linearly independent set, as they do\rin Baird��s counterexample, because then exact approx\u0026shy;imation is possible on\reach iteration and the method reduces to standard tabular DP. But of course the\rpoint here is to consider the case when an exact solution is notpossible. In this case stability is not guaranteed\reven when forming the best approximation at each iteration, as shown by the\rcounterexample in the box on the next page.\nAnother way to try to prevent\rinstability is to use special methods for function approximation. In\rparticular, stability is guaranteed for function approximation methods that do\rnot extrapolate from the observed targets. These methods, called averagers, include nearest neighbor methods and local\rweighted regression, but not popular methods such as tile coding and\rbackpropagation.\n\r\rTsitsiklis and Van Roy��s Counterexample to DP policy evaluation with\rleast- squares linear function approximation\nThe simplest full counterexample to the least-squares idea is the\rw-to-2w ex\u0026shy;ample (from earlier in this section) extended with a terminal state:\n\r\n\r\r\r\r\r\r1�� e\n\r\r\r\r\r\r@\n\r\r\r\r\r\u0026nbsp;\nAs before, the estimated value of the first state is\rw, and the estimated value of the second state is 2w. The reward is zero on all\rtransitions, so the true values are zero at both states, which is exactly\rrepresentable with w = 0. If we set wk+i at each step so as to minimize the\rMSVE between the estimated value and the expected one-step return, then we have\n\r\r\r\r\r2\n\r\r\r\r\r\r\r\rE (v\nsGS\n\r\r\r\r\r\r\r\r(s,w)\r- En[Rt+i + 7v(St+i,wk)\rI St = s]\n\r\r\r\r\r\r\r\rarg min\nwGR\n\r\r\r\r\r\r\r\rwk+i\n\r\r\r\r\r\u0026nbsp;\n\rargmin (w - y2w^ + (2w - (1- e)Y2wk)2 wGR\n\r\r\r(11.8)\n\r\r\r\r\r6-4e\n-7wk.\n5\nThe sequence {wk} diverges when 7\u0026gt; and wo = 0.\n11.3The Deadly Triad\nOur discussion so far can be summarized by saying that the danger of\rinstability and divergence arises whenever we combine all of the following\rthree elements, making up what we call the deadly triad:\nFunction approximation A powerful, scalable way of generalizing from a\rstate space much larger than the memory and computational resources (e.g.,\rlinear function approximation or artificial neural networks).\nBootstrapping Update targets that include existing estimates\rrather than rely\u0026shy;ing exclusively on actual rewards and complete returns (e.g.,\ras in dynamic programming or TD learning).\nOff-policy training Training\ron a distribution of transitions other than that pro\u0026shy;duced by the target\rpolicy. Sweeping through the state space and updating all states uniformly, as\rin dynamic programming, does not respect the target policy and is an example of\roff-policy training.\n\r\rIn particular, note that the danger is notdue to control, or to generalized policy iteration.\rThose cases are more complex to analyze, but the instability arises in the\rsimpler prediction case whenever it includes all three elements of the deadly\rtriad. The danger is also notdue to learning or to the uncertainties involved in\restimation, as it occurs just as strongly in planning methods, such as dynamic\rprogramming, in which the environment is completely known.\nAny two elements of the deadly triad can be present\rwithout the third without creating instability. It is natural, then, to go\rthrough the three and see if there is any one that can be given up.\nOf the three, function approximationmost clearly can not be given up. We need methods\rthat scale to large problems and to great expressive power. We need at least\rlinear function approximation with many features and parameters. State\raggregation or non-parametric methods whose complexity grows with data are too\rweak or too expensive. Least-squares methods such as LSTD are of quadratic\rcomplexity and are too expensive.\nDoing without bootstrappingis possible, at the cost of computational and data\refficiency. Perhaps most important are the losses in computational efficiency.\rMonte Carlo (non-bootstrapping) methods require memory to save everything that\rhappens between making each prediction and obtaining the final return, and all\rtheir com\u0026shy;putation is done once the final return is obtained. The cost of these\rcomputational issues is not apparent on serial von Neumann computers, but would\rbe on specialized hardware. With bootstrapping and eligibility traces (Chapter\r12), data can be dealt with when and where it is generated, then need never be\rused again. The savings in communication and memory made possible by\rbootstrapping are great.\nThe losses in data efficiency by giving up bootstrappingare also significant. We have seen this repeatedly,\rsuch as in Chapters 7 (Figure 7.2) and 9 (Figure 9.2), where some degree of\rbootstrapping performed much better than Monte Carlo methods on the random-walk\rprediction task, and in Chapter 10 where the same was seen on the Mountain-Car\rcontrol task (Figure 10.4). Many other problems show much faster learning with\rbootstrapping. Figure 12.14 shows a few examples on other small problems.\rBootstrapping allows learning to take advantage of the state prop\u0026shy;erty, the\rability to recognize a state upon returning to it. On problems where the state\rrepresentation is poor and causes poor generalization, bootstrapping can impair\rlearning. One example of this seems to be Tetris (see f^imsek, Algorta, and\rKothiyal, 2016). A poor state representation can also result in bias; this in\rthe reason for the poorer bound on the asymptotic approximation quality of\rbootstrapping methods (Equation 9.13). On balance, a bootstrapping ability has\rto be considered extremely valuable. One may sometimes choose not to use it by\rselecting long backups (or a large bootstrapping parameter,����1; see Chapter 12) but\roftimes bootstrapping greatly increases efficiency. It's an ability that we\rwould very much like to keep in our toolkit.\nFinally, there is off-policy\rlearning; can we give it up?\rOn-policy methods are often adequate. For model-free reinforcement learning,\rone can simply use Sarsa rather than Q-learning. Off-policy methods free\rbehavior from the target policy.\nThis could be considered an appealing convenience\rbut not a necessity. However, off-policy learning is\ressential to other anticipated use cases, cases that we have not yet\rmentioned in this book but may be important to the larger goal of creating a\rpowerful intelligent agent.\nIn these use cases, the agent learns not just a single value\rfunction and single policy, but large numbers of them in parallel. There is\rextensive psychological evidence that people and animals learn to predict many\rdifferent sensory events, not just rewards. We can be surprised by unusual\revents, and correct our predictions about them, even if they are of neutral\rvalence. This kind of prediction presumably underlies predictive models of the\rworld such as are used in planning. We predict what we will see after eye\rmovements, how long it will take to walk home, the probability of making a jump\rshot in basketball, and the satisfaction we will get from taking on a new\rproject. In all these cases, the events we would like to predict depend on our\racting in a certain way. To learn them all, in parallel, requires learning from\rthe one stream of experience. There are many target policies, and thus the one\rbehavior policy can not equal all of them. Yet parallel learning is\rconceptually possible, because the behavior policy may overlap in part with\rmany of the target policies. To take full advantage of this requires off-policy\rlearning.\n11.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rLinear Value-function Geometry\nTo better understand the stability challenge of\roff-policy learning, it is helpful to think about value function approximation\rmore abstractly. We can imagine the space of all possible state-value\rfunctions, of all functions from states to real num\u0026shy;bers v : S R. Most of these\rvalue functions do not correspond to any policy. More important for our\rpurposes is that most are not representable by the function approximator, which\rby design has far fewer parameters than there are states.\nGiven an enumeration of the state space S = {si, s2,��,s|g|}, any value function v corresponds to a vector listing the value of\reach state in order [v(si), v(s2),��,v(s|S|)]T. This vector\rrepresentation of a value function has as many components as there are states.\rIn most cases where we want to use function approximation that would be far too\rmany to represent the vector explicitly. Nevertheless, the idea of this vector\ris conceptually useful. In the following, we treat a value function and its\rvector representation interchangably.\nTo develop intuitions, consider the case with\rthree states S = {si, s2, s3} and two parameters w = (wi, w2)T. We can then view all value functions/vectors as points in a\rthree-dimensional space. The parameters provide an alternative coordinate\rsystem over a two-dimensional subspace. For any pair of numbers (x, y), we can\rset wi = x and w2= y to produce a parameter vector��a point\rin the two-dimensional subspace��and thus a complete value function vw lying\rwithin the subspace and that assigns values to all three states. In general the\rsubspace of representable functions could be curved and twisted, even not\rone-to-one, but in the case of linear value- function\rapproximation it is a simple plane, as suggested by Figure 11.3.\n\r\nFigure 11.3: The geometry of linear\rvalue-function approximation. Shown as a plane is the subspace of all\rfunctions representable by the function approximator. The three-dimensional\rspace above and below it is the much larger space of all value functions\r(functions from S to R). The true value function is in this larger space and\rprojects down to its best approximation in the value error (VE) sense. The\rbest approximators in the Bellman error (BE) and projected Bellman error\r(PBE) senses are different and are also shown in the lower right. (VE, BE,\rand PBE are all treated as the corresponding vectors in this figure.) The\rBellman operator takes a value function in the plane to one outside, which\rcan then be projected back. If you could iteratively apply the Bellman\roperator outside the space (shown in gray above) you would reach the true\rvalue function, as in conventional DP.\n\r\r\r\r\r\u0026nbsp;\nNow consider a single fixed\rpolicy n. We assume that its true value function, Vn, is too complex to be\rrepresented exactly as an approximation. Thus v^ is not in the subspace; in the\rfigure it is depicted as being above the planar subspace of representable\rfunctions.\nIf Vn can not be represented\rexactly, what representable value function is closest to it? This turns out to\rbe a subtle question, with multiple answers. To begin, we need a measure of the\rdistance between two value functions. Given two value functions\nV\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\ri and V2, we can talk about the vector difference between\rthem, v = Vi - V2. If v is small, then the two value functions are close to each\rother. But how are we to measure the size of this difference vector? The\rconventional Euclidean norm is not appropriate because, as discussed in Section\r9.2, some states are more important than others because they occur more\rfrequently or because we are more interested in them (Section 9.10). As in\rSection 9.2, let us use the weighting ^ : S R to specify the degree to which we\rcare about different states being accurately valued (often taken to be the\ron-policy distribution). We can then define the distance between\n\r\rvalue functions using the norm\n\r\r\u0026nbsp;\n\r\r\r\r\rI:\n\r\r\r\r\r\r\r\r(11.9)\n\r\r\r\r\rx^(s\u0026gt;��2\nseS\n\r\r\u0026nbsp;\n\r\rNote that the MSVE from Section 9.2 can be\rwritten simply using this norm as MSVE(w) = ||vw - VnII��. For any value function v, the\roperation of finding its closest value function in the subspace of\rrepresentable value functions is a projection operation. We define a projection\roperator n that takes an arbitrary value function to the representable function\rthat is closest in our norm:\n\r\r\u0026nbsp;\n\r\r\r\r\r(11.10)\n\r\r\r\r\r\r\r\rarg min\nw\n\r\r\r\r\rnv == Vw where w\n\r\r\u0026nbsp;\n\r\rThe representable value function that is closest to the true value\rfunction is thus its projection, nv^, as suggested in Figure 11.3. This is the\rsolution asymmptotically found by Monte Carlo methods, albeit often very\rslowly. The projection operation is discussed more fully in the box.\nThe projection matrix\nFor a linear function\rapproximator, the projection operation is linear, which implies that it can be\rrepresented as an | S| x | S| matrix:\ni\n\r\r\r(11.11)\n\r\r\r\r\rn = x(xt dx xtd\n\r\r\rwhere Ddenotes denotes the | S| x each state s:\n\r\r\r\r\rthe |S| x |S| diagonal matrix with /i on the diagonal, and X dmatrix whose rows are the feature vectors x(s)T,\rone for\nX1) o \u0026quot;\n\r\u0026nbsp;\n\r\u0026quot;-x(1)T-_\n\r\rM2)\n\r, X =\n\r-x(2)T-\n\r\r_\r0 \u0026quot;(|S|)_\n\r\u0026nbsp;\n\r-x(|S|)T-_\n\r\r\r\rD\n\r\r\r\r\r\u0026nbsp;\n(Formally,\rthe inverse in (11.11) may not exist, in which case the pseudoinverse is\rsubstituted.) Using these matrices, the norm of a vector can be written\n\r\r\u0026nbsp;\n\r\r\r\r\r(11.12)\n(11.13)\n\r\r\r\r\ri��ii��=��٤��\nand the\rapproximate linear value function can be written vw = Xw.\n\r\r\u0026nbsp;\n\r\rTD methods find different solutions. To understand their rationale,\rrecall that the Bellman equation for value function is\nvn(s) = ^2n(q|s^ p(s\u0026#12316;r|s,a)[r +\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (s')] ,\u0026nbsp; Vs G S.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (11.14)\na\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; s,��r\n\r\rvn is the only value function that solves this\requation exactly. For any approximate value function vw, the difference between\rthe right and left sides can be used as a measure of how far off vw is from v^,\rin a Bellman equation sense. We call this the Bellman errorat state s:\n\r\r\r(11.15)\nso that\n(11.16)\nBellman\n\r\r\r\r\r(^w(s) = I ^أ(a|s)Ep(s',r|s,a) [r + 7vw(s')] I - vw(s)\na\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; s7��r\nwhere we have subtracted the left-hand side from the right (vis.\r(11.14)) =E [Rt+i + 7vw(St+i) - vw (St) | St = s, At \u0026#12316;n],\nshows clearly the relationship of the Bellman\rerror to the TD error. The error is the expectation of the TD error.\nThe vector of all the Bellman errors, at all\rstates, ^w G R|S|, is called the Bellman error vector(shown as BE in Figure 11.3). The overall size of\rthis vector, in the norm, is an overall measure of the error in the value\rfunction:\nMSBE(w) = pwII' .\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (11.17)\nThis measure, or objective function, is called the\rMean Squared Bellman Error (MSBE). It is not possible in general to reduce the MSBE to zero (at\rwhich point vw = vn), but for linear function approximation there is\ra unique value of w for which the MSBE is minimized. This point in\rrepresentable value function space is shown in Figure 11.3 as different from\rthat which minimizes the MSVE, as it gen\u0026shy;erally is. Methods that seek to\rminimize the MSBE are discussed in the next two sections.\nThe Bellman error vector is shown in Figure 11.3\ras the result of applying the Bellman operatorBn : R|S|R|S|to the approximate value function. The Bellman operator is defined\rby\n(Bnv)(s)\u0026nbsp;\u0026nbsp;\u0026nbsp; y^n(a|s^\u0026nbsp;\u0026nbsp;\u0026nbsp; p(s',r|s,a)[r + 7v(s')]\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ,Vs\u0026nbsp;\u0026nbsp; G S, Vv: S\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; R.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (11.18)\na\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; s7��r\nThe Bellman error vector for v can be written ^w = Bn vw\r- vw.\nIf the Bellman operator is applied to a value\rfunction in the representable subspace, then, in general, it will produce a new\rvalue function that is outside the subspace, as suggested in the figure. In\rdynamic programming, this operator is applied repeatedly to the points outside\rthe representable space, as suggested by the gray arrows in the top of Figure\r11.3. Eventually that process converges to the true value function v^, the only fixedpoint for the Bellman operator,\rthe only value function for which\nvn = Bn vn,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (11.19)\nwhich is just another way of\rwriting the Bellman equation for n (11.14).\nWith function approximation,\rhowever, the intermediate value functions lying out\u0026shy;side the subspace cannot be\rrepresented. The gray arrows in the upper part of\nFigure 11.3 cannot be followed\rbecause after the first update (dark line) the value function must be projected\rback into something representable. The next iteration then begins within the\rsubspace; the value function is again taken outside of the sub\u0026shy;space by the\rBellman operator and then mapped back by the projection operator, as suggested\rby the lower gray arrow and line. Following these arrows is a DP-like process\rwith approximation.\nIn this case we are interested in the projection of\rthe Bellman error vector back into the representable space. This is the\rprojected Bellman error vector n^w, shown in Figure 11.3 as PBE. The\rsize of this vector, in the norm, is another measure of error in the\rapproximate value function. For any approximate value function v, we define the\rMean Square Projected Bellman Error(MSPBE) as\nMSPBE(w) = ||nlw ||��.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (11.20)\nWith linear function approximation there always exists an\rapproximate value func\u0026shy;tion within the subspace with zero MSPBE; this is the TD\rfixedpoint introduced in Section 9.4. As we have seen, this point is not always\rstable under semi-gradient TD methods and off-policy training. As shown in the\rfigure, this value function is gener\u0026shy;ally different from those minimizing MSVE\ror MSBE. Methods that are guaranteed to converge to it are discussed in\rSections 11.7 and 11.8.\n11.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rStochastic Gradient Descent in\rthe Bellman Error\nStochastic gradient descent (SGD, Section 9.3) is a\rpowerful and appealing approach to approximation that we would like to make\rfull use of in reinforcement learning. Among the algorithms investigated so far\rin this book, only the Monte Carlo meth\u0026shy;ods are true SGD methods. These\rconverge very robustly, under both on-policy and off-policy training as well as\rfor general non-linear (differentiable) function ap\u0026shy;proximators, though they\rare often slower than semi-gradient methods with boot\u0026shy;strapping, which are not\rSGD methods. Semi-gradient methods may diverge under off-policy training, as we\rhave seen earlier in this chapter, and under contrived cases of non-linear\rfunction approximation (Tsitsiklis and Van Roy, 1997). With a true SGD method\rthese kinds of divergence are not possible.\nThe appeal of true SGD is so\rstrong that great effort has gone into finding a prac\u0026shy;tical way of harnessing\rit for reinforcement learning. The starting place of all such efforts is the\rchoice of an error or objective function to optimize. In this and the next\rsection we explore the origins and limits of the most popular proposed objec\u0026shy;tive\rfunction, that based on the Bellman errorintroduced in the previous section. Although this has\rbeen a popular and influential approach, the conclusion that we reach here is\rthat it is a misstep and offers no good learning algorithms. On the other hand,\rthis approach fails in an interesting way that offers some insight into what\rmight constitute a good approach.\nTo begin, let us consider not the Bellman error, but\rsomething more immediate and naive. Temporal difference learning is driven by\rthe TD error. Why not take the minimization of the square of the TD error as the\robjective? In the general function-approximation case, the one-step TD error\rwith discounting can be written\n^t = Rt+i + 7v(St+i,wt) -\rv(St,wt).\nA possible objective function then is what one\rmight call the Mean Squared TD\rError, or MSTDE:\nMSTDE(w)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ^(s)E\r[��| St\r= s, At \u0026#12316;n]\ns�S\n=^ ^(s)E\r[pt^t2| St=s,At\r\u0026#12316;b]\ns�S\n=Eb[pt^2.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (if\r^ is the distribution encountered under b)\nThe last equation is of the form needed for SGD;\rit gives the objective as an expec\u0026shy;tation that can be sampled from experience\r(remember the experience is due to the behavior policy b. Thus, following the\rstandard SGD approach, one can derive the per-step update based on a sample of\rthis expected value:\nwt+i = wt - 1aV(pt^t2)\n=wt -\rapt^tV^t\n=wt + apt^t(Vv(St,wt) - 7V-0(St,wt)),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (11.21)\nwhich you will recognize as the same as the\rsemi-gradient TD algorithm (11.2) except for the additional final term. This\rterm completes the gradient and makes this a true SGD algorithm with excellent\rconvergence guarantees. Let us call this algorithm the naive residual-gradientalgorithm (after Baird, 1993).\nAlthough the naive residual-gradient algorithm\rconverges robustly, it does not always converge to a desireable place, as the A-split examplein the box on the next page shows. In this example a tabular\rrepresentation is used, so the true state values can be exactly represented,\ryet the naive residual-gradient algorithm finds different values, and these\rvalues have lower MSTDE than do the true values. Minimizing the MSTDE is naive;\rby penalizes all TD errors it achieves something more like temporal smoothing\rthan accurate prediction.\nA better idea would seem to be\rminimizing the Bellman error. If the exact values are learned, the Bellman\rerror is zero everywhere. Thus, a Bellman-error-minimizing algorithm should\rhave no trouble with the A-split example. We cannot expect to achieve zero\rBellman error in general, as it would involve finding the true value function,\rwhich we presume is outside the space of representable value functions. But\rgetting close to this ideal is a natural-seeming goal. As we have seen, the\rBellman error is also closely related to the TD error. The Bellman error for a\rstate is the expected TD error in that state. So let's repeat the derivation\rabove with the\nEpisodes begin in state A and then ��split�� stochastically, half the time going\rto B and\rthen invariably going on to terminate with a reward of 1, and half the time\rgoing to state C and\rthen invariably terminating with a reward of zero. Reward for the first\rtransition, out of A, is\ralways zero whichever way the episode goes. As this is an episodic problem, we\rcan take 7to be 1. We also assume on-policy training, so that\rpt is always 1, and tabular function\rapprox\u0026shy;imation, so that the learning algorithms are free to give arbitrary,\rindependent values to all three states. So it should be an easy problem.\n\r\r\rA-split example, showing the naivete of the naive\rresidual gradient algorithm\n\r\r\r\r\r\r\r\rConsider the following three-state episodic MRP:\n\r\r\r\r\rWhat should the values be? From A, half the time the return is 1, and half the time\rthe return is 0; A should\rhave value 2. From B the\rreturn is always\n1,\u0026nbsp; so its value should be 1, and similarly from C the return is always 0, so its value should be 0. These are the true values and, as this\ris a tabular problem, all the methods presented previously converge to them\rexactly.\nHowever, the naive\rresidual-gradient algorithm finds different values for B and C. It converges with B having a value of | and C having a value of 4 (A converges correctly to 2). These are in fact the\rvalues that minimize the MSTDE.\nLet us compute the MSTDE for these\rvalues. The first transition of each episode is either up from A��s 2 to B��s |, a change of |, or down from A��s 2 to C��s i, a change of -1. Because the reward is zero on these\rtransitions, and 7= 1,\rthese changes arethe\rTD errors, and thus the squared TD error is always ��on the first\rtransition. The second transition is similar; it is either up from B��s | to a reward of 1 (and a terminal state of value\r0), or down from C��s 4to a reward of 0 (again with a terminal state of\rvalue 0). Thus, the TD error is always ʿ4, for a\rsquared error of ��on the second step. Thus, for this set of values, the MSTDE on both\rsteps is ʿ.\nNow let��s compute the MSTDE for\rthe true values (B at 1, C at 0, and A at i). In this case the first transition is either from 2 up to 1,\rat B, or\rfrom 2 down to 0, at C; in either case the absolute error is 2 and the\rsquared error is 4. The\rsecond transition has zero error because the starting value, either 1 or\n0\u0026nbsp;\u0026nbsp; depending on whether the transition is from B or C, always exactly matches the immediate reward and return. Thus the\rsquared TD error is | on the first transition and 0on the second, for a mean reward over the two\rtransitions of 8. As 8is bigger that ��,this\rsolution is worse according to the MSTDE. On this simple problem, the true\rvalues do not have the smallest MSTDE.\n\r\rexpected TD\rerror (all expectations here are implicitly conditional on St):\n\r\r\r\r\rwt+i = wt\nwt\nwt\nwt\n\r\r\r\r\r\r\r\r(expectation here implicitly conditional on St)\n\r\r\r\r\r\r\r\r12 2aV(En[^t]2)\n12 2aV(E6[pt^t]2)\naE6[pt^t] VE6[pt^t]\naE6[pt(Rt+i + 7^(St+i,w) - v(St,w))] E6[ptWt]\n\r\r\r\r\r\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\r\r\r\rwt + a\n\r\r\r\r\rE6[pt(Rt+i + 7v(St+i,w)) - v(St,w) Vv(St,w) - yE6[pt����(St+i,w)]\n\r\r\u0026nbsp;\n\r\rThis update and\rvarious ways of sampling it are referred to as the residual gradient algorithm. If you simply used the sample values in all the\rexpectations, then the equation above reduces almost exactly to (11.21),\rthe naive residual-gradient algo- rithm.[18]\rBut this is naive, because the equation above involves the next state, St+i\rappearing in two expectations that are multiplied together. To get an unbiased\rsam\u0026shy;ple of the product, one two independent samples of the next state, but\rduring normal interaction with an external environment only one is obtained.\rOne can sample one expectation or the other, but not both.\nThere are\rtwo ways which the residual gradient algorithm can be made to work. One is in\rthe case of deterministic environments. If the next state is deterministic,\rthen the two samples will necessarily be the same, and the naive algorithm is\rvalid. The other way is to obtain twoindependent\rsamples of the next state St+i from St, one for the first expectation and\ranother for the second expectation. In real interaction with an environment,\rthis would not seem possible, but when interacting with a simulated environment,\rit is. One simply rolls back to the previous state and obtains an alternate\rnext state before proceeding forward from the first next state. In either of\rthese cases the residual gradient algorithm is guaranteed to converge to a\rminimum of the MSBE under the usual conditions on the step-size parameter. As a\rtrue SGD method, this convergence is robust, applying to both linear and\rnon-linear function approximators. In the linear case, convergence is always to\rthe uniquew that\rminimizes the MSBE.\nHowever,\rthere remain at least three ways in which the convergence of the resid\u0026shy;ual\rgradient method is unsatisfactory. The first of these is that empirically it is\rslow, much slower that semi-gradient methods. Indeed, proponents of this method\rhave proposed increasing its speed by combining it with faster semi-gradient\rmeth\u0026shy;ods initially, then gradually switching over to residual gradient for the\rconvergence guarantee (Baird and Moore, 1999). The second way in which the\rresidual-gradient algorithm is unsatisfactory is that it still seems to\rconverge to the wrong values. It must get the right values in all tabular\rcases, such as the A-split example, as for those an exact solution to the\rBellman equation is possible. But if we examine ex\u0026shy;amples with genuine function\rapproximation, then the residual-gradient algorithm,\nA-presplit example, a counterexample for the MSBE\nConsider the following three-state episodic MRP:\ni\\\nحkح\nEpisodes start in\reither A1 or A2, with equal probability. Because of function\rapproximation, these two states look exactly the same, like a single state A whose feature representation is distinct from and\runrelated to the feature representation of the other two states, B and C, which are also distinct from each other. Specifically, the\rparameter of the function approximator has three components, one giving the\rvalue of state B, one\rgiving the value of state C, and one giving the value of both states A1 and A2. Other than the selection of the initial state, the system is\rdeterministic. If it starts in A1, then it transitions to B with a reward of 0and\rthen on to termination with a reward of 1. If it starts in A2, then it transitions to C, and then to termination, with both rewards zero.\nTo a learning\ralgorithm, seeing only the features, the system looks identical to the A-split\rexample. The system seems to always start in A, followed by either B or C with equal probability, and then terminating with a 1or a 0 depending deterministically on the previous state. As in the A-split\rexample, the true values of B and C are 1\rand 0, and the best shared value of A1 and A2 is 2,\rby symmetry.\nBecause this\rproblem appears externally identical to the A-split example, we already know\rwhat values will be found by the algorithms. Semi-gradient TD converges to the\rideal values just mentioned, while the naive residual-gradient algorithm\rconverges to values of | and 4 for B and C respectively.\rAll state transitions are deterministic, so the non-naive residual-gradient\ralgorithm will also converge to these values (it is the same algorithm in this\rcase). It follows then that this ��naive�� solution must also be the one that\rminimizes the MSBE, and so it is. On a deterministic problem, the Bellman\rerrors and TD errors are all the same, so the MSBE is always the same as the\rMSTDE. Optimizing the MSBE on this example gives rise to the same failure mode\ras with the naive residual-gradient algorithm on the A-split example.\nand indeed the MSBE, seems to find the wrong values\rfunctions. One of the most telling such examples is the variation on the\rA-split example shown in the box on the previous page. On the A-presplit example the residual-gradient algorithm finds\rthe same poor solution as its naive version. This example shows intuitively\rthat minimizing the MSBE (which the residual-gradient algorithm surely does)\rmay not be a desirable goal.\nThe third way in which the convergence of the\rresidual-gradient algorithm is not satisfactory is also a problem more with the\rMSBE than with the algorithm as such. This is explained in the next section.\n11.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rLearnability of the Bellman\rError\nThe concept of learnability that we introduce in this\rsection is different from than that commonly used in machine learning. There, a\rhypothesis is said to be ��learnable�� if it is efficientlylearnable, meaning that it can be learned within a\rpolynomial rather than an exponential number of examples. Here we use the term\rin a more basic way, to mean learnable at all, with any amount of experience.\rIt turns out many quantities of apparent interest in reinforcement learning can\rnot be learned even from an infinite amount of experiential data. These quantities\rare well defined and can be computed given knowledge of the internal structure\rof the environment, but cannot be computed or estimated from the observed\rsequence of feature vectors, actions, and rewards.[19]We say that they are not learnable.It will turn out that the Bellman error introduced\rin the last two sections is not learnable in this sense. That the Bellman\rcannot be learned from the observable data is probably the strongest reason not\rto seek it as an objective.\nTo make the concept of learnability\rclear, let��s start with some simple examples. Consider the two Markov reward\rprocesses[20](MRPs) diagrammed below:\n0\n��cocy.\n2\nWhere two edges leave a state, both transitions are\rassumed to occur with equal probability, and the numbers indicate the reward\rreceived. All the states appear the same; they all produce the same\rsingle-component feature vector x= 1and have approximated\rvalue w. Thus, the only varying part of a data trajectory is the rewards. The\rleft MRP stays in the same state and emits an endless stream of 0s and 2s at\rrandom, each with 50-50 probability. The right MRP, on every step, either stays\rin its current state or switches to the other, with 50-50 probability. The\rreward is deterministic in this MRP, always a 0 from one state and always a 2\rfrom the other, but because the state is 50-50, the observable data is again an\rendless stream of 0s and 2s at random, identical to that produced by the left\rMDP. (We can assume the right MRP starts in one of two states at random with\requal probability.) Thus, even given even an infinite amount of data, it would\rnot be possible to tell which of these two MRPs was generating it. In\rparticular, we could not tell if the MRP has one state or two, is stochastic or\rdeterministic. These things are not learnable.\nThis pair of\rMRPs also illustrates that the MSVE objective (9.1) is not learnable. If 7= 0, then the true values of the three states, left\rto right, are 1, 0, and 2. Suppose w = 1. Then the MSVE is 0 for the left MDP\rand 1 for the right MRP. Because the MSVE is different in the two problems, yet\rthe data generated has the same distribution, the MSVE cannot be learned. The\rMSVE is not a unique function of the data distribution. And if it cannot be\rlearned, then how could the MSVE possibly be useful as an objective for\rlearning?\nIf an objective\rcannot be learned, it does indeed draw its utility into question. In the case\rof the MSVE, however, there is a way out. Note that the same solution, w = 1,\ris optimal for both MRPs above (assuming ^ is the same for the two indistin\u0026shy;guishable\rstates in the right MRP). Is this a coincidence, or could it be generally true\rthat all MDPs with the same data distribution also have the same optimal\rparameter vector? If this is true��and we will show next that it is��then the\rMSVE remains a usable objective. The MSVE is not learnable, but the parameter\rthat optimizes it is!\nTo understand this, it is useful\rto bring in another natural objective function, this time one that is clearly\rlearnable. One error that is always observable is that between the value\restimate at each time and the return from that time. The Mean Square Return\rError (MSRE) is the expectation, under \u0026quot;, of the square of this error. In\rthe on-policy case it is not difficult that the the MSRE can be written\nMSRE(w) = E\r[(Gt - v(St,w))2\n=MSVE(w) + E\r[(Gt - vn(St))2.\nThus, the two\robjectives are the same except for a variance term that does not depend on the\rparameter vector. The two objectives must therefore have the same optimal\rparameter value w*. The overall relationships are summarized in Figure 11.4.\nNow let us\rreturn to the MSBE. The MSBE is like the MSVE in that it can be computed from\rknowledge of the MDP but is not learnable from data. But it is not like the\rMSVE in that its minimum solution is not learnable. The counterexample in the\rbox (two pages ahead) gives two MRPs that generate the same data distribution\rbut whose minimizing parameter vector is different, proving that the optimal\rparam\u0026shy;eter vector is not a function of the data and thus cannot be learned from\rit. The other bootstrapping objectives that we have considered, the MSPBE and\rMSTDE, can be determined from data (are learnable) and determine optimal\rsolutions that are in general different from each other and the MSBE minimums.\rThe general case is summarize in Figure 11.5.\n\r\r\r\r\rData\ndistribution\n\r\r\r\r\r\r\r\rMDP:\n\r\r\r\r\r\r\r\rFigure 11.5: Causal relationships among the data distribution, MDPs��and errors for bootstrapping objectives. Two different MDPs can\rproduce the same data distri\u0026shy;bution ye: also produce different MSBEs andhave different minimizing\rparameter vectors; these are not learnable from the data distribution. The\rMSPBE and MSTDE objectives and their (different) minima can be directly\rdetermined from data and thus are learnable.\n\r\r\r\r\r\r\r\rData\ndistribution\n\r\r\r\r\r\r\r\r\u0026#10003;\n\r\r\r\r\rMSRE\n\r\n\r\r\r\r\r\rMS\n\r\r\r\r\r\rFigure\r11.4: Causal relationships among the data distribution, MDPs, and errors for\rMonte-Carlo objectives. Two different MDPs can produce the same data distri\u0026shy;bution\ryet also produce different MSVEs, proving that the MSVE objective cannot be\rdetermined from data and is not learnable. However, all such MSVEs must have\rthe same optimal parameter vector, w*! Moreover, this same w* can be\rdetermined from another objective, the MSRE, which isuniquely determined from the data distribution. Thus w* and the\rMSRE are learnable even though the MSVEs are not.\n\r\r\r\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\n\r\r\r\r\r\u0026nbsp;\nTo show the full range of\rpossibilities we need a slightly more complex pair of Markov reward processes\r(MRPs) than those considered earlier. Consider the following two MRPs:\n\r\n\r\r\r\r\r\u0026nbsp;\nWhere two edges leave a state, both transitions are\rassumed to occur with equal probability, and the numbers indicate the reward\rreceived. The MRP on the left has two states that are represented distinctly.\rThe MRP on the right has three states, two of which, B and B', appear the same and must be given the same approximate value.\rSpecifically, w has two components and the value of state A is given by the first component and the value of B and B' is given by the second. The second MRP has been designed so that\requal time is spent in all three states, so we can take \u0026quot;(s) = |, Vs.\nNote that the observable data\rdistribution is identical for the two MRPs. In both cases the agent will see\rsingle occurrences of A followed by a 0, then some number of apparent Bs, each followed by a -1 except the last, which is\rfollowed by a 1, then we start all over\ragain with a single A and a 0, etc.\rAll the statistical details are the same as well; in both MRPs, the probability\rof a string of kBs is 2-k.\nNow suppose w = 0. In the first\rMRP, this is an exact solution, and the MSBE is zero. In the second MRP, this\rsolution produces a squared error in both B and B' of 1, such that MSBE = \u0026quot;(B)1+ \u0026quot;(B')1 =��.These\rtwo MRPs, which generate the same data distribution, have different MSBEs; the\rMSBE is not learnable.\nMoreover (and\runlike the earlier example for the MSVE) the minimizing value of w is different\rfor the two MRPs. For the first MRP, w = 0 minimizes the MSBE for any 7. For the second MRP, the minimizing w is a\rcomplicated function of 7, but in\rthe limit, as 7\u0026nbsp;\u0026nbsp; 1, it is (-2, 0)T. Thus the solution that\nminimizes MSBE cannot be estimated from data alone;\rknowledge of the MRP beyond what is revealed in the data is required. In this\rsense, it is impossible in principle to pursue the MSBE as an objective for\rlearning.\nIt may be surprising that the\rMSBE-minimizing value of A is so far from zero. Recall that A has a dedicated weight and thus its value is\runconstrained by function approximation. A is followed by a reward of 0 and transition to a\rstate with a value of nearly 0, which suggests vw(A) should be 0; why is its optimal value substantially negative\rrather than 0? The answer is that making the value of A negative reduces the error upon arriving in A from B. The reward on this deterministic transition is 1, which implies\rthat B should\rhave a value 1 more than A. Because B��s value is approximately zero, A��s value is driven toward -1. The MSBE-minimizing\rvalue of c -1 for A is a\rcompromise between reducing the errors on leaving and on entering A.\nThus, the MSBE is not learnable; it cannot be estimated from feature\rvectors and other observable data. This limits the MSBE to model-based\rsettings. There can be no algorithm that minimizes the MSBE without access to\rthe underlying MDP states beyond the feature vectors. The residual-gradient\ralgorithm is only able to minimize MSBE because it is allowed to double sample\rfrom the same state��not a state that has the same feature vector, but one that\ris guaranteed to be the same underlying state. We can see now that there is no\rway around this. Minimizing the MSBE requires some such access to the nominal,\runderlying MDP. This is an important limitation of the MSBE beyond that\ridentified in the A-presplit example on page 286. All this directs more\rattention toward the MSPBE.\n11.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rGradient-TD Methods\nWe now consider SGD methods for minimizing the MSPBE.\rAs true SGD methods, these gradient-TD methodshave robust convergence properties even under\roff-policy training and non-linear function approximation. Remember that in the\rlinear case there it is always an exact solution, the TD fixedpoint wtd , at which the MSPBE is zero. This solution could\rbe found by least-squares methods (Section 9.7), but only by methods of\rquadratic O(d2)\rcomplexity in the number of parameters. We seek instead an SGD method, which\rshould be O(d) and have robust convergence properties.\nTo derive an SGD method for the MSPBE (assuming linear function\rapproxima\u0026shy;tion) we begin by expanding and rewriting the objective (11.20) in\rmatrix terms:\nMSPBE(w) = ||nlw If\n=(n^w)TDn^w\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (from (11.12))\n=G nTDn\u0026lt;!w\n=G DX(XTDX)-lXT̎\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��11.22)\n(using (11.11) and the identity nTDn = DX (XTDX)-iXTD)\n=(XTD^w )T(XtDX)-1(Xt\rDlw).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (11.23)\nThe gradient with respect to w is\nr\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; iT\nVMSPBE(w) = 2V XTD^w (XTDX)-l(XTD^w).\nTo turn this into an SGD method, we have to sample something on\revery time step that has this quantity as its expected value. Let us take\r\u0026quot; to be the distribution of states visited under the behavior policy. All\rthree of the factors above can then be written in terms of expectations under\rthis distribution. For example, the last factor can be written\nXTD^w =��\u0026quot;(s)x(s)^w(s) = E[xtpt^t]\nwhich is just the expectation of the semi-gradient TD(0) update\r(11.2). The first factor is the transpose of the gradient of this update:\n\r\r\u0026nbsp;\n\r\r\r\r\rPtV^TxT\n\r\r\r\r\r\r\r\rE\n\r\r\r\r\rVE[xtpt^t]\n\r\r\r\r\r(using episodic \u0026#12316;��\n\r\r\r\r\r\r\r\rPtV(Rt+i + 7wTxt+i - wTxt)TxT\n\r\r\r\r\r\r\r\rPt(7xt+i - xt)xT\n\r\r\r\r\r\r\r\rE\nE\n\r\r\r\r\r\u0026nbsp;\n\u0026nbsp;\n\rFinally, the middle factor is the inverse of the\rexpected outer-product matrix of the feature vectors:\nXtDX =\u0026nbsp; ^(s)x8xT = E xtxT\nSubstituting these expectations for the three\rfactors in our expression for the gradient of the MSPBE, we get\ni\n\r\r\r(11.24)\n\r\r\r\r\rVMSPBE(w) = 2E pt(7xt+i - xt)xT E xtxf\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; E[xtpt^t]\r\u0026#8226;\nIt might not be obvious that we have made any\rprogress by writing the gradient in this form. It is a product of three\rexpressions and the first and last are not independent. They both depend on the\rnext feature vector xt+i; we cannot simply sample both of these expectations\rand then multiply the samples. This would give us an unbiased estmate of the\rgradient just as in the naive residual-gradient algorithm.\nAnother idea would be to estimate the three\rexpectations separately and then combine them to produce an unbiased estimate\rof the gradient. This would work, but would require a lot of computational\rresources, particularly to store the first two expectations, which are dx dmatrices, and to compute the inverse of the\rsecond. This idea can be improved. If two of the three expectations are\restimated and stored, then the third could be sampled and used in conjunction\rwith the two stored quantities. For example, you could store estimates of the\rsecond two quantities (using the increment inverse-updating techniques in\rSection 9.7) and then sample the first expression. Unfortunately, the overall\ralgorithm would still be of quadratic complexity (of order O(d2)).\nThe idea of storing some\restimates separately and then combining them with samples is a good one and is\ralso used in gradient-TD methods. In these methods we estimate and store the productof the second two factors. These factors are an d x d matrix and an\rn-vector, so their product is just an n-vector, like w itself. We denote this\rsecond learned vector as v:\n\r\r\u0026nbsp;\n\r\ri\n\r\r\r(11.25)\n\r\r\r\r\r\r\r\rv % E\n\r\r\r\r\rxtxT\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; E[xtPt^t].\n\r\r\u0026nbsp;\n\r\rThis form is\rfamiliar to students of linear supervised learning. It is the solution to a\rlinear least-squares problem that tries to approximate pt^t from the features.\rThe standard SGD method for incrementally finding the vector v that minimizes\rthe\n\r\rexpected squared error (vTxt - pt\u0026amp;)\ris known as the Least Mean Square (LMS) rule:\nVt+l = vt + ¬pt (5t - vtTxt) xt,\nwhere ^ \u0026gt; 0 is another\rstep-size parameter. We can use this method to effectively achieve (11.25) with\rO(d) storage and per-step computation.\nGiven a stored estimate vt\rapproximating (11.25), we can update our main pa\u0026shy;rameter vector wt using SGD\rmethods based on (11.24). The simplest such rule is\n\r\r\r\r\rwt+i = wt - ^aVMSPBE(wt)\n\r\r\r\r\r\r\r\r-l\n\r\r\r\r\r\r\r\rpt(7xt+i - xt)xT E xtxT\n\r\r\r\r\r\r\r\rwt\nwt\nwt\nwt\n\r\r\r\r\r\r\r\r-l\n\r\r\r\r\r\r\r\rxtxtT\n\r\r\r\r\r\r\r\rE\nvt\n\r\r\r\r\r\r\r\rThis algorithm is called GTD2.Note that if the final inner product first, then the entire\ralgorithm is of O(d) complexity.\nA slightly better algorithm can\rbe derived by doing a few more analytic steps before substituting in vt.\rContinuing from (11.26):\n\r\r\r\r\r\r\r\r+ aE pt(xt - 7xt+i)xT + aE pt(xt -\r7xt+i)xT +\rapt (xt - 7xt+i) xT vt\n\r\r\r\r\r\r\r\r--- a2E\n2\n\r\r\r\r\r\r\r\r(the general SGD rule)\rEfxtpt^t]\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (from\r(11.24))\nEfxtpt^t]\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (11.26)\n(based on (11.25)) (sampling)\n(xfvt) is done\n\r\r\r\r\r\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\r-l\nwt+i = wt + aE pt(xt\r- 7xt+i)xT E xtxf\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Efxtpt^t]\n\r\r\r\r\r-i\n\r\r\r\r\r\r\r\rxtxT\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Efxtpt^t]\n\r\r\r\r\r\r\r\rwt + a (E\n\r\r\r\r\r\r\r\r- 7E\n\r\r\r\r\r\r\r\rE\n\r\r\r\r\r\r\r\rptxt+ixt\n\r\r\r\r\r\r\r\r-l\n\r\r\r\r\r\r\r\rwt + a QEfxtpt^t] - 7E wt + a (E[xtp0t] - 7E\n\r\r\r\r\r\r\r\rxtxT\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Efxtpt^t]\n\r\r\r\r\r\r\r\rE\n\r\r\r\r\r\r\r\rptxt+ixt\nptxt+ixt\n\r\r\r\r\r\r\r\rvt\n\r\r\r\r\r\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\r\r\r\r(sampling)\n\r\r\r\r\rwt + apt ^txt - 7xt+ixTvt^\nwhich again is O(d) if the\rfinal product is done first. This algorithm is known as either TD(0) with gradient correction (TDC)or, alternatively, as GTD(0).\nFigure 11.6 shows a sample and\rthe expected behavior of TDC on Baird��s coun\u0026shy;terexample. As intended, the MSPBE\rfalls to zero, but note that the individual components of the parameter vector\rdo not approach zero. In fact, these values are still far from an optimal\rsolution, v(s) = 0,Vs,\rfor which w would have to be propor\u0026shy;tional to (1,1,1,1,1,1, 4 - 2)t. After 1000 iterations we are still\rfar from an optimal solution, as we can see from the MSVE, which remains almost\r2. The system is actually converging to an optimal solution, but progress is\rextremely slow because the MSPBE is already so close to zero.\n\r\n\r\r\r\r\r\u0026nbsp;\nFigure 11.6: The behavior of the TDC\ralgorithm on Baird��s counterexample. On the left is shown a typical single run,\rand on the right is shown the expected behavior of this algorithm if the\rupdates are done in asynchronous sweeps (analogous to (11.7), except for the\rtwo TDC parameter vectors). The step sizes were a = 0.005 and /3= 0.05.\nGTD2 and TDC both involve two\rlearning processes, a primary one for w and a secondary one for v. The logic of\rthe primary learning process relies on the sec\u0026shy;ondary learning process having\rfinished, at least approximately, whereas the sec\u0026shy;ondary learning process\rproceeds without being influenced by the first. We call this sort of\rasymmetrical dependence a cascade. In cascades we often assume that the secondary learning process is\rproceeding faster and thus is always at its asymmptotic value, ready and\raccurate to assist the primary learning process. The convergence proofs for\rthese methods often make this assumption explicitly. These are called two-time-scaleproofs. The fast time scale is that of the secondary\rlearning process, and the slower time scale is that of the primary learning\rprocess. If a is the step size of the primary learning process, and Pis the step size of the secondary learning process,\rthen these convergence proofs will typically be in the limit as P 0and\n��^ 0 ^ ] 0.\nGradient-TD methods are currently the most well\runderstood and widely used stable off-policy methods. There are extensions to\raction values and control (GQ, Maei et al., 2010), to eligibility traces\r(GTD(A) and GQ(A), Maei, 2011; Maei and Sutton, 2010), and to nonlinear\rfunction approximation (Maei et al., 2009). There has also been proposed hybrid\ralgorithms midway between semi-gradient TD and gradient TD. The Hybrid TD (HTD,\rHackman, 2012; White and White, 2016) al\u0026shy;gorithm behaves like GTD in states\rwhere the target and behavior policies are very different and like\rsemi-gradient TD in states where they are the same. Finally, the gradient-TD\ridea has been combined with the ideas of proximal method and controlvariates to produce more efficient methods (Mahadevan et al., 2014).\n11.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rEmphatic-TD Methods\nWe turn now to the second major strategy that has\rbeen extensively explored for ob\u0026shy;taining a cheap and efficient off-policy\rlearning method with function approximation. Recall that linear semi-gradient\rTD methods are efficient and stable when trained under the on-policy\rdistribution, and that we showed in Section 9.4 that this has to do with the\rmatrix A and the match between the on-policy state distribution and the\rstate-transition probabilities p(s|s, a) under the target policy. In off-policy\rlearning, we reweight the state transitions using importance weighting so that\rthey become appropriate for learning about the target policy, but the state\rdistribution is still that of the behavior policy. There is a mismatch. A\rnatural idea is to somehow reweight the states, emphasizing some and\rde-emphasizing others, so as to return the distribution of updates to the\ron-policy distribution. There would then be a match, and stability and\rconvergence would follow from existing results. This is the idea of Emphatic-TD\rmethods, first introduced, for on-policy training, in Section 9.10.\nActually, the notion of ��the\ron-policy distribution�� is not quite right, as there are many on-policy\rdistributions, and any one of these is sufficient to guarantee stability.\rConsider an undiscounted episodic problem. The way episodes terminate is fully\rdetermined by the transition probabilities, but there may be several different\rways the episodes might begin. However the episodes start, if all state\rtransitions are due to the target policy, then the state distribution that\rresults is an on-policy distribution. You might start close to the terminal\rstate and visit only a few states with high probability before ending the\repisode. Or you might start far away and pass through many states before\rterminating. Both are on-policy distributions, and training on both with a\rlinear semi-gradient method would be guaranteed to be stable. However the\rprocess starts, an on-policy distribution results as long as all states\rencountered are updated up until termination.\nIf there is discounting, it can be\rtreated as partial or probabilistic termination for these purposes. If 7= 0.9, then we can consider that with probability\r0.1 the pro\u0026shy;cess terminates on every time step and then immediately restarts in\rthe state that is transitioned to. A discounted problem is one that is\rcontinually terminating and restarting with probability 1 - 7on every step. This way of thinking about discount\u0026shy;ing\ris an example of a more general notion of pseudo\rtermination��termination that does\rnot affect the sequence of state transitions, but does affect the learning\rprocess and the quantities being learned. This kind of pseudo termination is\rimportant to off-policy learning because the restarting is optional��remember we\rcan start any way we want to��and the termination relieves the need to keep\rincluding encoun\u0026shy;tered states within the on-policy distribution. That is, if we\rdon��t consider the new states as restarts, then discounting quickly give us a\rlimited on-policy distribution.\nThe one-step emphatic-TD algorithm\rfor learning episodic state values is defined by:\n^t = Rt+i + 7v(St+i,wt) - v(St,wt), wt+i\r= wt + aMtpt^t Vv(St,wt),\nMt = 7pt-iMt-i + It,\nwith It, the interest,being arbitrary and Mt, the emphasis,being\rinitialized to Mt_i = 0. How does this algorithm perform on Baird��s\rcounterexample? Figure 11.7 shows the trajectory in expectation of the\rcomponents of the parameter vector (for the case in which It = 1, Vt). There\rare some oscilations but eventually everything converges and the MSVE goes to\rzero. These trajectories are obtained by iteratively computing the expectation\rof the parameter vector trajectory without any of the variance due to sampling\rof transitions and rewards. We do not show the results of applying ETD directly\rbecause its variance on Baird��s counterexample is so high that it is nigh\rimpossible to get consistent results in computational experiments. The\ralgorithm converges to the optimal solution in theory on this problem, but in\rpractice it does not. We turn to the topic of reducing the variance of all\rthese algorithms in the next section.\n\r\nFigure 11.7: The behavior of the\rETD algorithm in expectation on Baird��s counterexample. The step size was a =\r0.03.\n\r\r\r\r\r\u0026nbsp;\n11.9\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rReducing Variance\nOff-policy learning is of\rinherently greater variance than on-policy learning. This is not surprising; if\ryou receive data less closely related to a policy, you should expect to learn\rless about the policy��s values. In the extreme, one may be able to learn\rnothing. You can��t expect to learn how to drive by cooking dinner, for example.\rOnly if the target and behavior policies are related, if they visit similar\rstates and \n\r\rtake\rsimilar actions, should one be able to make significant progress in off-policy\rtraining.\nOn the other hand, any policy has\rmany neighbors, many similar policies with considerable overlap in states\rvisited and actions chosen, and yet which are not identical. The raison d��etre\rof off-policy learning is to enable generalization to this vast number of\rrelated-but-not-identical policies. The problem remains of how to make the best\ruse of the experience. Now that we have some methods that are stable in\rexpected value (if the step sizes are set right), attention naturally turns to\rreducing the variance of the estimates. There are many possible ideas, and we\rcan just touch on of a few of them in this introductory text.\nWhy is controlling variance\respecially critical in off-policy methods based on im\u0026shy;portance sampling? As we\rhave seen, important sampling often involves products of policy ratios. The\rratios are always one in expectation, but their actual values may be much\rsignificantly higher or as low as zero. Successive ratios are uncorrelated, so their\rproducts are also always one in expected value, but can to be of great\rvariance. Recall that these ratios multiply the step size in SGD methods, so\rhigh variance means taking steps that vary greatly in their size. This is\rproblematic for SGD be\u0026shy;cause of the occasional very large steps. They must not\rbe so large as to take the parameter to a part of the space with a very\rdifferent gradient. SGD methods rely on averaging over multiple steps to get a\rgood sense of the gradient, and if they make large moves from single samples\rthey become unreliable. If the step-size parameter is set small enough to\rprevent this, then the expected step can end up being very small, resulting in\rvery slow learning. The notions of momentum (Derthick, 1984), of Polyak-Ruppert\raveraging (Polyak, 1991; Ruppert, 1988; Polyak and Juditsky, 1992), or further\rextensions of these ideas may significantly help. Methods for adap\u0026shy;tively\rsetting separate step sizes for different components of the parameter vector\rare also pertinent (e.g., Jacobs, 1988; Sutton, 1992), as are the ��importance\rweight aware�� updates of Karampatziakis and Langford (2010).\nIn Chapter 5 we saw how weighted\rimportance sampling is significantly better behaved, with lower variance\rupdates, than ordinary importance sampling. However, adapting weighted\rimportance sampling to function approximation is challenging and can probably\ronly be done approximately with O(d) complexity (Mahmood and Sutton, 2015).\nThe Tree Backup algorithm shows\rthat is is possible to perform some off-policy learning without using\rimportance sampling. This idea has been extended to the off-policy case to\rproduce stable and more efficient methods by Munos, Stepleton, Harutyunyan, and\rBellemare (2016) and by Mahmood, Yu and Sutton (2017).\nAnother, complementary strategy is\rto allow the target policy to be determined in part by the behavior policy, in\rsuch a way that it never can be so different from it to create large importance\rsampling ratios. For example, the target policy can be defined by reference to\rthe behavior policy, as in the ��recognizers�� proposed by Precup et al. (2005).\n11.10\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rSummary\nOff-policy learning is a tempting challenge, testing\rour ingenuity in designing sta\u0026shy;ble and efficient learning algorithms. Tabular\rQ-learning makes off-policy learning seem easy, and it has natural\rgeneralizations to Expected Sarsa and to the Tree Backup algorithm. But as we\rhave seen in this chapter, the extension of these ideas to significant function\rapproximation, even linear function approximation, involves new challenges and\rforces us to deepen our understanding of reinforcement learning algorithms.\nWhy go to such lengths? One reason\rto seek off-policy algorithms is to give flexibility in dealing with the\rtradeoff between exploration and exploitation. Another is to free behavior from\rlearning, and avoid the tyranny of the target policy. TD learning appears to\rhold out the possibility of learning about multiple things in parallel, of\rusing one stream of experience to solve many tasks simultaneously. We can certainly\rdo this in special cases, just not in every case that we would like to or as\refficiently as we would like to.\nIn this chapter we divided the\rchallenge of off-policy learning into two parts. The first part, correcting the\rtargets of learning for the behavior policy, is straightfor\u0026shy;wardly dealt with\rusing the techniques devised earlier for the tabular case, albiet at the cost\rof increasing the variance of the updates and thereby slowing learning. High\rvariance will probably always remains a challenge for off-policy learning.\nThe second part of the challenge\rof off-policy learning emerges as the instability of semi-gradient TD methods\rthat involve bootstrapping. We seek powerful function approximation, off-policy\rlearning, and the efficiency and flexibility of bootstrapping TD methods, but\rit is challenging to combine all three aspects of this deadly triad in one algorithm without introducing the potential\rfor instability. There have been several attempts. The most popular has been to\rseek to perform true stochastic gradient descent (SGD) in the Bellman error\r(a.k.a. the Bellman residual). However, our analysis concludes that this is not\ran appealing goal in many cases, and that anyway it is impossible to achieve\rwith a learning algorithm��that the gradient of the MSBE is not learnable from\rexperience that reveals only feature vectors and not underlying states. Another\rapproach, Gradient-TD methods, performs SGD in the projectedBellman error. The gradient of the MSPBE is\rlearnable with O(d) complexity, but at the cost of a second parameter vector\rwith a second step size. The newest family of methods, Emphatic-TD methods,\rrefine an old idea for reweighting updates, emphasizing some and de-emphasizing\rothers. In this way they restore the special properties that make on-policy\rlearning stable with computationally simple semi-gradient methods.\nThe whole area of off-policy\rlearning is relatively new and unsettled. Which meth\u0026shy;ods are best or even\radequate is not yet clear. Are the complexities of the new methods introduced\rat the end of this chapter really necessary? Which of them can be combined\reffectively with variance reductions methods? The potential for off-policy\rlearning remains tantalizing, the best way to achieve it still a mystery.\nBibliographical and Historical\rRemarks\n11.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe first\rsemi-gradient method was linear TD(A) (Sutton, 1988). The name ��semi-gradient��\ris more recent (Sutton, 2015a). Semi-gradient off-policy TD(0) with general\rimportance-sampling ratio may not have been explic\u0026shy;itly stated until Sutton, Mahmood,\rand White (2016), but the action-value forms were introduced by Precup, Sutton,\rand Singh (2000), who also did eligibility trace forms of these algorithms (see\rChapter 12). Their continu\u0026shy;ing, undiscounted forms have not been significantly\rexplored. The atomic multi-step forms given here are new.\n11.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe earliest\rw-to-2w example was given by Tsitsiklis and Van Roy (1996), who also introduced\rthe specific counterexample in the box on page 276. Baird��s counterexample is\rdue to Baird (1995), though the version we present here is slightly modified.\rAveraging methods for function approximation were developed by Gordon (1995,\r1996). Other examples of instability with off- policy DP methods and more\rcomplex methods of function approximation are given by Boyan and Moore (1995).\rBradtke (1993) gives an example in which Q-learning using linear function\rapproximation in a linear quadratic regulation problem converges to a\rdestabilizing policy.\n11.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe deadly triad\rwas first identified by Sutton (1995) and thoroughly ana\u0026shy;lyzed by Tsitsiklis\rand Van Roy (1997). The name ��deadly triad�� is due to Sutton (2015a).\n11.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThis kind of\rlinear analysis was pioneered by Tsitsiklis and Van Roy (1996; 1997), including\rthe dynamic programming operator. Diagrams like Fig\u0026shy;ure 11.3 were introduced by\rLagoudakis and Parr (2003).\n11.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe MSBE was\rfirst proposed as an objective function for dynamic program\u0026shy;ming by Schweitzer\rand Seidmann (1985). Baird (1995, 1999) extended it to TD learning based on\rstochastic gradient descent, and Engel, Mannor, and Meir (2003) extended it to\rleast squares (O(d2)) methods known as Gaussian Process TD learning.\rIn the literature, MSBE minimization is often referred to as Bellman residual\rminimization.\nThe earliest A-split example is due to Dayan (1992). The two forms given\rhere were introduced by Sutton et al. (2009).\n11.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe contents of\rthis section are new to this text.\n11.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rGradient-TD\rmethods were introduced by Sutton, Szepesvari, and Maei (2009). The methods\rhighlighted in this section were introduced by Sut\u0026shy;ton et al. (2009) and\rMahmood et al. (2014). The most sensitive empirical investigations to date of\rgradient-TD and related methods are given by Geist and Scherrer (2014), Dann,\rNeumann, and Peters (2014), and White (2015).\n11.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rEmphatic-TD\rmethods were introduced by Sutton, Mahmood, and White (2016). Full convergence proofs\rand other theory were later established by Yu (2015a; 2015b; Yu, Mahmood, and\rSutton, 2017) and Hallak, Tamar, Munos, and Mannor (2015).\n\r\rChapter 12\nEligibility Traces\nEligibility traces are one of the basic mechanisms of\rreinforcement learning. For example, in the popular TD(A) algorithm, the ��refers\rto the use of an eligibility trace. Almost any temporal-difference (TD) method,\rsuch as Q-learning or Sarsa, can be combined with eligibility traces to obtain\ra more general method that may learn more efficiently.\nEligibility traces unify and\rgeneralize TD and Monte Carlo methods. When TD methods are augmented with\religibility traces, they produce a family of methods spanning a spectrum that\rhas Monte Carlo methods at one end (A = 1) and one- step TD methods at the\rother (A = 0). In between are intermediate methods that are often better than\reither extreme method. Eligibility traces also provide a way of implementing\rMonte Carlo methods online and on continuing problems without episodes.\nOf course, we have already seen\rone way of unifying TD and Monte Carlo methods: the n-step TD methods of\rChapter 7. What eligibility traces offer beyond these is an elegant algorithmic\rmechanism with significant computational advantages. The mechanism is a\rshort-term memory vector, the eligibility traceet G Rd, that parallels the long-term\rweight vector wt G Rd. The rough idea is that when a component of wt\rparticipates in producing an estimated value, then the corresponding component\rof et is bumped up and then begins to fade away. Learning will then occur in\rthat component of wt if a nonzero TD error occurs before the trace falls back\rto zero. The trace-decay parameter A G [0,1] determines the rate at which the\rtrace falls.\nThe primary computational\radvantage of eligibility traces over n-step methods is that only a single trace\rvector is required rather than a store of the last n feature vectors. Learning\ralso occurs continually and uniformly in time rather than being delayed and\rthen catching up at the end of the episode. In addition learning can occur and\raffect behavior immediately after a state is encountered rather than being\rdelayed n steps.\nEligibility traces illustrate that\ra learning algorithm can sometimes be imple\u0026shy;mented in a different way to obtain\rcomputational advantages. Many algorithms are most naturally formulated and\runderstood as an update of a state��s value basedon\revents that follow that state over multiple future time steps. For example,\rMonte Carlo methods (Chapter 5) update a state based on all the future rewards,\rand n- step TD methods (Chapter 7) update based on the next nrewards and state nsteps in the future. Such\rformulations, based on looking forward from the updated state, are called forward views. Forward views are always\rsomewhat complex to imple\u0026shy;ment because the update depends on later things that\rare not available at the time. However, as we show in this chapter it is often\rpossible to achieve nearly the same updates��and sometimes exactlythe same updates��with an\ralgorithm that uses the current TD error, looking backward to recently visited\rstates using an eligibil\u0026shy;ity trace. These alternate ways of looking at and\rimplementing learning algorithms are called backward views. Backward views,\rtransformations between forward-views and backward-views, and equivalences\rbetween them date back to the introduction of temporal difference learning, but\rhave become much more powerful and sophisticated since 2014. Here we present\rthe basics of the modern view.\nAs usual, first we fully develop the ideas for\rstate values and prediction, then extend them to action values and control. We\rdevelop them first for the on-policy case then extend them to off-policy\rlearning. Our treatment pays special attention to the case of linear function\rapproximation, for which the results with eligibility traces are stronger. All\rthese results apply also to the tabular and state aggregation cases because\rthese are special cases of linear function approximation.\n12.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe A-return\n\r\r\rrewards plus the discounted\r(7.1). approximator, is\n\r\r\r\r\rIn Chapter 7 we defined an n-step\rreturn as the sum of the first n estimated value of the state reached in n\rsteps, each appropriately The general form of that equation, for any\rparameterized function\nGt:t+n = Rt+i\r+ 7Rt+2+' ' ' +7^ iRt+n + 7n6(St+n,wt+n-i), 0^ t\u0026lt; T-U.(12.1)\nWe noted in Chapter 7 that each\rn-step return, for n \u0026gt; 1, is a valid update target for a tabular learning\rupdate, just as it is for an approximate SGD learning update such as (9.6).\nNow we note that a valid update\rcan be done not just toward any n-step return, but toward any averageof n-step returns. For example, an update can be done toward a\rtarget that is half of a two-step return and half of a four-step return:\n2Gt��t+2+ 2\rGt:t+4. Any set of n-step returns can be averaged in\rthis way, even an infinite set, as long as the weights on the component returns\rare positive and sum to 1. The composite return possesses an error reduction\rproperty similar to that of individual n-step returns (7.3) and thus can be\rused to construct updates with guaranteed convergence properties. Averaging\rproduces a substantial new range of algorithms. For example, one could average\rone-step and infinite-step returns to obtain another way of interrelating TD\rand Monte Carlo methods. In principle, one could even average experience-based\rupdates with DP updates to get a simple combination of experience-based and\rmodel-based methods (cf. Chapter 8).\nAn update that averages simpler\rcomponent updates is called a com\u0026shy;pound update. The backup diagram for a compound update consists\rof the backup diagrams for each of the component updates with a hori\u0026shy;zontal line\rabove them and the weighting fractions below. For example, the compound update\rfor the case mentioned at the start of this sec\u0026shy;tion, mixing half of a two-step\rreturn and half of a four-step return, has the diagram shown to the right. A\rcompound update can only be done when the longest of its component updates is\rcomplete. The update at the right, for example, could only be done at time t+ 4 for the estimate formed at time t. In general\rone would like to limit the length of the longest component backup because of\rthe corresponding delay in the updates.\n\r\r\r��\n\r9\n\r\r1\n\rI\n\r\r2\n\r!\n?\n\r\r\u0026nbsp;\n\r��\n��\n\r\r\u0026nbsp;\n\ri\n\r\r\u0026nbsp;\n\r2\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\rThe TD(A) algorithm can be understood\ras one particular way of averaging n-step backups. This average contains all\rthe n-step backups, each weighted proportional to An-1, where A G\r[0,1], and normalized by a factor of 1 - A to ensure that the weights sum to 1\r(see Figure 12.1).\nThe resulting backup is toward a return, called the X-return, defined in its state-based form by\n\r\r\u0026nbsp;\n\r\r\r\r\r��\n\r\r\r\r\r\r\r\r(12.2)\n\r\r\r\r\r(1- A) A^ 1Gt:t+n\n\r\r\u0026nbsp;\n\r\rFigure 12.2 further illustrates the weighting on the sequence of\rn-step returns in the A-return. The one-step return is given the largest\rweight, 1 - A; the two-step return is given the next largest weight, (1- A)A; the three-step return is given the weight (1- A)A2; and so on. The weight fades by A\rwith each additional step. After a\nTD(A)\nSt\n\r\r\r\r\r��\n�� 1 - A\n\r\r\r\r\r\r\r\r��\n(i -��)��2\n\r\r\r\r\r\r\r\r��4\nO St+i Rt+i\n..��\u0026#12316;1\n(j) St+2 Rt + 2 \u0026#8226; At+2\n��\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; -i\n\u0026#8226; . EH St Rt\nXT-��1\n0, then the overall backup\rreduces to its 1, then the overall\rbackup reduces\n\r\r\r\r\r\r\r\rFigure 12.1: The backup digram for TD(A). If A first component, the\rone-step TD backup, whereas if A to its last component, the Monte Carlo\rbackup.\n\r\r\r\r\r\r\r\r��\n��\n(i һ��)��\n\r\r\r\r\r\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\r\r\r\rFigure 12.2: Weighting given in the A-return to each of the n-step returns.\n\r\r\r\r\r\r\r\r1\n\r\r\r\r\r\r\r\rTime\n\r\r\r\r\r\r\r\rWeight\n\r\r\r\r\r\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rterminal state has been reached, all subsequent n-step returns are\requal to Gt. If we want, we can\rseparate these post-termination terms from the main sum, yielding\nt-t-i\nG=\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (1\u0026nbsp; -\u0026nbsp; A)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; An iGt��t+n + aT��iGt,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (12.3)\nn=i\nas indicated in the figures. This equation makes it clearer what\rhappens when A = 1. In this case the main sum goes to zero, and the remaining\rterm reduces to the conventional return, Gt. Thus, for A = 1, backing up according to the A-return is a Monte\rCarlo algorithm. On the other hand, if A = 0, then the A-return reduces to Gt��t+i, the one-step return. Thus, for A = 0, backing up according to the A-return is a one-step\rTD method.\nExercise 12.1 Just as the return can be written\rrecursively in terms of the first reward and itself one-step later (3.3), so\rcan the A-return. Derive the analogous recursive relationship from (12.2) and (12.1).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 12.2 The parameter A characterizes how fast the exponential\rweighting in Figure 12.2 falls off, and thus how far into the future the\rA-return algorithm looks in determining its backup. But a rate factor such as A\ris sometimes an awkward way of characterizing the speed of the decay. For some\rpurposes it is better to specify a time constant, or half-life. What is the\requation relating A and the half-life, t����,the time by which the\rweighting sequence will have fallen to half of its initial value? ��\nWe are now ready to define our first learning\ralgorithm based on the A-return: the off-line\rX-return algorithm.As an\roff-line algorithm, it makes no changes to the weight vector during the\repisode. Then, at the end of the episode, a whole sequence of off-line updates\rare made according to our usual semi-gradient rule, using the A-return as the\rtarget:\nwt+i ʿwt + a G^s - v(St,wt) Vv(St,wt), t= 0,... ,T\r- 1.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (12.4)\nThe A-return gives us an\ralternative way of moving smoothly between Monte Carlo and one-step TD methods\rthat can be compared with the n-step TD way of\n\r\r\rn-step TD methods\n(from Chapter 7)\n\r\r\r\r\r\r\r\rRMS error at the end of the episode over the first 10 episodes\n\r\r\r\r\r\r\r\rOff-line X-return algorithm\n\r\r\r\r\ra\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; a\nFigure 12.3: 19-state Random walk results (Example 7.1): Performance\rof the offline X- return algorithm alongside that of the n-step TD methods. In\rboth case, intermediate values of the bootstrapping parameter (X or n)\rperformed best. The results with the off-line X-return algorithm are slighly\rbetter at the best values of aand X, and at high a.\nChapter 7. There we assessed effectiveness on a\r19-state random walk task (Example 7.1). Figure 12.3 shows the performance of\rthe off-line ��-return algorithm on this task alongside that of the\rn-step methods (repeated from Figure 7.2). The experiment was just as described\rearlier except that for the X-return algorithm we varied X instead of n. The\rperformance measure used is the estimated root-mean-squared error between the\rcorrect and estimated values of each state measured at the end of the episode,\raveraged over the first 10 episodes and the 19 states. Note that overall\rperformance of the off-line X-return algorithms is comparable to that of the\rn-step algorithms. In both cases we get best performance with an intermediate\rvalue of the bootstrapping parameter, n for n-step methods and ��for the offline ��-return algorithm.\nThe approach that we have been taking so far is what\rwe call the theoretical, or forward, view of a learning algorithm. For each state visited, we look\rforward in time to all the future rewards and decide how best to combine them.\rWe might imagine ourselves riding the stream of states, looking forward from\reach state to determine its update, as suggested by Figure 12.4. After looking\rforward from and updating\n\r\nFigure 12.4: The forward view. We decide how to\rupdate each state by looking forward to future rewards and states.\n\r\r\r\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\rand never have to work with the\rpreceding state hand, are viewed and processed repeatedly, once them.\n\r\r\r\r\r\r\r\rone state, we move on to the\rnext again. Future states, on the other from each vantage point preceding\n\r\r\r\r\r12.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; TD(A)\nTD(����is\rone of the oldest and most widely used algorithms in reinforcement learning. It\rwas the first algorithm for which a formal relationship was shown between a\rmore theoretical forward view and a more computational congenial backward view\rusing eligibility traces. Here we will show empirically that it approximates\rthe off-line X-return algorithm presented in the previous section.\nTD(����improves\rover the off-line ��-return algorithm in\rthree ways. First it updates the weight vector on every step of an episode\rrather than only at the end, and thus its estimates may be better sooner.\rSecond, its computations are equally distributed in time rather that all at the\rend of the episode. And third, it can be applied to continuing problems rather\rthan just episodic problems. In this section we present the semi-gradient\rversion of TD(����with\rfunction approximation.\nWith\rfunction approximation, the eligibility trace is a vector et G Rd\rwith the same number of components as the weight vector wt. Whereas the weight\rvector is a long-term memory, accumulating over the lifetime of the system, the\religibility trace is a short-term memory, typically lasting less time than the\rlength of an episode. Eligibility traces assist in the learning process; their\ronly consequence is that they affect the weight vector, and then the weight\rvector determines the estimated value.\nIn TD(��)��the eligibility trace\rvector is initialized to zero at the beginning of the episode, is incremented\ron each time step by the value gradient, and then fades away by yX:\ne-i= 0,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (12\r5)\net == 7Aet-i + Vv(St,wt),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0\r\u0026lt; t \u0026lt; T,\u0026nbsp;\u0026nbsp;\u0026nbsp; '\nwhere 7is the discount rate and ��is\rthe parameter introduced in the previous section. The eligibility trace keeps\rtrack of which components of the weight vector have contributed, positively or\rnegatively, to recent state valuations, where ��recent�� is defined in terms 7A. The trace is said to indicate the eligibility\rof each component of the weight vector for undergoing learning changes should a\rreinforcing event occur. The reinforcing events we are concerned with are the\rmoment-by-moment one-step TD errors. The TD error for state-value prediction is\n^t == Rt+i + 7��(St+i,wt) - v(St,wt).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (12.6)\n\r\r\r(12.7)\n\r\r\r\r\rIn TD(X), the weight vector is\rupdated on each step proportional to the scalar TD error and the vector\religibility trace:\nwt+i = wt + a^t et,\n\r\r\r\n\r\r\r\r\r\u0026nbsp;\nInput: the policy n to be\revaluated\nInput: a differentiable function v : S+ x Rd\rR such that v(terminal,-) = 0\nInitialize value-function weights\rw arbitrarily (e.g., w = 0)\nRepeat (for each episode):\nInitialize S\ne 0\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (An\rn-dimensional vector)\nRepeat (for each step of episode):\n.Choose A\u0026#12316;n(-|S)\n.Takeaction A,\robserve R, Sf.e �� 7Ae + V(0(S,w)\n.8�� R + 7v(S;,w) - v(S,w)\n. w �� w + a8e\nuntil Sfis terminal\nComplete pseudocode for TD(A) is given in the box,\rand a picture of its operation is suggested by Figure 12.5.\nTD(A) is oriented backward in\rtime. At each moment we look at the current TD error and assign it backward to\reach prior state according to how much that state contributed to the current\religibility trace at that time. We might imagine ourselves riding along the stream\rof states, computing TD errors, and shouting them back to the previously\rvisited states, as suggested by Figure 12.5. Where the TD error and traces come\rtogether, we get the update given by (12.7).\nTo better understand the backward view, consider what\rhappens at various values of A. If A = 0, then by (12.5) the trace at t is\rexactly the value gradient corresponding\n\r\nFigure 12.5:\rThe backward or mechanistic view. Each update depends on the current TD error\rcombined with eligibility traces of past events.\n\r\r\r\r\r\u0026nbsp;\nto St. Thus the TD(A) update (12.7) reduces to the\rone-step semi-gradient TD update treated in Chapter 9 (and, in the tabular\rcase, to the simple TD rule (6.2)). This is why that algorithm was called\rTD(0). In terms of Figure 12.5, TD(0) is the case in which only the one state\rpreceding the current one is changed by the TD error. For larger values of A,\rbut still A \u0026lt; 1, more of the preceding states are changed, but each more\rtemporally distant state is changed less because the corresponding eligibility trace\ris smaller, as suggested by the figure. We say that the earlier states are\rgiven less creditfor the TD error.\nIf A = 1, then the credit given to\rearlier states falls only by 7per\rstep. This turns out to be just the right thing to do to achieve Monte Carlo\rbehavior. For example, remember that the TD error, \u0026amp;, includes an\rundiscounted term of Rt+i. In passing this back k steps it needs to be\rdiscounted, like any reward in a return, by 7k, which is just what the falling\religibility trace achieves. If A = 1 and 7=1, then the eligibility traces do not decay at all with time. In\rthis case the method behaves like a Monte Carlo method for an undiscounted,\repisodic task. If A = 1, the algorithm is also known as TD(1).\nTD(1) is a way of implementing\rMonte Carlo algorithms that is more general than those presented earlier and\rthat significantly increases their range of applicability. Whereas the earlier\rMonte Carlo methods were limited to episodic tasks, TD(1) can be applied to\rdiscounted continuing tasks as well. Moreover, TD(1) can be performed\rincrementally and on-line. One disadvantage of Monte Carlo methods is that they\rlearn nothing from an episode until it is over. For example, if a Monte Carlo\rcontrol method takes an action that produces a very poor reward but does not\rend the episode, then the agent��s tendency to repeat the action will be\rundiminished during the episode. On-line TD(1), on the other hand, learns in an\rn-step TD way from the incomplete ongoing episode, where the n steps are all\rthe way up to the current step. If something unusually good or bad happens\rduring an episode, control methods based on TD(1) can learn immediately and\ralter their behavior on that same episode.\nIt is revealing to revisit the\r19-state random walk example (Example 7.1) to see how well TD(A) does in\rapproximating the off-line A-return algorithm. The results for both algorithms\rare shown in Figure 12.6. For each A value, if a is selected optimally for it\ror smaller, then the two algorithms perform virtually identically. If a is\rchosen larger, however, then the A-return algorithm is only a little worse\rwhereas TD(A) is much worse and may even be unstable. This is not a terrible\rproblem for TD(A) on this problem, as these higher parameter values are not\rwhat one would want to use anyway, but for other problems it can be a\rsignificant weakness.\n\r\r\rTD(�룩\n\r\r\r\r\r\r\r\rOff-line ��-return algorithm\n(from the previous section)\n\r\r\r\r\r\r\r\r0.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0.6\not\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; a\nFigure 12.6: 19-state Random walk results (Example 7.1): Performance\rof TD(X) alongside that of the off-line X-return algorithm. The two\ralgorithms performed virtually identically at low (less than optimal) a\rvalues, but TD(X) was worse at high a values.\n\r\r\r\r\r\r\r\r0.25 _______________________________________________________________________________ _________________________________________________ \n\r\r\r\r\r\r\r\r\u0026nbsp;\n\r0.55\n\r\rRMS error\n\r0.5\n0.45\n\r\rat the end\n\r\u0026nbsp;\n\r\rof the episode\n\r0.4\n\r\rover the first\n\r\u0026nbsp;\n\r\r10 episodes\n\r0.35\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\rLinear TD(A) has been proved to\rconverge in the on-policy case if the step-size parameter is reduced over time\raccording to the usual conditions (2.7). Just as discussed in Section 9.4,\rconvergence is not to the minimum-error weight vector, but to a nearby weight\rvector that depends on A. The bound on solution quality presented in that\rsection (9.13) can now be generalized to apply to any A. For thecontinuing\rdiscounted case,\n1 - 7X\n\r\r\rMSVE(w^)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026lt;\n\r\r\r\r\rminMSVE(w).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (12.8)\n1-7\n\r\r\rThat is, the asymptotic error is\rAs X approaches 1, the bound\n\r\r\r\r\rno more than i--^ times\rthe smallest possible error. approaches the minimum error (and it is loosest at\r��= 0). In practice, however,��=1 is often the poorest choice, as will be\rillustrated later in Figure 12.14.\nExercise 12.3 Some\rinsight into how TD(X) can closely approximate the off-line X-retum algorithm\rcan be gained by seeing that the latter��s error term (from (12.4)) can be\rwritten as the sum of TD errors (12.6) for a single fixed w. Show this, following the pattern of (6.6),\rand using the recursive relationship for the you obtained in Exercise 12.1.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n''Exercise 12.4 Although TD(X) only approximates the ��-�Źϴ�algorithm\rwhen done online, perhaps there��s a slightly different TD method that would\rmaintain the equivalence even in the on-line case. One idea is to define the TD\rerror instead as ^t == Rt+i + 7Vt(St+i)\r- Vt-i(St). Show that in this case the modified TDC') algorithm\rwould then achieve exactly\nAVt(St) = a[Gt��-Vt-i(St)\neven\rin the case of on-line updating with large a. In what ways might this modified\rTD(X) be better or worse than the conventional one described in the text?\rDescribe an experiment to assess the relative merits of the two algorithms.\u0026nbsp;\u0026nbsp; ��\n12.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rn-step Truncated A-return\rMethods\nThe off-line A-return algorithm is an important\rideal, but it��s of limited utility be\u0026shy;cause it uses the A-return (12.2), which\ris not known until the end of the episode. In the continuing case, the A-return\ris technically never known, as it depends on n- step returns for arbitrarily\rlarge n, and thus on rewards arbitrarily far in the future. However, the\rdependence gets weaker for long-delayed rewards, falling by 7A for each step of delay. A natural approximation\rthen would be to truncate the sequence after some number of steps. Our existing\rnotion of n-step returns provides a natural way to do this in which the missing\rrewards are replaced with estimated values.\nIn general, we define the truncatedA-returnfor time t, given data only up to some later\rhorizon, h, as\nh-t-i\nGi��h=\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (1- A)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; AiGt��t+n + Ah t iGt��h,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0^ t \u0026lt; h ^ T.(12.9)\nn=i\nIf you compare this equation with the A-return\r(12.3), it is clear that the horizon his playing the\rsame role as was previously played by T, the time of termination. Whereas in\rthe A-return there is a residual weighting given to the true return, here it is\rgiven to the longest available n-step return, the (h-t)-step return (Figure\r12.2).\nThe truncated A-return immediately gives rise to a\rfamily of n-step A-return algo\u0026shy;rithms similar to the n-step methods of Chapter\r7. In all these algorithms, updates are delayed by n steps and only take into\raccount the first n rewards, but now all the k-step returns are included for 1\u0026lt; k \u0026lt; n (whereas the earlier n-step\ralgorithms used only the n-step return), weighted geometrically as in Figure\r12.2. In the state- value case, this family of algorithms is known as truncated\rTD(A), or TTD(A). The compound backup diagram, shown in Figure 12.7, is similar\rto that for TD(A) (Fig\u0026shy;ure 12.1) except that the longest component backup is n\rsteps rather than all the way to the end of the episode. TTD(A) is defined by\r(cf. (9.14)):\nwt+n = wt+n-i + a G^��t+\u0026#8222; - v(St,wt+n-i) Vv(St,wt+\u0026#8222;-i),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0\r\u0026lt; t \u0026lt; T. (12.10)\nThis algorithm can be implemented efficiently so\rthat per-step computation does not scale with n (though of course memory must).\rMuch as in n-step TD methods, no updates are made on the first n - 1time steps, and n - 1additional updates are made upon termination.\rEfficient implementation relies on the fact that the k-step A-return can be\rwritten exactly as\nt+k-i\nGi��t+k �� v(St,wt��1)+ E (7A)i-t^t,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (12.11)\ni=t\nwhere\n\r\r\r(12.12)\n\r\r\r\r\r^t = Ri+i + 7^(Si+i,wi) - v(Si,wi-i).\n\r\rn-step truncated TD(A)\n\r\r\r\r\rSt\nAt\nSt+i\rRt+i At+i\nAt-i\n\r\r\r\r\r\r\r\r����\n\r\r\r\r\r\r\r\r��\n(i һ��)��\n\r\r\r\r\r\r\r\rC] St Rt\nXT-t-1\n\r\r\r\r\r\r\r\rAt+n-1 St+n\rRt+n\n\r\r\r\r\r\r\r\r��\nAn-1\n\r\r\r\r\r\r\r\r��\n(1 -��)��2\n\r\r\r\r\r\r\r\rif tʮn \u0026gt; T\n\r\r\r\r\r\r\r\r��\n��\n1-��\n\r\r\r\r\r\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rFigure 12.7: The backup diagram\rfor truncated TD(A).\nExercise 12.5 Several\rtimes in this book (often in exercises) we have established that returns can be\rwritten as sums of TD errors if the value function is held constant. Why is\r(12.11) another instance of this? Prove (12.11).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 12.6 The parameter A characterizes how fast\rthe exponential weighting in Figure 12.2 falls off, and thus how far into the\rfuture the A-return algorithm looks in determining its backup. But a rate\rfactor such as A is sometimes an awkward way of characterizing the speed of the\rdecay. For some purposes it is better to specify a time constant, or half-life.\rWhat is the equation relating A and the half-life, t����,the time by\rwhich the weighting sequence will have fallen to half of its initial value? ��\n12.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rRedoing Updates: The Online\rA-return Algorithm\nChoosing the truncation\rparameter n in Truncated TD(A) involves a tradeoff. n should be large so that\rthe method closely approximates the off-line A-return al\u0026shy;gorithm, but it should\ralso be small so that the updates can be made quicker and influence behavior\rquicker. Can we get the best of both? Well, yes, in principle we can, albeit at\rthe cost of computational complexity.\nThe idea is\rthat, on each time step as you gather a new increment of data, you go back and\rredo all the updates since the beginning of the current episode. The new\rupdates will be better than the ones you previously made because now they can\rtake into account the time step��s new data. That is, the updates are always\rtowards an n-step truncated A-return target, but they always use the latest\rhorizon. In each pass over that episode you can use a slightly longer horizon\rand obtain slightly better\nresults. Recall that the n-step truncated A-return is defined by\nh-t-1\n\r\r\u0026nbsp;\n\r\r\r\r\r(12.9)\n\r\r\r\r\r\r\r\rGi��h\n\r\r\r\r\r(1- A)XniGt��t+n + Ah t iGt��h\n\r\r\u0026nbsp;\n\r\rLet us step through how this\rtarget could ideally be used if computational com\u0026shy;plexity was not an issue. The\repisode begins with an estimate at time 0 using the weights wo from the end of the previous episode. Learning\rbegins when the data horizon is extended to time step 1. The target for the\restimate at step 0, given the data up to horizon 1, could only be the one-step return Go��i,which includes Ri and bootstraps from the\restimate v(Si,wo).\rIn (12.9), this is exactly what Gq��iis, taking the last part of the equation. Using this update target,\rwe construct wi. Then, after\radvancing the data horizon to step 2, what do we do? We have new data in the\rform of R2and S2, as well as the new wi, so now we can construct a better update target Gq��2for the first update from So as well as a better\rupdate target Gq��2for the\rsecond update from Si. We perform both of these updates in sequence to produce w2. Now we advance the horizon to step 3 and repeat,\rgoing all the way back to produce three new updates and finally w3, and so on.\nThis\rconceptual algorithm involves multiple passes over the episode, one at each\rhorizon, each generating a different sequence of weight vectors. To describe it\rclearly we have to distinguish between the weight vectors computed at the\rdifferent horizons. Let us use wf to denote the weights used to generate the value at time t in the\rsequence at horizon h. The first weight vector in each sequence is that\rinherited from the previous episode, wh == wo, and the last weight vector in each sequence defines the ultimate\rweight-vector sequence of the algorithm wh == wh. At the final\rhorizon h = T we obtain the final weights wt == wT which will be passed on to form the initial\rweights wo of the next episode. With these conventions, the\rthree first sequences described in the previous paragraph can be given\rexplicitly:\nh = 1:w]��= w^ + a Gq��1- v(So,wi) Vv(So,wi),\n\r\r\r\r\r22 wi = wo + a\nw2== w2+ a\n\r\r\r\r\r\r\r\rw^)Vv(So,w2),\nw2)\u0026quot;| Vv(Si,w2),\n\r\r\r\r\r\r\r\rG0��2- v(So G^��2- v(Si\n\r\r\r\r\r\r\r\rh=3\n\r\r\r\r\r\r\r\rw\nw\nw3\n\r\r\r\r\r\r\r\rw0)w0)w0)\n\r\r\r\r\r\r\r\rVv(So,w2),\nVV(Si,w3),\n��V(S2,w3).\n\r\r\r\r\r\r\r\rV(So\nv(Si\nV(S2\n\r\r\r\r\r\r\r\rG0\nGo��3 -\nG0\nG1��3 - G0\nG2��3 -\n\r\r\r\r\r\r\r\rw0+ a w3+ a 3\nw2+ a\n\r\r\r\r\r\r\r\rh=2\n\r\r\r\r\r\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rThe general\rform for the update is\n\r\r\r\r\r(12.13)\n\r\r\r\r\r\r\r\rh+1= wh + a\n\r\r\r\r\r\r\r\rw\n\r\r\r\r\r\r\r\rG0��h- V(St,wth) VV(St,wth),\r0 \u0026lt; t\u0026lt;h \u0026lt; T.\n\r\r\r\r\r\u0026nbsp;\n\ra\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; a\n\r\r\rOn-line ��-return algorithm\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Off-line��-return algorithm\n\r\r\r\r\rFigure 12.8: 19-state Random walk\rresults (Example 7.1): Performance of online and off\u0026shy;line X-return algorithms.\rThe performance measure here is the MSVE at the end of the episode, which\rshould be the best case for the off-line algorithm. Nevertheless, the on-line\ralgorithm performs subtlely better. For comparison, the X = 0 line is the same\rfor both methods.\nThis update, together with wt == w\\defines the online X-return\ralgorithm.\nThe online X-return algorithm is fully online,\rdetermining a new weight vector wt at each step t during an episode, using only\rinformation available at time t. It��s main drawback is that it is\rcomputationally complex, passing over the entire episode so far on every step.\rNote that it is strictly more complex than the off-line X-return algorithm,\rwhich passes through all the steps at the time of termination but does not make\rany updates during the episode. In return, the online algorithm can be expected\rto perform better than the off-line one, not only during the episode when it\rmakes an update while the off-line algorithm makes none, but also at the end of\rthe episode because the weight vector used in bootstrapping (in G;^) has had a\rgreater number of informative updates. This effect can be seen if one looks\rcarefully at Figure 12.8, which compares the two algorithms on the 19-state\rrandom walk task.\n12.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rTrue Online TD(A)\nThe on-line X-return algorithm just presented is\rcurrently the best performing temporal- difference algorithm. As presented,\rhowever, it is very complex. Is there a way to invert this forward-view\ralgorithm to produce an efficient backward-view algorithm using eligibility\rtraces? It turns out that there is indeed an exact computationally congenial\rimplementation of the on-line X-return algorithm for the case of linear\rfunction approximation. This implementation is known as the true online TD(X)\ralgorithm because it is ��truer�� to the idea of the online TD(X) algorithm,\rtruer even than the TD(X) algorithm itself.\nThe derivation of true on-line\rTD(X) is a little too complex to present here (see the next section and the\rappendix to the paper by van Seijen et al., 2016) but its\nstrategy is simple. The sequence of weight vectors produce by the\ron-line A-return algorithm can be arranged in a triangle:\n\r\r\r\r\r(12.14)\n\r\r\r\r\r\r\r\rw0\nw0\nw0\nw0\nw0\n\r\r\r\r\r\r\r\rw\nw\nw\n\r\r\r\r\r\r\r\rw| w��\n\r\r\r\r\r\r\r\rWT\nT3\nw\nT2\n.w\nTl\n.w\nTo\n\r\r\r\r\r\r\r\r\r\r\r\r\r\r\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\r\r\r\rthen we arrive at the\n\r\r\r\r\rOne row of this triangle is produced\ron each time step. Really only the weight vectors on the diagonal, the w^ need\rto be produced by the algorithm. The first, w0, is the input, the\rlast, w^, is the output, and each weight vector along the way, wf, plays a role\rin bootstrapping in the n-step returns of the updates. In the final algorithm\rthe diagonal weight vectors are renamed without a superscript, wt == w^. The\rstrategy then is to find a compact, efficient way of computing each w\\from the one before. If this is done, for the linear\rcase in which {)(s,w) = wTx(s), true online TD(A) algorithm:\n\r\r\r(12.15)\n\r\r\r\r\rwt+i = wt + a8tet + a\r(wtTxt - w^��xt) (et\r- xt),\nwhere we have used the shorthand xt == x(St), 8t is defined as in TD(A) (12.6), and et is defined by\n\r\r\ret\n\r\r\r\r\ryAet-i + (1- aYAe^xt) xt.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (12.16)\nThis algorithm has been proven to produce exactly the\rsame sequence of weight vec\u0026shy;tors, wt, 0 \u0026lt; t \u0026lt; T, as the on-line A-return\ralgorithm (van Siejen et al. 2016). Thus the results on the random walk task on\rthe left of Figure 12.8 are also its results on that task. Now, however, the\ralgorithm is much less expensive. The memory re\u0026shy;quirements of true online TD(A)\rare identical to those of conventional TD(A), while the per-step computation is\rincreased by about 50% (there is one more inner prod\u0026shy;uct in the\religibility-trace update). Overall, the per-step computational complexity\rremains of O(d), the same as TD(A). Pseudocode for the complete algorithm is\rgiven in the box on the next page.\nThe eligibility trace (12.16) used in true online TD(A)\ris called a dutch traceto distinguish it from the trace (12.5) used in TD(A), which is\rcalled an accumulating trace.Earlier work often used a third kind of trace called the replacing\rtrace, defined only for the tabular case or binary feature vectors such as are\rproduced by tile coding. The replacing trace is defined on a\rcomponent-by-component basis depending on whether the component of the feature\rvector was 1or 0:\n\u0026#12316;={YAei,t-i otSwi\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (12.17)\nNow, however, use of the replacing trace it\rdeprecated; a dutch trace should almost always be used instead.\n\r\rTrue Online TD(A)\rfor estimating wTx ^ Vn\nInput: the policy n\rto be evaluated\nInitialize value-function weights w arbitrarily (e.g., w = 0) Repeat (for each episode):\n\r\r\rx\n(an n-dimensional vector)\n(a scalar temporary variable)\nof the next state)\n\r\r\r\r\rInitialize state and obtain\rinitial feature vector e 0\nVold��\r0\nRepeat (for each step of episode):\n| Choose A \u0026#12316;n\n| Take action A, observe R, x!(feature vector | V�� wTx |\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Vf�� wTx;\n|\u0026nbsp;\u0026nbsp; 5 �� R + 7W - V\n| e �� 7Ae + (1- a7A eTx) x | w �� w + a(5 + V - Vold)e - a(V - V\u0026gt;ld)x |\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Vold�� V'\n| x �� x'\nuntil x' = 0(signaling arrival at a\rterminal state)\n12.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rDutch Traces in Monte Carlo\rLearning\nAlthough eligibility traces are\rclosely associated historically with TD learning, in fact they have nothing to\rdo with it. In fact, eligibility traces arise even in Monte Carlo learning, as\rwe show in this section. We show that the linear MC algorithm (Chapter 9),\rtaken as a forward view, can be used to derive an equivalent yet com\u0026shy;putationally\rcheaper backward-view algorithm using dutch traces. This is the only\requivalence of forward- and backward-views that we explicitly demonstrate in\rthis book. It gives some of the flavor of the proof of equivalence of true\ronline TD(A) and the on-line A-return algorithm, but is much simpler.\nThe linear version\rof the gradient Monte Carlo prediction algorithm (page 216) makes the following\rsequence of updates, one for each time step of the episode:\nwt+i == wt + a G -\rwfxt xt,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0\r\u0026lt; t \u0026lt; T.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (12.18)\nTo make the example a simpler, we assume here that\rthe return G is a single reward received at the end of the episode (this is why\rG is not subscripted by time) and that there is no discounting. In this case\rthe update is also known as the least mean square (LMS) rule. As a Monte Carlo\ralgorithm, all the updates depend on the final reward/return, so none can be\rmade until the end of the episode. The MC algorithm is an offline algorithm and\rwe do not seek to improve this aspect of it. Rather we seek merely an\rimplementation of this algorithm with computational advantages. We will still\rupdate the weight vector only at episode��s end, but we will do somecomputation\rduring each step of the episode and less at its end. This will give a more\requal distribution of computation��O(d) per step��and also remove the need to\rstore the feature vectors at each step for use later at the end of each\repisode. Instead, we will introduce an additional vector memory, the\religibility trace, keeping in it a summary of all the vectors seen so far. This\rwill be sufficient, at episode��s end to efficiently recreate exactly the same\roverall update as the sequence of MC updates (12.18).\nwt = wt-i + a (G - wr_ixr_i) xt-i\n=wt-i +\raxT-i (-xj_iwr_i) + aGxT-i =(I - axT_ixj_i) wt-i + aGxT_i =Ft\r_iwt_i + aGxT _i\nI - axtxj is a forgetting,or fading,matrix. Now, recursing,\nwhere Ft\nFt_i (Ft_2wt_2+ aGx^_2) + Gax^_i\rFt _iFt-2wt-2+ aG (Ft\r-ixt-2+ xt _i)\nFt_iFt-2(Ftswt-3+ aGx^-3) + aG (Ft-ixt-2+ xt_i)\nFt_iFt-2Ft_3wt-3+ aG (Ft-iFt_2xt-3+ Ft-ixt-2+\rxt_i)\nt-i\nFt_iFt_2�� �� �� Fowo + aG\u0026nbsp;\u0026nbsp; Ft_iFt-2�� �� �� Fk+ix^\nv\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; arci\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; '\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ,\neT-i\n=aT _i + aGeT _i,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (12.19)\nwhere aT_i and eT_i are the values at time T\r-1 of two auxilary memory vectors that can be updated incrementally without\rknowledge of G, and with O(d) complexity per time step. The et vector is in fact a dutch-style eligibility trace. It\ris initialized to eo =.\rxo and then updated according to\net ^=E FtF\rt-i ' ' ' Fk+ixk,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 1\u0026lt; t \u0026lt; T\nk=0 t-i\n=^ FtFt-i �� �� �� Ffc+ixfc + axt\n,t-i . . . \u0026#8226;\nk=0\nt-i\nFt ^ Ft-iFt-2�� �� �� Ffc+ixfc+ axt fc=o Ftet-i + xt\n(I - axtxj ) et-i + xt\n=et-i - axtx;et-i + xt =et-i - a (e^xt) xt + xt =et-i + (1 - ae^xt)\rxt,\nwhich is the dutch\rtrace for the case of 7A = 1\r(cf. Eq. 12.16). The at auxilary vector is initialized to ao = wo and then\rupdated according to\nat = FtFt-i ��Fowo = Ftat-i = at-i - axtxfat-i,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 1\u0026nbsp; \u0026lt;t\u0026lt;T.\u0026nbsp; (12.20)\nThe auxiliary\rvectors, at and et, are updated on each time step t \u0026lt; T and then, at time T when Gis observed, they are used in (12.19) to compute\rwt. In this way we achieve exactly the same final result as the MC/LMS\ralgorithm with poor computational properties (12.18), but with an incremental\ralgorithm whose time and memory complexity per step is O(d).\rThis is surprising and intriguing because the notion of an eligibility trace\r(and the dutch trace in particular) has arisen in a setting without temporal-difference\r(TD) learning (in contrast to Van Seijen \u0026amp; Sutton\r2014). It seems eligibility traces are not specific to TD learning at all; they\rare more fundamental than that. The need for eligibility traces seems to arise\rwhenever one tries to learn long-term predictions in an efficient manner.\n12.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rSarsa(A)\nVery few changes in the ideas already presented in\rthis chapter are required in order to extend eligibility-traces to action-value\rmethods. To learn approximate action values, q(s, a, w), rather than\rapproximate state values, v(s,w), we need to use the action-value form of the n-step return, from Chapter 10:\nGt:t+n = Rt+l + ... + 7^ lRt+n + T'KSt+n, At+n, wt+n-l),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (10.4)\nfor all n and t such that n \u0026gt; 1\rand 0 \u0026lt;\rt \u0026lt; T -n. Using this, we can\rform the action- value form of the truncated A-return, which is otherwise identical to the state-value form\r(12.9). The action-value form of the off-line A-return algorithm (12.4) simply uses qrather than v:\n\r\r\u0026nbsp;\n\r\r\r\r\rA\n\r\r\r\r\r\r\r\rwt+i = wt + a\n\r\r\r\r\r-q(St, Atwt) Vq(St, At, wt), t = 0,����T - 1,\u0026nbsp;\u0026nbsp; (12.21)\n\r\r\u0026nbsp;\n\r\rwhere G^ == G^��^.The compound backup diagram for this forward view\ris shown in Figure 12.9. Notice the similarity to the diagram of the TD(A)\ralgorithm (Fig\u0026shy;ure 12.1). The first backup looks ahead one full step, to the\rnext state-action pair, the second looks ahead two steps, to the second\rstate-action pair, and so on. A final backup is based on the complete return.\rThe weighting of each n-step backup in the A-return is just as in TD(A) and the\rA-return algorithm (12.3).\n\r\r\r(12.7)\n\r\r\r\r\rThe temporal-difference method for\raction values, known as Sarsa(A), approxi\u0026shy;mates this forward view. It has the\rsame update rule as given earlier for TD(A):\nwt+i = wt + a8t et,\n\r\rSarsa(A)\n\r\r\r\r\r1 - A\n\r\r\r\r\rS At St+i Rt+i At+i St+2 Rt\r+ 2 At+2\n\r\r\u0026nbsp;\n\r\r(1 -��)��\n\r\r\u0026nbsp;\n\u0026nbsp;\n\r\r\r\r\r(i -��)��2\n\r\r\r\r\r��At-i\nI ISt Rt\nAt -\nFigure 12.9: Sarsa(A),s backup diagram.\rCompare with Figure 12.1.\nexcept, naturally, using the action-value form of\rthe TD error:\n5t = Rt+i + 7q(St+i,\rAt+i, wt) - q(St, At, wt),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (12.22)\nand the action-value form of\rthe eligibility trace:\ne-i.=. 0,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (12.23)\net == 7Aet-i + Vq(St, At, wt),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0 \u0026lt; t \u0026lt; T\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (�A�A����\n(or the alternate replacing\rtrace given by (12.17)). Complete pseudocode for Sarsa(A) with linear function\rapproximation, binary features, and either accumulating or re\u0026shy;place traces is\rgiven in the box. This pseudocode highlights a few optimizations possible in\rthe special case of binary features.\nSarsaQ) with binary\rfeatures and linear function approximation for estimating q^ or q^\nInput: a\rfunction F(s, a) returning the set of (indices of) active features for s, a Input: a policy n to be evaluated, if any\nInitialize parameter vector w = (wi,..., wn) arbitrarily (e.g., w = 0)\nLoop for each episode:\nInitialize S\nChoose A \u0026#12316;n(-|S) or e-greedy according to q(S, ��, w) e 0\nLoop for each step of episode:\nTake action A, observe R, Sf5�� R\nLoop for iin F(S, A):\n\r\r\r(accumulating traces) (replacing\rtraces)\n\r\r\r\r\r\r\r\rnear greedily\u0026nbsp;\u0026nbsp;\u0026nbsp; q(S;, ��, w)\n5 �� 5 + 7Wi\n\r\r\r\r\r5 �� 5 - Wi ei �� ei + 1 or ei �� 1\rIf S1is terminal then: w �� w + a5 e Go\rto next episode Choose A;\u0026#12316;n(-|S;)\ror Loop for i in F(SA;): w �� w + a5 e e �� T^e\nS\u0026nbsp; t S;; At#\nExample 12.1: Traces in Gridworld The use of eligibility traces can substan\u0026shy;tially increase the efficiency\rof control algorithms over one-step method and even over n-step methods. The\rreason for this is illustrated by gridworld example below.\nAction values increased Action\rvalues increased Action values increased Path taken\u0026nbsp;\u0026nbsp; by\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; one-step\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Sarsaby\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 10-step\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Sarsaby Sarsa(!) with !=0.9\n\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r*\n\r-\n\r\u0026nbsp;\n\rt\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r*\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r+\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r-\n\r\u0026raquo;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r+\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026#8226;\n\r\u0026nbsp;\n\r'\n\r\u0026nbsp;\n\r\u0026nbsp;\n\rG\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\u0026nbsp;\n\r\r\r\r\u0026nbsp;\nThe first panel shows the path\rtaken by an agent in a single episode. In this example the values were all\roriginally zero, and all rewards were zero except for a positive reward at the\rgoal location marked by the G. The arrows in\rthe other panels show which actions would be strengthened upon reaching the\rgoal by various algorithms. A one-step method would update only the last\raction, whereas an n-step method would equally update the last n actions, and\ran eligibility trace method would update all the actions up to the beginning of\rthe episode to different degrees, fading with recency.\nThe fading strategy is often the best tradeoff,\rstrongly learning how to reach the goal from the right, yet not as strongly\rlearning the roundabout path to the goal from the left that was taken in this\repisode.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExercise 12.7 Modifiy the pseudocode for Sarsa(A) to\ruse dutch traces (12.16) alone without the other features of a true online\ralgorithm. Continue to assume linear function approximation and binary\rfeatures.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nExample 12.2: Sarsa(A) on Mountain Car Figure 12.10 shows results\rwith Sarsa(A) on the Mountain Car task introduced in Example 10.1. The function\rap\u0026shy;proximation, action selection, and environmental details were exactly as in\rChap\u0026shy;ter 10such that these results can be numerically compared\rwith those for an n-step Sarsa. Those results varied the backup length n\rwhereas here for Sarsa(A) we vary the trace parameter A, which plays a similar\rrole. The fading-trace bootstrapping strategy of Sarsa(A) appears to result in\rmore efficient learning on this problem.\n\r\r\rn-step Sarsa\n\r\r\r\r\rSarsa(����with\rreplacing traces\n\r\n\r\r\r\r\r\rMountain\rCar\nSteps per episode\naveraged over first 50 episodes and\r100 runs\n\r\r\r\r\r\rFigure 12.10: Early performance on\rthe Mountain Car task of Sarsa(A) with replacing traces and n-step Sarsa\r(copied from Figure 10.4) as a function of the step size, a.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n\r\r\r\r\r\u0026nbsp;\nThere is also an action-value\rversion of our ideal TD method, the onlineA-return algorithm presented in Section 12.4. Everything in that\rsection goes through without change other than to use the action-value form of\rthe n-step return given at the beginning of this section. In the case of linear\rfunction approximation, the ideal algorithm again has an exact, efficient O(d)\rimplementation, called True Online Sarsa(A).The analyses in Sections 12.5 and 12.6 carry through\rwithout change other than to use state-action feature vectors xt =\rx(St, At) instead of state feature vectors xt\r= x(St). The pseudocode for this algorithm given in the a box on the\rnext page. Figure 12.11compares the performance of various versions of Sarsa(A) on the Mountain Car\rexample.\n\r\rTrue Online Sarsa(A)\rfor estimating wTx ^ ��or q^\nInput: a\rfeature function x : S+ x ARd s.t. x(terminal, ��) = 0 Input: the policy n to be evaluated,\rif any\nInitialize parameter w arbitrarily (e.g., w = 0)\nLoop for each episode:\nInitialize S\nChoose A \u0026#12316;n(-|S)\ror near greedily from S using w; x �� x(S, A) e �� 0\nQoid �� 0\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (ascalartemporary\rvariable)\nLoop for each step of episode:\n|Take\raction A, observe R, Sح\n|Choose\rA'\u0026#12316;n(-|S;) or near\rgreedily from Sfusing w; x; �� x(S\u0026#12316;A;)\n|Q�� wTx\n|Q'\r�� wTx'\n|8�� R + yQ' - Q\n|e\r�� 7Ae + (1- aYA\reTx) x\n1\u0026nbsp;\u0026nbsp;\u0026nbsp; w\r�� w+ a(8+ Q - Qo1d)e\r- a(Q - Qo1d)x\nI\u0026nbsp;\u0026nbsp;\u0026nbsp;\rQo1d �� Q'\n|x �� xZ\n|AL ��#\nuntil S' is terminal\n\r\n\r\r\r\r\r\rMountain Car\nReward\rper episode averaged over\rfirst 20 episodes and 100 runs\n\r\r\r\r\r\rSarsa(����with\rreplacing traces\n\r\r\r\r\r\rFigure\r12.11: Summary comparison of Sarsa(A) algorithms on the Mountain Car task.\rTrue Online Sarsa(A) performed better than regular Sarsa(A) with both\raccumulating and replacing traces. Also included is a version of Sarsa(A)\rwith replacing traces in which, on each time step, the traces for the state\rand the actions not selected were set to zero.\n\r\r\r\r\r\u0026nbsp;\n12.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rVariable A and 7\nWe are starting now to reach the end of our\rdevelopment of fundamental TD learning algorithms. To present the final\ralgorithms in their most general forms, it is useful to generalize the degree\rof bootstrapping and discounting beyond constant parameters to functions\rpotentially dependent on the state and action. That is, each time step will\rhave a different ��and 7, denoted Xt and 7t. We change notation now so that ��:S x A[0,1] is now a whole function from states and actions to the unit\rinterval such that Xt == XfSt, At), and similarly, 7: S [0,1] is a function from\rstates to the unit interval such that 7t == 7(St).\nThe latter generalization, to state-dependent\rdiscounting, is particularly\rsignificant because it changes the return, the fundamental random variable\rwhose expectation we seek to estimate. Now the return is defined more generally\ras\nGt = Rt+i + 7t+iGt+i\n=Rt+i + 7t+iRt+2 + 7t+i7t+2Rt+3 + 7t+i 71+2 7t+3Rt+4 +\n\u0026nbsp;TOC \\o \u0026quot;1-5\u0026quot; \\h \\z ^\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; k\n=^ Rk+i\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 7i,\nk=t\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; i=t+i\nwhere, to assure the sums are finite, we require that\rH^k=t 7k = 0with probability one for all t. One convenient\raspect of this definition is that it allows us to dispense with episodes, start\rand terminal states, and T as a special cases and quantities. A terminal state\rjust becomes a state at which 7(s) = 0\rand which transitions to the start state. In that way (and by choosing 7(-) as a constant function) we can recover the\rclassical episodic setting as a special case. State dependent discounting\rincludes other prediction cases such as soft\rtermination, when we seek to\rpredict a quantity that becomes complete but does not alter the flow of the\rMarkov process. Discounted returns themselves can be thought of as such a\rquantity, and state de\u0026shy;pendent discounting is a deep unification of the\repisodic and discounted-continuing cases. (The undiscounted-continuing case\rstill needs some special treatment.)\nThe generalization to variable\rbootstrapping is not a change in the problem, like discounting, but a change in\rthe solution strategy. The generalization affects the X-returns for states and\ractions. The new state-based X-return can be written recur\u0026shy;sively as\nG;s= Rt+i + 7t+i ((1- Xt+i)(\u0026amp;(St+i,wt) + Xt+iG��+1) ,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (12.24)\nwhere now we have added\rthe ��s�� to the superscript X to remind us that this is a return that bootstraps\rfrom state values, distinguishing it from returns that boot\u0026shy;strap from action\rvalues, which we present below with ��a�� in the superscript. This equation says\rthat the X-return is the first reward, undiscounted and unaffected by\rbootstrapping, plus possibly a second term to the extent that we are not\rdiscount\u0026shy;ing at the next state (that is, according to 7t+i��recall that this is zero if the next state is terminal). To the\rextent that we aren��t terminating at the next state, wehave a\rsecond term which is itself divided into two cases depending on the degree of\rbootstrapping in the state. To the extent we are bootstrapping, this term is\rthe estimated value at the state, whereas, to the extent that we not\rbootstrapping, the\nnext time step. The action-based X-return is\reither the\nterm is the X-return for the Sarsa form\n\r\r\r(12.25)\n\r\r\r\r\rG;a= Rt+i + 7t+i ((1 - Xt+i)3(St+i,\rAt+i, wt) + Xt+iG;+i or the Expected Sarsa\rform,\n\r\r\u0026nbsp;SHAPE \u0026nbsp;\\* MERGEFORMAT \r\r\r\u0026nbsp;\n\r\r\r\r\r\r\u0026nbsp;\n\r\r\r\r\r-Xt+i)Ot+i + Xt+iGj^i\n\r\r\r\r\r\r\r\r(12.26)\n\r\r\r\r\rG;a= Rt+i + 7t+i ((1\nwhere\n\r\r\u0026nbsp;\n\r\r\r\r\r(12.27)\n\r\r\r\r\rQt == ^n(a|St)q(St,a, wt_i).\nExercise 12.8 Generalize\rthe three recursive equations above to their truncated versions, defining G��h and G;h.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n12.9\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rOff-policy Eligibility Traces\nThe final step is to incorporate importance\rsampling. Unlike in the case of n-step methods, for full non-truncated\rX-returns one does not have a practical option in which the importance sampling\ris done outside the target return. Instead, we move directly to the\rbootstrapping generalization of per-reward importance sampling (Sec\u0026shy;tion 7.4).\rIn the state case, our final definition of the X-return generalizes (12.24),\rafter the model of (7.10), to\nG;s= pt(Rt+i\r+ 7t+i((1 -Xt+i)v(St+i,wt)\r+ Xt+iG��+1)) +(1 -pt)v(St,wt)\r(12.28)\nwhere pt =ū��)is the usual single-step importance sampling ratio. Much like the\rother returns we have seen in this book, the truncated version of this return\rcan be approximated simply in terms of sums of the state-based TD error,\n5f = Rt+i +\r7t+iV(St+i,wt) - v(St,wt),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (12.29)\nas\n^ k\nG��s~ v(St,wt) + ptY^5s n 7iXiPi\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (12.30)\nk=t i=t+i\nwith the approximation becoming exact if the\rapproximate value function does not change.\nExercise 12.9 Prove\rthat (12.30) becomes exact\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; if\u0026nbsp;\u0026nbsp; thevaluefunction\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; does\rnot\nchange. To save\rwriting, consider the case of\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; t\u0026nbsp; =0,and\u0026nbsp; usethenotation\rVk ==\nv(Sk ,w).\nExercise 12.10 The truncated version of general off-policy is\rdenoted GAh. Guess the correct equation, based on (12.30).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nThe above form of the A-return is convenient to use in a\rforward-view update,\n\r\r\u0026nbsp;\n\r\r\r\r\r-rAs\n\r\r\r\r\r\r\r\rwt+i = wt + a\n\r\r\r\r\r-V(St,wt)) Vv(St,wt)\n\r\r\u0026nbsp;\n\r\r~\rwt + apt حE\nv k=t\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; i=t+l\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; )\nwhich to the\rexperienced eye looks like an eligibility-based TD updateһthe product is like an eligibility trace and it is\rmultiplied by TD errors. But this is just one time step of a forward view. The\rrelationship that we are looking for is that the forward-view update, summed\rover time, is approximately equal to a backward-view update, summed over time\r(this relationship is only approximate because again we ignore changes in the\rvalue function). The sum of the forward-view update over time is\nyi(wt+i- wt)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; apt8s\rVv(St,wt^ n 7i AiPi\n\u0026nbsp;TOC \\o \u0026quot;1-5\u0026quot; \\h \\z t=l\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; t=l\rk=t\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; i=t+l\n^ k\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; k\n=naPtV^(St ,wt)8s\rn 7i\rAiPi\nk=lt=l\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; i=t+l\n(using the summationrule: eLx ELt = ELx E*k=x) ^k\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; k\na8SX!ptV^(St,wt^ n 7iAiPi, k=lt=l\u0026nbsp;\u0026nbsp;\u0026nbsp; i=t+l\nwhich would be in\rthe form of the sum of a backward-view TD update if the entire expression from\rthe second sum left could be written and updated incrementally as an\religibility trace, which we now show can be done. That is, we show that if this\rexpression was the trace at time k, then we could update it from its value at\rtime k- 1 by:\nek\n^^PtW(St,wt)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; YiAiPi\n\u0026nbsp;TOC \\o \u0026quot;1-5\u0026quot; \\h \\z t=l\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; i=t+l\nk-1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; k\n\r\r\r+ Pk V-0(Sk ,wk)\n7 iAiPi + Pk V-0(Sk ,wk)\n\r\r\r\r\r^ptVv(St,wt^\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; YiAiPi\nt=l\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; i=t+l\nk-l\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; k-l\n7 k Ak Pk[ ptV^(St,wt)n\n=t+i\nek-1\n=Pk (7k Ak\rek-i +\rVv(Sk ,wk)),\nwhich, changing the index from k to t, is the general accumulating trace update for state values:\n\r\r\r(12.31)\n\r\r\r\r\ret == pt(7tAtet-i + ViD(St,wt^,\n\r\rThis eligibility trace, together with the usual\rsemi-gradient parameter-update rule for TD(A) (12.7), forms a general TD(A)\ralgorithm that can be applied to either on-policy or off-policy data. In the\ron-policy case, the algorithm is exactly TD(A) because pt is alway 1 and\r(12.31) becomes the usual accumulating trace (12.5) (extended to variable A and\r7). In the off-policy case, the algorithm often\rworks well but, as an semi-gradient method, is not guaranteed to be stable. In\rthe next few sections we will consider extensions of it that do guarantee\rstability.\nA very similar series of steps can be followed to\rderive the off-policy eligibility traces for action-value\rmethods and corresponding general Sarsa(A) algorithms. One can start with\reither recursive form for the general action-based A-return, (12.25) or\r(12.26). Let's use the latter here, as it is more different than the\rstate-based case that we have already done. We extend (12.26) to the off-policy\rcase after the model of (7.11) to produce\n\r\r\r-rAa\n\r\r\r\r\rRt+i\r+ 7t+i ((1- At+i)6t+i + At+i (pt+iG��+i\r+ (1-Pt+i)6t+i)) (12.32)\nwhere Qt+i is as given by\r(12.27). Again the A-return can be written approximately as the sum of TD\rerrors,\n\r\r\r-rAa\n\r\r\r\r\r~ q(St, At, wt) + L �{II 7iAiPi,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (12.33)\nk=t\u0026nbsp;\u0026nbsp;\u0026nbsp; i=t+1\nThis time using the expectation-based form of the\rTD error,\n= Rt+1+ 71+1^51+1 - q(St; At, wt).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (12.34)\nAs before, the approximation becomes exact if the\rapproximate value function does not change.\nExercise 12.11 The\rtruncated version of general off-polincy is denoted G;^. Guess the correct\requation, based on (12.33).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nUsing steps entirely analogous to those for the\rstate case, one can write a forward- view update based on (12.33), transform\rthe sum of the updates using the summation rule, and finally derive the following\rform for the eligibility trace for action values:\net = 7tAtPtet-i + Vq(St, At, wt),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (12.35)\nThis eligibility trace, together with the usual\rsemi-gradient parameter-update rule (12.7), forms a general Expected Sarsa(A)\ralgorithm that can be applied to either on- policy or off-policy data though,\rin the off-policy case it is not stable unless combined with one of the methods\rpresented in the following sections.\nExercise 12.12 Show in\rdetail the steps outlined above for deriving (12.35) from (12.33).��\nExercise 12.13 Show how similar steps can be followed starting\rfrom the Sarsa form of the action-based A-return (12.25) to derive the same\religibility trace algorithm as (12.35), but with the Sarsa TD error:\n��=Rt+1+ 7q(St+i, At+i, wt) -\rq(St, At, wt),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (12.36)\nto establish a general Sarsa(X) algorithm, applicable to both\ron-policy and off-policy data, that is the same as the Sarsa(X) algorithm that\rpresented in Section 12.7 in the on-policy case with constant X and 7.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nAt X = 1, these\ralgorithms become closely related to corresponding Monte Carlo algorithms. One\rmight expect that an exact equivalence would hold for episodic problems and\roff-line updating, but in fact the relationship is subtler and slightly weaker\rthan that. Under these most favorable conditions still there is not an episode\rby episode equivalence of updates, only of their expectations. This should not\rbe surprising as these method make irrevocable updates as a trajectory unfolds,\rwhereas true Monte Carlo methods would make no update for a trajectory if any\raction within it has zero probability under the target policy. In particular,\rall of these methods, even at X = 1, still bootstrap in the sense that their\rtargets depend on the current value estimates��its just that the dependence cancels\rout in expected value. Whether this is a good or bad property in practice is\ranother question. Recently methods have been proposed that do achieve an exact\requivalence (Sutton, Mahmood, Precup and van Hasselt, 2014). These methods\rrequire an additional table of ��provisional values�� that keep track of updates\rwhich have been made but may need to be retracted (or emphasized) depending on\rthe actions taken later. The state and state-action versions of these methods\rare called PTD(X) and PQ(X) respectively, where the ��P�� stands for Provisional.\nThe practical\rconsequences of all these new off-policy methods have not yet been established.\rUndoubtedly, issues of high variance will arise as they do in all off-policy\rmethods using importance sampling (Section 11.9).\nIf X \u0026lt; 1,\rthen all these off-policy algorithms involve bootstrapping and the deadly triad\rapplies (Section 11.3), meaning that they can be guaranteed stable only for the\rtabular case, for state aggregation, and for other limited forms of function\rapproxi\u0026shy;mation. For linear and more-general forms of function approximation the\rparameter vector may diverge to infinity as in the examples in Chapter 11. As we discussed there, the challenge of off-policy\rlearning has two parts. Off-policy eligibility traces deal effectively with the\rfirst part of the challenge, correcting for the expected value of the targets,\rbut not at all with the second part of the challenge, having to do with the\rdistribution of updates. Three algorithmic strategies for meeting the second part\rof the challenge of off-policy learning with eligibility traces are presented\rin the next three sections.\nExercise 12.14 What are the dutch-trace and replacing-trace versions\rof off-policy eligibility traces for state-value and action-value methods?\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\n12.10\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rWatkins��sQ(A) to Tree-Backup(A)\n\r\r\rg to eligibility es in the usual after the first n Figure 12.12. licy\rversion of it to arbitrary our treatment In Chapter 7, Tree Backup, ng. It\rremains well call Tree- to Q-learning ough it can be\niagram in Fig- ed in the usual\rled equations, parameters, it action values,\n\r\r\r\r\rSeveral methods have been proposed\rover the years to extend Q-learnii traces. The original is Watkins's Q(A),which decays its eligibility trac way as long as a\rgreedy action was taken, then cuts the traces to zero non-greedy action. The\rbackup diagram for Watkins��s Q(A) is shown i] In Chapter 6, we unified Q-learning and Expected Sarsa in the\roff-po the latter, which includes Q-learning as a special case, and generalizes\rtarget policies, and in the previous section of this chapter we completed of\rExpected Sarsa by generalizing it to off-policy eligibility traces. however, we\rdistinguished multi-step Expected Sarsa from multi-step where the latter\rretained the property of not using importance sampli then to present the\religibility trace version of Tree Backup, which we Backup(A),or TB(A)for short. This is arguable the true successor because it retains\rits appealing lack of importance sampling even th applied to off-policy data.\nThe concept of TB(A) is straightforward.\rAs shown in its backup d ure 12.13, the tree backups of each length (from\rSection 7.5) are weight way dependent on the bootstrapping parameter A. To get\rthe deta with the right indexes on the general bootstrapping and discounting is\rbest to start with a recursive form (12.26) for the A-return using\nWatkins��s Q(A)\nSt\rAt St+i Rt+i At+i\n\r\r\r\r\r+o��I *o��I\n\r\r\r\r\r\r\r\rSt+2 Rt+2 At+2\n\r\r\r\r\r\r\r\rponent backups hichever comes\n\r\r\r\r\r\r\r\r1 - A\n\r\r\r\r\r\r\r\rOR\n\r\r\r\r\r\r\r\rn-greedy action\n\r\r\r\r\r\r\r\r(1 - A)A\n\r\r\r\r\r\r\r\r\r\r\r\r\r\r\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rTree\rBackup(A)\n\r\r\r\r\r\u0026#8226; \u0026#8226; \u0026#8226;\n\r\r\r\r\r\r\r\r1- A\n\r\r\r\r\rSt At St+i Rt+1 At+i St\r+ 2 Rt+2\nAt + 2\n\r\r\u0026nbsp;\n\r\r(1-��)��\n\r\r\u0026nbsp;\n\u0026nbsp;\n\r\r\r\r\r(i -��)��2\n\r\r\r\r\rSt-i\n��\u0026#8226; At -i\nI I\u0026nbsp;\u0026nbsp; St Rt\nAt\r-\nFigure 12.13:\rThe backup diagram for the X version of the Tree Backup algorithm.\nand then expand\rthe bootstrapping case of the target after the model of (7.13):\nRt+i + 7t+i\r((1- Xt+i)^Qt+i + Xt+i (\n^ n(a|St+i)q(St+i,a,wt)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; + أ(At+ilSt+JG^+i))\n=Rt+i + 7t+i(6t+i + Xt+in(At+i|St+i^G��+i - q(St+i,At+i, wt)))\nAs per the usual pattern, it can also be written\rapproximately (ignoring changes in the approximate value function) as a sum of\rTD errors,\n-rAa\n\r\r\r\r\r(12.37)\n\r\r\r\r\r\r\r\r~ q(St, At, wt) + t 5g n 7iXiPi,\n\r\r\r\r\r\r\r\rt+i\n\r\r\r\r\r\r\r\r-rAa\n\r\r\r\r\r\r\r\rk=t\n\r\r\r\r\r\u0026nbsp;\n\rusing the expectation form of the action-based TD\rerror (12.34). Following the same steps as in the previous section, we arrive\rat a special eligibility trace update involving the target-policy probabilities\rof the selected actions,\n\r\r\r(12.38)\n\r\r\r\r\ret == 7tXtn(At|St)et_i + Vq(St,\rAt, wt).\nThis, together with the\rusual parameter-update rule (12.7), defines the TB(X) al\u0026shy;gorithm. Like all\rsemi-gradient algorithms, TB(X) is not guaranteed to be stable when used with\roff-policy data and with a powerful function approximator. For that it would\rhave to be combined with one of the methods presented in the next two sections.\n\r\r12.11\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rStable Off-policy Methods with\rTraces\nSeveral methods using\religibility traces have been proposed that achieve guarantees of stability\runder off-policy training, and here we present four of the most important using\rthis book��s standard notation, including general bootstrapping and discount\u0026shy;ing\rfunctions. All are based on either the Gradient-TD or the Emphatic-TD ideas\rpresented in Sections 11.7 and 11.8. All the algorithms assume linear function\rap\u0026shy;proximation, though extensions to nonlinear function approximation can also\rbe found in the literature.\nGTD(A)is the eligibility-trace algorithm analogous to TDC, the better of\rthe two state-value Gradient-TD prediction algorithms discussed in Section\r11.7. It��s goal is to learn a parameter wt such that {)(s,w) ==\rw7x(s)��Vn(s)\reven from data that is due to following another policy b. Its update is\nwt+i = wt + a8tset - a7t+i(1 - At+i) (efvt) xt+i,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (12.39)\nwith 8|, et, and pt defined in the usual ways for state values (12.29) (12.31)\r(11.1), and\nvt+l =¬��et - P(vtTxt) xt,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (12.40)\nwhere, as in Section 11.7, v G Rd is a vector of the same dimension as\rw, initialized to vo = 0, and P \u0026gt; 0 is a second step-size parameter.\nGQ(A)is the Gradient-TD algorithm for action values with eligibility\rtraces. It��s goal is to learn a parameter wt such that q(s, a, wt) == wTx(s,\ra)��q^(s,\ra) from off- policy data. If the target policy is e-greedy, or otherwise biased\rtoward the greedy policy for q, then GQ(A) can be used as a control algorithm.\rIts update is\nwt+i = wt +\ra8t*et - a7t+i(1 - At+i) (efvt) xt+i,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (12.41)\nwhere xt is the average feature vector for St under the\rtarget policy,\nxt y^n(a|St)x(St,a),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (12.42)\na\n8^ is the expectation form of the TD error, which\rcan be written,\n8ta == Rt+i + 7t+iwTxt+i - wTxt,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (12.43)\net is defined in the usual ways for action values\r(12.35), and the rest is as in GTD(A), including the update for vt (12.40).\nHTD(A)is a hybrid state-value algorithm combining\raspects of GTD(A) and TD(A). Its most appealing feature is that it is a strict\rgeneralization of TD(A) to off-policy learning, meaning that if the behavior\rpolicy happens to be the same as the target policy, then HTD(A) becomes the\rsame as TD(A), which is not true for GTD(A). This is appealing because TD(A) is\roften faster than GTD(A) when both\nalgorithms converge, and TD(X) requires setting only a single step\rsize. HTD(X) is defined by\n\r\r\u0026nbsp;\n\r\r\r\r\rwt+i\nvt+i\net\n\r\r\r\r\r=wt + a5tset + a ((et - eQTvt) (xt - 7t+ixt+i)\n\r\r\rvo =\n\r.0,\n\r\re-i\n\r= 0,\n\r\reb\ne-i\n\r= 0,\n\r\r\r\r\u0026nbsp;\n\r\r\r\r\r=vt + ¬5tSet - P(4����)��xt - 7t+ixt+i),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; -u\r- ^(12\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 44)\n==Pt(71Xtet-i + xt),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; '\u0026quot;\n=. 7tXtetb\ri+ xt,\n\r\r\u0026nbsp;\n\r\rwhere P \u0026gt; 0again is a second step-size parameter that becomes irrelevant in\rthe on-policy case in which b= n. In addition to the second set of weights, vt, HTD(X) also has\ra second set of eligibility traces, ej?. These are a conventional accumulating\religibility trace for the behavior policy and become equal to et if all the pt\rare 1, which causes the second term in the wt update to be\rzero and the overall update to reduce to TD(X).\nEmphatic TD(X)is the extension of the one-step Emphatic-TD\ralgorithm from Section 11.8 to eligibility traces. The resultant algorithm\rretains strong off-policy convergence guarantees while enabling any degree of\rbootstrapping, albiet at the cost of high variance and potentially slow\rconvergence. Emphatic TD(X) is defined by\n\r\r\rittt\rʮe MF\nA\n\r\r\r\r\rOt+ a5tet\nRt+i + 7t+ioTxt+i - OtTxt\npt(71Xtet-i + Mtxt),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; with\re_i = 0\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (12.45)\nXt h+ (1- Xt)Ft\npt_i7tFt-i + It,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; with\rFo = i(So),\nwhere Mt \u0026gt; 0is the general form of emphasis,Ft \u0026gt; 0is termed the followon trace, and It \u0026gt; 0 is the interest,as described in Section 11.8. Note that Mt, like 5t,\ris not really an additional memory variable. It can be removed from the\ralgorithm by substituting its definition into the eligibility-trace equation.\rPseudocode and software for the true online version of ��צ��ذ\u0026amp;^����0(����are available on the\rweb (Sutton, 2015b).\nIn the on-policy case (pt = 0, Vt), Emphatic-TDQ) is\rsimilar to conventional TDQ), but still significantly different. In fact,\rwhereas Emphatic-TDfX) is guaranteed to converge for all state-dependent ��functions,\rTD(X) is not. TD(X) is guaranteed convergent only for all constant ��.SeeYu��s counterexample (Ghiassian, Rafiee, and Sutton,\r2016).\n12.12\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rImplementation Issues\nIt might at first appear that methods using\religibility traces are much more com\u0026shy;plex than one-step methods. A naive\rimplementation would require every state (or state-action pair) to update both\rits value estimate and its eligibility trace on every time step. This would not\rbe a problem for implementations on single-instruction, multiple-data, parallel\rcomputers or in plausible neural implementations, but it is a problem for\rimplementations on conventional serial computers. Fortunately, fortypical values of A and 7the\religibility traces of almost all states are almost always nearly zero; only\rthose that have recently been visited will have traces significantly greater\rthan zero. In practice, only these few states need to be updated to closely\rapproximate these algorithms.\nIn practice, then, implementations on conventional\rcomputers may keep track of and update only the few states with nonzero traces.\rUsing this trick, the com\u0026shy;putational expense of using traces is typically just\ra few times that of a one-step method. The exact multiple of course depends on\rA and 7and on the expense of the other computations. Note\rthat the tabular case is in some sense the worst case for the computational\rcomplexity of eligibility traces. When function approximation is used, the\rcomputational advantages of not using traces generally decrease. For example,\rif artificial neural networks and backpropagation are used, then eligibility\rtraces generally cause only a doubling of the required memory and computation\rper step. Truncated A-return methods (Section 12.3) can be computationally\refficient on conventional computers though always require some additional\rmemory.\n12.13\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rConclusions\nEligibility traces in conjunction with TD errors\rprovide an efficient, incremental way of shifting and choosing between Monte\rCarlo and TD methods. The atomic multi\u0026shy;step methods of Chapter 7 also enabled\rthis, but eligibility trace methods are more general, often faster to learn,\rand offer different computational complexity tradeoffs. This chapter has\roffered an introduction to the elegant, emerging theoretical under\u0026shy;standing of\religibility traces for on- and off-policy learning and for variable boot\u0026shy;strapping\rand discounting. One aspect of this elegant theory is true online methods,\rwhich exactly reproduce the behavior of expensive ideal methods while retaining\rthe computational congeniality of conventional TD methods. Another aspect is\rthe possi\u0026shy;bility of derivations that automatically convert from intuitive forward-viewmethods to more efficient incremental backward-view\ralgorithms. We illustrated this general idea in a derivation that started with\ra classical, expensive Monte Carlo algorithm and ended with a cheap incremental\rnon-TD implementation using the same novel eligibility trace used in true\ronline TD methods.\nAs we mentioned in Chapter 5,\rMonte Carlo methods may have advantages in non-Markov tasks because they do not\rbootstrap. Because eligibility traces make TD methods more like Monte Carlo\rmethods, they also can have advantages in these cases. If one wants to use TD\rmethods because of their other advantages, but the task is at least partially\rnon-Markov, then the use of an eligibility trace method is indicated.\rEligibility traces are the first line of defense against both long-delayed\rrewards and non-Markov tasks.\nBy adjusting A, we can place\religibility trace methods anywhere along a continuum from Monte Carlo to\rone-step TD methods. Where shall we place them? We do not yet have a good\rtheoretical answer to this question, but a clear empirical answer appears to be\remerging. On tasks with many steps per episode, or many stepswithin the half-life of discounting, it appears significantly better to use\religibility traces than not to (e.g., see Figure 12.14). On the other hand, if\rthe traces are so long as to produce a pure Monte Carlo method, or nearly so,\rthen performance degrades sharply. An intermediate mixture appears to be the\rbest choice. Eligibility traces should be used to bring us toward Monte Carlo\rmethods, but not all the way there. In the future it may be possible to vary\rthe trade-off between TD and Monte Carlo methods more finely by using variable\rA, but at present it is not clear how this can be done reliably and usefully.\n\r\r\rMountain Car\n\r\r\r\r\r\r\r\rdpr\n��so epi\n��e\n\r\r\r\r\r\r\r\r��\n\r\r\r\r\r\r\r\rRandom Walk\n\r\r\r\r\r\r\r\rRMS error\n\r\r\r\r\r\r\r\r0.4\n\r\r\r\r\r\r\r\r0.3\n\r\r\r\r\r\r\r\r0.2\n\r\r\r\r\r\r\r\r0.5\n\r\r\r\r\r\r\r\rPuddle World\n\r\r\r\r\r\r\r\r��\n\r\r\r\r\r\r\r\rCart and Pole\n\r\r\r\r\r\r\r\r0\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 0.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 1\n��\n\r\r\r\r\r\r\r\rFailures per 100,000 steps\n\r\r\r\r\r\r\r\rFigure 12.14: The effect of A on reinforcement learning performance in\rfour different test problems. In all cases, lowernumbers represent better\rperformance. The two left panels are applications to simple continuous-state\rcontrol tasks using the Sarsa(A) algorithm and tile coding, with either\rreplacing or accumulating traces (Sutton, 1996). The upper-right panel is\rfor policy evaluation on a random walk task using TD(A) (Singh and Sutton,\r1996). The lower right panel is unpublished data for the pole-balancing\rtask (Example 3.4) from an earlier study (Sutton, 1984).\n\r\r\r\r\rMethods using eligibility traces\rrequire more computation than one-step methods, but in return they offer\rsignificantly faster learning, particularly when rewards are delayed by many\rsteps. Thus it often makes sense to use eligibility traces when data are scarce\rand cannot be repeatedly processed, as is often the case in on\u0026shy;line\rapplications. On the other hand, in off-line applications in which data can be\rgenerated cheaply, perhaps from an inexpensive simulation, then it often does\rnot pay to use eligibility traces. In these cases the objective is not to get\rmore out of alimited amount of data, but simply to process as much data as possible as quickly\ras possible. In these cases the speedup per datum due to traces is typically\rnot worth their computational cost, and one-step methods are favored.\nExercise 12.15 Write pseudocode for Expected Sarsa(A) with dutch\rtraces. �� sKExercise 12.16 How might Double Expected Sarsa be\rextended to eligibility traces? ��\nBibliographical and\rHistorical Remarks\nEligibility traces came into reinforcement learning via the fecund\rideas of Klopf (1972). Our use of eligibility traces is based on Klopf��s work\r(Sutton, 1978a, 1978b, 1978c; Barto and Sutton, 1981a, 1981b; Sutton and Barto,\r1981a; Barto, Sutton, and Anderson, 1983; Sutton, 1984). We may have been the\rfirst to use the term ��eligibility trace�� (Sutton and Barto, 1981). The idea\rthat stimuli produce aftereffects in the nervous system that are important for\rlearning is very old. See Chapter 14. Some of the earliest uses of eligibility\rtraces were in the actor-critic methods discussed in Chapter 13 (Barto, Sutton,\rand Anderson, 1983; Sutton, 1984).\n12.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe A-return and\rits error-reduction properties were introduced by Watkins (1989) and further\rdeveloped by Jaakkola, Jordan and Singh (1994). The random walk results in this\rand subsequent sections are new to this text, as are the terms ��forward view��\rand ��backward view.�� The notion of A-return algorithm was introduced in the\rfirst edition of this text. The more refined treatment presented here was\rdeveloped in conjunction with Harm van Seijen (e.g., van Seijen and Sutton,\r2014).\n12.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rTD(A) with\raccumulating traces was introduced by Sutton (1988, 1984). Con\u0026shy;vergence in the\rmean was proved by Dayan (1992), and with probability 1 by many researchers,\rincluding Peng (1993), Dayan and Sejnowski (1994), and Tsitsiklis (1994) and\rGurvits, Lin, and Hanson (1994). The bound on the error of the asymptotic\rA-dependent solution of linear TD(A) is due to Tsitsiklis and Van Roy (1997).\n12.3-5 Truncated TD methods were developed by Cichosz\r(1995) and van Seijen (2016). True online TD(A) and the other ideas presented\rin these sections are primarily due to work of van Seijen (van Seijen and\rSutton, 2014; van Seijen et al., 2016) Replacing traces are due to Singh and\rSutton (1996).\n12.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe material in\rthis section is from van Hasselt and Sutton (2015).\n12.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rSarsa(A) with\raccumulating traces was first explored as a control method by Rummery and\rNiranjan (1994; Rummery, 1995). True Online Sarsa(A) was introduced by van\rSeijen and Sutton (2014). The algorithm on page 321 was adapted from van Seijen\ret al. (2016). The Mountain Car results were made new for this text, except for\rFigure 12.11 which is adapted from van Seijen and Sutton (2014).\n12.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rPerhaps the\rfirst published discussion of variable ��was by Watkins (1989),\rwho pointed out that the cutting off of the backup sequence (Figure 12.12) in\rhis Q(X) when a nongreedy action was selected could be implemented by\rtemporarily setting X to 0.\nVariable X was introduced in the first edition of this text. The\rroots of variable 7are in\rthe work on options (Sutton, Precup, and Singh, 1999) and its precursors\r(Sutton, 1995), becoming explicit in the ����(����paper (Maei and Sutton, 2010), which also\rintroduced some of these recursive forms for the X-returns.\nA different notion of variable ��has been developed by\rYu (2012).\n12.9\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rOff-policy\religibility traces were introduced by Precup et al. (2000, 2001), then further\rdeveloped by Bertsekas and Yu (2009), Maei (2011; Maei and Sutton, 2010), Yu\r(2012), and by Sutton, Mahmood, Precup, and van Hasselt (2014). The latter\rreference in particular gives a powerful forward view for off- policy TD\rmethods with general state-dependent X and 7. The presentation here seems to be new.\n12.10\u0026nbsp; Watkins��s Qp) is due to Watkins (1989). Convergence\rhas still not been proved for any control method for 0 \u0026lt; ��\u0026lt; 1. Tree Backup(X)\ris due to Precup, Sutton, and Singh (2000).\n12.11\u0026nbsp;\rGTDQ) is due to\rMaei (2011). GQQ) is due to Maei and Sutton (2010). HTD(X) is due to White and\rWhite (2016) based on the one-step HTD al\u0026shy;gorithm introduced by Hackman (2012).\rEmphatic TD(X) was introduced by Sutton, Mahmood, and White (2016), who proved\rits stability, then was proved to be convergent by Yu (2015a,b), and developed\rfurther by Hallak, Tamar, Munos, and Mannor (2016).\n\r\rChapter 13\nPolicy Gradient Methods\nIn this chapter we\rconsider something new. So far in this book almost all the methods have learned\rthe values of actions and then selected actions based on their estimated action\rvalues[21];\rtheir policies would not even exist without the action-value estimates. In this\rchapter we consider methods that instead learn a parameterized policythat can select actions without consulting a\rvalue function. A value function may still be used to learnthe policy parameter, but is not required for action selection. We\ruse the notation 6G Rd for the policy��s parameter vector.\rThus we write n(a|s, 6)= Pr{At = a | St = s, 6t= 6} for the\rprobability that action a is taken at time t given that the agent is in state s\rat time t with parameter 6. If a method uses a learned value function as\rwell, then the value function��s weight vector is denoted w G Rm, as in V(s,w).\nIn this chapter we consider methods for learning\rthe policy parameter based on the gradient of some performance measure J(6) with respect to the policy parameter. These methods seek to maximizeperformance, so their updates approximate gradient ascentin J:\n6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rt+i = 6t + aVJ (6t),(13.1)\nһ����\nwhere VJ(6t) is a stochastic estimate whose expectation approximates the\rgradient of the performance measure with respect to its argument 6t. All methods that follow this general schema we call policy gradient methods, whether or not they also learn an approximate\rvalue function. Methods that learn approximations to both policy and value\rfunctions are often called actor-critic\rmethods,where ��actor�� is a\rreference to the learned policy, and ��critic�� refers to the learned value\rfunction, usually a state- value function. First we treat the episodic case, in\rwhich performance is defined as the value of the start state under the\rparameterized policy, before going on to consider the continuing case, in which\rperformance is defined as the average reward rate, as in Section 10.3. In the\rend we are able to express the algorithms for both cases in very similar terms.\n13.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rPolicy Approximation and its\rAdvantages\nIn policy gradient methods, the policy can be\rparameterized in any way, as long as n(a|s, d)is differentiable with respect to its parameters,\rthat is, as long as Ven(a|s, d) exists\rand is always finite. In practice, to ensure exploration we generally require\rthat the policy never becomes deterministic (i.e., that n(a|s, d)G (0,1) Vs, a, 6. In this section we introduce the most common parameterization for\rdiscrete action spaces and point out the advantages it offers over action-value\rmethods. Policy- based methods also offer useful ways of dealing with\rcontinuous action spaces, as we describe later in Section 13.7.\nIf the action space is discrete\rand not too large, then a natural kind of param\u0026shy;eterization is to form\rparameterized numerical preferences h(s, a, 6) G R for\reach state-action pair. The most preferred actions in each state are given the\rhighest probability of being selected, for example, according to an exponential\rsoftmax dis\u0026shy;tribution:\nexp(h(s, a, 6))\n\r\r\r(13.2)\n\r\r\r\r\rn(a|s, 6)\nEb exp(h(s, b, 6)��\nwhere exp��=ex, where\re c 2.71828 is the base of the natural logarithm. Note that the denominator\rhere is just what is required so that the action probabilities in each state to\rsum to one. The preferences themselves can be parameterized arbitrarily. For\rexample, they might be computed by a deep neural network, where 6is the vector of all the connection weights of the\rnetwork (as in the AlphaGo system described in Section 16.7). Or the\rpreferences could simply be linear in features,\nh(s, a, 6) = 6Tx(s, a),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (13.3)\nusing feature vectors x(s, a) G Rd constructed by any of the\rmethods described in Chapter 9.\nAn immediate advantage of\rselecting actions according to the softmax in action preferences (13.2) is that\rthe approximate policy can approach determinism, whereas with e-greedy action\rselection over action values there is always an eprobability of selecting a random action. Of course,\rone could select according to a softmax over action values, but this alone\rwould not approach determinism. Instead, the action- value estimates would\rconverge to their corresponding true values, which would differ by a finite\ramount, translating to specific probabilities other than 0 and 1. If the\rsoftmax included a temperature parameter, then the temperature could be reduced\rover time to approach determinism, but in practice it would be difficult to\rchoose the reduction schedule, or even the initial temperature, without more\rknowledge of the true action values than we would like to assume. Action\rpreferences are different because they do not approach specific values; instead\rthey are driven to produce the optimal stochastic policy. If the optimal policy\ris deterministic, then the preferences of the optimal actions will be driven\rinfinitely higher than all suboptimal actions (if permited by the\rparameterization).\nPerhaps the simplest advantage\rthat policy parameterization may have over action- value parameterization is\rthat the policy may be a simpler function to approximate.\nProblems vary in the complexity of their policies and\raction-value functions. For some, the action-value function is simpler and thus\reasier to approximate. For others, the policy is simpler. In the latter case a\rpolicy-based method will typically be faster to learn and yield a superior\rasymptotic policy (as seems to be the case with Tetris; see ��imsek, AlgcSrta,\rand Kothiyal, 2016).\nIn problems with significant function approximation,\rthe best approximate policy may be stochastic. For example, in card games with\rimperfect information the opti\u0026shy;mal play is often to do two different things\rwith specific probabilities, such as when bluffing in Poker. Action-value\rmethods have no natural way of finding stochastic op\u0026shy;timal policies, whereas\rpolicy approximating methods can, as shown in Example 13.1. This is a third\rsignificant advantage of policy-based methods.\nExample 13.1 Short corridor with\rswitched actions\nConsider the small corridor gridworld shown inset in\rthe graph below. The reward is -1 per step, as usual. In each of the three\rnonterminal states there are only two actions, right and left. These actions have their usual consequences in the first and third\rstates, but in the second state they are reversed, so that right moves to the left and left moves to the right. The problem is difficult because\rall the states appear identical under the function approximation. In\rparticular, we define x(s, right)=\r=[1, 0]T\rand x(s, left)= [0,1]T, for all s. An action-value\rmethod with e-greedy action selection is forced to choose between just two\rpolicies: choosing right with\rhigh probability\n1\u0026nbsp; - s/2on all\rsteps or choosing left with\rthe same high probability on all time steps. If e= 0.1, then these two policies achieve a value (at\rthe start state) of less than -44 and -82, respectively, as shown in the graph.\rA method can do significantly better if it can learn a specific probability\rwith which to select right. The best probability is about 0.59, which achieves a value of\rabout\n11.6.\n\r\n\r\r\r\r\r\rJ(\u0026#10003;) = v^e\n\r\r\r\r\r\rprobability of right action\n\r\r\r\r\r\u0026nbsp;\nExercise 13.1 Use your\rknowledge of the gridworld and its dynamics to determine an exactsymbolic expression for the optimal probability of\rselecting the right action\rin Example 13.1.\u0026nbsp;\u0026nbsp; ��\nFinally, we note that the choice of policy\rparameterization is sometimes a good way of injecting prior knowledge about the\rdesired form of the policy into the rein\u0026shy;forcement learning system.\n13.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe Policy Gradient Theorem\nIn addition to the\rpractical advantages of policy parameterization over e-greedy action selection,\rthere is also an important theoretical advantage. With continuous policy\rparameterization, the action probabilities changes smoothly as a function of\rthe learned parameter, whereas in e-greedy selection the action probabilities\rmay change dramatically for an arbitrarily small change in the estimated action\rvalues, if that change results in a different action having the maximal value.\rBecause of this, stronger convergence guarantees are available for\rpolicy-gradient methods than for action-value methods. In particular, it is the\rcontinuity of the parameterized policy that enables policy-gradient methods\rthat approximate gradient ascent (13.1).\nThe episodic\rand continuing cases define the performance measure, J(6), differ\u0026shy;ently and thus have to be treated separately to some\rextent. Nevertheless, we will try to present both cases uniformly, and we\rdevelop a notation so that the major theoretical results can be decribed with a\rsingle set of equations.\nIn this section we treat the episodic case, for\rwhich we define the performance measure as the value of the start state of the\repisode. We can simplify the notation without losing any meaningful generality\rby assuming that every episode starts in some particular (non-random) state so.\rThen, in the episodic case we define perfor\u0026shy;mance as\nJ (6) = v ߵ(so),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (13.4)\nwhere is the true\rvalue function for , the policy determined by 6.\nWith\rfunction approximation, it may seem challenging to change the policy param\u0026shy;eter\rin a way that ensures improvement. The problem is that performance depends on\rboth the action selections and the distribution of states in which those selections\rare made, and that both of these are affected by the policy parametre. Given a\rstate, the effect of the policy parameter on the actions, and thus on reward,\rcan be computed in a relatively straightforward way from knowledge of the\rparameteriza\u0026shy;tion. But the effect of the policy on the state distribution is\rcompletely a function of the environment and is typically unknown. How can we\restimate the performance gradient with respect to the policy parameter, when\rthe gradient depends on the unknown effect of changing the policy on the state\rdistribution?\nFortunately,\rthere is an excellent theoretical answer to this challenge in the form of the policy gradient theorem,which provides us an analytic expression for the\rgradient of performance with respect to the policy parameter (which is what we\rneed to approximate for gradient ascent (13.1)) that does notinvolve the derivative\n\r\r\r\r\rProof of the Policy Gradient Theorem (episodic case)\n\r\r\r\r\r\r\r\rVn(a|s)qn(s, a) + n(a|s)\na\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; sf\nE [Vn(a'|s')qn(s', a') + n(a'|s') E 7P(s''|s', a')Vv^(s'')]\na;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; s��\n=^ y^7kPr(sx,k,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Vn(a|x)q^(x, a),\nx�S k=0\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; a\nafter repeated unrolling, where Pr(s x, k, n) is the probability of\rtransition\u0026shy;ing from state s to state x in k steps under policy n. It is\rthen immediate that\nVJ (6) = VVn (so)\n=H7kPr(so ^ s, k,أ)[Vn(a|s)qn(s, a)\ns k=0\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; a\n=E \u0026#12316;(s)E\rVn(a|s)qn(s, a).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Q.E.D.\n\r\r\r\r\r\r\r\rVn(a|s)qn(s, a) + أ(a|s)����p(s\u0026#12316;r|s, a) (r+ 7\u0026#12316;(s'))\n(Exercise 3.12 and Equation 3.8)\n\r\r\r\r\r\r\r\rVn(a|s)qn(s, a) + n(a|s^ ^ 7p(s'|s, a)Vv^(s')\n\r\r\r\r\r\r\r\rVn(a|s)qn(s, a) + n(a|s)Vq^(s, a)\n\r\r\r\r\r\r\r\r^n(a|s)qn (s,a)\n\r\r\r\r\r\r\r\r(Eq. 3.10) (unrolling)\n\r\r\r\r\r\r\r\rVVn (s)\n\r\r\r\r\r\r\r\r(Exercise 3.11) (product rule)\n\r\r\r\r\r\r\r\rV\nE\na\nE\na\nE\na\nE\n\r\r\r\r\r\r\r\rWith just elementary calculus\rand re-arranging terms we can prove the policy gradient theorem from first\rprinciples. To keep the notation simple, we leave it implicit in all cases\rthat n is a function of 6, and all gradients are also implicitly with respect to 6. First note that the gradient of the state-value function can be\rwritten in terms of the action-value function as\n\r\r\r\r\r\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rof the state\rdistribution. The policy gradient theorem is that\n\r\r\r(13.5)\n\r\r\r\r\rVJ(6) = L \u0026quot;آ(s) E\rqn(s, a)Ven(a|s, 6),\nwhere the gradients in all cases are the column\rvectors of partial derivatives with respect to the components of 6,and n denotes the policy corresponding to parameter vector 6. The notion of the distribution ��here\rshould be clear from what transpired in Chapters 9 and 10. That is, in the\repisodic case, (s) is defined to be the expected number of time steps t on\rwhich St = s in a randomly generated episode starting in so and following n and\rthe dynamics of the MDP. The policy gradient theorem is proved for the episodic\rcase in the box.\n13.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rREINFORCE: Monte Carlo Policy\rGradient\nWe are now ready for\rour first policy-gradient learning algorithm. Recall our overall strategy of\rstochastic gradient ascent (13.1), for which we need a way of obtaining samples\rwhose expectation is equal to the performance gradient. The policy gradient\rtheorem gives us an exact expression for this gradient; all we need is some way\rof sampling whose expectation equals or approximates this expression. Notice\rthat the right-hand side of the policy gradient theorem is a sum over states\rweighted by how often the states occurs under the target policy n, weighted\ragain by 7times how many steps it takes to get to those\rstates; if we just follow n we will encounter states in these proportions,\rwhich we can then weight by 7tto preserve the expected value.\nThus\n\r\r\r(13.5)\n\r\r\r\r\rVJ(6) = L \u0026quot;آ(s) E\rqn(s, a)V0n(a|s, 6),\nEn 7*^2,qn(St,a)V0n(a|St, 6).\nThis is good progress, and we would like to carry\rit further and handle the action in the same way (replacing a with the sample\raction At). The remaining part of the expectation above is a sum over actions;\rif only each term was weighted by the probability of selecting the actions,\rthat is, according to n(a|St, 6). So let us make\rit that way, multiplying and dividing by this probability. Continuing from the\rprevious equation, this gives us\n\r\r\rVJ(6) = En 7^ n(a|St, 6)��(St, a)\n\n\r\r\r\r\rVn(a|St, 6)\rn(a|St, 6)\n(replacing a by the sample At \u0026#12316;n)\n(because En[Gt|St, At]=��(St, At))\nwhich is exactly what we want, a\rquantity that we can sample on each time step whose expectation is equal to the\rgradient. Using this sample to instantiate our generic stochastic gradient\rascent algorithm (13.1), we obtain the update\n6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\r6, ��t G ����(a ��6)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (13\n6t+[22]= 6t+ a7 Gt n(At|St, 6) .\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (13.6)\nWe call this algorithm\rREINFORCE (after Williams, 1992). Its update has an intuitive appeal. Each\rincrement is proportional to the product of a return Gt and a vector, the\rgradient of the probability of taking the action actually taken, divided by the\rprobability of taking that action. The vector is the direction in parameter\rspace that most increases the probability of repeating the action At on future\rvisits to state\nSt. The update\rincreases the parameter vector\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; inthisdirection\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; proportional\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; to\rthe\nreturn, and inversely proportional to the action\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; probability.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Theformer makes sense\nbecause it\rcauses the parameter to move most in the directions that favor actions that\ryield the highest return. The latter makes sense because otherwise actions that\rare selected frequently are at an advantage (the updates will be more often in\rtheir direction) and might win out even if they do not yield the highest return.\nNote that\rREINFORCE uses the complete return from time t, which includes all future\rrewards up until the end of the episode. In this sense REINFORCE is a Monte\rCarlo algorithm and is well defined only for the episodic case with all updates\rmade in retrospect after the episode is completed (like the Monte Carlo\ralgorithms in Chapter 5). This is shown explicitly in the boxed pseudocode\rbelow.\nREINFORCE, A\rMonte-Carlo Policy-Gradient Method (episodic)\nInput: a differentiable policy parameterization n(a|s, 6), Va G A,s G S, 6G Rd Initialize policy parameter 6 Repeat forever:\nGenerate an\repisode So, Ao, Ri,...,St_i, At_i, Rt, following n(-|-, 6)\nFor each step\rof the episode t= 0,...,T - 1:\nG return from\rstep t\n6\u0026nbsp; �� 6 + \u0026laquo;7tG Ve logn(At|St, 6)\nAs a stochastic gradient method,\rREINFORCE has good theoretical convergence properties. By construction, the\rexpected update over an episode is in the same direction as the performance\rgradient.[23]This assures an improvement in expected performance\rfor sufficiently small a, and convergence to a local optimum under standard\rstochastic approximation conditions for decreasing a. However, as a Monte Carlo\rmethod REINFORCE may be of high variance and thus slow to learn.\nExercise\r13.2 Prove (13.7) using the\rdefinitions and elementary calculus. ��\n13.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rREINFORCE with Baseline\nThe policy gradient theorem\r(13.5) can be generalized to include a comparison of the action value to an\rarbitrary baselineb(s):\nVJ(O) =\rL (s) L (��(s,a) - b(s^ Ven(a|s, O).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (13.8)\ns\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; a\nThe baseline can be any function,\reven a random variable, as long as it does not vary with a; the equation\rremains true, because the the subtracted quantity is zero:\ny^b(s)Ven(a|s, O) = b(s)Ve ^ n(a|s, O) = b(s)Ve 1 =0Vs G\rS.\naa\nHowever, after\rwe convert the policy gradient theorem to an expectation and an update rule,\rusing the same steps as in the previous section, then the baseline can have a\rsignificant effect on the varianceof the update rule.\nThe update rule that we end up with\ris a new version of REINFORCE that includes a general baseline:\nOt+i ^Ot+a71(Gt - b(St))\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; . (13.9)\nAs the\rbaseline could be uniformly zero, this update is a strict generalization of\rREINFORCE. In general, the baseline leaves the expected value of the update un\u0026shy;changed,\rbut it can have a large effect on its variance. For example, we saw in Section\r2.8 that an analogous baseline can significantly reduce the variance (and thus\rspeed the learning) of gradient bandit algorithms. In the bandit algorithms the\rbaseline was just a number (the average of the rewards seen so far), but for\rMDPs the baseline should vary with state. In some states all actions have high\rvalues and we need a high baseline to differentiate the higher valued actions\rfrom the less highly valued ones; in other states all actions will have low\rvalues and a low baseline is appropriate.\n\r\rOne natural\rchoice for the baseline is an estimate of the state value, V(St,w), where w G Rm is a weight vector learned by one of the methods\rpresented in previous chapters. Because REINFORCE is a Monte Carlo method for\rlearning the policy parameter, 6, it seems natural to also use a Monte Carlo method to learn the\rstate- value weights, w. A\rcomplete pseudocode algorithm for REINFORCE with baseline is given in the box\rusing such a learned state-value function as the baseline.\nREINFORCE with Baseline\r(episodic)\nInput: a\rdifferentiable policy parameterization n(a|s, 6), Va G A, s G S, 6G Rd Input: a differentiable state-value parameterization\rV(s,w), Vs G S, w G Rm Parameters: step sizes a \u0026gt; 0, P \u0026gt; 0\nInitialize policy parameter 6and state-value weights w Repeat forever:\nGenerate an episode So, Ao, Ri,...,St_l, At_i, Rt,following n(-|-, 6)\nFor each step of the episode t = 0,...,T - 1:\nGt\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; return from step t\n8�� Gt -\rV(St,w)w �� w + P8Vw V(St,w)\n6\u0026nbsp;\u0026nbsp;\r�� 6+ a7t8V^ log\rn(At|St, 6)\nThis algorithm\rhas two step sizes, a and P. The step size for values (here P) is relatively\reasy; in the linear case we have rules of thumb for setting it, such as P =\r0.1/E[||xt||^]. For action values though it is much less clear. It depends on\rthe range of variation of the rewards and on the policy parameterization.\n13.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rActor��Critic Methods\nAlthough the\rREINFORCE-with-baseline method learns both a policy and a state- value\rfunction, we do not consider it to be an actor-critic method because its state-\rvalue function is used only as a baseline, not as a critic. That is, it is not\rused for bootstrapping (updating a state from the estimated values of\rsubsequent states), but only as a baseline for the state being updated. This is\ra useful distinction, for only through bootstrapping do we introduce bias and\ran asymptotic dependence on the quality of the function approximation. As we\rhave seen, the bias introduced through bootstrapping and reliance on the state\rrepresentation is often on balance beneficial because it reduces variance and\raccelerates learning. REINFORCE with baseline is unbiased and will converge\rasymptotically to a local minimum, but like all Monte Carlo methods it tends to\rbe slow to learn (of high variance) and inconvenient to implement online or for\rcontinuing problems. As we have seen earlier in this book, with\rtemporal-difference methods we can eliminate these inconveniences, and through\rmulti-step methods we can flexibly choose the degree of bootstrapping. In order\rto gain these advantages in the case of policy gradient methods we use actor-critic\rmethods with a true bootstrapping critic.\nFirst consider one-step actor-critic methods, the analog of the TD methods\rintro\u0026shy;duced in Chapter 6such as TD(0),\rSarsa(0), and Q-learning. The main appeal of one-step methods is that they are\rfully online and incremental, yet avoid the com\u0026shy;plexities of eligibility\rtraces. They are a special case of the eligibility trace methods, and not as\rgeneral, but easier to understand. One-step actor-critic methods replace the\rfull return of REINFORCE (13.9) with the one-step return (and use a learned\rstate-value function as the baseline) as follow:\nVe n(At|St,\rO)\nOt+[24] = Ot+ a7(Gt:t+i - V(St,w)7 n(At|St, O)\n=Ot + a7t(Rt+i + 7v(St+i,w)\r- v(St,w)) Veأ(At|St��O)\n=Ot + a71 \u0026#12316;Ve n(At|St,O)\n\r\r\r(13.11)\n\r\r\r\r\r\r\r\rn(At|St, O)\n\r\r\r\r\r(13.10)\n(13.12)\nn(At|St, O)'\nThe natural state-value-function learning method to pair with this\ris semi-gradient TD(0). Pseudocode for the complete algorithm is given in the\rbox below. Note that it is now a fully online, incremental algorithm, with\rstates, actions, and rewards processed as they occur and then never revisited.\nOne-step Actor-Critic (episodic)\nInput: a differentiable policy\rparameterization n(a|s, O), Va G A, s G S, O G Rd Input: a differentiable state-value\rparameterization v(s,w), Vs G S, w\rG Rm Parameters: step sizes a \u0026gt; 0, P \u0026gt; 0\nInitialize\rpolicy parameter O and state-value weights w Repeat forever:\nInitialize\rS (first state of episode)\nI ^ 1\nWhile S\ris not terminal:\nA \u0026#12316;n(-|S, O)\nTake action A, observe S', R\n5R + 7v(S',w) - v(S,w)(if Sحis terminal, then {)(S/,w) = 0)\nw �� w + P5 Vw v(S,w)\n0\u0026nbsp;\u0026nbsp;\r�� O +\raI5 Ve log n(A|S, O)\n1\u0026nbsp;\u0026nbsp;\r�� 7I\nS\u0026nbsp; �� Sf\nActor-Critic with\rEligibility Traces (episodic)\nInput: a\rdifferentiable policy parameterization n(a|s, 6), Va G A, s G S, 6G Rd Input: a differentiable state-value parameterization\rv(s,w), Vs G S, w G Rm Parameters: step sizes a\u0026gt; 0, ^ \u0026gt; 0\nInitialize policy parameter 6 and state-value weights w Repeat forever (for each episode):\nInitialize S\r(first state of episode)\nee �� 0 (n-component\religibility trace vector)\new�� 0 (m-component\religibility trace vector)\nI �� 1\nWhile S is not\rterminal (for each time step):\nA \u0026#12316;n(-|S, 6)\nTake action A,\robserve S', R\n8�� R + 7v(S',w) - v(S,w)(if S' is terminal, then {)(S',w) == 0)\new�� 7Awew+ I VwV(S,w) ee �� 7Aeee + I Ve log n(A|S, 6) w �� w + ^8ew\n6�� 6 + ee\nI\u0026nbsp; \u0026nbsp;TOC \\o \u0026quot;1-5\u0026quot; \\h \\z �� 7I\nS�� S'\n13.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rPolicy Gradient\rfor Continuing Problems\nAs discussed in Section 10.3, for continuing\rproblems without episode boundaries we need to define performance in terms of\rthe average rate of reward per time step:\n1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rT\nJ (6)=ʮ��=limtY^ E[Rt I Ao��t-i \u0026#12316;��\n��400 �A^\nt=1\n=limE[Rt | Ao��t-i \u0026#12316;n] ,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (10.6)\nt^^\n=E ��(s)En(a|s)E p(s', r|s, a)r,\ns\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; a\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; s;,r\nwhere is the steady-state\rdistribution under n,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (s)\r= limt^^ Pr{St = s|Ao:t \u0026#12316;n},\nwhich is assumed to exist and to be independent of So (an ergodicity\rassumption). Remember that this is the special distribution under which, if you\rselect actions according to n, you remain in the same distribution:\n^ ��(s) [ n(a|s, 6)p(s'|s, a)=��(s').\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (10.7)\nsa\nWe also define values,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Vn(s)\u0026nbsp; ==\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; En[Gt|St= s]\u0026nbsp; andqn(s,a)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ==En[Gt|St = s, At\u0026nbsp;\u0026nbsp;\u0026nbsp; =\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; a],\nwith respect to the differential\rreturn:\nGt == Rt+i\r-n(n) + Rt+2-n(n) + Rt+3-n(n) + ��.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (10.8)\nWith these alternate definitions,\rand 7= 1, the policy gradient theorem as given for the\repisodic case (13.5) remains true for the continuing case. A proof is given in\rthe box on the next page. The forward and backward view equations also remain\rthe same. Complete pseudocode for the backward view is given in the box below.\nActor-Critic with\rEligibility Traces (continuing)\nInput: a\rdifferentiable policy parameterization n(a|s, 6), Va G A, s G S, 6G RdInput: a differentiable state-value parameterization\rV(s,w), Vs G S, w G RmParameters: step sizes a \u0026gt; 0, P \u0026gt; 0, n \u0026gt; 0\ne���� 0(n-component eligibility trace vector) ew �� 0(m-component eligibility trace vector)\nInitialize R G R (e.g.,\rto 0)\nInitialize policy parameter 6and state-value weights w (e.g., to 0)\nInitialize S G S\r(e.g., to so)\nRepeat forever:\nA \u0026#12316;n(-|S, 6)\nTake action A,\robserve S;, R\n8 �� R - R + V(S;,w) - V(S,w)(if Sحis terminal, then {)(S/,w) == 0)\nR �� R + n8\new �� Awew + VwV(S,w)\ne���� A6e0+ Velogn(A|S, 6) w �� w + P8 ew\n6\u0026nbsp;\u0026nbsp;\r�� 6+ a8 e0\nS\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; S/\n\r\rProof of the Policy Gradient Theorem\r(continuing case)\nThe proof of\rthe policy gradient theorem for the continuing case begins sim\u0026shy;ilarly to the\repisodic case. Again we leave it implicit in all cases that n is a function of\rO and that the gradients are with respect to O. Recall that in the continuing\rcase J(O) = r(n) (10.6) and that and denote values with re\u0026shy;spect to the\rdifferential return (10.8). The gradient of the state-value function can be\rwritten as\n\r\r\r\r\r(Exercise 3.15) (product rule)\n\r\r\r\r\r\r\r\r^\rVn(a|s)qn(s,a) + n(a|s)Vq^(s,a)\n\r\r\r\r\r\r\r\rVVn (s) = V\n\r\r\r\r\r\r\r\r^n(a|s)qn (s,a)\n\r\r\r\r\r\r\r\rsS\n\r\r\r\r\r\u0026nbsp;\n\u0026nbsp;\n\r=��Vn(a|s)qn(s,a) + أ(a|s)����p(s\u0026#12316;r|s,a)(r-\rr(O) + 7Vn(s'))\n\u0026nbsp;TOC \\o \u0026quot;1-5\u0026quot; \\h \\z a\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; s,,r\n=^ Vn(a|s)qn(s, a) + n(a|s) [-Vr(O) + ^7p(s'|s, a)Vvn(s')]\na\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; s7\nAfter re-arranging terms, we\robtain\nV\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rr(O) =\u0026nbsp;\u0026nbsp;\u0026nbsp; Vn(a|s)qn(s,\ra)+n(a|s^ ^ 7p(s'|s, a)Vv^(s') -Vv^(s), Vs G S.\na\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; s!\nNotice that\rthe left-hand side can be written VJ(O) and that it does not depend on s. Thus the right-hand side does\rnot depend on s either, and we can safely sum it over all s G S, weighted by \u0026quot;أ(s), without changing it\n(because\rEs\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (s)\r= 1). Thus\nV\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rJ(O) = L\u0026nbsp; (s) E Vn(a|s)qn(s, a) + أ(a|s) ��7P(s'|s, a)Vvn(s') - Vvn(s:\ns\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; a\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; s!\n(s)YlVn(a|s)qn (s, a)\nsa\n+ ��(s)[أ(a|s) [ 7P(s'|s, a)Vvn(s')-\u0026#12316;(s) [\rVv^(s)\na\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; s7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; a\n(s)YlVn(a|s)qn (s, a)\nsa\n+\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��(s)I]\rn(a|s)p(s'|s, a) Vv^(s') - ^ ��(s)Vv^(s)\nMn (s7) (i0.7)\n[\u0026#12316;(s)[ Vn(a|s)qn(s, a) + [��(s')Vv^(s')-[��(s)Vv^(s)\n\r\r\rQ.E.D.\n\r\r\r\r\rآ(s) L Vn(a|s)qn(s,a).\n13.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rPolicy Parameterization for\rContinuous Actions\nPolicy-based\rmethods offer practical ways of dealing with large actions spaces, even\rcontinuous spaces with an infinite number of actions. Instead of computing\rlearned probabilities for each of the many actions, we instead compute learned\rthe statistics of the probability distribution. For example, the action set\rmight be the real numbers, with actions chosen from a normal (Gaussian)\rdistribution.\nThe conventional probability\rdensity function for the normal distribution is written\n��exp(-\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (13.13)\nwhere ^ and ahere are the mean and standard deviation of the normal distribution, and of\rcourse n here is just the number n ��3.14159. The\rprobability density function for several different means and standard\rdeviations is shown in Figure 13.1. The value p(x) is the densityof the probability at x, not the probability. It can be greater\rthan 1; it is the total area under p(x) that must sum to 1. In general, one can\rtake the integral under p(x) for any range of x values to get the probability\rof x falling within that range.\nTo produce a policy\rparameterization, we can define the policy as the normal prob\u0026shy;ability density\rover a real-valued scalar action, with mean and standard deviation give by\rparametric function approximators. That is, we define\nn(a|s, 6)==����expf-(a-�ɡ�6^^ .\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (13.14)\nV\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\r'\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\u0026nbsp;\u0026nbsp; a(s,\r6)^\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; V\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 2a(s, 6)2J\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; !\nTo complete\rthe example we need only give a form for the approximators for the mean and\rstandard-deviation functions. For this we divide the policy��s parameter vector\rinto two parts, 6 = [6M, 6��]T, one part to be used for the\rapproximation of the mean and one part for the approximation of the standard\rdeviation. The mean\n\r\r\r\r\r(x �� /j)2 2a2\n\r\r\r\r\r\r\r\r1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 5\n\r\r\r\r\r\r\r\rp(x)\n\r\r\r\r\r\r\r\rexp\n\r\r\r\r\r\r\r\rr\\/2n\n\r\r\r\r\r\r\r\rFigure 13.1:\rThe probability density function of the normal distribution for different\rmeans and variances.\n\r\r\r\r\r\r\r\r-5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; -4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; -3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; -2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; -1\n\r\r\r\r\r\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rcan be approximated as a linear function. The standard deviation\rmust always be positive and is better approximated as the exponential of a\rlinear function. Thus\n^(s, w) == 6\u0026quot;Tx(s)\rand a(s, 6) == exp(6��Tx(s)) ,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (13.15)\nwhere x(s) is a state feature vector constructed perhaps by one of\rthe methods described in Chapter 9. With these definitions, all the algorithms\rdescribed in the rest of this chapter can be applied to learn to select\rreal-valued actions.\nExercise 13.3 A Bernoulli-logistic\runitis a stochastic neuron-like unit used in some artificial neural\rnetworks (see Section 9.6). Its input at time t is a fea\u0026shy;ture vector x(St); its\routput, At, is a random variable having two values, 0and 1, with Pr{At = 1} = Pt and Pr{At = 0} = 1- Pt (the\rBernoulli distribution). Let h(s, 0, 6) and h(s, 1, 6) be the preferences in state s for for the unit��s two ac\u0026shy;tions\rgiven policy parameter 6. Assume that the difference\rbetween the prefer\u0026shy;ences is given by a weighted sum of the unit��s input vector,\rthat is, assume that h(s, 1, 6) - h(s, 0, 6) = 6Tx(s), where 6is the unit��s weight vector.\n(a)\u0026nbsp;\u0026nbsp;\rShow that if the exponential\rsoftmax distribution (13.2) is used to convert pref\u0026shy;erences to policies, then\rPt = n(1|St, 6t) = 1/(1+ exp(-6Tx(St)))\r(the logistic function).\n(b)\u0026nbsp;\u0026nbsp;\u0026nbsp;\r\u0026nbsp;What is the Monte-Carlo REINFORCE update\rof 6t to 6t+i upon receipt of return Gt?\n(c)\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\r\u0026nbsp;Express the eligibility Ve logn(a|s, 6) for a Bernoulli-logistic unit, in terms of a, x(s), and n(a|s, 6) by calculating the gradient. Hint: separately for each action\rcompute the derivative of the log first with respect to p= n(1|s, 6), combine the two results into one expression that depends on a and\rp, and then use the chain rule, noting that the derivative of the logistic\rfunction f(x) is f(x)(1- f(x)).��\n13.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rSummary\nPrior to this chapter, this book has\rfocused on action-value methods��meaning\rmeth\u0026shy;ods that learn action values and then use them to determine action\rselections. In this chapter, on the other hand, we have considered methods that\rlearn a parameterized policy that enables actions to be taken without\rconsulting action-value estimates�� though action-value estimates may still be\rlearned and used to update the policy pa\u0026shy;rameter. In particular, we have\rconsidered policy-gradient methods��meaning\rmeth\u0026shy;ods that update the policy parameter on each step in the direction of an\restimate of performance with respect to the policy parameter.\nMethods that learn and store a\rpolicy parameter have many advantages. They can learn specific probabilities\rfor their actions. They can learn appropriate levels of exploration and\rapproach determinism asymptotically. They can naturally handle continuous state\rspaces. All these things are easy for policy-based methods, but awkward or\rimpossible for e-greedy methods and for action-value methods in general. In\raddition, on some problems the policy is just simpler to represent\rparametrically than the value function; these are more suited to parameterized\rpolicy methods.\nParameterized policy methods also have an important theoretical\radvantage over action-value methods in the form of the policy\rgradient theorem, which gives an\rexact formula for how performance is affected by the policy parameter that does\rnot involve derivatives of the state distribution. This theorem provides a\rtheoretical foundation for all policy gradient methods.\nThe REINFORCEmethod follows directly from the policy gradient theorem. Adding a\rstate-value function as a baselinereduces REINFORCE��s variance without introducing bias. Using the\rstate-value function for bootstrapping results introduces bias, but is often\rdesirable for the same reason that bootstrapping TD methods are often superior\rto Monte Carlo methods (substantially reduced variance). The state-value\rfunction assigns credit to��critizes��the policy��s action selections, and\raccordingly the former is termed the criticand the latter the actor, and these overall methods are sometimes termed actor-criticmethods.\nOverall, policy-gradient\rmethods provide a significantly different set of proclivi\u0026shy;ties, strengths, and\rweaknesses than action-value methods. Today they are less well understood, but\ra subject of excitement and ongoing research.\nBibliographical\rand Historical Remarks\nMethods that\rwe now see as related to policy gradients were actually some of the earli\u0026shy;est\rto be studied in reinforcement learning (Witten, 1977; Barto, Sutton, and Ander\u0026shy;son,\r1983; Sutton, 1984; Williams, 1987, 1992) and in predecessor fields (Phansalkar\rand Thathachar, 1995). They were largely supplanted in the 1990s by the action-\rvalue methods that are the focus of the other chapters of this book. In recent\ryears, however, extensive attention has returned to actor-critic methods and to\rpolicy- gradient methods in general. Among the further developments beyond what\rwe cover here are natural-gradient methods (Amari, 1998; Kakade, 2002, Peters,\rVi- jayakumar and Schaal, 2005; Peters and Schall, 2008; Park, Kim and Kang,\r2005; Bhatnagar, Sutton, Ghavamzadeh and Lee, 2009; see Grondman, Busoniu,\rLopes and Babuska, 2012), and deterministic policy gradient (Silver et al.,\r2014). Major applications include acrobatic helicopter autopilots and AlphaGo\r(see Section 16.7).\nOur\rpresentation in this chapter is based primarily on that by Sutton, McAllester,\rSingh, and Mansour (2000), who introduced the term ��policy gradient methods��. A\ruseful overview is provided by Bhatnagar et al. (2003). One of the earliest\rrelated works is by Aleksandrov, Sysoyev, and Shemeneva (1968).\n13.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rExample 13.1\rwas implemented by Eric Graves.\n13.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe policy\rgradient theorem was first obtained by Marbach and Tsitsiklis (1998, 2001) and\rthen independently by Sutton et al. (2000). A similar expression was obtained\rby Cao and Chen (1997). Other early results are due to Konda and Tsitsiklis\r(2000, 2003) and Baxter and Bartlett (2000).\n\r\r\r13.3\n\r\r\r\r\rREINFORCE is due to Williams (1987,\r1992). The use of a power of the \n\r\rdiscount\rfactor in the update is due to Thomas (2014).\nPhansalkar and Thathachar (1995)\rproved both local and global convergence theorems for modified versions of\rREINFORCE algorithms.\n13.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe baseline\rwas introduced in Williams��s (1987, 1992) original work. Green- smith,\rBartlett, and Baxter (2004) analyzed an arguable better baseline (see Dick,\r2015).\n13.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rActor-critic\rmethods were among the earliest to be investigated in reinforce\u0026shy;ment learning\r(Witten, 1977; Barto, Sutton, and Anderson, 1983; Sutton, 1984). The algorithms\rpresented here and in Section 13.6 are based on the work of Degris, White, and\rSutton (2012), who also introduced the study of off-policy policy-gradient\rmethods.\n13.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; The first to show how continuous actions could be\rhandled this way appears\nto have been\rWilliams (1987, 1992).\n\r\rPart III: Looking Deeper\nIn this last\rpart of the book we look beyond the standard reinforcement learning ideas\rpresented in the first two parts of the book to briefly survey their\rrelationships with psychology and neuroscience, a sampling of reinforcement\rlearning applications, and some of the active frontiers for future\rreinforcement learning research.\n\r\rChapter 14\nPsychology\nIn previous\rchapters we developed ideas for algorithms based on computational con\u0026shy;siderations\ralone. In this chapter we look at some of these algorithms from another\rperspective: the perspective of psychology and its study of how animals learn.\rThe goals of this chapter are, first, to discuss ways that reinforcement\rlearning ideas and algorithms correspond to what psychologists have discovered\rabout animal learning, and second, to explain the influence reinforcement\rlearning is having on the study of animal learning. The clear formalism\rprovided by reinforcement learning that sys- temizes tasks, returns, and\ralgorithms is proving to be enormously useful in making sense of experimental\rdata, in suggesting new kinds of experiments, and in pointing to factors that\rmay be critical to manipulate and to measure. The idea of optimizing return\rover the long term that is at the core of reinforcement learning is\rcontributing to our understanding of otherwise puzzling features of animal\rlearning and behavior.\nSome of the correspondences between reinforcement learning and\rpsychological theories are not surprising because the development of\rreinforcement learning drew inspiration from psychological learning theories.\rHowever, as developed in this book, reinforcement learning explores idealized situations\rfrom the perspective of an ar\u0026shy;tificial intelligence researcher or engineer,\rwith the goal of solving computational problems with efficient algorithms,\rrather than to to replicate or explain in detail how animals learn. As a\rresult, some of the correspondences we describe connect ideas that arose\rindependently in their respective fields. We believe these points of contact\rare specially meaningful because they expose computational principles important\rto learning, whether it is learning by artificial or by natural systems.\nFor the most part, we describe correspondences between reinforcement\rlearning and learning theories developed to explain how animals like rats,\rpigeons, and rab\u0026shy;bits learn in controlled laboratory experiments. Thousands of\rthese experiments were conducted throughout the 20th century, and many are still being conducted to\u0026shy;day.\rAlthough sometimes dismissed as irrelevant to wider issues in psychology, these\rexperiments probe subtle properties of animal learning, often motivated by\rprecise theoretical questions. As psychology shifted its focus to more\rcognitive aspects of behavior, that is, to mental processes such as thought and\rreasoning, animal learning experiments came to play less of a role in\rpsychology than they once did. But this experimentation led to the discovery of\rlearning principles that are elemental and widespread throughout the animal\rkingdom, principles that should not be neglected in designing artificial\rlearning systems. In addition, as we shall see, some aspects of cognitive\rprocessing connect naturally to the computational perspective provided by\rreinforcement learning.\nThis chapter��s\rfinal section includes references relevant to the connections we dis\u0026shy;cuss as\rwell as to connections we neglect. We hope this chapter encourages readers to\rprobe all of these connections more deeply. Also included in this final section\ris a discussion of how the terminology used in reinforcement learning relates\rto that of psychology. Many of the terms and phrases used in reinforcement learning\rare bor\u0026shy;rowed from animal learning theories, but the computational/engineering\rmeanings of these terms and phrases do not always coincide with their meanings\rin psychology.\n14.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rPrediction and Control\nThe algorithms\rwe describe in this book fall into two broad categories: algorithms for predictionand algorithms for control. These categories arise naturally in solution\rmethods for the reinforcement learning problem presented in Chapter 3. In many\rways these categories respectively correspond to categories of learning\rextensively studied by psychologists: classical,or Pavlovian, conditioningand instrumental,or operant, conditioning. These correspondences are not completely accidental because of\rpsychology��s influence on reinforcement learning, but they are nevertheless\rstriking because they connect ideas arising from different objectives.\nThe prediction algorithms presented in this book estimate quantities\rthat depend on how features of an agent��s environment are expected to unfold\rover the future. We specifically focus on estimating the amount of reward an\ragent can expect to receive over the future while it interacts with its\renvironment. In this role, prediction algo\u0026shy;rithms are policy\revaluation algorithms, which are\rintegral components of algorithms for improving policies. But prediction\ralgorithms are not limited to predicting future reward; they can predict any\rfeature of the environment (see, for example, Modayil, White, and Sutton,\r2014). The correspondence between prediction algorithms and classical\rconditioning rests on their common property of predicting upcoming stimuli,\rwhether or not those stimuli are rewarding (or punishing).\nThe situation\rin an instrumental, or operant, conditioning experiment is different. Here, the\rexperimental apparatus is set up so that an animal is given something it likes\r(a reward) or something it dislikes (a penalty) depending on what the animal\rdid. The animal learns to increase its tendency to produce rewarded behavior\rand to decrease its tendency to produce penalized behavior. The reinforcing\rstimulus is said to be contingenton the animal��s behavior, whereas in classical\rconditioning it is not (although it is difficult to remove all behavior\rcontingencies in a classical conditioning experiment). Instrumental\rconditioning experiments are like those that inspired Thorndike��s Law of Effect\rthat we briefly discuss in Chapter 1. Controlis at the core of this form of learning, which\rcorresponds to the operation of reinforcement learning��s policy-improvement\ralgorithms.[25]\nThinking of classical conditioning in terms of prediction, and\rinstrumental condi\u0026shy;tioning in terms of control, is a starting point for connecting\rour computational view of reinforcement learning to animal learning, but in\rreality, the situation is more complicated than this. There is more to\rclassical conditioning than prediction; it also involves action, and so is a\rmode of control, sometimes called Pavlovian con\u0026shy;trol.\rFurther, classical and instrumental conditioning interact in interesting ways,\rwith both sorts of learning likely being engaged in most experimental\rsituations. Despite these complications, aligning the classical/instrumental distinction\rwith the prediction/control distinction is a convenient first approximation in\rconnecting rein\u0026shy;forcement learning to animal learning.\nIn psychology,\rthe term reinforcement is used to describe learning in both classical and\rinstrumental conditioning. Originally referring only to the strengthening a pat\u0026shy;tern\rof behavior, it is frequently also used for the weakening of a pattern of\rbehavior. A stimulus considered to be the cause of the change in behavior is\rcalled a reinforcer, wether or not it is contingent on the animal��s previous\rbehavior. At the end of this chapter we discuss this terminology in more detail\rand how it relates to terminology used in machine learning.\n14.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rClassical Conditioning\nWhile studying the activity of\rthe digestive system, the celebrated Russian physiolo\u0026shy;gist Ivan Pavlov found\rthat an animal��s innate responses to certain triggering stimuli can come to be\rtriggered by other stimuli that are quite unrelated to the inborn triggers. His\rexperimental subjects were dogs that had undergone minor surgery to allow the\rintensity of their salivary reflex to be accurately measured. In one case he\rdescribes, the dog did not salivate under most circumstances, but about 5\rseconds after being presented with food it produced about six drops of saliva\rover the next several seconds. After several repetitions of presenting another\rstimulus, one not re\u0026shy;lated to food, in this case the sound of a metronome,\rshortly before the introduction of food, the dog salivated in response to the\rsound of the metronome in the same way it did to the food. ��The activity of the\rsalivary gland has thus been called into play by impulses of sound��a stimulus\rquite alien to food�� (Pavlov, 1927, p. 22). Summarizing the significance of\rthis finding, Pavlov wrote:\nIt is pretty\revident that under natural conditions the normal animal must respond not only\rto stimuli which themselves bring immediate benefit or harm, but also to other\rphysical or chemical agencies��waves of sound, light, and the like��which in\rthemselves only signalthe approach of these stimuli; though it is not the\rsight and sound of the beast of prey which is\n\r\rDelay Conditioning CS\nUS\n\r\r\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\r\rISI\n\r\r\u0026nbsp;\n\r\rTrace Conditioning CS\n\r\r\u0026nbsp;\n\r\rUS\n\r\r\u0026nbsp;\n\u0026nbsp;\n\r\rt\nFigure 14.1: Arrangement of\rstimuli in two types of classical conditioning experiments. In delay\rconditioning, the CS extends throughout the interstimulus interval, or ISI,\rwhich is the time interval between the CS onset and the US onset (often with\rthe CS and US ending at the same time as shown here). In trace conditioning,\rthere is a time interval, called the trace interval, between CS offset and US\ronset.\nin itself\rharmful to the smaller animal, but its teeth and claws. (Pavlov, 1927, p. 14)\n\r\r\rConnecting new stimuli to innate Pavlovian,\rconditioning. Pavlov (or\n\r\r\r\r\rreflexes in this way is now called\rclassical, or more exactly, his translators) called inborn re\u0026shy;sponses (e.g.,\rsalivation in his demonstration described above) ��unconditioned re\u0026shy;sponses��\r(URs), their natural triggering stimuli (e.g., food) ��unconditioned stimuli��\r(USs), and new responses triggered by predictive stimuli (e.g., here also\rsalivation) ��conditioned responses�� (CRs). A stimulus that is initially\rneutral, meaning that it does not normally elicit strong responses (e.g., the\rmetronome sound), becomes a ��conditioned stimulus�� (CS) as the animal learns\rthat it predicts the US and so comes to produce a CR in response to the CS.\rThese terms are still used in describ\u0026shy;ing classical conditioning experiments\r(though better translations would have been ��conditional�� and ��unconditional��\rinstead of conditioned and unconditioned). The US is called a reinforcer\rbecause it reinforces producing a CR in response to the CS.\nFigure 14.1 shows the arrangement of stimuli in two types of\rclassical conditioning experiments: in delay conditioning, the CS extends\rthroughout the interstimulus interval, or ISI, which is the time interval\rbetween the CS onset and the US onset (with the CS ending when the US ends in a\rcommon version shown here). In trace conditioning, the US begins after the CS\rends, and the time interval between CS offset and US onset is called the trace\rinterval.\nThe salivation\rof Pavlov��s dogs to the sound of a metronome is just one example of classical\rconditioning, which has been intensively studied across many response systems\rof many species of animals. URs are often preparatory in some way, like the\rsalivation of Pavlov��s dog, or protective in some way, like an eye blink in\rresponse to something irritating to the eye, or freezing in response to seeing\ra predator. Ex\u0026shy;periencing the CS-US predictive relationship over a series of\rtrials causes the animal to learn that the CS predicts the US so that the\ranimal can respond to the CS with a CR that prepares the animal for, or\rprotects it from, the predicted US. Some CRs are similar to the UR but begin\rearlier and differ in ways that increase their effectiveness. In one\rintensively studied type of experiment, for example, a tone CS reliably\rpredicts a puff of air (the US) to a rabbit��s eye, triggering a UR consisting\rof the closure of a protective inner eyelid called the nictitating membrane.\rAfter one or more trials, the tone comes to trigger a CR consisting of membrane\rclosure that begins before the air puff and eventually becomes timed so that\rpeak closure occurs just when the air puff is likely to occur. This CR, being\rinitiated in anticipation of the air puff and appropriately timed, offers\rbetter protection than simply initiating closure as a reaction to the\rirritating US. The ability to act in anticipation of impor\u0026shy;tant events by\rlearning about predictive relationships among stimuli is so beneficial that it\ris widely present across the animal kingdom.\n14.2.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rBlocking and\rHigher-order Conditioning\nMany\rinteresting properties of classical conditioning have been observed in exper\u0026shy;iments.\rBeyond the anticipatory nature of CRs, two widely observed properties figured\rprominently in the development of classical conditioning models: blocking and higher-order conditioning.Blocking occurs when an animal fails to learn a CR\rwhen a potential CS in presented along with another CS that had been used previ\u0026shy;ously\rto condition the animal to produce that CR. For example, in the first stage of\ra blocking experiment involving rabbit nictitating membrane conditioning, a\rrabbit is first conditioned with a tone CS and an air puff US to produce the CR\rof closing its nictitating membrane in anticipation of the air puff. The\rexperiment��s second stage consists of additional trials in which a second\rstimulus, say a light, is added to the tone to form a compound tone/light CS\rfollowed by the same air puff US. In the experiment��s third phase, the second\rstimulus alone��the light��is presented to the rabbit to see if the rabbit has\rlearned to respond to it with a CR. It turns out that the rabbit produces very\rfew, or no, CRs in response to the light: learning to the light had been blockedby the previous learning to the tone.[26]Blocking results like this challenged the idea that\rconditioning depends only on simple temporal contigu\u0026shy;ity, that is, that a\rnecessary and sufficient condition for conditioning is that a US frequently\rfollows a CS closely in time. In the next section we describe the Rescorla-\rWagner model(Rescorla\rand Wagner, 1972) that offered an influential explanation\nfor blocking.\nHigher-order conditioning occurs when a previously-conditioned CS\racts as a US in conditioning another initially neutral stimulus. Pavlov\rdescribed an experiment in which his assistant first conditioned a dog to\rsalivate to the sound of a metronome that predicted a food US, as described\rabove. After this stage of conditioning, a number of trials were conducted in\rwhich a black square, to which the dog was initially indifferent, was placed in\rthe dog��s line of vision followed by the sound of the metronome��and this was notfollowed by food. In just ten trials, the dog began\rto salivate merely upon seeing the black square, despite the fact that the\rsight of it had never been followed by food. The sound of the metronome itself\racted as a US in conditioning a salivation CR to the black square CS. This was\rsecond-order conditioning. If the black square had been used as a US to\restablish salivation CRs to another otherwise neutral CS, it would have been\rthird-order conditioning, and so on. Higher-order conditioning is difficult to\rdemonstrate, especially above the second order, in part because a higher-order\rreinforcer loses its reinforcing value due to not being repeatedly followed by\rthe original US during higher-order conditioning trials. But under the right\rconditions, such as intermixing first-order trials with higher- order trials or\rby providing a general energizing stimulus, higher-order conditioning beyond\rthe second order can be demonstrated. As we describe below, the TD model of\rclassical conditioninguses\rthe backup idea that is central to our approach to extend the Rescorla-Wagner\rmodel��s account of blocking to include both the anticipatory nature of CRs and\rhigher-order conditioning.\nHigher-order instrumental conditioning occurs as well. In this case,\ra stimulus that consistently predicts primary reinforcement becomes a\rreinforcer itself, where reinforcement is primary if its rewarding or\rpenalizing quality has been built into the animal by evolution. The predicting\rstimulus becomes a secondary reinforcer, or more generally, a higher-orderor conditioned reinforcer��the latter being a bet\u0026shy;ter term when the predicted\rreinforcing stimulus is itself a secondary, or an even higher-order,\rreinforcer. A conditioned reinforcer delivers conditioned\rreinforcement: conditioned\rreward or conditioned penalty. Conditioned reinforcement acts like pri\u0026shy;mary\rreinforcement in increasing an animal��s tendency to produce behavior that leads\rto conditioned reward, and to decrease an animal��s tendency to produce behavior\rthat leads to conditioned penalty. (See our comments at the end of this chapter\rthat explain how our terminology sometimes differs, as it does here, from\rterminology used in psychology.)\nConditioned\rreinforcement is a key phenomenon that explains, for instance, why we work for\rthe conditioned reinforcer money, whose worth derives solely from what is\rpredicted by having it. In actor-critic methods described in Section 13.5 (and\rdiscussed in the context of neuroscience in Sections 15.7 and 15.8), the critic\ruses a TD method to evaluate the actor��s policy, and its value estimates\rprovide conditioned reinforcement to the actor, allowing the actor to improve\rits policy. This analog of higher-order instrumental conditioning helps address\rthe credit-assignment problem mentioned in Section 1.7 because the critic gives\rmoment-by-moment reinforcement to the actor when the primary reward signal is\rdelayed. We discuss this more below in Section 14.4.\n14.2.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe\rRescorla��Wagner Model\nRescorla and Wagner created their model mainly to account for\rblocking. The core idea of the Rescorla-Wagner model is that an animal only\rlearns when events violate its expectations, in other words, only when the\ranimal is surprised (although with\u0026shy;out necessarily implying any consciousexpectation or emotion). We first present Rescorla and Wagner��s\rmodel using their terminology and notation before shifting to the terminology\rand notation we use to describe the TD model.\nHere is how Rescorla and Wagner described their\rmodel. The model adjusts the ��associative strength�� of each component stimulus\rof a compound CS, which is a number representing how strongly or reliably that\rcomponent is predictive of a US. When a compound CS consisting of several\rcomponent stimuli is presented in a clas\u0026shy;sical conditioning trial, the\rassociative strength of each component stimulus changes in a way that depends\ron an associative strength associated with the entire stimulus compound, called\rthe ��aggregate associative strength,�� and not just on the associa\u0026shy;tive strength\rof each component itself.\nRescorla and\rWagner considered a compound CS AX, consisting of component stimuli A and X,\rwhere the animal may have already experienced stimulus A, and stimulus X might\rbe new to the animal. Let Va,VX, and Vaxrespectively\rdenote the associative strengths of stimuli A, X, and the compound AX. Suppose\rthat on a trial the compound CS AX is followed by a US, which we label stimulus\rY. Then the associative strengths of the stimulus components change according\rto these ex\u0026shy;pressions:\nAFa= aAPY (ry - Vax)\nAVX = axPY (Ry -\rVAx),\nwhere aAPY and axPY are\rthe step-size parameters, which depend on the identities of the CS components\rand the US, and RY is the\rasymptotic level of associative strength that the US Y can support. (Rescorla\rand Wagner used A here instead of R, but we use R to avoid confusion with our\ruse of A and because we usually think of this as the magnitude of a reward\rsignal, with the caveat that the US in classical conditioning is not\rnecessarily rewarding or penalizing.) A key assumption of the model is that the\raggregate associative strength VAxis equal to VA + VX. The\rassociative strengths as changed by these As become the associative strengths\rat the beginning of the next trial.\nTo be complete, the model needs a\rresponse-generation mechanism, which is a way of mapping values of Vs to\rCRs. Since this mapping would depend on details of the experimental situation,\rRescorla and Wagner did not specify a mapping but simply assumed that larger Vs\rwould produce stronger or more likely CRs, and that negative Vs would mean that\rthere would be no CRs.\nThe Rescorla-Wagner model accounts for the\racquisition of CRs in a way that\n\r\rexplains blocking. As long as the aggregate associative strength, Vax, of the stim\u0026shy;ulus compound is below the asymptotic level\rof associative strength, Ry, that the US Y can support, the prediction error Ry\r- VAx is positive. This means that over successive trials the associative\rstrengths VA and VX of the component stimuli in\u0026shy;crease until the aggregate\rassociative strength VAx equals Ry, at which point the associative strengths\rstop changing (unless the US changes). When a new compo\u0026shy;nent is added to a compound\rCS to which the animal has already been conditioned, further conditioning with\rthe augmented compound produces little or no increase in the associative\rstrength of the added CS component because the error has already been reduced\rto zero, or to a low value. The occurrence of the US is already pre\u0026shy;dicted\rnearly perfectly, so little or no error��or surprise��is introduced by the new CS\rcomponent. Prior learning blocks learning to the new component.\nTo transition from Rescorla and Wagner��s model to the TD model of\rclassical conditioning (which we just call the TD model), we first recast their\rmodel in terms of the concepts that we are using throughout this book.\rSpecifically, we match the notation we use for learning with linear function\rapproximation (Section 9.4), and we think of the conditioning process as one of\rlearning to predict the ��magnitude of the US�� on a trial on the basis of the\rcompound CS presented on that trial, where the magnitude of a US Y is the Ry of\rthe Rescorla-Wagner model as given above. We also introduce states. Because the\rRescorla-Wagner model is a trial-level model,\rmeaning that it deals with how associative strengths change from trial to trial\rwithout considering any details about what happens within and between trials,\rwe do not have to consider how states change during a trial until we present\rthe full TD model in the following section. Instead, here we simply think of a\rstate as a way of labeling a trial in terms of the collection of component CSs\rthat are present on the trial.\nTherefore, assume that trial-type, or state, s is described by a\rreal-valued vector of features x(s) =\r(xi(s), X2(s),...,xn(s))T\rwhere Xi(s) = 1if CSi, the ith component of a compound CS, is present\ron the trial and 0 otherwise. Then if the n-dimensional vector of associative\rstrengths is w, the aggregate\rassociative strength for trial-type s is\n\r\r\r(14.1)\nin reinforcement learning, and we think of it as\nnumber of a complete trial and\rnot its usual to t��s usual meaning when we extend this to that St is the\rstate corresponding to trial t.\n\r\r\r\r\r{)(s,w) = wTx(s).\nThis corresponds to a value estimate the US prediction.\nNow temporally let t denote the meaning as\ra time step (we revert the TD model below), and assume Conditioning trial t\rupdates the associative strength vector wt to wt+i as follows:\nwt+i = wt + a5t x(St),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (14.2)\nwhere a is the step-size parameter,\rand��because here we are describing the Rescorla- Wagner model��5t is the prediction\rerror\nRt is the\rtarget of the prediction on trial t, that is, the magnitude of the US, or in\rRescorla and Wagner��s terms, the associative strength that the US on the trial\rcan support. Note that because of the factor x(St) in (14.2), only the associative strengths of CS\rcomponents present on a trial are adjusted as a result of that trial. You can\rthink of the prediction error as a measure of surprise, and the aggregate\rassociative strength as the animal��s expectation that is violated when it does\rnot match the target US magnitude.\nFrom the perspective of machine learning, the Rescorla-Wagner model\ris an error- correction supervised learning rule. It is essentially the same as\rthe Least Mean Square (LMS), or Widrow-Hoff, learning rule (Widrow and Hoff,\r1960) that finds the weights��here the associative strengths��that make the\raverage of the squares of all the errors as close to zero as possible. It is a\r��curve-fitting,�� or regression, algorithm that is widely used in engineering\rand scientific applications (see Section 9.4).[27]\nThe Rescorla-Wagner model was very influential in the history of\ranimal learning theory because it showed that a ��mechanistic�� theory could\raccount for the main facts about blocking without resorting to more complex\rcognitive theories involv\u0026shy;ing, for example, an animal��s explicit recognition\rthat another stimulus component had been added and then scanning its short-term\rmemory backward to reassess the predictive relationships involving the US. The\rRescorla-Wagner model showed how traditional contiguity theories of\rconditioning��that temporal contiguity of stimuli was a necessary and sufficient\rcondition for learning��could be adjusted in a simple way to account for\rblocking (Moore and Schmajuk, 2008).\nThe\rRescorla-Wagner model provides a simple account of blocking and some other\rfeatures of classical conditioning, but it is not a complete or perfect model\rof classical conditioning. Different ideas account for a variety of other\robserved effects, and progress is still being made toward understanding the\rmany subtleties of classical conditioning. The TD model, which we describe\rnext, though also not a complete or perfect model model of classical\rconditioning, extends the Rescorla-Wagner model to address how within-trial and\rbetween-trial timing relationships among stimuli can influence learning and how\rhigher-order conditioning might arise.\n14.2.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe TD Model\nThe TD model\ris a real-timemodel, as opposed to a trial-level model like the\rRescorla-Wagner model. A single step t in the our formulation of Rescorla and\rWagner��s model above represents an entire conditioning trial. The model does\rnot apply to details about what happens during the time a trial is taking\rplace, or what might happen between trials. Within each trial an animal might\rexperience various stimuli whose onsets occur at particular times and that have\rparticular durations. These timing relationships strongly influence learning.\rThe Rescorla-Wagner model\n\r\ralso does not\rinclude a mechanism for higher-order conditioning, whereas for the TD model,\rhigher-order conditioning is a natural consequence of the backup idea that is\rat the base of TD algorithms.\nTo describe the TD model we begin with the formulation of the\rRescorla-Wagner model above, but tnow labels time\rsteps within or between trials instead of complete trials. Think of the time\rbetween t and t +1 as a small time interval, say .01 second, and think of a\rtrial as a sequences of states, one associated with each time step, where the\rstate at step t now represents details of how stimuli are represented at t\rinstead of just a label for the CS components present on a trial. In fact, we\rcan completely abandon the idea of trials. From the point of view of the\ranimal, a trial is just a fragment of its continuing experience interacting\rwith its world. Following our usual view of an agent interacting with its\renvironment, imagine that the animal is experiencing an endless sequence of\rstates s, each represented by a feature vector x(s). That said, it is still often convenient to\rrefer to trials as fragments of time during which patterns of stimuli repeat in\ran experiment.\nState features are not restricted to describing the external stimuli\rthat an animal experiences; they can describe neural activity patterns that\rexternal stimuli produce in an animal��s brain, and these patterns can be\rhistory-dependent, meaning that they can be persistent patterns produced by\rsequences of external stimuli. Of course, we do not know exactly what these\rneural activity patterns are, but a real-time model like the TD model allows\rone to explore the consequences on learning of different hypotheses about the\rinternal representations of external stimuli. For these reasons, the TD model\rdoes not commit to any particular state representation. In addition, because\rthe TD model includes discounting and eligibility traces that span time\rintervals between stimuli, the model also makes it possible to explore how\rdiscounting and eligibility traces interact with stimulus representations in\rmaking predictions about the results of classical conditioning experiments.\nBelow we\rdescribe some of the state representations that have been used with the TD\rmodel and some of their implications, but for the moment we stay agnostic about\rthe representation and just assume that each state s is represented by a\rfeature vector x(s) =\r(xi(s), X2(s),...,xn(s))T.\rThen the aggregate associative strength corresponding to a state s is given by\r(14.1), the same as for the Rescorla-Wgner model, but the TD model updates the\rassociative strength vector, w, differently. With t now labeling a time step instead of a complete\rtrial, the TD model governs learning according to this update:\nwt+i = wt + a8t et,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (14.4)\nwhich replaces xt (St) in the Rescorla-Wagner update (14.2) with et, a vector of eligibility traces, and instead of\rthe 8t of (14.3), here 8t is a TD error:\n8t = Rt+i + 7v(St+i,wt) -\rv(St,wt),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (14.5)\nwhere 7is a discount factor (between 0and 1), Rt is the prediction target at time t, and v(St+i,wt) and v(St,wt) are\raggregate associative strengths at t + 1and t as defined by (14.1).\nEach component\riof the eligibility-trace vector et increments or decrements ac\u0026shy;cording to the component x^(St) of the\rfeature vector x(St),\rand otherwise decays with a rate determined by 7A:\net+i = yAet + x(St).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (14.6)\nHere A is the usual eligibility trace decay\rparameter.\nNote that if 7= 0, the TD model reduces to the Rescorla-Wagner\rmodel with the exceptions that: the meaning of t is different in each case (a\rtrial number for the Rescorla-Wagner model and a time step for the TD model),\rand in the TD model there is a one-time-step lead in the prediction target R.\rThe TD model is equivalent to the backward view of the semi-gradient TD(A)\ralgorithm with linear function approximation (Chapter 12), except that Rt in\rthe model does not have to be a reward signal as it does when the TD algorithm\ris used to learn a value function for policy-improvement.\n14.2.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rTD Model\rSimulations\nReal-time\rconditioning models like the TD model are interesting primarily because they\rmake predictions for a wide range of situations that cannot be represented by\rtrial-level models. These situations involve the timing and durations of\rconditionable stimuli, the timing of these stimuli in relation to the timing of\rthe US, and the timing and shapes of CRs. For example, the US generally must\rbegin after the onset of a neutral stimulus for conditioning to occur, with the\rrate and effectiveness of learning depending on the inter-stimulus interval, or\rISI, the interval between the onsets of the CS and the US. When CRs appear,\rthey generally begin before the appearance of the US and their temporal\rprofiles change during learning. In conditioning with compound CSs, the\rcomponent stimuli of the compound CSs may not all begin and end at the same\rtime, sometimes forming what is called a serial compoundin\rwhich the component stimuli occur in a sequence over time. Timing\rconsiderations like these make it important to consider how stimuli are\rrepresented, how these representations unfold over time during and between\rtrials, and how they interact with discounting and eligibility traces.\nFigure 14.2 shows three of the stimulus representations that have\rbeen used in exploring the behavior of the TD model: the complete serial compound(CSC), the microstimulus(MS),\rand the presencerepresentations (Ludvig, Sutton, and Kehoe, 2012).\rThese representations differ in the degree to which they force generalization\ramong nearby time points during which a stimulus is present.\nThe simplest of the representations shown in Figure 14.2 is the\rpresence repre\u0026shy;sentation in the figure��s right column. This representation has\ra single feature for each component CS present on a trial, where the feature\rhas value 1 whenever that component is present, and 0 otherwise.[28]The presence representation is not a real\u0026shy;istic\rhypothesis about how stimuli are represented in an animal��s brain, but as we\rdescribe below, the TD model with this representation can produce many of the\rtiming phenomena seen in classical conditioning.\nFor the CSC representation (left\rcolumn of Figure 14.2), the onset of each exter\u0026shy;nal stimulus initiates a\rsequence of precisely-timed short-duration internal signals that continues\runtil the external stimulus ends.5This is like assuming the animal��s nervous system has a clock that\rkeeps precise track of time during stimulus presenta-\nthroughout the trial, there is a feature, xi,for each component CSi, i=\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; where\rXi(St)= 1\nfor all times twhen the CSi is\rpresent, and equals zero otherwise.\n5In our formalism,\rfor each CS component CSi present\ron a trial, and for each time step tduring a trial,\rthere is a separate feature xit, where xt(St^)=\r1 if t= tحfor\rany tحat\rwhich CSi is present, and equals\r0 otherwise. This is different from the CSC representation in Sutton and Barto\r(1990) in which there are the same distinct features for each time step but no\rreference to external stimuli; hence the name complete serial compound.\nComplete\rSerial ,ߊ.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; i.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ^\nCompound M(Crost(mul(\rPresence\n\r\nFigure 14.2: Three stimulus representations (in columns) sometimes\rused with the TD model. Each row represents one element of the stimulus\rrepresentation. The three representations vary along a temporal\rgeneralization gradient, with no general\u0026shy;ization between nearby time points\rin the complete serial compound (left column) and complete generalization between\rnearby time points in the presence representation (right column). The\rmicrostimulus representation occupies a middle ground. The degree of temporal\rgeneralization determines the temporal granularity with which US predictions\rare learned. Adapted with minor changes from Learning \u0026amp; Behavior, Evaluating the TD Model of Classical Conditioning, volume 40,\r2012, p. 311, E. A. Ludvig, R. S. Sutton, E. J. Kehoe. With permission of\rSpringer.\n\r\r\r\r\r\u0026nbsp;\ntions; it is\rwhat engineers call a ��tapped delay line.�� Like the presence representation,\rthe CSC representation is unrealistic as a hypothesis about how the brain\rinternally represents stimuli, but Ludvig et al. (2012) call it a ��useful\rfiction�� because it can reveal details of how the TD model works when\rrelatively unconstrained by the stimulus representation. The CSC representation\ris also used in most TD models of dopamine-producing neurons in the brain, a\rtopic we take up in Chapter 15. The CSC representation is often viewed as an\ressential part of the TD model, although this view is mistaken.\nThe MS representation (center column of Figure 14.2) is like the CSC\rrepresen\u0026shy;tation in that each external stimulus initiates a cascade of internal\rstimuli, but in this case the internal stimuli��the microstimuli��are not of such\rlimited and non\u0026shy;overlapping form; they are extended over time and overlap. As\rtime elapses from stimulus onset, different sets of microstimuli become more or\rless active, and each subsequent microstimulus becomes progressively wider in\rtime and reaches a lower maximal level. Of course, there are many MS\rrepresentations depending on the nature of the microstimuli, and a number of\rexamples of MS representations have been studied in the literature, in some\rcases along with proposals for how an ani\u0026shy;mal��s brain might generate them (see\rthe Bibliographic and Historical Comments at the end of this chapter). MS\rrepresentations are more realistic than the presence or CSC representations as\rhypotheses about neural representations of stimuli, and they allow the behavior\rof the TD model to be related to a broader collection of phenomena observed in\ranimal experiments. In particular, by assuming that cas\u0026shy;cades of microstimuli\rare initiated by USs as well as by CSs, and by studying the significant effects\ron learning of interactions between microstimuli, eligibility traces, and\rdiscounting, the TD model is helping to frame hypotheses to account for many of\rthe subtle phenomena of classical conditioning and how an animal��s brain might\rproduce them. We say more about this below, particularly in Chapter 15 where we\rdiscuss reinforcement learning and neuroscience.\nEven with the simple presence representation, however, the TD model\rproduces all the basic properties of classical conditioning that are accounted\rfor by the Rescorla- Wagner model, plus features of conditioning that are\rbeyond the scope of trial-level models. For example, as we have already\rmentioned, a conspicuous feature of clas\u0026shy;sical conditioning is that the US\rgenerally must begin afterthe onset of a neutral stimulus for conditioning to\roccur, and that after conditioning, the CR begins before the appearance of the US. In other words, conditioning generally\rrequires a positive ISI, and the CR generally anticipates the US. How the\rstrength of conditioning (e.g., the percentage of CRs elicited by a CS) depends\ron the ISI varies substantially across species and response systems, but it\rtypically has the following properties: it is neg\u0026shy;ligible for a zero or\rnegative ISI, i.e., when the US onset occurs simultaneously with, or earlier\rthan, the CS onset (although research has found that associative strengths\rsometimes increase slightly or become negative with negative ISIs); it\rincreases to a maximum at a positive ISI where conditioning is most effective;\rand it then decreases to zero after an interval that varies widely with\rresponse systems. The precise shape of this dependency for the TD model depends\ron the values of its parameters anddetails of the stimulus representation, but these basic features of\rISI-dependency are core properties of the TD model.\nOne of the theoretical issues arising with serial-compound\rconditioning, that is, conditioning with a compound CS whose components occur\rin a sequence, concerns the facilitation of remote associations. It has been\rfound that if the empty trace interval between the CS and the US is filled with\ra second CS to form a serial- compound stimulus, then conditioning to the first\rCS is facilitated. Figure 14.3 shows the behavior of the TD model with the\rpresence representation in a simulation of such an experiment whose timing\rdetails are shown at the top of the figure. Consistent with the experimental\rresults (Kehoe, 1982), the model shows facilitation of both\nof the first CS due\nthe rate of\rconditioning and the asymptotic level of conditioning to the presence of the\rsecond CS.\n\r\r\rA\n\r\r\r\r\r\r\r\rFigure 14.3: Facilitation of a remote association\rby an intervening\n\r\r\r\r\rCSA- CSB- US -\n\r\r\rTRIALS\n\r\r\r\r\rB\nstimulus in the TD\nmodel. Top: temporal\rrelationships among stimuli within a trial. Bottom: behavior over trials of\rCSA��s associative strength when CSA is presented in a serial compound as shown\rin the top panel, and when presented in an identical temporal relationship to\rthe US, only without CSB. Adapted from Sutton and Barto (1990).\nA well-known demonstration of the effects on conditioning of\rtemporal relation\u0026shy;ships among stimuli within a trial is an experiment by Egger\rand Miller (1962) that involved two overlapping CSs in a delay configuration as\rshown in the top panel of Figure 14.4. Although CSB was in a better temporal\rrelationship with the US, the presence of CSA substantially reduced\rconditioning to CSB as compared to controls in which CSA was absent. The bottom\rpanel of Figure 14.4 shows the same result be\u0026shy;ing generated by the TD model in\ra simulation of this experiment with the presence representation.\n\rCSA------ 1\u0026quot;\n\n1.6\n\r\r\r\r\r\u0026nbsp;\nCSA ABSENT\n\r\r\rWCSA\n\r\r\r\r\rB\n\r\n\r\r\r\r\r\u0026nbsp;\n0\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\r80 TRIALS\nFigure 14.4: The Egger-Miller, or\rprimacy, effect in the TD model. Top: temporal rela\u0026shy;tionships among stimuli\rwithin a trial. Bottom: behavior over trials of CSB��s associative strength when\rCSB is presented with and without CSA. Adapted from Sutton and Barto (1990).\nThe TD model accounts for blocking because it is an error-correcting\rlearning rule like the Rescorla-Wagner model. Beyond accounting for basic\rblocking results, however, the TD model predicts (with the presence\rrepresentation and more complex representations a well) that blocking is\rreversed if the blocked stimulus is moved earlier in time so that its onset\roccurs before the onset of the blocking stimulus. This feature of the TD\rmodel��s behavior deserves attention because it had not been observed at the\rtime of the model��s introduction. Recall that in blocking, if an animal has\ralready learned that one CS predicts a US, then learning that a newly-added\rsecond CS also predicts the US is much reduced, i.e., is blocked. But if the\rnewly- added second CS begins earlier than the pretrained CS, then��according to\rthe TD model�� learning to the newly-added CS is not blocked. In fact, as\rtraining continues and the newly-added CS gains associative strength, and the\rpretrained CS loses associative strength. The behavior of the TD model under\rthese conditions is shown in Figure 14.5. This simulation experiment differed\rfrom the Egger-Miller experiment of Figure 14.4 in that the shorter CS with the\rlater onset was given prior training until it was fully associated with the US.\rThis surprising prediction led Kehoe, Scheurs, and Graham (1987) to conduct the\rexperiment using the well-studied rabbit nictitating membrane preparation.\rTheir results confirmed the model��s prediction, and they noted that non-TD\rmodels have considerable difficulty explaining their data.\n\r\r\r080\n\r\r\r\r\r\r\r\rFigure 14.5:\rTemporal primacy overriding blocking in the TD model. Top: temporal relationships\rbetween stimuli. Bottom: behavior over trials of CSB��s associative strength\rwhen CSB is presented with and without CSA. The only difference between\rthis simulation and that shown in Figure 14.4 was that here CSB started out\rfully conditioned��CSB��s associative strength was initially set to 1.653,\rthe final level reached when CSB was presented alone for 80 trials, as in\rthe ��CSA-absent�� case in Figure 14.4. Adapted from Sutton and Barto (1990).\n\r\r\r\r\r\r\r\rTRIALS\n\r\r\r\r\rWith the TD model, an earlier\rpredictive stimulus takes precedence over a later predictive stimulus because,\rlike all the prediction methods described in this book, the TD model is based\ron the backup idea: updates to associative strengths shift the strengths at a\rparticular state toward a ��backed-up�� strength for that state. An\u0026shy;other\rconsequence of backups is that the TD model provides an account of higher-order conditioning, a feature of classical conditioning that is beyond the\rscope of the Rescoral-Wagner and similar models. As we described above,\rhigher-order con\u0026shy;ditioning is the phenomenon in which a previously-conditioned\rCS can act as a US in conditioning another initially neutral stimulus. Figure\r14.6 shows the behavior of the TD model (again with the presence\rrepresentation) in a higher-order condi\u0026shy;tioning experiment��in this case it is\rsecond-order conditioning. In the first phase (not shown in the figure), CSB is\rtrained to predict a US so that its associative strength increases, here to\r1.6. In the second phase, CSA is paired with CSB in the absence of the US, in\rthe sequential arrangement shown at the top of the figure. CSA acquires\rassociative strength even though it is never paired with the US. With continued\rtraining, CSA��s associative strength reaches a peak and then decreases because\rthe associative strength of CSB, the secondary reinforcer, decreases so that it\rloses its ability to provide secondary reinforcement. CSB��s associative\rstrength decreases because the US does not occur in these higher-order\rconditioning trials. These are extinction\rtrialsfor CSB because its\rpredictive relationship to the US is disrupted so that its ability to act as a\rreinforcer decreases. This same pattern is seen in animal experiments. This\rextinction of conditioned reinforcement in higher- order conditioning trials\rmakes it difficult to demonstrate higher-order conditioning unless the original\rpredictive relationships are periodically refreshed by occasionally inserting\rfirst-order trials.\nThe TD model produces an analog of second- and higher-order conditioning\rbe-\n\r\r\rFigure 14.6: Second-order\rconditioning with the TD model. Top: temporal relationships between\rstimuli. Bottom: behavior of the associative strengths associated with CSA\rand CSB over trials. The second stimulus, CSB, has an initial associative\rstrength of 1.653 at the beginning of the simulation. Adapted from Sutton\rand Barto (1990).\n\r\r\r\r\r\r\r\rTRIALS\n\r\r\r\r\rcause 7{)(St+i,wt) - v(St,wt)\rappears in the TD error \u0026amp; (14.5). This means that as a result of previous\rlearning, 7v(St+i,wt) can differ from v(St,wt), making \u0026#12316;non-zero (a temporal difference). This difference has the same\rstatus as Rt+i in (14.5), im\u0026shy;plying that as far as learning is concerned there\ris no difference between a temporal difference and the occurrence of a US. In\rfact, this feature of the TD algorithm is one of the major reasons for its\rdevelopment, which we now understand through its connection to dynamic\rprogramming as described in Chapter 6. Backing up values is\rintimately related to second-order, and higher-order, conditioning.\nIn the examples of the TD\rmodel��s behavior described above, we examined only the changes in the\rassociative strengths of the CS components; we did not look at what the model\rpredicts about properties of an animal��s conditioned responses (CRs): their\rtiming, shape, and how they develop over conditioning trials. These properties\rdepend on the species, the response system being observed, and parameters of\rthe conditioning trials, but in many experiments with different animals and\rdifferent response systems, the magnitude of the CR, or the probability of a\rCR, increases as the expected time of the US approaches. For example, in\rclassical conditioning of a rabbit��s nictitating membrane response that we\rmentioned above, over conditioning trials the delay from CS onset to when the\rnictitating membrane begins to move across the eye decreases over trials, and\rthe amplitude of this anticipatory closure gradually increases over the\rinterval between the CS and the US until the membrane reaches maximal closure\rat the expected time of the US. The timing and shape of this CR is critical to\rits adaptive significance��covering the eye too early reduces vision (even\rthough the nictitating membrane is translucent), while covering it too late is\rof little protective value. Capturing CR features like these is challenging for\rmodels of classical conditioning.\nThe TD model does not include as part of its\rdefinition any mechanism for trans\u0026shy;lating the time\rcourse of the US prediction, {)(St,wt), into a profile that can be compared\rwith the properties of an animal��s CR. The simplest choice is to let the time\rcourse of a simulated CR equal the time course of the US prediction. In this\rcase, features of simulated CRs and how they change over trials depend only on\rthe stimulus representation chosen and the values of the model��s parameters a, 7, and A.\nFigure 14.7 shows the time courses of US predictions at different\rpoints during learning with the three representations shown in Figure 14.2. For\rthese simulations the US occurred 25 times steps after the onset of the CS, and\ra= .05, A = .95 and 7= .97. With the CSC representation (Figure 14.7 left), the curve of\rthe US prediction formed by the TD model increases exponentially throughout the\rinterval between the CS and the US until it reaches a maximum exactly when the\rUS occurs (at time step 25). This exponential increase is the result of\rdiscounting in the TD model learning rule. With the presence representation\r(Figure 14.7 middle), the US prediction is nearly constant while the stimulus\ris present because there is only one weight, or associative strength, to be\rlearned for each stimulus. Consequently, the TD model with the presence\rrepresentation cannot recreate many features of CR timing. With an MS\rrepresentation (Figure 14.7 right), the development of the TD modePs US\rprediction is more complicated. After 200 trials the prediction��s profile is a\rreasonable approximation of the US prediction curve produced with the CSC\rrepresentation.\n\r\r\u0026nbsp;SHAPE \u0026nbsp;\\* MERGEFORMAT \r\r\r\u0026nbsp;\n\r\r\r\r\r\r\u0026nbsp;\n\r\r\rComplete Serial Compound\n\n\r\r\r\r\u0026nbsp;\n\r\r\rMicrostimulus\n\r\r\r\r\r\r\r\r-Trial 200 -Trial 50 Trial 25\n\r\r\r\r\r\r\r\rTime Steps\n\r\r\r\r\r\r\r\rPresence\n\r\r\r\r\rTime Steps\n\r\r\u0026nbsp;\n\r\rFigure 14.7:\rTime course of US prediction over the course of acquisition for the TD model\rwith three different stimulus representations. Left: With the complete serial\rcompound (CSC),the US prediction increases exponentially through\rthe interval, peaking at the time of the US. At asymptote (trial 200), the US\rprediction peaks at the US intensity (1 in these simulations). Middle: With the\rpresence representa\u0026shy;tion, the US prediction converges to an almost constant\rlevel. This constant level is determined by the US intensity and the length of\rthe CS-US interval. Right: With the microstimulus representation, at asymptote,\rthe TD model approximates the exponentially increasing time course depicted\rwith the CSC through a linear combi\u0026shy;nation of the different microstimuli.\rAdapted with minor changes from Learning\r\u0026amp;\rBehavior,Evaluating the TD Model of Classical Conditioning, volume 40, 2012,\rE. A. Ludvig, R. S. Sutton, E. J. Kehoe. With permission of Springer.\nThe US prediction curves shown in Figure 14.7 were not intended to\rprecisely match profiles of CRs as they develop during conditioning in any\rparticular animal experiment, but they illustrate the strong influence that the\rstimulus representation has on predictions derived from the TD model. Further,\ralthough we can only men\u0026shy;tion it here, how the stimulus representation\rinteracts with discounting and eligibility traces is important in determining\rproperties of the US prediction profiles produced by the TD model. Another\rdimension beyond what we can discuss here is the influ\u0026shy;ence of different\rresponse-generation mechanisms that translate US predictions into CR profiles;\rthe profiles shown in Figure 14.7 are ��raw�� US prediction profiles. Even\rwithout any special assumption about how an animal��s brain might produce overt\rresponses from US predictions, however, the profiles in Figure 14.7 for the CSC\rand MS representations increase as the time of the US approaches and reach a\rmaximum at the time of the US, as is seen in many animal conditioning\rexperiments.\nThe TD model, when combined with particular stimulus representations\rand response- generation mechanisms, is able to account for a surprisingly-wide\rrange of phenomena observed in animal classical conditioning experiments, but\rit is far from being a per\u0026shy;fect model. To generate other details of classical\rconditioning the model needs to be extended, perhaps by adding model-based\relements and mechanisms for adap\u0026shy;tively altering some of its parameters. Other\rapproaches to modeling classical condi\u0026shy;tioning depart significantly from the\rRescorla-Wagner-style error-correction process. Bayesian models, for example,\rwork within a probabilistic framework in which expe\u0026shy;rience revises probability\restimates. All of these models usefully contribute to our understanding of\rclassical conditioning.\nPerhaps the most notable feature of the TD model is that it is based\ron a theory�� the theory we have described in this book��that suggests an account\rof what an animal��s nervous system is trying to dowhile\rundergoing conditioning: it is trying to form accurate long-term predictions,consistent with the limitations imposed by the way stimuli are\rrepresented and how the nervous system works. In other words, it suggests a normative accountof classical conditioning in which long-term, instead of immediate,\rprediction is a key feature.\nThe development of the TD model of classical conditioning is one\rinstance in which the explicit goal was to model some of the details of animal\rlearning behavior. In addition to its standing as an algorithm, then, TD learning is also the basis of this modelof aspects of biological learning. As we discuss in Chapter 15, TD learning has\ralso turned out to underlie an influential model of the activity of neurons\rthat produce dopamine, a chemical in the brain of mammals that is deeply\rinvolved in reward processing. These are instances in which reinforcement\rlearning theory makes detailed contact with animal behavioral and neural data.\nWe now turn to considering correspondences between reinforcement learning\rand animal behavior in instrumental conditioning experiments, the other major\rtype of laboratory experiment studied by animal learning psychologists.\n\r\r14.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rInstrumental Conditioning\nIn instrumental\rconditioningexperiments\rlearning depends on the consequences of behavior: the delivery of a reinforcing\rstimulus is contingent on what the animal does. In classical conditioning\rexperiments, in contrast, the reinforcing stimulus�� the US��is delivered\rindependently of the animal��s behavior. Instrumental condi\u0026shy;tioning is usually\rconsidered to be the same as operant conditioning,the term B. F. Skinner (1938, 1961) introduced for\rexperiments with behavior-contingent reinforce\u0026shy;ment, though the experiments and\rtheories of those who use these two terms differ in a number of ways, some of\rwhich we touch on below. We will exclusively use the term instrumental\rconditioning for experiments in which reinforcement is contingent upon\rbehavior. The roots of instrumental conditioning go back to experiments per\u0026shy;formed\rby the American psychologist Edward Thorndike one hundred years before\rpublication of the first edition of this book.\nThorndike observed the behavior of cats when they were placed in\r��puzzle boxes�� from which they could escape by appropriate actions (Figure\r14.8). For example, a cat could open the door of one box by performing a\rsequence of three separate actions: depressing a platform at the back of the\rbox, pulling a string by clawing at it, and pushing a bar up or down. When\rfirst placed in a puzzle box, with food visible outside, all but a few of\rThorndike��s cats displayed ��evident signs of discomfort�� and extraordinarily\rvigorous activity ��to strive instinctively to escape from confinement��\r(Thorndike, 1898).\nIn experiments\rwith different cats and boxes with different escape mechanisms, Thorndike\rrecorded the amounts of time each cat took to escape over multiple ex\u0026shy;periences\rin each box. He observed that the time almost invariably decreased with\rsuccessive experiences, for example, from 300 seconds to 6or 7 seconds. He described\n\r\nFigure 14.8: One of Thorndike��s\rpuzzle boxes. Reprinted from Thorndike, Animal Intelli\u0026shy;gence: An Experimental\rStudy of the Associative Processes in Animals, The\rPsychological Review, Series of Monograph Supplements,II(4), Macmillan, New York, 1898, permission\rpending.\n\r\r\r\r\r\u0026nbsp;\ncats�� behavior in a puzzle box like this:\nThe cat that is clawing all over\rthe box in her impulsive struggle will probably claw the string or loop or\rbutton so as to open the door. And gradually all the other non-successful\rimpulses will be stamped out and the particular impulse leading to the\rsuccessful act will be stamped in by the resulting pleasure, until, after many\rtrials, the cat will, when put in the box, immediately claw the button or loop in\ra definite way. (Thorndike 1898, p. 13)\nThese and\rother experiments (some with dogs, chicks, monkeys, and even fish) led\rThorndike to formulate a number of ��laws�� of learning, the most influential\rbeing the Law of Effect,a version of which we quoted in Chapter 1. This law describes what\ris generally known as learning by trial and error. As mentioned in Chapter 1, many aspects of the Law of Effect have generated\rcontroversy, and its details have been modified over the years. Still the\rlaw��in one form or anotherһexpresses\ran enduring principle of learning.\nEssential features of reinforcement learning algorithms correspond\rto features of animal learning described by the Law of Effect. First,\rreinforcement learning algo\u0026shy;rithms are selectional, meaning that they try alternatives and select\ramong them by comparing their consequences. Second, reinforcement learning\ralgorithms are associative, meaning that the alternatives found by selection are associated\rwith particular situations, or states, to form the agent��s policy. Like\rlearning described by the Law of Effect, reinforcement learning is not just the\rprocess of findingactions that produce a lot of reward, but also of connectingthese actions to situations or states. Thorndike\rused the phrase learning by ��selecting and connecting�� (Hilgard, 1956). Natural\rselection in evolution is a prime example of a selectional process, but it is\rnot associative (at least as it is commonly understood); supervised learning is\rassociative, but it is not selectional because it relies on instructions that\rdirectly tell the agent how to change its behavior.\nIn computational terms, the Law of Effect describes an elementary\rway of com\u0026shy;bining searchand memory:\rsearch in the form of trying and selecting among many actions in each situation,\rand memory in the form of associations linking situations with the actions\rfound��so far��to work best in those situations. Search and memory are essential\rcomponents of all reinforcement learning algorithms, whether memory takes the\rform of an agent��s policy, value function, or environment model.\nA reinforcement learning algorithm��s need to search means that it\rhas to explore in some way. Animals clearly explore as well, and early animal\rlearning researchers disagreed about the degree of guidance an animal uses in\rselecting its actions in sit\u0026shy;uations like Thorndike��s puzzle boxes. Are actions\rthe result of ��absolutely random, blind groping�� (Woodworth, 1938, p. 777), or\ris there some degree of guidance, either from prior learning, reasoning, or\rother means? Although some thinkers, including Thorndike, seem to have taken\rthe former position, others favored more deliberate exploration. Reinforcement\rlearning algorithms allow wide latitude for how much guidance an agent can\remploy in selecting actions. The forms of exploration we have used in the\ralgorithms presented in this book, such as e-greedy and upper-confidence- bound\raction selection, are merely among the simplest. More sophisticated methods are\rpossible, with the only stipulation being that there has to be someform of exploration for the algorithms to work effectively.\nThe feature of our treatment of reinforcement learning allowing the\rset of actions available at any time to depend on the environment��s current\rstate echoes something Thorndike observed in his cats�� puzzle-box behaviors.\rThe cats selected actions from those that they instinctively perform in their\rcurrent situation, which Thorndike called their ��instinctual impulses.�� First\rplaced in a puzzle box, a cat instinctively scratches, claws, and bites with\rgreat energy: a cat��s instinctual responses to finding itself in a confined\rspace. Successful actions are selected from these and not from every possible\raction or activity. This is like the feature of our formalism where the action\rselected from a state sbelongs to a set of admissible actions, A(s).Specifying these sets is an important aspect of reinforcement\rlearning because it can radically simplify learning. They are like an animal��s\rinstinctual impulses. On the other hand, Thorndike��s cats might have been\rexploring according to an instinctual context- specific orderingover actions rather than by just selecting from a set of\rinstinctual impulses. This is another way to make reinforcement learning\reasier.\nAmong the most prominent animal learning researchers influenced by\rthe Law of Effect were Clark Hull (e.g., Hull, 1943) and B. F. Skinner (e.g.,\rSkinner, 1938). At the center of their research was the idea of selecting\rbehavior on the basis of its consequences. Reinforcement learning has features\rin common with Hull��s theory, which included eligibility-like mechanisms and\rsecondary reinforcement to account for the ability to learn when there is a\rsignificant time interval between an action and the consequent reinforcing\rstimulus (see Section 14.4). Randomness also played a role in Hull��s theory\rthrough what he called ��behavioral oscillation�� to introduce exploratory\rbehavior.\nSkinner did not fully subscribe to the memory aspect of the Law of\rEffect. Be\u0026shy;ing averse to the idea of associative linkages, he instead\remphasized selection from spontaneously-emitted behavior. He introduced the\rterm ��operant�� to emphasize the key role of an action��s effects on an animal��s\renvironment. Unlike the experiments of Thorndike and others, which consisted of\rsequences of separate trials, Skinner��s operant conditioning experiments\rallowed animal subjects to behave for extended periods of time without\rinterruption. He invented the operant conditioning cham\u0026shy;ber, now called a\r��Skinner box,�� the most basic version of which contains a lever or key that an\ranimal can press to obtain a reward, such as food or water, which would be\rdelivered according to a well-defined rule, called a reinforcement schedule. By\rrecording the cumulative number of lever presses as a function of time, Skinner\rand his followers could investigate the effect of different reinforcement\rschedules on the animal��s rate of lever-pressing. Modeling results from\rexperiments likes these using the reinforcement learning principles we present\rin this book is not well developed, but we mention some exceptions in the\rBibliographic and Historical Remarks section at the end of this chapter.\nAnother of Skinner��s\rcontributions resulted from his recognition of the effective\u0026shy;ness of training\ran animal by reinforcing successive approximations of the desired behavior, a\rprocess he called shaping.Although this technique had been used by others,\rincluding Skinner himself, its significance was impressed upon him when he and\rcolleagues were attempting to train a pigeon to bowl by swiping a wooden ball\rwith its beak. After waiting for a long time without seeing any swipe that they\rcould reinforce, they\n... decided to reinforce any\rresponse that had the slightest resemblance to a swipe��perhaps, at first,\rmerely the behavior of looking at the ball�� and then to select responses which\rmore closely approximated the final form. The result amazed us. In a few\rminutes, the ball was caroming off the walls of the box as if the pigeon had\rbeen a champion squash player. (Skinner, 1958, p. 94)\nNot only did\rthe pigeon learn a behavior that is unusual for pigeons, it learned quickly\rthrough an interactive process in which its behavior and the reinforcement\rcontingencies changed in response to each other. Skinner compared the process\rof altering reinforcement contingencies to the work of a sculptor shaping clay\rinto a de\u0026shy;sired form. Shaping is a powerful technique for computational\rreinforcement learning systems as well. When it is difficult for an agent to\rreceive any non-zero reward sig\u0026shy;nal at all, either due to sparseness of\rrewarding situations or their inaccessibility given initial behavior, starting\rwith an easier problem and incrementally increasing its difficulty as the agent\rlearns can be an effective, and sometimes indispensable, strategy.\nA concept from psychology that is especially relevant in the context\rof instrumental conditioning is motivation, which refers to processes that influence the\rdirection and strength, or vigor, of behavior. Thorndike��s cats, for example,\rwere motivated to escape from puzzle boxes because they wanted the food that\rwas sitting just outside. Obtaining this goal was rewarding to them and\rreinforced the actions allowing them to escape. It is difficult to link the\rconcept of motivation, which has many dimensions, in a precise way to\rreinforcement learning��s computational perspective, but there are clear links\rwith some of its dimensions.\nIn one sense, a reinforcement learning agent��s reward signal is at\rthe base of its motivation: the agent is motivated to maximize the total reward\rit receives over the long run. A key facet of motivation, then, is what makes\ran agent��s experience rewarding. In reinforcement learning, reward signals\rdepend on the state of the reinforcement learning agent��s environment and the\ragent��s actions. Further, as pointed out in Chapter 1, the state of the agent��s\renvironment not only includes information about what is external to the\rmachine, like an organism or a robot, that houses the agent, but also what is\rinternal to this machine. Some internal state components correspond to what\rpsychologists call an animal��s motivational\rstate, which influences what is\rrewarding to the animal. For example, an animal will be more rewarded by eating\rwhen it is hungry than when it has just finished a satisfying meal. The concept\rof state dependence is broad enough to allow for many types of modulating\rinfluences on the generation of reward signals.\nValue functions provide a further link to psychologists�� concept of\rmotivation. If the most basic motive for selecting an action is to obtain as\rmuch reward as possible, for a reinforcement learning agent that selects\ractions using a value function, a more proximal motive is to ascend the gradient of its value function, that is, to select actions expected to lead to the\rmost highly-valued next states (or what is essentially the same thing, to\rselect actions with the greatest action-values). For these agents, value\rfunctions are the main driving force determining the direction of their\rbehavior.\nAnother dimension of motivation is that an animal��s motivational\rstate not only influences learning, but also influences the strength, or vigor,\rof the animal��s behavior after learning. For example, after learning to find\rfood in the goal box of a maze, a hungry rat will run faster to the goal box\rthan one that is not hungry. This aspect of motivation does not link so cleanly\rto the reinforcement learning framework we present here, but in the\rBibliographical and Historical Remarks section at the end of this chapter we\rcite several publications that propose theories of behavioral vigor based on\rreinforcement learning.\nWe turn now to\rthe subject of learning when reinforcing stimuli occur well after the events\rthey reinforce. The mechanisms used by reinforcement learning algorithms to\renable learning with delayed reinforcement��eligibility traces and TD learning��\rclosely correspond to psychologists�� hypotheses about how animals can learn\runder these conditions.\n14.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rDelayed Reinforcement\nThe Law of\rEffect requires a backward effect on connections, and some early critics of the\rlaw could not conceive of how the present could affect something that was in\rthe past. This concern was amplified by the fact that learning can even occur\rwhen there is a considerable delay between an action and the consequent reward\ror penalty. Similarly, in classical conditioning, learning can occur when US\ronset follows CS offset by a non-negligible time interval. We call this the\rproblem of delayed reinforcement, which is related to what Minsky (1961) called\rthe ��credit-assignment problem for learning systems��: how do you distribute\rcredit for success among the many decisions that may have been involved in\rproducing it? The reinforcement learning algorithms presented in this book\rinclude two basic mechanisms for addressing this problem. The first is the use\rof eligibility traces, and the second is the use of TD methods to learn value\rfunctions that provide nearly immediate evaluations of actions (in tasks like\rinstrumental conditioning experiments) or that provide immediate prediction\rtargets (in tasks like classical conditioning experiments). Both of these\rmethods correspond to similar mechanisms proposed in theories of animal\rlearning.\nPavlov (1927) pointed out that every stimulus must leave a trace in\rthe nervous system that persists for some time after the stimulus ends, and he\rproposed that stimulus traces make learning possible when there is a temporal\rgap between the CS offset and the US onset. To this day, conditioning under\rthese conditions is called trace\rconditioning(Figure 14.1).\rAssuming a trace of the CS remains when the US \n\r\rarrives, learning occurs through the simultaneous presence of the\rtrace and the US. We discuss some proposals for trace mechanisms in the nervous\rsystem in Chapter 15.\nStimulus traces were also proposed as a means for\rbridging the time interval be\u0026shy;tween actions and consequent rewards or penalties\rin instrumental conditioning. In Hull��s influential learning theory, for\rexample, ��molar stimulus traces�� accounted for what he called an animal��s goal gradient, a description of how the maximum strength of an\rinstrumentally-conditioned response decreases with increasing delay of\rreinforcement (Hull, 1932, 1943). Hull hypothesized that an animal��s actions\rleave internal stimuli whose traces decay exponentially as functions of time\rsince an action was taken. Looking at the animal learning data available at the\rtime, he hypothesized that the traces effectively reach zero after 30 to 40\rseconds.\nThe eligibility traces used in the algorithms\rdescribed in this book are like Hull��s traces: they are decaying traces of past\rstate visitations, or of past state-action pairs. Eligibility traces were\rintroduced by Klopf (1972) in his neuronal theory in which they are\rtemporally-extended traces of past activity at synapses, the connections\rbetween neurons. Klopf��s traces are more complex than the exponentially-decaying\rtraces our algorithms use, and we discuss this more when we take up his theory\rin Section 15.9.\nTo account for goal gradients that extend over\rlonger time periods than spanned by stimulus traces, Hull (1943) proposed that\rlonger gradients result from conditioned reinforcement passing backwards from\rthe goal, a process acting in conjunction with his molar stimulus traces.\rAnimal experiments showed that if conditions favor the development of\rconditioned reinforcement during a delay period, learning does not decrease\rwith increased delay as much as it does under conditions that obstruct sec\u0026shy;ondary\rreinforcement. Conditioned reinforcement is favored if there are stimuli that\rregularly occur during the delay interval. Then it is as if reward is not\ractually delayed because there is more immediate conditioned reinforcement.\rHull therefore envisioned that there is a primary gradient based on the delay\rof the primary rein\u0026shy;forcement mediated by stimulus traces, and that this is\rprogressively modified, and lengthened, by conditioned reinforcement.\nAlgorithms\rpresented in this book that use both eligibility traces and value func\u0026shy;tions to\renable learning with delayed reinforcement correspond to Hull��s hypothesis\rabout how animals are able to learn under these conditions. The actor-critic ar\u0026shy;chitecture\rdiscussed in Sections 13.5, 15.7, and 15.8 illustrates this correspondence most\rclearly. The critic uses a TD algorithm to learn a value function associated\rwith the system��s current behavior, that is, to predict the current policy��s\rreturn. The actor updates the current policy based on the critic��s predictions,\ror more exactly, on changes in the critic��s predictions. The TD error produced\rby the critic acts as a conditioned reinforcement signal for the actor, providing\ran immediate evaluation of performance even when the primary reward signal\ritself is considerably delayed. Algorithms that estimate action-value\rfunctions, such as Q-learning and Sarsa, sim\u0026shy;ilarly use TD learning principles\rto enable learning with delayed reinforcement by means of conditioned\rreinforcement. The close parallel between TD learning and the activity of\rdopamine producing neurons that we discuss in Chapter 15 lends addi\u0026shy;tional\rsupport to links between reinforcement learning algorithms and this aspect of\rHull��s learning theory.\n14.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rCognitive Maps\nModel-based\rreinforcement learning algorithms use environment models that have elements in\rcommon with what psychologists call cognitive maps.Recall from our discussion of planning and learning in Chapter 8that by an environment model we mean anything an\ragent can use to predict how its environment will respond to its actions in\rterms of state transitions and rewards, and by planning we mean any process\rthat computes a policy from such a model. Environment models consist of two\rparts: the state-transition part encodes knowledge about the effect of actions\ron state changes, and the reward-model part encodes knowledge about the reward\rsignals expected for each state or each state-action pair. A model-based\ralgorithm selects actions by using a model to predict the consequences of\rpossible courses of action in terms of future states and the reward signals\rexpected to arise from those states. The simplest kind of planning is to\rcompare the predicted consequences of collections of ��imagined�� sequences of\rdecisions.\nQuestions about whether or not animals use environment models, and\rif so, what are the models like and how are lthey learned, have played\rinfluential roles in the history of animal learning research. Some researchers\rchallenged the then-prevailing stimulus-response (S-R) view of learning and\rbehavior, which corresponds to the simplest model-free way of learning\rpolicies, by demonstrating latent\rlearning. In the earliest latent\rlearning experiment, two groups of rats were run in a maze. For the\rexperimental group, there was no reward during the first stage of the\rexperiment, but food was suddenly introduced into the goal box of the maze at\rthe start of the second stage. For the control group, food was in the goal box\rthroughout both stages. The question was whether or not rats in the\rexperimental group would have learned anything during the first stage in the\rabsence of food reward. Although the experimental rats did not appearto learn much during the first, unrewarded, stage, as soon as they\rdiscovered the food that was introduced in the second stage, they rapidly\rcaught up with the rats in the control group. It was concluded that ��during the\rnon-reward period, the rats [in the experimental group] were developing a\rlatent learning of the maze which they were able to utilize as soon as reward\rwas introduced�� (Blodgett, 1929).\nLatent learning is most closely associated with the psychologist\rEdward Tolman, who interpreted this result, and others like it, as showing that\ranimals could learn a ��cognitive map of the environment�� in the absence of\rrewards or penalties, and that they could use the map later when they were\rmotivated to reach a goal (Tolman, 1948). A cognitive map could also allow a\rrat to plan a route to the goal that was different from the route the rat had\rused in its initial exploration. Explanations of results like these led to the\renduring controversy lying at the heart of the behav- iorist/cognitive\rdichotomy in psychology. In modern terms, cognitive maps are not restricted to\rmodels of spatial layouts but are more generally environment models, or models of an animal��s ��task\rspace�� (e.g., Wilson, Takahashi, Schoenbaum, and Niv, 2014). The cognitive map\rexplanation of latent learning experiments is analogous to the claim that\ranimals use model-based algorithms, and that environment models can be learned\reven without explicit rewards or penalties. Models are then used for planning\rwhen the animal is motivated by the appearance of rewards or penalties.\nTolman��s account of how animals learn cognitive maps was that they\rlearn stimulus- stimulus, or S-S, associations by experiencing successions of\rstimuli as they explore an environment. In psychology this is called expectancy theory: given S-S associa\u0026shy;tions, the occurrence\rof a stimulus generates an expectation about the stimulus to come next. This is\rmuch like what control engineers call system identification,\rin which a model of a system with unknown dynamics is learned from labeled\rtraining examples. In the simplest discrete-time versions, training examples\rare S-S; pairs, where S is a state and S;, the subsequent\rstate, is the label. When S is observed, the model creates the ��expectation��\rthat S' will be observed next. Models more useful for planning involve actions\ras well, so that examples look like SA-S', where S' is expected when action A\ris executed in state S. It is also useful to learn how the environment\rgenerates rewards. In this case, examples are of the form S-R or SA-R, where R\ris a reward signal associated with S or the SA pair. These are all forms of\rsupervised learning by which an agent can acquire cognitive-like maps whether\ror not it receives any non-zero reward signals while exploring its environment.\n14.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rHabitual and Goal-directed\rBehavior\nThe distinction between model-free and\rmodel-based reinforcement learning algo\u0026shy;rithms corresponds to the distinction\rpsychologists make between habitual and goal- directed control of learned behavioral patterns.\rHabits are behavior patterns trig\u0026shy;gered by appropriate stimuli and then\rperformed more-or-less automatically. Goal- directed behavior, according to how\rpsychologists use the phrase, is purposeful in the sense that it is controlled\rby knowledge of the value of goals and the relationship between actions and\rtheir consequences. Habits are sometimes said to be controlled by antecedent\rstimuli, whereas goal-directed behavior is said to be controlled by its\rconsequences (Dickinson, 1980, 1985). Goal-directed control has the advantage\rthat it can rapidly change an animal��s behavior when the environment changes\rits way of reacting to the animal��s actions. While habitual behavior responds\rquickly to input from an accustomed environment, it is unable to quickly adjust\rto changes in the environment. The development of goal-directed behavioral\rcontrol was likely a major advance in the evolution of animal intelligence.\nFigure 14.9 illustrates the difference between\rmodel-free and model-based decision strategies in a hypothetical task in which\ra rat has to navigate a maze that has distinctive goal boxes, each delivering\ran associated reward of the magnitude shown (Figure 14.9 top). Starting at Si,\rthe rat has to first select left (L) or right (R) and then has to select L or R\ragain at S2or S3to reach one of the goal boxes. The goal boxes are the terminal\rstates of each episode of the rat��s episodic task. A model-free strategy\r(Figure 14.9 lower left) relies on stored values for state-action pairs. These\raction values (Q-values) are estimates of the highest return the rat can expect\rfor each action taken from each (nonterminal) state. They are obtained over\rmany trials of running the maze from start to finish. When the action values\rhave become good enough estimates of the optimal returns, the rat just has to\rselect at each state the action with the largest action value in order to make\roptimal decisions. In this case, when the action-value estimates become\raccurate enough, the rat selects L from Si and R from S2to obtain\rthe maximum return of 4. A different model-free strategy might simply rely on a\rcached policy instead of action values, making direct links from Si to L and\rfrom S2to R. In neither of these strategies do decisions rely on an\renvironment model. There is no need to consult a state-transition model, and no\rconnection is required between the features of the goal boxes and the rewards\rthey deliver.\nFigure 14.9 (lower right)\rillustrates a model-based strategy. It uses an environment model consisting of\ra state-transition model and a reward model. The state-transition model is\rshown as a decision tree, and the reward model associates the distinctive\rfeatures of the goal boxes with the rewards to be found in each. (The rewards\rassociated with states Si, S2, and S3are also part of the reward model, but here they are zero and are\rnot shown.) A model-based agent can decide which way to turn at each state by\rusing the model to simulate sequences of action choices to find a path yielding\rthe highest return. In this case the return is the reward obtained from the\routcome at the end of the path. Here, with a sufficiently accurate model, the\rrat would select L and then R to obtain reward of 4. Comparing the predicted\rreturns of simulated paths is a simple form of planning, which can be done in a\rvariety of ways as discussed in Chapter 8.\nWhen the environment of a\rmodel-free agent changes the way it reacts to the agent��s actions, the agent\rhas to acquire new experience in the changed environment during which it can\rupdate its policy and/or value function. In the model-free strategy shown in\rFigure 14.9 (lower left), for example, if one of the goal boxes were to somehow\rshift to delivering a different reward, the rat would have to traverse the\rmaze, possibly many times, to experience the new reward upon reaching that goal\rbox, all the while updating either its policy or its action-value function (or\rboth) based on this experience. The key point is that for a model-free agent to\rchange the action its policy specifies for a state, or to change an action\rvalue associated with a state, it has to move to that state, act from it,\rpossibly many times, and experience the consequences of its actions.\nA model-based agent can\raccommodate changes in its environment without this kind of ��personal\rexperience�� with the states and actions affected by the change. A change in its\rmodel automatically (through planning) changes its policy. Planning can\rdetermine the consequences of changes in the environment that have never been\rlinked together in the agent��s own experience. For example, again referring to\rthe maze task of Figure 14.9, imagine that a rat with a previously learned\rtransition and reward model is placed directly in the goal box to the right of\rS2to find that the reward available there now has value 1 instead of\r4. The rat��s reward model will change even though the action choices required\rto find that goal box in the maze\n\r\n\r\r\r\r\r\u0026nbsp;\nModel-Free\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Model-Based\nFigure 14.9:\rModel-based and model-free strategies to solve a hypothetical sequential\raction- selection problem. Top: a rat navigates a maze with distinctive goal\rboxes, each associated with a reward having the value shown. Lower left: a\rmodel-free strategy relies on stored action values for all the state-action\rpairs obtained over many learning trials. To make decisions the rat just has to\rselect at each state the action with the largest action value for that state.\rLower right: in a model-based strategy, the rat learns an environment model,\rconsisting of knowledge of state-action-next-state transitions and a reward\rmodel consisting of knowledge of the reward associated with each distinctive\rgoal box. The rat can decide which way to turn at each state by using the model\rto simulate sequences of action choices to find a path yielding the highest\rreturn. Adapted from Trends in Cognitive Science,volume 10, number 8, Y. Niv, D. Joel, and P. Dayan,\rA Normative Perspective on Motivation, p. 376, 2006, with permission from\rElsevier.\nwere not involved. The planning\rprocess will bring knowledge of the new reward to bear on maze running without\rthe need for additional experience in the maze; in this case changing the\rpolicy to right turns at both Si and S3to obtain a return of\r3.\nExactly this logic is the\rbasis of outcome-devaluation experimentswith animals. Results from these experiments provide insight into\rwhether an animal has learned a habit or if its behavior is under goal-directed\rcontrol. Outcome-devaluation ex\u0026shy;periments are like latent-learning experiments\rin that the reward changes from one stage to the next. After an initial\rrewarded stage of learning, the reward value of an outcome is changed,\rincluding being shifted to zero or even to a negative value.\nAn early important experiment\rof this type was conducted by Adams and Dickin\u0026shy;son (1981). They trained rats\rvia instrumental conditioning until the rats energet\u0026shy;ically pressed a lever for\rsucrose pellets in a training chamber. The rats were then placed in the same\rchamber with the lever retracted and allowed non-contingent food, meaning that\rpellets were made available to them independently of their actions. Af\u0026shy;ter\r15-minutes of this free-access to the pellets, rats in one group were injected\rwith the nausea-inducing poison lithium chloride. This was repeated for three\rsessions, in the last of which none of the injected rats consumed any of the\rnon-contingent pellets, indicating that the reward value of the pellets had\rbeen decreased��the pel\u0026shy;lets had been devalued. In the next stage taking place a\rday later, the rats were again placed in the chamber and given a session of\rextinction training, meaning that the response lever was back in place but\rdisconnected from the pellet dispenser so that pressing it did not release\rpellets. The question was whether the rats that had the reward value of the\rpellets decreased would lever-press less than rats that did not have the reward\rvalue of the pellets decreased, even without experiencing the devalued reward\ras a result of lever-pressing. It turned out that the injected rats had\rsignificantly lower response rates than the non-injected rats right from the\rstart of the extinction trials.\nAdams and Dickinson concluded\rthat the injected rats associated lever pressing with consequent nausea by\rmeans of a cognitive map linking lever pressing to pellets, and pellets to\rnausea. Hence, in the extinction trials, the rats ��knew�� that the consequences\rof pressing the lever would be something they did not want, and so they reduced\rtheir lever-pressing right from the start. The important point is that they\rreduced lever-pressing without ever having experienced lever-pressing directly\rfollowed by being sick: no lever was present when they were made sick. They\rseemed able to combine knowledge of the outcome of a behavioral choice\r(pressing the lever will be followed by getting a pellet) with the reward value\rof the outcome (pellets are to be avoided) and hence could alter their behavior\raccordingly. Not every psychologist agrees with this ��cognitive�� account of\rthis kind of experiment, and it is not the only possible way to explain these\rresults, but the model-based planning explanation is widely accepted.\nNothing prevents an agent from\rusing both model-free and model-based algo\u0026shy;rithms, and there are good reasons\rfor using both. We know from our own experience that with enough repetition,\rgoal-directed behavior tends to turn into habitual be\u0026shy;havior. Experiments show\rthat this happens for rats too. Adams (1982) conducted \n\r\ran experiment to see if extended training would convert\rgoal-directed behavior into habitual behavior. He did this by comparing the\reffect of outcome devaluation on rats that experienced different amounts of\rtraining. If extended training made the rats less sensitive to devaluation\rcompared to rats that received less training, this would be evidence that\rextended training made the behavior more habitual. Adams�� exper\u0026shy;iment closely\rfollowed the Adams and Dickinson (1981) experiment just described. Simplifying\ra bit, rats in one group were trained until they made 100 rewarded lever-\rpresses, and rats in the other group��the overtrained group��were trained until\rthey made 500 rewarded lever-presses. After this training, the reward value of\rthe pel\u0026shy;lets was decreased (using lithium chloride injections) for rats in both\rgroups. Then both groups of rats were given a session of extinction training.\rAdams�� question was whether devaluation would effect the rate of lever-pressing\rfor the overtrained rats less than it would for the non-overtrained rats, which\rwould be evidence that extended training reduces sensitivity to outcome\rdevaluation. It turned out that de\u0026shy;valuation strongly decreased the\rlever-pressing rate of the non-overtrained rats. For the overtrained rats, in\rcontrast, devaluation had little effect on their lever-pressing; in fact, if\ranything, it made it more vigorous. (The full experiment included con\u0026shy;trol\rgroups showing that the different amounts of training did not by themselves\rsignificantly effect lever-pressing rates after learning.) This result\rsuggested that while the non-overtrained rats were acting in a goal-directed\rmanner sensitive to their knowledge of the outcome of their actions, the\rovertrained rats had developed a lever-pressing habit.\nViewing this and other results like it from a computational perspective\rprovides insight as to why one might expect animals to behave habitually in\rsome circum\u0026shy;stances, in a goal-directed way in others, and why they shift from\rone mode of control to another as they continue to learn. While animals\rundoubtedly use algorithms that do not exactly match those we have presented in\rthis book, one can gain insight into animal behavior by considering the\rtradeoffs that various reinforcement learning al\u0026shy;gorithms imply. An idea\rdeveloped by computational neuroscientists Daw, Niv, and Dayan (2005) is that\ranimals use both model-free and model-based processes. Each process proposes an\raction, and the action chosen for execution is the one proposed by the process\rjudged to be the more trustworthy of the two as determined by mea\u0026shy;sures of\rconfidence that are maintained throughout learning. Early in learning the\rplanning process of a model-based system is more trustworthy because it chains\rto\u0026shy;gether short-term predictions which can become accurate with less experience\rthan long-term predictions of the model-free process. But with continued\rexperience, the model-free process becomes more trustworthy because planning is\rprone to mak\u0026shy;ing mistakes due to model inaccuracies and short-cuts necessary to\rmake planning feasible, such as various forms of tree-pruning. According to\rthis idea one would expect a shift from goal-directed behavior to habitual\rbehavior as more experience accumulates. Other ideas have been proposed for how\ranimals arbitrate between goal-directed and habitual control, and both\rbehavioral and neuroscience research continues to examine this and related\rquestions.\nThe\rdistinction between model-free and model-based algorithms is proving to be\ruseful for this research. One can examine the computational implications of\rthese types of algorithms in abstract settings that expose basic advantages and\rlimitations of each type. This serves both to suggest and to sharpen questions\rthat guide the de\u0026shy;sign of experiments necessary for increasing psychologists��\runderstanding of habitual and goal-directed behavioral control.\n14.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rSummary\nOur goal in\rthis chapter has been to discuss correspondences between reinforcement learning\rand the experimental study of animal learning in psychology. We emphasized at\rthe outset that reinforcement learning as described in this book is not\rintended to model details of animal behavior. It is an abstract computational\rframework that explores idealized situations from the perspective of artificial\rintelligence and engineering. But many of the basic reinforcement learning\ralgorithms were inspired by psychological theories, and in some cases, these\ralgorithms have contributed to the development of new animal learning models.\rThis chapter described the most conspicuous of these correspondences.\nThe distinction in reinforcement learning between algorithms for prediction\rand algorithms for control parallels animal learning theory��s distinction\rbetween classi\u0026shy;cal, or Pavlovian, conditioning and instrumental conditioning.\rThe key difference between instrumental and classical conditioning experiments\ris that in the former the reinforcing stimulus is contingent upon the animal��s\rbehavior, whereas in the latter it is not. Learning to predict via a TD\ralgorithm corresponds to classical con\u0026shy;ditioning, and we described the TD model of\rclassical conditioningas one\rinstance in which reinforcement learning principles account for some details of\ranimal learning behavior. This model generalizes the influential\rRescorla-Wagner model by including the temporal dimension where events within\rindividual trials influence learning, and it provides an account of\rsecond-order conditioning, where predictors of reinforcing stimuli become\rreinforcing themselves. It also is the basis of an influential view of the\ractivity of dopamine neurons in the brain, something we take up in Chapter 15.\nLearning by trial and error is at the base of the control aspect of\rreinforcement learning. We presented some details about Thorndike��s experiments\rwith cats and other animals that led to his Law of Effect,which we discussed here and in Chap\u0026shy;ter 1. We pointed\rout that in reinforcement learning, exploration does not have to be limited to\r��blind groping��; trials can be generated by sophisticated methods using innate\rand previously learned knowledge as long as there is someexploration. We discussed the training method B. F.\rSkinner called shapingin which reward contin\u0026shy;gencies are progressively altered to train an\ranimal to successively approximate a desired behavior. Shaping is not only\rindispensable for animal training, it is also an effective tool for training\rreinforcement learning agents. There is also a connection to the idea of an\ranimal��s motivational state, which influences what an animal will approach or\ravoid and what events are rewarding or punishing for the animal.\nThe reinforcement learning algorithms presented in this book include\rtwo basic\nmechanisms for\raddressing the problem of delayed reinforcement: eligibility traces and value\rfunctions learned via TD algorithms. Both mechanisms have antecedents in\rtheories of animal learning. Eligibility traces are similar to stimulus traces\rof early theories, and value functions correspond to the role of secondary\rreinforcement in providing nearly immediate evaluative feedback.\nThe next correspondence the chapter addressed is that between\rreinforcement learning��s environment models and what psychologists call cognitive maps.Exper\u0026shy;iments conducted in the mid 20th century purported to\rdemonstrate the ability of animals to learn cognitive maps as alternatives to,\ror as additions to, state-action associations, and later use them to guide\rbehavior, especially when the environment changes unexpectedly. Environment\rmodels in reinforcement learning are like cog\u0026shy;nitive maps in that they can be\rlearned by supervised learning methods without relying on reward signals, and\rthen they can be used later to plan behavior.\nReinforcement learning��s distinction between model-freeand model-basedalgo\u0026shy;rithms corresponds to the distinction in\rpsychology between habitualand goal-directed\rbehavior. Model-free algorithms\rmake decisions by accessing information that has been strored in a policy or an\raction-value function, whereas model-based methods select actions as the result\rof planning ahead using a model of the agent��s envi\u0026shy;ronment.\rOutcome-devaluation experiments provide information about whether an animal��s\rbehavior is habitual or under goal-directed control. Reinforcement learning\rtheory has helped clarify thinking about these issues.\nAnimal learning clearly informs reinforcement learning, but as a\rtype of machine learning, reinforcement learning is directed toward designing\rand understanding ef\u0026shy;fective learning algorithms, not toward replicating or\rexplaining details of animal behavior. We focused on aspects of animal learning\rthat relate in clear ways to methods for solving prediction and control\rproblems, highlighting the fruitful two\u0026shy;way flow of ideas between reinforcement\rlearning and psychology without venturing deeply into many of the behavioral\rdetails and controversies that have occupied the attention of animal learning\rresearchers. Future development of reinforcement learn\u0026shy;ing theory and\ralgorithms will likely exploit links to many other features of animal learning\ras the computational utility of these features becomes better appreciated. We\rexpect that a flow of ideas between reinforcement learning and psychology will\rcontinue to bear fruit for both disciplines.\nMany connections between reinforcement learning and areas of\rpsychology and other behavioral sciences are beyond the scope of this chapter.\rWe largely omit discussing links to the psychology of decision-making, which\rfocusses on how actions are selected, or how decisions are made, afterlearning has taken place. We also do not discuss links to ecological and\revolutionary aspects of behavior studied by ethologists and behavioral\recologists: how animals relate to one another and to their physical\rsurroundings, and how their behavior contributes to evolutionary fitness.\rOptimization, MDPs, and dynamic programming figure prominently in these fields,\rand our emphasis on agent interaction with dynamic environments connects to the\rstudy of agent behavior in complex ��ecologies.�� Multi-agent reinforcement\rlearning, omitted in this book, has connections to social aspects of behavior.\rDespite the\nlack of treatment here, reinforcement\rlearning should by no means be interpreted as dismissing evolutionary\rperspectives. Nothing about reinforcement learning implies a tabula rasaview of learning and behavior. Indeed, experience with engineering\rapplications has highlighted the importance of building into reinforcement\rlearning systems knowledge that is analogous to what evolution provides to\ranimals.\nBibliographical\rand Historical Remarks\nLudvig, Bellemare, and Pearson\r(2011) and Shah (2012) review reinforcement learn\u0026shy;ing in the contexts of\rpsychology and neuroscience. These publications are useful companions to this\rchapter and the following chapter on reinforcement learning and neuroscience.\n14.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rDayan, Niv,\rSeymour, and Daw (2006) focused on interactions between clas\u0026shy;sical and instrumental\rconditioning, particularly situations where classically- conditioned and\rinstrumental responses are in conflict. They proposed a Q- learning framework\rfor modeling aspects of this interaction. Modayil and Sut\u0026shy;ton (2014) used a\rmobile robot to demonstrate the effectiveness of a control method combining a\rfixed response with online prediction learning. Calling this Pavlovian control, they emphasized that it differs from the usual control methods of\rreinforcement learning, being based on predictively executing fixed responses\rand not on reward maximization. The electro-mechanical machine of Ross (1933)\rand especially the learning version of Walter��s tur\u0026shy;tle (Walter, 1951) were\rvery early illustrations of Pavlovian control. What is now called Pavlovian-instrumental\rtransfer was first observed by Estes (1943, 1948).\n14.2.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\rKamin (1968)\rfirst reported blocking, now commonly known as Kamin block\u0026shy;ing, in classical\rconditioning. Moore and Schmajuk (2008) provide an excel\u0026shy;lent summary of the\rblocking phenomenon, the research it stimulated, and its lasting influence on\ranimal learning theory. Gibbs, Cool, Land, Kehoe, and Gormezano (1991) describe\rsecond-order conditioning of the rabbit��s nic\u0026shy;titating membrane response and\rits relationship to conditioning with serial- compound stimuli. Finch and\rCuller (1934) reported obtaining fifth-order conditioning of a dog��s foreleg\rwithdrawal ��when the motivationof the ani\u0026shy;mal is maintained through the various\rorders.��\n14.2.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe idea built\rinto the Rescorla-Wagner model that learning occurs when animals are surprised\ris derived from Kamin (1969). Models of classical conditioning other than\rRescorla and Wagner��s include the models of Klopf (1988), Grossberg (1975),\rMackintosh (1975), Moore and Stickney (1980), Pearce and Hall (1980), and\rCourville, Daw, and Touretzky (2006). Schmajuk (2008) review models of\rclassical conditioning.\n\r\r14.2.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\rAn early\rversion of the TD model of classical conditioning appeared in Sut\u0026shy;ton and Barto\r(1981), which also included the early model��s prediction that temporal primacy\roverrides blocking, later shown by Kehoe, Scheurs, and Graham (1987) to occur\rin the rabbit nictitating membrane preparation. Sutton and Barto (1981)\rcontains the earliest recognition of the near identity between the\rRescorla-Wagner model and the Least-Mean-Square (LMS), or Widrow-Hoff, learning\rrule (Widrow and Hoff, 1960). This early model was revised following Sutton��s\rdevelopment of the TD algorithm (Sutton, 1984, 1988) and was first presented as\rthe TD model in Sutton and Barto (1987) and more completely in Sutton and Barto\r(1990), upon which this section is largely based. Additional exploration of the\rTD model and its possible neural implementation was conducted by Moore and\rcolleagues (Moore, Desmond, Berthier, Blazis, Sutton, and Barto, 1986; Moore\rand Blazis, 1989; Moore, Choi, and Brunzell, 1998; Moore, Marks, Castagna, and\rPolewan, 2001). Klopf��s (1988) drive-reinforcement theory of classical\rconditioning extends the TD model to address additional experimental details,\rsuch as the S- shape of acquisition curves. In some of these publications TD is\rtaken to mean Time Derivative instead of Temporal Difference.\n14.2.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\rLudvig, Sutton,\rand Kehoe (2012) evaluated the performance of the TD model in previously\runexplored tasks involving classical conditioning and examined the influence of\rvarious stimulus representations, including the mi\u0026shy;crostimulus representation\rthat they introduced earlier (Ludvig, Sutton, and Kehoe, 2008). Earlier\rinvestigations of the influence of various stimulus repre\u0026shy;sentations and their\rpossible neural implementations on response timing and topography in the\rcontext of the TD model are those of Moore and colleagues cited above. Although\rnot in the context of the TD model, representations like the microstimulus\rrepresentation of Ludvig et al. (2012) have been pro\u0026shy;posed and studied by Grossberg\rand Schmajuk (1989), Brown, Bullock, and Grossberg (1999), Buhusi and Schmajuk\r(1999), and Machado (1997).\n14.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rSection 1.7\rincludes comments on the history of trial-and-error learning and the Law of\rEffect. The idea that Thorndikes cats might have been exploring according to an\rinstinctual context-specific ordering over actions rather than by just\rselecting from a set of instinctual impulses was suggested by Peter Dayan\r(personal communication). Selfridge, Sutton, and Barto (1985) illus\u0026shy;trated the\reffectiveness of shaping in a pole-balancing reinforcement learning task. Other\rexamples of shaping in reinforcement learning are Gullapalli and Barto (1992),\rMahadevan and Connell (1992), Mataric (1994), Dorigo and Colombette (1994),\rSaksida, Raymond, and Touretzky (1997), and Randl0v and Alstr0m (1998). Ng (2003) and Ng, Harada, and Russell (1999) used the term\rshaping in a sense somewhat different from Skinner��s, focussing on the problem\rof how to alter the reward signal without altering the set of optimal policies.\nDickinson and Balleine (2002) discuss the complexity of the\rinteraction be-\ntween learning and motivation.\rWise (2004) provides an overview of rein\u0026shy;forcement learning and its relation to\rmotivation. Daw and Shohamy (2008) link motivation and learning to aspects of\rreinforcement learning theory. See also McClure, Daw, and Montague (2003), Niv,\rJoel, and Dayan (2006), Rangel et al. (2008), and Dayan and Berridge (2014).\rMcClure et al. (2003), Niv, Daw, and Dayan (2005), and Niv, Daw, Joel, and Dayan\r(2007) present theories of behavioral vigor related to the reinforcement\rlearning framework.\n14.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rSpence, Hull��s\rstudent and collaborator at Yale, elaborated the role of higher- order\rreinforcement in addressing the problem of delayed reinforcement (Spence,\r1947). Learning over very long delays, as in taste-aversion conditioning with\rdelays up to several hours, led to interference theories as alternatives to\rdecaying-trace theories (e.g., Revusky and Garcia, 1970; Boakes and Costa,\r2014). Other views of learning under delayed reinforcement invoke roles for\rawareness and working memory (e.g., Clark and Squire, 1998; Seo, Barra- clough,\rand Lee, 2007).\n14.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThistlethwaite\r(1951) is an extensive review of latent learning experiments up to the time of\rits publication. Ljung (1998) is an overview of model learning, or system\ridentification, techniques in engineering. Gopnik, Glymour, Sobel, Schulz,\rKushnir, and Danks (2004) present a Bayesian theory about how children learn\rmodels.\n14.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rConnections\rbetween habitual and goal-directed behavior and model-free and model-based\rreinforcement learning were first proposed by Daw, Niv, and Dayan (2005). The\rhypothetical maze task used to explain habitual and goal-directed behavioral\rcontrol is based on the explanation of Niv, Joel, and Dayan (2006). Dolan and\rDayan (2013) review four generations of experi\u0026shy;mental research related to this\rissue and discuss how it can move forward on the basis of reinforcement\rlearning��s model-free/model-based distinction. Dickinson (1980, 1985) and Dickinson\rand Balleine (2002) discuss exper\u0026shy;imental evidence related to this distinction.\rDonahoe and Burgos (2000) alternatively argue that model-free processes can\raccount for the results of outcome-devaluation experiments. Dayan and Berridge\r(2014) argue that classical conditioning involves model-based processes.\rRangel, Camerer, and Montague (2008) review many of the outstanding issues\rinvolving habitual, goal-directed, and Pavlovian modes of control.\nComments on Terminology�� The traditional meaning of reinforcementin psy\u0026shy;chology\ris the strengthening of a pattern of behavior (by increasing either its\rintensity or frequency) as a result of an animal receiving a stimulus (or\rexperiencing the omis\u0026shy;sion of a stimulus) in an appropriate temporal\rrelationship with another stimulus or with a response. Reinforcement produces\rchanges that remain in future behavior. Sometimes in psychology reinforcement\rrefers to the process of producing lasting changes in behavior, whether the\rchanges strengthen or weaken a behavior pattern\n(Mackintosh,\r1983). Letting reinforcement refer to weakening in addition to strength\u0026shy;ening\ris at odds with the everyday meaning of reinforce, and its traditional use in\rpsychology, but it is a useful extension that we have adopted here. In either case,\ra stimulus considered to be the cause of the behavioral change is called a reinforcer.\nPsychologists do not generally use the specific phrase reinforcement\rlearningas we do. Animal\rlearning pioneers probably regarded reinforcement and learning as being\rsynonymous, so it would be redundant to use both words. Our use of the phrase\rfollows its use in computational and engineering research, influenced mostly by\rMinsky (1961). But the phrase is lately gaining currency in psychology and\rneuroscience, likely because strong parallels have surfaced between\rreinforcement learning algorithms and animal learning��parallels described in\rthis chapter and the next.\nAccording to common usage, a rewardis an object or event that an animal will approach\rand work for. A reward may be given to an animal in recognition of its ��good��\rbehavior, or given in order to make the animal��s behavior ��better.�� Similarly,\ra penaltyis an object or event that the animal usually avoids and that is\rgiven as a consequence of ��bad�� behavior, usually in order to change that\rbehavior. Primary rewardis reward due to machinery built into an animal��s nervous system by\revolution to improve its chances of survival and reproduction, e.g., reward\rproduced by the taste of nourishing food, sexual contact, successful escape,\rand many other stimuli and events that predicted reproductive success over the\ranimal��s ancestral history. As explained in Section 14.2.1, higher-order\rrewardis reward delivered by\rstimuli that predict primary reward, either directly or indirectly by\rpredicting other stimuli that predict primary reward. Reward is secondaryif its rewarding quality is the result of directly\rpredicting primary reward.\nI\u0026nbsp; this book we call Rthe ��reward signal\rat time Vor sometimes just the ��reward at time V,�� but we do\rnot think of it as an object or event in the agent��s environment. Because R is\ra number��not an object or an event��it is more like a reward signal in\rneuroscience, which is a signal internal to the brain, like the activity of\rneurons, that influences decision making and learning. This signal might be\rtriggered when the animal perceives an attractive (or an aversive) object, but\rit can also be triggered by things that do not physically exist in the animal��s\rexternal environment, such as memories, ideas, or hallucinations. Because our Rcan\rbe positive, negative, or zero, it might be better to call a negative R a\rpenalty, and an R equal to zero a neutral signal, but for simplicity we\rgenerally avoid these terms.\nIn reinforcement learning, the process that generates all theR^s\rdefines the problem the agent is trying to solve. The agent��s objective is to\rkeep the magnitude of R as large as possible over time. In this respect, R is\rlike primary reward for an animal if we think of the problem the animal faces\ras the problem of obtaining as much primary reward as possible over its\rlifetime (and thereby, through the prospective ��wisdom�� of evolution, improve\rits chances of solving its real problem, which is to pass its genes on to\rfuture generations. However, as we suggest in Chapter 15, it is unlikely that\rthere is a single ��master�� reward signal like Rin an animal��s\rbrain.)\nNot all reinforcers are rewards or penalties.\rSometimes reinforcement is not the result of an animal receiving a stimulus\rthat evaluates its behavior by labeling the behavior good or bad. A behavior\rpattern can be reinforced by a stimulus that arrives to an animal no matter how\rthe animal behaved. As described in Section 14.1, whether the delivery of\rreinforcer depends, or does not depend, on preceding behavior is the defining\rdifference between instrumental, or operant, conditioning experiments and\rclassical, or Pavlovian, conditioning experiments. Reinforcement is at work in\rboth types of experiments, but only in the former is it feedback that evaluates\rpast behavior. (Though It has often been pointed out that even when the\rreinforcing US in a classical conditioning experiment is not contingent on the\rsubject��s preceding behavior, its reinforcing value can be influenced by this behavior,\ran example being that a closed eye makes an air puff to the eye less aversive.)\nThe distinction between reward signals and\rreinforcement signals is a crucial point when we discuss neural correlates of\rthese signals in the next chapter. Like a reward signal, for us, the\rreinforcement signal at any specific time is a positive or negative number, or\rzero. A reinforcement signal is the major factor directing changes a learn\u0026shy;ing\ralgorithm makes in an agent��s policy, value estimates, or environment models.\rThe definition that makes the most sense to us is that a reinforcement signal\rat any time is a number that multiplies (possibly along with some constants) a\rvector to determine parameter updates in some learning algorithm.\nFor some algorithms, the reward signal alone is the\rcritical multiplier in the parameter-update equation. For these algorithms the\rreinforcement signal is the same as the reward signal. But for most of the\ralgorithms we discuss in this book, reinforcement signals include terms in addition\rto the reward signal, an example being a TD error \u0026#12316;=Rt+i + 7V(St+i) �� V(St), which is the reinforcement signal for TD\rstate-value learning (and analogous TD errors for action-value learning). In\rthis reinforcement signal, Rt+i is the primary reinforcementcontribution, and the temporal difference in\rpredicted values, 7V(St+i) �� V(St) (or an analogous temporal difference for\raction values), is the conditioned\rreinforcementcontribution.\rThus, whenever 7V(St+i) �� V(St) = 0, \u0026amp; signals ��pure�� primary reinforcement;\rand when\u0026shy;ever Rt+i = 0,\rit signals ��pure�� conditioned reinforcement, but it often signals a mixture of\rthese. Note as we mentioned in Section 6.1, this \u0026#12316;is\rnot available until time t+ 1. We therefore think of \u0026#12316;as the reinforcement signal at time t+\r1, which is fitting because it reinforces predictions and/or actions made\rearlier at step t.\nA possible source of confusion is the terminology\rused by the famous psycholo\u0026shy;gist B.F. Skinner and his followers. For Skinner,\rpositive reinforcement occurs when the consequences of an animal��s behavior\rincrease the frequency of that behavior; punishment occurs when the behavior��s\rconsequences decrease that behavior��s fre\u0026shy;quency. Negative reinforcement occurs\rwhen behavior leads to the removal of an aversive stimulus (that is, a stimulus\rthe animal does not like), thereby increasing the frequency of that behavior.\rNegative punishment, on the other hand, occurs when behavior leads to the\rremoval of an appetitive stimulus (that is, a stimulus the animal likes),\rthereby decreasing the frequency of that behavior. We find no critical need for\rthese distinctions because our approach is more abstract than this, with both\rreward and reinforcement signals allowed to take on both positive and negative \n\r\rvalues. (But note especially that when our\rreinforcement signal is negative, it is not the same as Skinner��s negative\rreinforcement.)\nOn the other hand, it has often been pointed out that using a single\rnumber as a reward or a penalty signal, depending only on its sign, is at odds\rwith the fact that animals�� appetitive and aversive systems have qualitatively\rdifferent properties and involve different brain mechanisms. This points to a\rdirection in which the reinforce\u0026shy;ment learning framework might be developed in\rthe future to exploit computational advantages of separate appetitive and\raversive systems, but for now we are passing over these possibilities.\nAnother discrepancy in terminology is how we use the word action. To many cognitive scientists, an action is purposeful in the sense\rof being the result of an animal��s knowledge about the relationship between the\rbehavior in question and the consequences of that behavior. An action is\rgoal-directed and the result of a decision, in contrast to a response, which is\rtriggered by a stimulus; the result of a reflex or a habit. We use the word\raction without differentiating among what others call actions, decisions, and\rresponses. These are important distinctions, but for us they are encompassed by\rdifferences between model-free and model-based reinforcement learning\ralgorithms, which we discussed above in relation to habitual and goal-directed\rbehavior in Section 14.6. Dickinson (1985) discusses the distinction between\rresponses and actions.\nA term used a lot in this book is control. What we mean\rby control is entirely different from what it means to animal learning\rpsychologists. By control we mean that an agent influences its environment to\rbring about states or events that the agent prefers: the agent exerts control\rover its environment. This is the sense of control used by control engineers.\rIn psychology, on the other hand, control typically means that an animal��s\rbehavior is influenced by��is controlled by��the stimuli the animal receives\r(stimulus control) or the reinforcement schedule it experiences. Here the\renvironment is controlling the agent. Control in this sense is the basis of\rbehavior modification therapy. Of course, both of these directions of control\rare at play when an agent interacts with its environment, but our focus is on\rthe agent as controller; not the environment as controller. A view equivalent\rto ours, and perhaps more illuminating, is that the agent is actually\rcontrolling the input it receives from its environment (Powers, 1973). This is notwhat psychologists mean by stimulus control.\nSometimes reinforcement learning is understood to refer solely to\rlearning policies directly from rewards (and penalties) without the involvement\rof value functions or environment models. This is what psychologists call\rstimulus-response, or S\u0026shy;R, learning. But for us, along with most of today��s\rpsychologists, reinforcement learning is much broader than this, including in\raddition to S-R learning, methods involving value functions, environment\rmodels, planning, and other processes that are commonly thought to belong to\rthe more cognitive side of mental functioning.\n392\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; CHAPTER\u0026nbsp; 14.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; PSYCHOLOGY\n\r\rChapter 15\nNeuroscience\nNeuroscience\ris the multidisciplinary study of nervous systems: how they regulate bodily\rfunctions; control behavior; change over time as a result of development,\rlearning, and aging; and how cellular and molecular mechanisms make these\rfunctions possible. One of the most exciting aspects of reinforcement learning\ris the mounting evidence from neuroscience that the nervous systems of humans\rand many other animals implement algorithms that correspond in striking ways to\rreinforcement learning algorithms. The main objective of this chapter is to\rexplain these parallels and what they suggest about the neural basis of\rreward-related learning in animals.\nThe most remarkable point of contact between reinforcement learning\rand neuro\u0026shy;science involves dopamine, a chemical deeply involved in reward\rprocessing in the brains of mammals. Dopamine appears to convey\rtemporal-difference (TD) errors to brain structures where learning and decision\rmaking take place. This parallel is expressed by the reward prediction error hypothesis of dopamine\rneuron activity,a hy\u0026shy;pothesis\rthat resulted from the convergence of computational reinforcement learning and\rresults of neuroscience experiments. In this chapter we discuss this\rhypothesis, the neuroscience findings that led to it, and why it is a\rsignificant contribution to un\u0026shy;derstanding brain reward systems. We also\rdiscuss parallels between reinforcement learning and neuroscience that are less\rstriking than this dopamine/TD-error parallel but that provide useful\rconceptual tools for thinking about reward-based learning in animals. Other\relements of reinforcement learning have the potential to impact the study of\rnervous systems, but their connections to neuroscience are still relatively\rundeveloped. We discuss several of these evolving connections that we think\rwill grow in importance over time.\nAs we outlined\rin the history section of this book��s introductory chapter (Sec\u0026shy;tion 1.7), many\raspects of reinforcement learning were influenced by neuroscience. A second\robjective of this chapter is to acquaint readers with ideas about brain\rfunction that have contributed to our approach to reinforcement learning. Some\relements of reinforcement learning are easier to understand when seen in light\rof theories of brain function. This is particularly true for the idea of the\religibility trace, one of the ba\u0026shy;sic mechanisms of reinforcement learning, that\roriginated as a conjectured property of synapses, the structures by which nerve\rcells��neurons��communicate with one another.\nIn this chapter we do not delve very deeply into the enormous\rcomplexity of the neural systems underlying reward-based learning in animals:\rthis chapter too short, and we are not neuroscientists. We do not try to\rdescribe��or even to name��the very many brain structures and pathways, or any of\rthe molecular mechanisms, be\u0026shy;lieved to be involved in these processes. We also\rdo not do justice to hypotheses and models that are alternatives to those that\ralign so well with reinforcement learning. It should not be surprising that\rthere are differing views among experts in the field. We can only provide a\rglimpse into this fascinating and developing story. We hope, though, that this\rchapter convinces you that a very fruitful channel has emerged con\u0026shy;necting reinforcement\rlearning and its theoretical underpinnings to the neuroscience of reward-based\rlearning in animals.\nMany excellent\rpublications cover links between reinforcement learning and neu\u0026shy;roscience, some\rof which we cite in this chapter��s final section. Our treatment differs from\rmost of these because we assume familiarity with reinforcement learning as\rpresented in the earlier chapters of this book, but we do not assume knowledge\rof neuroscience. We begin with a brief introduction to the neuroscience concepts\rneeded for a basic understanding of what is to follow.\n15.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rNeuroscience Basics\nSome basic\rinformation about nervous systems is helpful for following what we cover in\rthis chapter. Terms that we refer to later are italicized. Skipping this\rsection will not be a problem if you already have an elementary knowledge of\rneuroscience.\nNeurons,the main components of nervous systems, are cells\rspecialized for pro\u0026shy;cessing and transmitting information using electrical and\rchemical signals. They come in many forms, but a neuron typically has a cell\rbody, dendrites, and a single axon.Dendrites are structures that branch from the cell body to receive\rinput from other neurons (or to also receive external signals in the case of\rsensory neurons). A neuron��s axon is a fiber that carries the neuron��s output\rto other neurons (or to muscles or glands). A neuron��s output consists of\rsequences of electrical pulses called action\rpotentialsthat\rtravel along the axon. Action potentials are also called spikes, and a neuron is said to firewhen it generates a spike. In models of neural\rnetworks it is common to use real numbers to represent a neuron��s firing rate, the average number of spikes per some unit of\rtime.\nA neuron��s axon can branch widely so that the neuron��s action potentials\rreach many targets. The branching structure of a neuron��s axon is called the\rneuron��s axonal arbor. Because the conduction of an action potential is an active\rprocess, not unlike the burning of a fuse, when an action potential reaches an\raxonal branch point it ��lights up�� action potentials on all of the outgoing\rbranches (although propagation to a branch can sometimes fail). As a result,\rthe activity of a neuron with a large axonal arbor can influence many target\rsites.\nA synapseis a structure generally at the termination of an axon branch that\rmedi\u0026shy;ates the communication of one neuron to another. A synapse transmits\rinformation from the presynapticneuron��s axon to a dendrite or cell body of the postsynaptic neuron. With a few exceptions, synapses release a\rchemical neurotransmitterupon the arrival of an action potential from the presynaptic neuron.\r(The exceptions are cases of direct electric coupling between neurons, but\rthese will not concern us here.) Neurotransmitter molecules released from the presynaptic\rside of the synapse diffuse across the synaptic cleft, the very small space between the presynaptic\rending and the postsynaptic neuron, and then bind to receptors on the surface\rof the postsy- naptic neuron to excite or inhibit its spike-generating\ractivity, or to modulate its behavior in other ways. A particular\rneurotransmitter may bind to several different types of receptors, with each\rproducing a different effect on the postsynaptic neuron. For example, there are\rat least five different receptor types by which the neurotrans\u0026shy;mitter dopamine\rcan affect a postsynaptic neuron. Many different chemicals have been identified\ras neurotransmitters in animal nervous systems.\nA neuron��s backgroundactivity is its level of activity, usually its\rfiring rate, when the neuron does not appear to be driven by synaptic input\rrelated to the task of in\u0026shy;terest to the experimenter, for example, when the\rneuron��s activity is not correlated with a stimulus delivered to a subject as\rpart of an experiment. Background activity can be irregular due to input from\rthe wider network, or due to noise within the neu\u0026shy;ron or its synapses.\rSometimes background activity is the result of dynamic processes intrinsic to\rthe neuron. A neuron��s phasicactivity, in contrast to its background ac\u0026shy;tivity, consists of\rbursts of spiking activity usually caused by synaptic input. Activity that\rvaries slowly and often in a graded manner, whether as background activity or\rnot, is called a neuron��s tonicactivity.\nThe strength or effectiveness by which the neurotransmitter released\rat a synapse influences the postsynaptic neuron is the synapse��s efficacy.One way a nervous system can change through\rexperience is through changes in synaptic efficacies as a result of\rcombinations of the activities of the presynaptic and postsynaptic neurons, and\rsometimes by the presence of a neuromodulator, which is a neurotransmitter having effects other than, or in\raddition to, direct fast excitation or inhibition.\nBrains contain several different neuromodulation systems consisting\rof clusters of neurons with widely branching axonal arbors, with each system\rusing a different neu\u0026shy;rotransmitter. Neuromodulation can alter the function of\rneural circuits, mediate motivation, arousal, attention, memory, mood, emotion,\rsleep, and body tempera\u0026shy;ture. Important here is that a neuromodulatory system\rcan distribute something like a scalar signal, such as a reinforcement signal,\rto alter the operation of synapses in widely distributed sites critical for\rlearning.\nThe ability of synaptic efficacies to change is called synaptic\rplasticity. It is one of the\rprimary mechanisms responsible for learning. The parameters, or weights,\radjusted by learning algorithms correspond to synaptic efficacies. As we detail\rbelow, modulation of synaptic plasticity via the neuromodulator dopamine is a\rplausible mechanism for how the brain might implement learning algorithms like\rmany of those described in this book.\n15.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rReward Signals, Reinforcement\rSignals, Values, and Prediction Errors\nLinks between neuroscience\rand computational reinforcement learning begin as paral\u0026shy;lels between signals in\rthe brain and signals playing prominent roles in reinforcement learning theory\rand algorithms. In Chapter 3 we said that any problem of learn\u0026shy;ing\rgoal-directed behavior can be reduced to the three signals representing\ractions, states, and rewards. However, to explain links that have been made\rbetween neuro\u0026shy;science and reinforcement learning, we have to be less abstract\rthan this and consider other reinforcement learning signals that correspond, in\rcertain ways, to signals in the brain. In addition to reward signals, these\rinclude reinforcement signals (which we argue are different from reward\rsignals), value signals, and signals conveying pre\u0026shy;diction errors. When we label\ra signal by its function in this way, we are doing it in the context of\rreinforcement learning theory in which the signal corresponds to a term in an\requation or an algorithm. On the other hand, when we refer to a signal in the\rbrain, we mean a physiological event such as a burst of action potentials or\rthe secretion of a neurotransmitter. Labeling a neural signal by its function,\rfor example calling the phasic activity of a dopamine neuron a reinforcement\rsignal, means that the neural signal behaves like, and is conjectured to\rfunction like, the corresponding theoretical signal.\nUncovering evidence for these correspondences involves many\rchallenges. Neu\u0026shy;ral activity related to reward processing can be found in\rnearly every part of the brain, and it is difficult to interpret results\runambiguously because representations of different reward-related signals tend\rto be highly correlated with one another. Experiments need to be carefully\rdesigned to allow one type of reward-related signal to be distinguished with\rany degree of certainty from others��or from an abundance of other signals not\rrelated to reward processing. Despite these difficulties, many experiments have\rbeen conducted with the aim of reconciling aspects of reinforce\u0026shy;ment learning\rtheory and algorithms with neural signals, and some compelling links have been\restablished. To prepare for examining these links, in the rest of this sec\u0026shy;tion\rwe remind the reader of what various reward-related signals mean according to\rreinforcement learning theory.\nIn our Comments on Terminology at the end of the previous chapter,\rwe said that Rt is like a reward signal in an animal��s brain and not as an\robject or event in the animal��s environment. In reinforcement learning, the\rreward signal (along with an agent��s environment) defines the problem a\rreinforcement learning agent is trying to solve. It this respect, Rt is like a\rsignal in an animal��s brain that distributes primary reward to sites throughout\rthe brain. But it is unlikely that a unitary master reward signal like Rt\rexists in an animal��s brain. It is best to think of Rt as an abstraction\rsummarizing the overall effect of a multitude of neural signals generated by\rmany systems in the brain that assess the rewarding or punishing qualities of\rsensations and states.\nReinforcement signalsin reinforcement learning are different from reward signals. The function of a\rreinforcement signal is to direct the changes a learning algo-\n\r\rrithm makes in an agent��s policy, value estimates, or environment\rmodels. For a TD method, for instance, the reinforcement signal at time V is\rthe TD error \u0026#12316;_i = Rt + 7V(St) ��\rV(St-i).[29]\rThe reinforcement signal for some algorithms could be just the reward signal,\rbut for most of the algorithms we consider the reinforce\u0026shy;ment signal is the\rreward signal adjusted by other information, such as the value estimates in TD\rerrors.\nEstimates of state values or of action values, that\ris, V or Q, specify what is good or bad for the agent over the long run. They\rare predictions of the total reward an agent can expect to accumulate over the\rfuture. Agents make good decisions by selecting actions leading to states with\rthe largest estimated state values, or by selecting actions with the largest\restimated action values.\nPrediction errors measure discrepancies between\rexpected and actual signals or sensations. Reward prediction errors (RPEs)\rspecifically measure discrepancies be\u0026shy;tween the expected and the received\rreward signal, being positive when the reward signal is greater than expected,\rand negative otherwise. TD errors like (6.5) are spe\u0026shy;cial kinds RPEs that\rsignal discrepancies between current and earlier expectations of reward over\rthe long-term. When neuroscientists refer to RPEs they generally (though not\ralways) mean TD RPEs, which we simply call TD errors throughout this chapter.\rAlso in this chapter, a TD error is generally one that does not depend on\ractions, as opposed to TD errors used in learning action-values by algorithms\rlike Sarsa and Q-learning. This is because the most well-known links to\rneuroscience are stated in terms of action-free TD errors, but we do not mean\rto rule out possible similar links involving action-dependent TD errors. (TD\rerrors for predicting signals other than rewards are useful too, but that case\rwill not concern us here. See, for example, Modayil, White, and Sutton, 2014.)\nOne can ask many questions about links between\rneuroscience data and these theoretically-defined signals. Is an observed\rsignal more like a reward signal, a value signal, a prediction error, a\rreinforcement signal, or something altogether different? And if it is an error\rsignal, is it an RPE, a TD error, or a simpler error like the Rescorla-Wagner\rerror (14.3)? And if it is a TD error, does it depend on actions like the TD\rerror of Q-learning or Sarsa? As indicated above, probing the brain to answer\rquestions like these is extremely difficult. But experimental evidence suggests\rthat one neurotransmitter, specifically the neurotransmitter dopamine, signals\rRPEs, and further, that the phasic activity of dopamine-producing neurons in\rfact conveys TD errors (see Section 15.1 for a definition of phasic activity).\rThis evidence led to the reward\rprediction error hypothesis of dopamine neuron activity, which we describe next.\n\r\r15.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe Reward Prediction Error\rHypothesis\nThe reward prediction error hypothesis of dopamine neuron\ractivityproposes that one of\rthe functions of the phasic activity of dopamine-producing neurons in mammals\ris to deliver an error between an old and a new estimate of expected future\rreward to target areas throughout the brain. This hypothesis (though not in\rthese exact words) was first explicitly stated by Montague, Dayan, and\rSejnowski (1996), who showed how the TD error concept from reinforcement\rlearning accounts for many features of the phasic activity of dopamine neurons\rin mammals. The experiments that led to this hypothesis were performed in the\r1980s and early 1990s in the laboratory of neuroscientist Wolfram Schultz.\rSection 15.5 describes these influential experiments, Section 15.6 explains how\rthe results of these experiments align with TD errors, and the Bibliographical\rand Historical Remarks section at the end of this chapter includes a guide to\rthe literature surrounding the development of this influential hypothesis.\nMontague et al. (1996) compared the TD errors of the TD model of\rclassical con\u0026shy;ditioning with the phasic activity of dopamine-producing neurons\rduring classical conditioning experiments. Recall from Section 14.2 that the TD\rmodel of classi\u0026shy;cal conditioning is basically the semi-gradient-descent TD(A)\ralgorithm with linear function approximation. Montague et al. made several\rassumptions to set up this comparison. First, since a TD error can be negative\rbut neurons cannot have a neg\u0026shy;ative firing rate, they assumed that the quantity\rcorresponding to dopamine neuron activity is \u0026amp;_i + bt, where bt is the\rbackground firing rate of the neuron. A neg\u0026shy;ative TD error corresponds to a\rdrop in a dopamine neuron��s firing rate below its background rate.[30]\nA second assumption was needed about the states visited in each\rclassical condi\u0026shy;tioning trial and how they are represented as inputs to the\rlearning algorithm. This is the same issue we discussed in Section 14.2.4 for\rthe TD model. Montague et al. chose a complete serial compound (CSC)representation as shown in the left column of Figure 14.2, but\rwhere the sequence of short-duration internal signals continues until the onset\rof the US, which here is the arrival of a non-zero reward signal. This\rrepresentation allows the TD error to mimic the fact that dopamine neuron activ\u0026shy;ity\rnot only predicts a future reward, but that it is also sensitive to whenafter a predictive cue that reward is expected to arrive. There has to be some\rway to keep track of the time between sensory cues and the arrival of reward.\rIf a stimulus ini\u0026shy;tiates a sequence of internal signals that continues after\rthe stimulus ends, and if there is a different signal for each time step\rfollowing the stimulus, then each time step after the stimulus is represented\rby a distinct state. Thus, the TD error, being state-dependent, can be\rsensitive to the timing of events within a trial.\nIn simulated trials with these assumptions about background firing\rrate and input representation, TD errors of the TD model are remarkably similar\rto dopamine neu\u0026shy;ron phasic activity. Previewing our description of details\rabout these similarities in Section 15.5 below, the TD errors parallel the\rfollowing features of dopamine neuron activity: 1) the phasic response of a\rdopamine neuron only occurs when a rewarding event is unpredicted; 2) early in\rlearning, neutral cues that precede a reward do not cause substantial phasic\rdopamine responses, but with continued learning these cues gain predictive\rvalue and come to elicit phasic dopamine responses; 3) if an even ear\u0026shy;lier cue\rreliably precedes a cue that has already acquired predictive value, the phasic\rdopamine response shifts to the earlier cue, ceasing for the later cue; and 3)\rif after learning, the predicted rewarding event is omitted, a dopamine\rneuron��s response decreases below its baseline level shortly after the expected\rtime of the rewarding event.\nAlthough not every dopamine neuron monitored in the experiments of\rSchultz and colleagues behaved in all of these ways, the striking\rcorrespondence between the activities of most of the monitored neurons and TD\rerrors lends strong support to the reward prediction error hypothesis. There\rare situations, however, in which predictions based on the hypothesis do not\rmatch what is observed in experiments. The choice of input representation is\rcritical to how closely TD errors match some of the details of dopamine neuron\ractivity, particularly details about the timing of dopamine neuron responses.\rDifferent ideas, some of which we discuss below, have been proposed about input\rrepresentations and other features of TD learning to make the TD errors fit the\rdata better, though the main parallels appear with the CSC representation that\rMontague et al. used. Overall, the reward prediction error hy\u0026shy;pothesis has\rreceived wide acceptance among neuroscientists studying reward-based learning,\rand it has proven to be remarkably resilient in the face of accumulating\rresults from neuroscience experiments.\nTo prepare for\rour description of the neuroscience experiments supporting the re\u0026shy;ward\rprediction error hypothesis, and to provide some context so that the\rsignificance of the hypothesis can be appreciated, we next present some of what\ris known about dopamine, the brain structures it influences, and how it is\rinvolved in reward-based learning.\n15.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rDopamine\nDopamine is\rproduced as a neurotransmitter by neurons whose cell bodies lie mainly in two\rclusters of neurons in the midbrain of mammals: the substantia nigra pars\rcompacta (SNpc) and the ventral tegmental area (VTA). Dopamine plays essen\u0026shy;tial\rroles in many processes in the mammalian brain. Prominent among these are\rmotivation, learning, action-selection, most forms of addiction, and the\rdisorders schizophrenia and Parkinson��s disease. Dopamine is called a\rneuromodulator be\u0026shy;cause it performs many functions other than direct fast excitation\ror inhibition of targeted neurons. Although much remains unknown about\rdopamine��s functions and details of its cellular effects, it is clear that it\ris fundamental to reward processing in the mammalian brain. Dopamine is not the\ronly neuromodulator involved in reward processing, and its role in aversive\rsituations��punishment��remains controversial. Dopamine also can function\rdifferently in non-mammals. But no one doubts that dopamine is essential for\rreward-related processes in mammals, including humans.\nAn early, traditional view is that dopamine neurons broadcast a\rreward signal to multiple brain regions implicated in learning and motivation.\rThis view followed from a famous 1954 paper by James Olds and Peter Milner that\rdescribed the effects of electrical stimulation on certain areas of a rat��s\rbrain. They found that electrical stimulation to particular regions acted as a\rvery powerful reward in controlling the rat��s behavior: ��... the control\rexercised over the animal��s behavior by means of this reward is extreme,\rpossibly exceeding that exercised by any other reward previously used in animal\rexperimentation�� (Olds and Milner, 1954). Later research revealed that the\rsites at which stimulation was most effective in producing this rewarding\reffect excited dopamine pathways, either directly or indirectly, that\rordinarily are excited by natural rewarding stimuli. Effects similar to these\rwith rats were also observed with human subjects. These observations strongly\rsuggested that dopamine neuron activity signals reward.\nBut if the reward prediction error hypothesis is correct��even if it\raccounts for only some features of a dopamine neuron��s activity��this\rtraditional view of dopamine neuron activity is not entirely correct: phasic\rresponses of dopamine neurons signal reward prediction errors, not reward\ritself. In reinforcement learning��s terms, a dopamine neuron��s phasic response\rat a time tcorresponds to \u0026amp;_i = Rt + 7V(St)��\nV\u0026nbsp; (St_i), not to Rt.\nReinforcement learning theory and algorithms help reconcile the reward-prediction-\rerror view with the conventional notion that dopamine signals reward. In many\rof the algorithms we discuss in this book, 5functions as a\rreinforcement signal, mean\u0026shy;ing that it is the main driver of learning. For\rexample, 5 is the critical factor in the TD model of classical conditioning,\rand 5 is the reinforcement signal for learn\u0026shy;ing both a value function and a\rpolicy in an actor-critic architecture (Sections 13.5 and 15.7).\rAction-dependent forms of 5 are reinforcement signals for Q-learning and Sarsa.\rThe reward signal Rt is a crucial component of 5t_i, but it is not the com\u0026shy;plete determinant of its\rreinforcing effect in these algorithms. The additional term 7V(St) �� V(St_i) is the higher-order reinforcement part of 5t_i, and even if reward occurs (Rt = 0), the TD error\rcan be silent if the reward is fully predicted (which is fully explained in\rSection 15.6 below).\nA closer look at Olds�� and Milner��s 1954 paper, in fact, reveals\rthat it is mainly about the reinforcing effect of electrical stimulation in an\rinstrumental condition\u0026shy;ing task. Electrical stimulation not only energized the\rrats�� behavior��through dopamine��s effect on motivation��it also led to the rats\rquickly learning to stimulate themselves by pressing a lever, which they would\rdo frequently for long periods of time. The activity of dopamine neurons\rtriggered by electrical stimulation reinforced the rats�� lever pressing.\nMore recent experiments using optogenetic methods clinch the role of\rphasic re\u0026shy;sponses of dopamine neurons as reinforcement signals. These methods\rallow neuro\u0026shy;scientists to precisely control the activity of selected neuron\rtypes at a millisecond timescale in awake behaving animals. Optogenetic methods\rintroduce light-sensitive proteins into selected neuron types so that these\rneurons can be activated or silenced by means of flashes of laser light. The\rfirst experiment using optogenetic methods to study dopamine neurons showed that\roptogenetic stimulation producing phasic acti\u0026shy;vation of dopamine neurons in\rmice was enough to condition the mice to prefer the side of a chamber where\rthey received this stimulation as compared to the chamber��s other side where\rthey received no, or lower-frequency, stimulation (Tsai et al. 2009). In\ranother example, Steinberg et al. (2013) used optogenetic activation of\rdopamine neurons to create artificial bursts of dopamine neuron activity in\rrats at the times when rewarding stimuli were expected but omitted��times when\rdopamine neuron activity normally pauses. With these pauses replaced by\rartificial bursts, responding was sustained when it would ordinarily decrease\rdue to lack of reinforcement (in extinction trials), and learning was enabled\rwhen it would ordinarily be blocked due to the reward being already predicted\r(the blocking paradigm; Section 14.2.1).\nAdditional evidence for the reinforcing function of dopamine comes\rfrom optoge- netic experiments with fruit flies, except in these animals\rdopamine��s effect is the opposite of its effect in mammals: optically triggered\rbursts of dopamine neuron ac\u0026shy;tivity act just like electric foot shock in\rreinforcing avoidance behavior, at least for the population of dopamine neurons\ractivated (Claridge-Chang et al. 2009). Although none of these optogenetic\rexperiments showed that phasic dopamine neuron activity is specifically like a\rTD error, they convincingly demonstrated that phasic dopamine neuron activity\racts just like 5acts (or perhaps like minus 5acts in fruit flies) as the reinforcement signal in\ralgorithms for both prediction (classical conditioning) and control\r(instrumental conditioning).\nDopamine neurons are particularly well suited to broadcasting a\rreinforcement signal to many areas of the brain. These neurons have huge axonal\rarbors, each releasing dopamine at 100to 1,000times more synaptic sites than reached by the axons of typical\rneurons. Figure 15.1 shows the axonal arbor of a single dopamine neuron whose\rcell body is in the SNpc of a rat��s brain. Each axon of a SNpc or VTA dopamine\rneuron makes roughly 500,000 synaptic contacts on the dendrites of neurons in\rtargeted brain areas.\nIf dopamine neurons broadcast a reinforcement signal\rlike reinforcement learning��s 5, then since this is a scalar signal, i.e., a\rsingle number, all dopamine neurons in both the SNpc and VTA would be expected\rto activate more-or-less identically so that they would act in near synchrony\rto send the same signal to all of the sites their axons target. Although it has\rbeen a common belief that dopamine neurons do act together like this, modern\revidence is pointing to the more complicated picture that different subpopulations\rof dopamine neurons respond to input differently depending on the structures to\rwhich they send their signals and the different ways these signals act on their\rtarget structures. Dopamine has functions other than signaling RPEs, and even for\rdopamine neurons that do signal RPEs, it can make sense to send different RPEs\rto different structures depending on the roles these structures play in\rproducing reinforced behavior. This is beyond what we treat in any detail in\rthis book, but vector-valued RPE signals make sense from the perspective of\rreinforcement learning when decisions can be decomposed into separate\rsub-decisions, or more generally, as a way to address the structuralversion of the credit assignment problem: How do\ryou distribute credit for success (or blame for failure) of a decision among\rthe many component structures that could have been involved in producing it? We\rsay a bit more about this in Section 15.10 below.\nThe axons of\rmost dopamine neurons make synaptic contact with neurons in the frontal cortex\rand the basal ganglia, areas of the brain involved in voluntary move\u0026shy;ment,\rdecision-making, learning, and cognitive functions such a planning. Since most\rideas relating dopamine to reinforcement learning focus on the basal ganglia, and\rthe connections from dopamine neurons are particularly dense there, we focus on\rthe basal ganglia here. The basal ganglia are a collection neuron groups, or\rnuclei, lying at the base of the forebrain. The main input structure of the\rbasal ganglia is called the striatum. Essentially all of the cerebral cortex,\ramong other structures, provides input to the striatum. The activity of\rcortical neurons conveys a wealth of informa\u0026shy;tion about sensory input, internal\rstates, and motor activity. The axons of cortical neurons make synaptic\rcontacts on the dendrites of the main input/output neurons of the striatum,\rcalled medium spiny neurons. Output from the striatum loops back via other\rbasal ganglia nuclei and the thalamus to frontal areas of cortex, and to motor\rareas, making it possible for the striatum to influence movement, abstract\rdecision processes, and reward processing. Two main subdivisions of the\rstriatum are important for reinforcement learning: the dorsal striatum,\rprimarily implicated\n\r\nFigure 15.1: Axonal arbor of a single\rneuron producing dopamine as a neurotransmitter whose cell body is in the\rSNpc of a rat��s brain. These axons make synaptic contacts with a huge number\rof dendrites of neurons in targeted brain areas. Adapted from Journal\rof Neuroscience,Matsuda,\rFuruta, Nakamura, Hioki, Fujiyama, Arai, and Kaneko, volume 29, 2009, page\r451.\n\r\r\r\r\r\u0026nbsp;\n\r\nFigure 15.2: Spine of a striatal\rneuron showing input from both cortical and dopamine neurons. Axons of\rcortical neurons influence striatal neurons via corticostriatal synapses\rreleasing the neurotransmitter glutamate at the tips of spines covering the\rdendrites of striatal neurons. An axon of a VTA or SNpc dopamine neuron is\rshown passing by the spine (from the lower right). ��Dopamine varicosities�� on\rthis axon release dopamine at or near the spine stem, in an arrangement that\rbrings together presynaptic input from cortex, postsynaptic activity of the\rstriatal neuron, and dopamine, making it possible that several types of\rlearning rules govern the plasticity of corticostriatal synapses. Each axon\rof a dopamine neuron makes synaptic contact with the stems of roughly 500,000\rspines. Some of the complexity omitted from our discussion is shown here by\rother neurotransmitter pathways and multiple receptor types, such as D1 an D2\rdopamine receptors by which dopamine can produce different effects at spines\rand other postsynaptic sites. From Journal\rof Neurophysiology,W.\rSchultz, vol. 80, 1998, page 10.\n\r\r\r\r\r\u0026nbsp;\nin influencing\raction selection, and the ventral striatum, thought to be critical for\rdifferent aspects of reward processing, including the assignment of affective\rvalue to sensations.\nThe dendrites of medium spiny neurons are covered with spines on\rwhose tips the axons of neurons in the cortex make synaptic contact. Also\rmaking synaptic contact with these spines��in this case contacting the spine\rstems��are axons of dopamine neurons (Figure 15.2). This arrangement brings\rtogether presynaptic ac\u0026shy;tivity of cortical neurons, postsynaptic activity of\rmedium spiny neurons, and input from dopamine neurons. What actually occurs at\rthese spines is complex and not completely understood. Figure 15.2 hints at the\rcomplexity by showing two types of receptors for dopamine, receptors for\rglutamate��the neurotransmitter of the cor\u0026shy;tical inputs��and multiple ways that\rthe various signals can interact. But evidence is mounting that changes in the\refficacies of the synapses on the pathway from the cortex to the striatum,\rwhich neuroscientists call corticostriatal synapses,depend critically on appropriately-timed dopamine\rsignals.\n15.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rExperimental\rSupport for the Reward Prediction Error Hypothesis\nDopamine\rneurons respond with bursts of activity to intense, novel, or unexpected visual\rand auditory stimuli that trigger eye and body movements, but very little of\rtheir activity is related to the movements themselves. This is surprising\rbecause degeneration of dopamine neurons is a cause of Parkinson��s disease,\rwhose symptoms include motor disorders, particularly deficits in self-initiated\rmovement. Motivated by the weak relationship between dopamine neuron activity\rand stimulus-triggered eye and body movements, Romo and Schultz (1990) and\rSchultz and Romo (1990) took the first steps toward the reward prediction error\rhypothesis by recording the activity of dopamine neurons and muscle activity\rwhile monkeys moved their arms.\nThey trained two monkeys to reach from a resting hand position into\ra bin con\u0026shy;taining a bit of apple, a piece of cookie, or a raisin, when the\rmonkey saw and heard the bin��s door open. The monkey could then grab and bring\rthe food to its mouth. After a monkey became good at this, it was trained on\rtwo additional tasks. The purpose of the first task was to see what dopamine\rneurons do when movements are self-initiated. The bin was left open but covered\rfrom above so that the monkey could not see inside but could reach in from\rbelow. No triggering stimuli were pre\u0026shy;sented, and after the monkey reached for\rand ate the food morsel, the experimenter usually (though not always), silently\rand unseen by the monkey, replaced food in the bin by sticking it onto a rigid\rwire. Here too, the activity of the dopamine neurons Romo and Schultz monitored\rwas not related to the monkey��s movements, but a large percentage of these\rneurons produced phasic responses whenever the monkey first touched a food\rmorsel. These neurons did not respond when the monkey touched just the wire or\rexplored the bin when no food was there. This was good evidence that the\rneurons were responding to the food and not to other aspects of the task.\nThe purpose of Romo and Schultz��s second task was to see what\rhappens when movements are triggered by stimuli. This task used a different bin\rwith a moveable cover. The sight and sound of the bin opening triggered\rreaching movements to the bin. In this case, Romo and Schultz found that after\rsome period of training, the dopamine neurons no longer responded to the touch\rof the food but instead responded to the sight and sound of the opening cover\rof the food bin. The phasic responses of these neurons had shifted from the\rreward itself to stimuli predicting the availability of the reward. In a\rfollowup study, Romo and Schultz found that most of the dopamine neurons whose\ractivity they monitored did not respond to the sight and sound of the bin\ropening outside the context of the behavioral task. These observations\rsuggested that the dopamine neurons were responding neither to the initiation\rof a movement nor to the sensory properties of the stimuli, but were rather\rsignaling an expectation of reward.\nSchultz��s group conducted many additional studies involving both\rSNpc and VTA dopamine neurons. A particular series of experiments was\rinfluential in suggesting that the phasic responses of dopamine neurons\rcorrespond to TD errors and not to simpler errors like those in the\rRescorla-Wagner model (14.3). In the first of \n\r\rthese experiments (Ljungberg, Apicella, and Schultz, 1992), monkeys\rwere trained to depress a lever after a light was illuminated as a ��trigger\rcue�� to obtain a drop of apple juice. As Romo and Schultz had observed earlier,\rmany dopamine neurons initially responded to the reward��the drop of juice\r(Figure 15.3, top panel). But many of these neurons lost that reward response\ras training continued and devel\u0026shy;oped responses instead to the illumination of\rthe light that predicted the reward (Figure 15.3, middle panel). With continued\rtraining, lever pressing became faster while the number of dopamine neurons\rresponding to the trigger cue decreased.\n\r\nFigure 15.3: The response of dopamine\rneurons shifts from initial responses to primary reward to earlier predictive\rstimuli. These are plots of the number of action potentials produced by\rmonitored dopamine neurons within small time intervals, averaged over all the\rmonitored dopamine neurons (ranging from 23 to 44 neurons for these data).\rTop: dopamine neurons are activated by the unpredicted delivery of drop of\rapple juice. Middle: with learning, dopamine neurons developed responses to\rthe reward-predicting trigger cue and lost responsiveness to the delivery of\rreward. Bottom: with the addition of an instruction cue preceding the trigger\rcue by 1second, dopamine neurons shifted their responses from the trigger cue to the\rearlier instruction cue. From Schultz et al. (1995), MIT Press.\n\r\r\r\r\r\u0026nbsp;\nFollowing this study, the same monkeys were trained on a new task\r(Schultz, Apicella, and Ljungberg, 1993). Here the monkeys faced two levers,\reach with a light above it. Illuminating one of these lights was an\r��instruction cue�� indicating which of the two levers would produce a drop of\rapple juice. In this task, the instruction cue preceded the trigger cue of the\rprevious task by a fixed interval of\n1\u0026nbsp;\rsecond. The\rmonkeys learned to withhold reaching until seeing the trigger cue, and dopamine\rneuron activity increased, but now the responses of the monitored dopamine\rneurons occurred almost exclusively to the earlier instruction cue and not to\rthe trigger cue (Figure 15.3, bottom panel). Here again the number of dopamine\rneurons responding to the instruction cue was much reduced when the task was\rwell learned. During learning across these tasks, dopamine neuron activity\rshifted from \n\r\rinitially responding to the reward to responding to the earlier\rpredictive stimuli, first progressing to the trigger stimulus then to the still\rearlier instruction cue. As responding moved earlier in time it disappeared\rfrom the later stimuli. This shifting of responses to earlier reward\rpredictors, while losing responses to later predictors is a hallmark of TD\rlearning (see, for example, Figure 14.5).\nThe task just described revealed another property of dopamine neuron\ractivity shared with TD learning. The monkeys sometimes pressed the wrong key,\rthat is, the key other than the instructed one, and consequently received no\rreward. In these trials, many of the dopamine neurons showed a sharp decrease\rin their firing rates below baseline shortly after the reward��s usual time of\rdelivery, and this happened without the availability of any external cue to\rmark the usual time of reward delivery (Figure 15.4). Somehow the monkeys were internally\rkeeping track of the timing of the reward. (Response timing is one area where\rthe simplest version of TD learning needs to be modified to account for some of\rthe details of the timing of dopamine neuron responses. We consider this issue\rin the following section.)\nThe\robservations from the studies described above led Schultz and his group to\rconclude that dopamine neurons respond to unpredicted rewards, to the earliest\rpredictors of reward, and that dopamine neuron activity decreases below baseline\rif a reward, or a predictor of reward, does not occur at its expected time.\rResearchers familiar with reinforcement learning were quick to recognize that\rthese results are strikingly similar to how the TD error behaves as the\rreinforcement signal in a TD algorithm. The next section explores this\rsimilarity by working through a specific example in detail.\n15.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rTD\rError/Dopamine Correspondence\nThis section\rexplains the correspondence between the TD error 5and the phasic responses of dopamine neurons\robserved in the experiments just described. We examine how 5 changes over the\rcourse of learning in a task something like the one described above where a\rmonkey first sees an instruction cue and then a fixed time later has to respond\rcorrectly to a trigger cue in order to obtain reward. We use a simple idealized\rversion of this task, but we go into a lot more detail than is usual because we\rwant to emphasize the theoretical basis of the parallel between TD errors and\rdopamine neuron activity.\n\r\r\rFigure 15.4: The response of dopamine neurons drops\rbelow baseline time when an expected reward fails to occur. Top: dopamine\rneurons the unpredicted delivery of a drop of apple juice. Middle: dopamine\rneurons respond to a conditioned stimulus (CS) that predicts reward and do\rnot respond to the reward itself. Bottom: when the reward predicted by the\rCS fails to occur, the activity of dopamine neurons drops below baseline\rshortly after the time the reward is expected to occur. At the top of each\rof these panels is shown the average number of action potentials produced\rby monitored dopamine neurons within small time intervals around the\rindicated times. The raster plots below show the activity patterns of the\rindividual dopamine neurons that were monitored; each dot represents an\raction potential. From Schultz, Dayan, and Montague, A Neural Substrate of\rPrediction and Reward, Science,vol. 275, issue\r5306, pages 1593-1598, March 14, 1997. Reprinted with permission from AAAS.\n\r\r\r\r\r\r\r\rshortly after the are activated by\n\r\r\r\r\rThe first simplifying assumption is\rthat the agent has already learned the actions required to obtain reward. Then\rits task is just to learn accurate predictions of future reward for the\rsequence of states it experiences. This is then a prediction task, or more\rtechnically, a policy-evaluation task: learning the value function for a fixed\rpolicy (Sections 4.1 and 6.1). The value function to be learned assigns to each\rstate a value that predicts the return that will follow that state if the agent\rselects actions according to the given policy, where the return is the\r(possibly discounted) sum of all the future rewards. This is unrealistic as a\rmodel of the monkey��s situation because the monkey would likely learn these predictions\rat the same time that it is learning toact correctly (as would a reinforcement learning algorithm that learns policies\ras well as value functions, such as an actor-critic algorithm), but this\rscenario is simpler to describe than one in which a policy and a value function\rare learned simultaneously.\nNow imagine that the agent��s experience divides into multiple\rtrials, in each of which the same sequence of states repeats, with a distinct\rstate occurring on each time step during the trial. Further imagine that the\rreturn being predicted is limited to the return over a trial, which makes a\rtrial analogous to a reinforcement learning episode as we have defined it. In\rreality, of course, the returns being predicted are not confined to single\rtrials, and the time interval between trials is an important factor in\rdetermining what an animal learns. This is true for TD learning as well, but\rhere we assume that returns do not accumulate over multiple trials. Given this,\rthen, a trial in experiments like those conducted by Schultz and colleagues is\requivalent to an episode of reinforcement learning. (Though in this discussion,\rwe will use the term trial instead of episode to relate better to the\rexperiments.)\nAs usual, we also need to make an assumption about how states are\rrepresented as inputs to the learning algorithm, an assumption that influences\rhow closely the TD error corresponds to dopamine neuron activity. We discuss\rthis issue later, but for now we assume the same CSC representation used by\rMontague et al. (1996) in which there is a separate internal stimulus for each\rstate visited at each time step in a trial. This reduces the process to the\rtabular case covered in the first part of this book. Finally, we assume that\rthe agent uses TD(0) to learn a value function, V, stored in a lookup table initialized to be zero\rfor all the states. We also assume that this is a deterministic task and that\rthe discount factor, 7, is very nearly one so that we can ignore it.\nFigure 15.5 shows the time courses of R, V, and 5 at several stages\rof learning in this policy-evaluation task. The time axes represent the time\rinterval over which a sequence of states is visited in a trial (where for\rclarity we omit showing individual states). The reward signal is zero throughout\reach trial except when the agent reaches the rewarding state, shown near the\rright end of the time line, when the reward signal becomes some positive\rnumber, say R*. The goal of TD learning is to predict the return for each state\rvisited in a trial, which in this undiscounted case and given our assumption\rthat predictions are confined to individual trials, is simply R* for each\rstate.\nPreceding the rewarding state is a sequence of reward-predicting\rstates, with the earliest\rreward-predicting stateshown near the left end of the time line. This is like the state\rnear the start of a trial, for example like the state marked by the instruction\rcue in a trial of the monkey experiment of Schultz et al. (1993) described\rabove. It is the first state in a trial that reliably predicts that trial��s\rreward. (Of course, in reality states visited on preceding trials are even\rearlier reward-predicting states, but since we are confining predictions to\rindividual trials, these do not qualify as predictors of thistrial��s\rreward. Below we give a more satisfactory, though more abstract, description of\ran earliest reward-predicting state.) The latest reward-predicting state in a trial is the state immediately preceding the trial��s rewarding\rstate. This is the state near the far right end of the time line in Figure\r15.5. Note that the rewarding state of a trial does not predict the return for\rthat trial: the value of this state would come to predict the return over all\rthe followingtrials, which here we are assuming to be zero in\rthis episodic formulation.\nFigure 15.5 shows the first-trial time courses of V and 5 as the\rgraphs labeled ��early in learning.�� Because the reward signal is zero\rthroughout the trial except when the rewarding state is reached, and all the\rV-values are zero, the TD error is also zero until it becomes R* at the\rrewarding state. This follows because 5t-i =\nR\n\u0026lt; regular predictors of Rover this interval -\nearly in V\rlearning\nlearning\ncomplete\nRomitted S\nFigure 15.5: The behavior of the TD error 5 during TD learning is consistent with features of the phasic\ractivation of dopamine neurons. (Here 5 is the TD\rerror available at time t, i.e., 5t��1). Top: a sequence of states, shown as an\rinterval of regular predictors, is followed by a non-zero reward R*. Early in learning: the initial value\rfunction, V, and initial 5, which at first is equal to R*. Learning complete: the value function accurately predicts\rfuture reward, 5 is positive at the\rearliest predictive state, and 5 = 0 at the\rtime of the non-zero reward. R* omitted: at the time\rthe predicted reward is omitted, 5 becomes\rnegative. See text for a complete explanation of why this happens.\nRt + Vt �� Vt_i = Rt + 0 �� 0 = Rt, which is zero until it equals R*\rwhen the reward occurs. Here Vt and Vt_i are respectively the estimated values\rof the states visited at times t and t �� 1 in a trial. The TD error at this\rstage of learning is analogous to a dopamine neuron responding to an\runpredicted reward, e.g., a drop apple juice, at the start of training.\nThroughout this first trial\rand all successive trials, TD(0) backups occur at each state transition as\rdescribed in Chapter 6. This successively increases the values\rof the reward-predicting states, with the increases spreading backwards from\rthe rewarding state, until the values converge to the correct return\rpredictions. In this case (since we are assuming no discounting) the correct\rpredictions are equal to R* for all the reward-predicting states. This can be\rseen in Figure 15.5 as the graph of\nV\u0026nbsp;\u0026nbsp;\rlabeled ��learning complete��\rwhere the values of all the states from the earliest to the latest\rreward-predicting states all equal R*. The values of the states preceding the\rearliest reward-predicting state remain low (which Figure 15.5 shows as zero)\rbecause they are not reliable predictors of reward.\nWhen learning is complete, that is, when V\rattains its correct values, the TD errors associated with transitions from any reward-predicting state are zero because the\rpredictions are now accurate. This is because for a transition from a reward-\rpredicting state to another reward-predicting state, we have 5t-i =\rRt + Vt �� Vt_i = 0+ R* �� R* = 0, and for\rthe transition from the latest reward-predicting state to the rewarding state,\rwe have 5t-i = Rt + Vt �� Vt_i = R* + 0 �� R* = 0. On the other hand,\rthe TD error on a transition from any state to the\rearliest reward-predicting state is positive because of the mismatch between\rthis state��s low value and the larger value of the following reward-predicting\rstate. Indeed, if the value of a state preceding the earliest reward-predicting\rstate were zero, then after the transition to the earliest reward-predicting\rstate, we would have that 5t-i = Rt + Vt �� Vt-i = 0 + R*\r�� 0 = R*. The ��learning complete�� graph of 5 in Figure 15.5 shows this positive\rvalue at the earliest reward-predicting state, and zeros everywhere else.\nThe positive TD error upon transitioning to the\rearliest reward-predicting state is analogous to the persistence of dopamine\rresponses to the earliest stimuli predicting reward. By the same token, when\rlearning is complete, a transition from the latest reward-predicting state to\rthe rewarding state produces a zero TD error because the latest\rreward-predicting state��s value, being correct, cancels the reward. This\rparallels the observation that fewer dopamine neurons generate a phasic\rresponse to a fully predicted reward than to an unpredicted reward.\nAfter learning, if the reward is suddenly\romitted, the TD error goes negative at the usual time of reward because the\rvalue of the latest reward-predicting state is then too high: 5t-i =\rRt + Vt �� Vt-i = 0+ 0�� R* = ��R*, as shown at the right end of the ��R omitted�� graph of 5\rin Figure 15.5. This is like dopamine neuron activity decreasing below baseline\rat the time an expected reward is omitted as seen in the experiment of Schultz\ret al. (1993) described above and shown in Figure 15.4.\nThe idea of an earliest\rreward-predicting state deserves more attention. In the scenario\rdescribed above, since experience is divided into trials, and we assumed that\rpredictions are confined to individual trials, the earliest reward-predicting\rstate is always the first state of a trial. Clearly this is artificial. A more\rgeneral way to think of an earliest reward-predicting state is that it is an unpredicted predictor of reward, and there can be many such\rstates. In an animal��s life, many different states may precede an earliest\rreward-predicting state. However, because these states are more often followed\rby other states that do not predict reward, their\rreward-predicting powers, that is, their values, remain low. A TD algorithm, if\roperating throughout the animal��s life, would back up values to these states\rtoo, but the backups would not consistently accumulate because, by assumption,\rnone of these states reliably precedes an earliest reward-predicting state. If\rany of them did, they would be reward-predicting states as well. This might\rexplain why with overtraining, dopamine responses decrease to even the earliest\rreward-predicting stimulus in a trial. With overtraining one would expect that\reven a formerly-unpredicted predictor state would become predicted by stimuli\rassociated with earlier states: the animal��s interaction with its environment\rboth inside and outside of an experimental task would become commonplace. Upon\rbreaking this routine with the introduction of a new task, however, one would\rsee TD errors reappear, as indeed is observed in dopamine neuron\nactivity.\nThe example described above explains why the TD error shares key\rfeatures with the phasic activity of dopamine neurons when the animal is\rlearning in a task similar to the idealized task of our example. But not every\rproperty of the phasic activity of dopamine neurons coincides so neatly with\rproperties of 5. One of the most troubling discrepancies involves what happens\rwhen a reward occurs earlierthan expected. We have seen that the omission of an\rexpected reward produces a negative prediction error at the reward��s expected\rtime, which corresponds to the activity of dopamine neurons decreasing below\rbaseline when this happens. If the reward arrives later than expected, it is\rthen an unexpected reward and generates a positive prediction error. This\rhappens with both TD errors and dopamine neuron responses. But when reward\rarrives earlier than expected, dopamine neurons do not do what the TD error\rdoes��at least with the CSC representation used by Montague et al. (1996) and by\rus in our example. Dopamine neurons do respond to the early reward, which is\rconsistent with a positive TD error because the reward is not predicted to\roccur then. However, at the later time when the reward is expected but omitted,\rthe TD error is negative whereas, in contrast to this prediction, dopamine\rneuron activity does not drop below baseline in the way the TD model predicts\r(Hollerman and Schultz, 1998). Something more complicated is going on in the\ranimal��s brain than simply TD learning with a CSC representation.\nSome of the mismatches between the TD error and dopamine neuron\ractivity can be addressed by selecting suitable parameter values for the TD\ralgorithm and by using stimulus representations other than the CSC\rrepresentation. For instance, to address the early-reward mismatch just\rdescribed, Suri and Schultz (1999) proposed a CSC representation in which the\rsequences of internal signals initiated by earlier stimuli are cancelled by the\roccurrence of a reward. Another proposal by Daw, Courville, and Touretzky\r(2006) is that the brain��s TD system uses representations produced by\rstatistical modeling carried out in sensory cortex rather than simpler\rrepresentations based on raw sensory input. Ludvig, Sutton, and Kehoe (2008)\rfound that TD learning with a microstimulus (MS) representation (Figure 14.2)\rfits the activity of dopamine neurons in the early-reward and other situations\rbetter than when a CSC representation is used. Pan, Schmidt, Wickens, and\rHyland (2005) found that even with the CSC representation, prolonged eligibility\rtraces improve the fit of the TD error to some aspects of dopamine neuron\ractivity. In general, many fine details of TD-error behavior depend on subtle\rinteractions between eligibility traces, discounting, and stimulus\rrepresentations. Findings like these elaborate and refine the reward prediction\rerror hypothesis without refuting its core claim that the phasic activity of\rdopamine neurons is well characterized as signaling TD errors.\nOn the other hand, there are other discrepancies between the TD\rtheory and ex\u0026shy;perimental data that are not so easily accommodated by selecting\rparameter values and stimulus representations (we mention some of these\rdiscrepancies in the Bib\u0026shy;liographical and Historical Remarks section at the end\rof this chapter), and more mismatches are likely to be discovered as\rneuroscientists conduct ever more refined experiments. But the reward\rprediction error hypothesis has been functioning very effectively as a catalyst\rfor improving our understanding of how the brain��s reward system works.\rIntricate experiments have been designed to validate or refute pre\u0026shy;dictions\rderived from the hypothesis, and experimental results have, in turn, led to\rrefinement and elaboration of the TD error/dopamine hypothesis.\nA remarkable aspect of these developments is that the reinforcement\rlearning algo\u0026shy;rithms and theory that connect so well with properties of the\rdopamine system were developed from a computational perspective in total\rabsence of any knowledge about the relevant properties of dopamine neurons��remember,\rTD learning and its con\u0026shy;nections to optimal control and dynamic programming\rwere developed many years before any of the experiments were conducted that\rrevealed the TD-like nature of dopamine neuron activity. This unplanned\rcorrespondence, despite not being per\u0026shy;fect, suggests that the TD error/dopamine\rparallel captures something significant about brain reward processes.\nIn addition to\raccounting for many features of the phasic activity of dopamine neurons, the\rreward prediction error hypothesis links neuroscience to other aspects of\rreinforcement learning, in particular, to learning algorithms that use TD\rerrors as reinforcement signals. Neuroscience is still far from reaching\rcomplete understand\u0026shy;ing of the circuits, molecular mechanisms, and functions of\rthe phasic activity of dopamine neurons, but evidence supporting the reward\rprediction error hypothe\u0026shy;sis, along with evidence that phasic dopamine\rresponses are reinforcement signals for learning, suggest that the brain might\rimplement something like an actor-critic algorithm in which TD errors play\rcritical roles. Other reinforcement learning algo\u0026shy;rithms are plausible\rcandidates too, but actor-critic algorithms fit the anatomy and physiology of\rthe mammalian brain particularly well, as we describe in the following two\rsections.\n15.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rNeural\rActor��Critic\nActor-critic\ralgorithms learn both policies and value functions. The ��actor�� is the\rcomponent that learns policies, and the ��critic�� is the component that learns\rabout whatever policy is currently being followed by the actor in order to\r��criticize�� the actor��s action choices. The critic uses a TD algorithm to learn\rthe state-value function for the actor��s current policy. The value function\rallows the critic to critique the actor��s action choices by sending TD errors,\r5, to the actor. A positive 5 means that the action was ��good�� because it led\rto a state with a better-than-expected value; a negative 5 means that the\raction was ��bad�� because it led to a state with a worse-than-expected value.\rBased on these critiques, the actor continually updates its policy.\nTwo distinctive features of actor-critic algorithms are responsible\rfor thinking that the brain might implement an algorithm like this. First, the\rtwo components of an actor-critic algorithm��the actor and the critic��suggest\rthat two parts of the striatum��the dorsal and ventral subdivisions (Section\r15.4), both critical for reward- based learning��may function respectively\rsomething like an actor and a critic. A \n\r\rsecond property of actor-critic algorithms that suggests a brain\rimplementation is that the TD error has the dual role of being the\rreinforcement signal for both the actor and the critic, though it has a\rdifferent influence on learning in each of these components. This fits well\rwith several properties of the neural circuitry: axons of dopamine neurons\rtarget both the dorsal and ventral subdivisions of the striatum; dopamine\rappears to be critical for modulating synaptic plasticity in both structures;\rand how a neuromodulator such as dopamine acts on a target structure depends on\rproperties of the target structure and not just on properties of the\rneuromodulator.\nSection 13.5 presents actor-critic algorithms as policy gradient\rmethods, but the actor-critic algorithm of Barto, Sutton, and Anderson (1983)\rwas simpler and was presented as an artificial neural network. Here we describe\ran artificial neural net\u0026shy;work implementation something like that of Barto et\ral., and we follow Takahashi, Schoenbaum, and Niv (2008) in giving a schematic\rproposal for how this artificial neural network might be implemented by real\rneural networks in the brain. We postpone discussion of the actor and critic\rlearning rules until Section 15.8, where we present them as special cases of\rthe policy-gradient formulation and discuss what they suggest about how\rdopamine might modulate synaptic plasticity.\nFigure 15.6a shows an implementation of an actor-critic algorithm as\ran artificial neural network with component networks implementing the actor and\rthe critic. The critic consists of a single neuron-like unit, V , whose output\ractivity represents state values, and a component shown as the diamond labeled\rTD that computes TD errors by combining V ��s output with reward signals and\rwith previous state values (as suggested by the loop from the TD diamond to\ritself). The actor network has a single layer of kactor units labeled Ai, i= 1,...,k.The output of each actor unit is a component of a\rk-dimensional action vector. An alternative is that there are k separate\ractions, one commanded by each actor unit, that compete with one another to be\rexecuted, but here we will think of the entire ��-vector as\ran action.\nBoth the critic and actor networks receive input consisting of\rmultiple features representing the state of the agent��s environment. (Recall\rfrom Chapter 1 that the environment of a reinforcement learning agent includes\rcomponents both inside and outside of the ��organism�� containing the agent.) The\rfigure shows these features as the circles labeled xi, X2,...,xn, shown twice just to keep the figure simple. A weight\rrepresenting the efficacy of a synapse is associated with each connection from\reach feature Xi to the critic unit, V, and to each of the action units, Ai. The weights\rin the critic network parameterize the value function, and the weights in the\ractor network parameterize the policy. The networks learn as these weights\rchange according to the critic and actor learning rules that we describe in the\rfollowing section.\nThe TD error produced by circuitry in the critic is the\rreinforcement signal for changing the weights in both the critic and the actor\rnetworks. This is shown in Figure 15.6a by the line labeled ��TD error 5��\rextending across all of the connections in the critic and actor networks. This\raspect of the network implementation, together with the reward prediction error\rhypothesis and the fact that the activity of dopamine neurons is so widely\rdistributed by the extensive axonal arbors of these neurons, suggests that an\ractor-critic network something like this may not be too farfetched\nas a hypothesis about how reward-related learning might happen in\rthe brain.\nFigure 15.6b\rsuggests��very schematically��how the artificial neural network on the figure��s\rleft might map onto structures in the brain according to the hypothesis of\rTakahashi et al. (2008). The hypothesis puts the actor and the value-learning\rpart of the critic respectively in the dorsal and ventral subdivisions of the\rstriatum, the input structure of the basal ganglia. Recall from Section 15.4\rthat the dorsal striatum is primarily implicated in influencing action selection,\rand the ventral stria\u0026shy;tum is thought to be critical for different aspects of\rreward processing, including the assignment of affective value to sensations.\rThe cerebral cortex, along with other structures, sends input to the striatum\rconveying information about stimuli, internal states, and motor activity.\nIn this\rhypothetical actor-critic brain implementation, the ventral striatum sends\rvalue information to the VTA and SNpc, where dopamine neurons in these nuclei\n\r\r\r\r\rEnsIJls\rlrtSJoQ\n\r\r\r\r\r\r\r\rjlnlujls/SJBls\n\r\r\r\r\r\r\r\r= n\u0026pound;jls/s\u0026lt;ulnjls\n\r\r\r\r\r\r\r\r(SE.Jrt9ld!llnlu)\rX91JOU\n\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rFigure 15.6:\rActor-critic artificial neural network and a hypothetical neural implementa\u0026shy;tion.\ra) Actor-critic algorithm as an artificial neural network. The actor adjusts a\rpolicy based on the TD error 5 it receives from the critic; the critic adjusts\rstate-value parameters using the same 5. The critic produces a TD error from\rthe reward signal, R, and the current change in its estimate of state values.\rThe actor does not have direct access to the reward signal, and the critic does\rnot have direct access to the action. b) Hypothetical neural im\u0026shy;plementation of\ran actor-critic algorithm. The actor and the value-learning part of the critic\rare respectively placed in the ventral and dorsal subdivisions of the striatum.\rThe TD error is transmitted by dopamine neurons located in the VTA and SNpc to\rmodulate changes in synaptic efficacies of input from cortical areas to the\rventral and dorsal striatum. Adapted from Frontiers in Neuroscience, vol. 2(1),\r2008, Y. Takahashi, G. Schoenbaum, and Y. Niv, Silencing the critics:\rUnderstanding the effects of cocaine sensitization on dorsolateral and ventral\rstriatum in the context of an Actor/Critic model.\ncombine it\rwith information about reward to generate activity corresponding to TD errors\r(though exactly how dopaminergic neurons calculate these errors is not yet un\u0026shy;derstood).\rThe ��TD error 5�� line in Figure 15.6a becomes the line labeled ��Dopamine�� in\rFigure 15.6b, which represents the widely branching axons of dopamine neurons\rwhose cell bodies are in the VTA and SNpc. Referring back to Figure 15.2, these\raxons make synaptic contact with the spines on the dendrites of medium spiny\rneu\u0026shy;rons, the main input/output neurons of both the dorsal and ventral\rdivisions of the striatum. Axons of the cortical neurons that send input to the\rstriatum make synaptic contact on the tips of these spines. According to the\rhypothesis, it is at these spines where changes in the efficacies of the\rsynapses from cortical regions to the stratum are governed by learning rules\rthat critically depend on a reinforcement signal supplied by dopamine.\nAn important implication of the hypothesis illustrated in Figure\r15.6b is that the dopamine signal is not the ��master�� reward signal like the\rscalar Rt of reinforcement learning. In fact, the hypothesis implies that one\rshould not necessarily be able to probe the brain and record any signal like Rt\rin the activity of any single neuron. Many interconnected neural systems\rgenerate reward-related information, with dif\u0026shy;ferent structures being recruited\rdepending on different types of rewards. Dopamine neurons receive information\rfrom many different brain areas, so the input to the SNpc and VTA labeled\r��Reward�� in Figure 15.6b should be thought of as vector of reward-related\rinformation arriving to neurons in these nuclei along multiple input channels.\rWhat the theoretical scalar reward signal Rt might correspond to, then, is the\rnet contribution of all reward-related information to dopamine neuron activity.\rIt is the result of a pattern of activity across many neurons in different\rareas of the brain.\nAlthough the\ractor-critic neural implementation illustrated in Figure 15.6b may be correct\ron some counts, it clearly needs to be refined, extended, and modified to qualify\ras a full-fledged model of the function of the phasic activity of dopamine\rneurons. The Historical and Bibliographic Remarks section at the end of this\rchapter cites publications that discuss in more detail both empirical support\rfor this hypoth\u0026shy;esis and places where it falls short. We now look in detail at\rwhat the actor and critic learning algorithms suggest about the rules governing\rchanges in synaptic efficacies of corticostriatal synapses.\n15.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rActor and\rCritic Learning Rules\nIf the brain\rdoes implement something like the actor-critic algorithm��and assuming\rpopulations of dopamine neurons broadcast a common reinforcement signal to the\rcorticostriatal synapses of both the dorsal and ventral striatum as illustrated\rin Figure 15.6b (which is likely an oversimplification as we mentioned\rabove)��then this reinforcement signal affects the synapses of these two\rstructures in different ways. The learning rules for the critic and the actor\ruse the same reinforcement signal, the TD error 5, but its effect on learning\ris different for these two components. The TD error (combined with eligibility\rtraces) tells the actor how to update action probabilities in order to reach higher-valued states. Learning by\rthe actor is like instrumental conditioning using a Law-of-Effect-type learning\rrule (Section 1.7): the actor works to keep 5 as positive as possible. On the\rother hand, the TD error (when combined with eligibility traces) tells the\rcritic the direction and magnitude in which to change the parameters of the\rvalue function in order to improve its predictive accuracy. The critic works to\rreduce 5��s magnitude to be as close to zero as possible using a learning rule\rlike the TD model of classical conditioning (Section 14.2). The difference\rbetween the critic and actor learning rules is relatively simple, but this\rdifference has a profound effect on learning and is essential to how the\ractor-critic algorithm works. The difference lies solely in the eligibility\rtraces each type of learning rule uses.\nMore than one set of learning\rrules can be used in actor-critic neural networks like those in Figure 15.6b,\rbut to be specific, here we focus on rules based on the REINFORCE\rpolicy-gradient formulation described in Section 13.5. The box below recaps\rthat method using the pseudocode for a policy-gradient actor-critic with\religibility traces from the box in Section 13.5.\nPolicy-Gradient Actor-Critic\nInput: a differentiable policy\rparameterization n(a|s, 0), Va G A, s G S, Q G Rd\rInput: a differentiable state-value parameterization v(s,w), Vs\rG S, w G Rm Hyperparameters: step sizes a\r\u0026gt; 0, ^ \u0026gt; 0\nAt each iteration:\nCurrent state is S Take\raction A \u0026#12316;n(-|S, Q), observe S!,\rR\n5\u0026nbsp;\r�� R + yv(S ;,w) �� {)(S,w) ew�� Awew+ Vw ��(S,w) e�� �� A6e0 +\rVe logn(A|S, Q) w �� w + ¬5 ewQ �� Q + a5 e0\nThe first step is to think of\rthe value function as the output of a single linear neuron-like unit, called\rthe critic unit and labeled V in Figure 15.6a. Then\rthe value function is a linear function of the feature-vector representation of\rstate s, x(s) = (xi(s),..., xn(s))T,\rparameterized by a weight vector wT= (wi,...,\rwn):\nv(s,w) = wTx(s).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (15.1)\nEach Xi(s) is like the presynaptic signal to a\rneuron��s synapse whose efficacy is Wi. The weights are updated according to the\rrule in the box above: w\r�� w + ^5ew,\rwhere the reinforcement signal, 5, corresponds to a dopamine signal being\rbroadcast to all of the critic unit��s synapses. The eligibility trace vector, ew, for the critic unit is a trace of VwV(s,w) for\rpast states s. Since v(s,w) is linear in the weights,\nV\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rw v(s,w) = x(s).\nIn neural terms, this means that each synapse has\rits own eligibility trace, which is one component of the vector ew. A synapse��s eligibility trace accumulates according to the level of activity arriving at that synapse,\rthat is, the level of presynaptic activity, represented here by the component\rof the feature vector x(s)\rarriving at that synapse. The trace otherwise decays toward zero at a rate\rgoverned by the fraction Aw. A synapse is eligible\rfor modificationas long\ras its eligibility trace is non-zero. How the synapse��s efficacy is actually\rmodified depends on the reinforcement signals that arrive while the synapse is\religible. We call eligibility traces like these of the critic unit��s synapses non-contingent\religibility tracesbecause\rthey only depend on presynaptic activity and are not contingent in any way on\rpostsynaptic activity.\nThe non-contingent eligibility traces of the critic unit��s synapses\rmean that the critic unit��s learning rule is essentially the TD model of\rclassical conditioning de\u0026shy;scribed in Section 14.2. With the definition we have\rgiven above of the critic unit and its learning rule, the critic in Figure\r15.6a is the same as the critic in the neural network actor-critic of Barto et\ral. (1983). Clearly, a critic like this consisting of just one linear\rneuron-like unit is the simplest starting point; this critic unit is a proxy\rfor a more complicated neural network able to learn value functions of greater\rcomplexity.\nThe actor in\rFigure 15.6a is a one-layer network of kneuron-like\ractor units, each receiving the same feature vector, x(s), that the critic unit receives. Each actor unit j, j= 1,��,k,has its own weight vector, , but since the actor\runits are all identical, we describe just one of the units and omit the\rsubscript. One way for these units to follow the policy-gradient formulation in\rthe box above is for each to be a Bernoulli-logistic unitwith a REINFORCE policy-gradient learning rule. This\rmeans that the output of each actor unit is a random variable, A, taking value\r0 or 1. Think of value 1 as the neuron firing, that is, emitting an action\rpotential. The weighted sum, 0Tx(s), of a unit��s input vector determines the unit��s\raction probabilities via the exponential softmax distribution (13.2), which for\rtwo actions is the logistic function:\nn(1|s,0) = 1�� n(0|s, $= 1+ exp(��^Tx(s)).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (15.2)\nThe weights of\reach actor unit are updated by the rule in the box above: Q^�D d+ ¬5 e��, where 5 again corresponds to the dopamine signal:\rthe same reinforcement signal that is sent to all the critic unit��s synapses.\rFigure 15.6a shows 5 being broadcast to all the synapses of all the actor units\r(which makes this actor network a teamof\rreinforcement learning agents, something we discuss in Section 15.10 below).\rThe actor eligibility trace vector e9is a trace of Velog n(A|s, Q) for past states s. To understand this eligibility\rtrace refer to Exercise 13.7, which defines this kind of unit and asks you to\rgive the REINFORCE learning rule for it. That exercise asked you to express Ve\rlogn(A|s, Q) in terms of A, x(s), and n(A|s, Q) by calculating the gradient. The answer we were\rlooking for is:\nVen(A|s, Q) = (A �� n(A|s, Q))x(s).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (15.3)\nUnlike the non-contingent eligibility trace of a critic synapse that\ronly accumu\u0026shy;lates the presynaptic activity x(s), the eligibility trace of an actor unit��s\rsynapse in addition depends on the activity of the actor unit itself. We call\rthis a contingent eligibility tracebecause it is contingent on this postsynaptic\ractivity. The eligibility trace at each synapse continually decays, but\rincrements or decrements depending on the activity of the presynaptic neuron\rAND whether or not the postsynaptic neuron fires. The factor A�� n(A|s, d)in (15.3) is positive when A= 1and negative other\u0026shy;wise. The postsynaptic\rcontingency in the eligibility traces of actor units is the only difference\rbetween the critic and actor learning rules.By keeping information about what actions were taken in what\rstates, contingent eligibility traces allow credit for reward (positive 5), or\rblame for punishment (negative 5), to be apportioned among the policy\rparameters (the efficacies of the actor units�� synapses) according to the\rcontributions these parameters made to the units�� outputs that could have influ\u0026shy;enced\rlater values of 5. Contingent eligibility traces mark the synapses as to how\rthey should be modified to alter the units�� future responses to favor positive\rvalues of 5.\nWhat do the critic and actor learning rules suggest about how\refficacies of corti- costriatal synapses change? Both learning rules are\rrelated to Donald Hebb��s classic proposal that whenever a presynaptic signal\rparticipates in activating the postsy\u0026shy;naptic neuron, the synapse��s efficacy\rincreases (Hebb, 1949). The critic and actor learning rules share with Hebb��s\rproposal the idea that changes in a synapse��s ef\u0026shy;ficacy depend on the\rinteraction of several factors. In the critic learning rule the interaction is\rbetween the reinforcement signal 5 and eligibility traces that depend only on\rpresynaptic signals. Neuroscientists call this a two-factor\rlearning rulebe\u0026shy;cause the\rinteraction is between two signals or quantities. The actor learning rule, on\rthe other hand, is a three-factor learning rulebecause, in addition to depending on 5, its\religibility traces depend on both presynaptic and postsynaptic activity. Unlike\rHebb��s proposal, however, the relative timing of the factors is critical to how\rsynaptic efficacies change, with eligibility traces intervening to allow the\rreinforcement signal to affect synapses that were active in the recent past.\nSome subtleties about signal timing for the actor and critic\rlearning rules de\u0026shy;serve closer attention. In defining the neuron-like actor and\rcritic units, we ignored the small amount of time it takes synaptic input to\reffect the firing of a real neu\u0026shy;ron. When an action potential from the\rpresynaptic neuron arrives at a synapse, neurotransmitter molecules are\rreleased that diffuse across the synaptic cleft to the postsynaptic neuron,\rwhere they bind to receptors on the postsynaptic neuron��s sur\u0026shy;face; this\ractivates molecular machinery that causes the postsynaptic neuron to fire (or\rto inhibit its firing in the case of inhibitory synaptic input). This process\rcan take several tens of milliseconds. According to (15.1) and (15.2), though,\rthe in\u0026shy;put to a critic and actor unit instantaneously produces the unit��s\routput. Ignoring activation time like this is common in abstract models of\rHebbian-style plasticity in which synaptic efficacies change according to a\rsimple product of simultaneous pre- and postsynaptic activity. More realistic\rmodels must take activation time into account.\nActivation time is especially important for a more realistic actor\runit because it influences how contingent eligibility traces have to work in\rorder to properly\n\r\rapportion\rcredit for reinforcement to the appropriate synapses. The expression (A ��\rn(A|s, Q))x(s) defining contingent eligibility traces for the\ractor unit��s learning rule given above includes the postsynaptic factor (A ��\rn(A|s, Q))\rand the presynaptic factor x(s). This works because by ignoring activation time,\rthe presynaptic activity x(s) participates in causingthe postsynaptic activity appearing in (A �� n(A|s, Q)). To assign credit for reinforcement correctly,\rthe presynaptic factor defining the eli\u0026shy;gibility trace must be a cause of the\rpostsynaptic factor that also defines the trace. Contingent eligibility traces\rfor a more realistic actor unit would have to take ac\u0026shy;tivation time into\raccount. (Activation time should not be confused with the time required for a\rneuron to receive a reinforcement signal influenced by that neuron��s activity.\rThe function of eligibility traces is to span this time interval which is gen\u0026shy;erally\rmuch longer than the activation time. We discuss this further in the following\rsection.)\nThere are hints from neuroscience for how this process might work in\rthe brain. Neuroscientists have discovered a form of Hebbian plasticity called spike-timing-\rdependent plasticity(STDP) that\rlends plausibility to the existence of actor-like synaptic plasticity in the\rbrain. STDP is a Hebbian-style plasticity, but changes in a synapse��s efficacy\rdepend on the relative timing of presynaptic and postsynaptic action\rpotentials. The dependence can take different forms, but in the one most\rstudied, a synapse increases in strength if spikes incoming via that synapse\rarrive shortly before the postsynaptic neuron fires. If the timing relation is\rreversed, with a presynaptic spike arriving shortly after the postsynaptic\rneuron fires, then the strength of the synapse decreases. STDP is a type of\rHebbian plasticity that takes the activation time of a neuron into account,\rwhich is one of the ingredients needed for actor-like learning.\nThe discovery of STDP has led neuroscientists to investigate the\rpossibility of a three-factor form of STDP in which neuromodulatory input must\rfollow appropriately- timed pre- and postsynaptic spikes. This form of synaptic\rplasticity, called reward- modulated STDP,is much like the actor learning rule discussed\rhere. Synaptic changes that would be produced by regular STDP only occur if\rthere is neuromodu- latory input within a time window after a presynaptic spike\ris closely followed by a postsynaptic spike. Evidence is accumulating that\rreward-modulated STDP occurs at the spines of medium spiny neurons of the\rdorsal striatum, with dopamine pro\u0026shy;viding the neuromodulatory factor��the sites\rwhere actor learning takes place in the hypothetical neural implementation of\ran actor-critic algorithm illustrated in Fig\u0026shy;ure 15.6b. Experiments have\rdemonstrated reward-modulated STDP in which lasting changes in the efficacies\rof corticostriatal synapses occur only if a neuromodulatory pulse arrives\rwithin a time window that can last up to 10seconds after a presynaptic spike is closely\rfollowed by a postsynaptic spike (Yagishita et al. 2014). Although the evidence\ris indirect, these experiments point to the existence of contingent eligibility\rtraces having prolonged time courses. The molecular mechanisms producing these\rtraces, as well as the much shorter traces that likely underly STDP, are not\ryet un\u0026shy;derstood, but research focusing on time-dependent and\rneuromodulator-dependent synaptic plasticity is continuing.\nThe\rneuron-like actor unit that we have described here, with its Law-of-Effect-\rstyle learning rule, appeared in somewhat simpler form in the actor-critic\rnetwork of Barto et al. (1983). That network was inspired by the ��hedonistic\rneuron�� hypothesis proposed by physiologist A. H. Klopf (1972, 1982). Not all\rthe details of Klopf��s hypothesis are consistent with what has been learned\rabout synaptic plasticity, but the discovery of STDP and the growing evidence\rfor a reward-modulated form of STDP suggest that Klopf��s ideas may not have\rbeen far off the mark. We discuss Klopf��s hedonistic neuron hypothesis next.\n15.9\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rHedonistic\rNeurons\nIn his\rhedonistic neuron hypothesis, Klopf (1972, 1982) conjectured that individual\rneurons seek to maximize the difference between synaptic input treated as\rrewarding and synaptic input treated as punishing by adjusting the efficacies\rof their synapses on the basis of rewarding or punishing consequences of their\rown action potentials. In other words, individual neurons can be trained with\rresponse-contingent rein\u0026shy;forcement like an animal can be trained in an\rinstrumental conditioning task. His hypothesis included the idea that rewards\rand punishments are conveyed to a neuron via the same synaptic input that\rexcites or inhibits the neuron��s spike-generating ac\u0026shy;tivity. (Had Klopf known\rwhat we know today about neuromodulatory systems, he might have assigned the\rreinforcing role to neuromodulatory input, but he wanted to avoid any\rcentralized source of training information.) Synaptically-local traces of past\rpre- and postsynaptic activity had the key function in Klopf��s hypothesis of\rmaking synapses eligible��the term he introduced��for modification by later re\u0026shy;ward\ror punishment. He conjectured that these traces are implemented by molecular\rmechanisms local to each synapse and therefore different from the electrical\ractivity of both the pre- and the postsynaptic neurons. In the Bibliographical\rand Historical Remarks section of this chapter we bring attention to some\rsimilar proposals made by others.\nKlopf specifically conjectured that synaptic efficacies change in\rthe following way. When a neuron fires an action potential, all of its synapses\rthat were active in contributing to that action potential become eligible to\rundergo changes in their efficacies. If the action potential is followed within\ran appropriate time period by an increase of reward, the efficacies of all the\religible synapses increase. Symmetrically, if the action potential is followed\rwithin an appropriate time period by an increase of punishment, the efficacies\rof eligible synapses decrease. This is implemented by triggering an eligibility\rtrace at a synapse upon a coincidence of presynaptic and postsynaptic activity\r(or more exactly, upon pairing of presynaptic activity with the postsynaptic\ractivity that that presynaptic activity participates in causing)��what we call a\rcontingent eligibility trace. This is essentially the three-factor learning\rrule of an actor unit described in the previous section.\nThe shape and time course of an eligibility trace in Klopf��s theory\rreflects the dura\u0026shy;tions of the many feedback loops in which the neuron is\rembedded, some of which lie entirely within the brain and body of the organism,\rwhile others extend out through the organism��s external environment as mediated\rby its motor and sensory systems. His idea was that the shape of a synaptic\religibility trace is like a histogram of the durations of the feedback loops in\rwhich the neuron is embedded. The peak of an eligibility trace would then occur\rat the duration of the most prevalent feedback loops in which that neuron\rparticipates. The eligibility traces used by algorithms described in this book\rare simplified versions of Klopf��s original idea, being expo\u0026shy;nentially (or\rgeometrically) decreasing functions controlled by the parameters ��and Y. This simplifies simulations as well as\rtheory, but we regard these simple eligi\u0026shy;bility traces as a placeholders for\rtraces closer to Klopf��s original conception, which would have computational\radvantages in complex reinforcement learning systems by refining the\rcredit-assignment process.\nKlopf��s hedonistic neuron hypothesis is not as implausible as it may\rat first appear. A well-studied example of a single cell that seeks some stimuli\rand avoids others is the bacterium Escherichia coli. The movement of this single-cell organism is\rinfluenced by chemical stimuli in its environment, behavior known as\rchemotaxis. It swims in its liquid environment by rotating hairlike structures\rcalled flagella attached to its surface. (Yes, it rotates them!) Molecules in\rthe bacterium��s environment bind to receptors on its surface. Binding events\rmodulate the frequency with which the bacterium reverses flagellar rotation.\rEach reversal causes the bacterium to tumble in place and then head off in a\rrandom new direction. A little chemical memory and computation causes the\rfrequency of flagellar reversal to decrease when the bacterium swims toward\rhigher concentrations of molecules it needs to survive (attractants) and\rincrease when the bacterium swims toward higher concentrations of molecules\rthat are harmful (repellants). The result is that the bacterium tends to\rpersist in swimming up attractant gradients and tends to avoid swimming up\rrepellant gradients.\nThe chemotactic behavior just described is called klinokinesis. It\ris a kind of trial- and-error behavior, although it is unlikely that learning\ris involved: the bacterium needs a modicum of short-term memory to detect\rmolecular concentration gradients, but it probably does not maintain long-term\rmemories. Artificial intelligence pioneer Oliver Selfridge called this strategy\r��run and twiddle,�� pointing out its utility as a basic adaptive strategy: ��keep\rgoing in the same way if things are getting better, and otherwise move around����Selfridge, 1978, 1984). Similarly, one might think\rof a neuron ��swimming�� (not literally of course) in a medium composed of the\rcom\u0026shy;plex collection of feedback loops in which it is embedded, acting to obtain\rone type of input signal and to avoid others. Unlike the bacterium, however,\rthe neuron��s synaptic strengths retain information about its past\rtrial-and-error behavior. If this view of the behavior of a neuron (or just one\rtype of neuron) is plausible, then the closed-loop nature of how the neuron\rinteracts with its environment is important for understanding its behavior,\rwhere the neuron��s environment consists of the rest of the animal together with\rthe environment with which the animal as a whole interacts.\nKlopf��s hedonistic\rneuron hypothesis extended beyond the idea that individual neurons are\rreinforcement learning agents. He argued that many aspects of intelligent\rbehavior can be understood as the result of the collective behavior of a\rpopulation of self-interested hedonistic\rneurons interacting with one another in an immense society or economic system\rmaking up an animal��s nervous system. Whether or not this view of nervous\rsystems is useful, the collective behavior of reinforcement learning agents has\rimplications for neuroscience. We take up this subject next.\n15.10\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rCollective\rReinforcement Learning\nThe behavior of populations of\rreinforcement learning agents is deeply relevant to the study of social and\reconomic systems, and if anything like Klopf��s hedonistic neuron hypothesis is\rcorrect, to neuroscience as well. The hypothesis described above about how an\ractor-critic algorithm might be implemented in the brain only narrowly\raddresses the implications of the fact that the dorsal and ventral subdivisions\rof the striatum, the respective locations of the actor and the critic according\rto the hypothesis, each contain millions of medium spiny neurons whose synapses\rundergo change modulated by phasic bursts of dopamine neuron activity.\nThe actor in Figure 15.6a is a\rsingle-layer network of k actor units. The actions produced by this network are\rvectors (Ai, A2, �� �� �� , Ak)T presumed to drive the ani\u0026shy;mal��s behavior.\rChanges in the efficacies of the synapses of all of these units depend on the\rreinforcement signal 5. Because actor units attempt to make 5 as large as\rpossible, 5 effectively acts as a reward signal for them (so in this case\rreinforcement is the same as reward). Thus, each actor unit is itself a\rreinforcement learning agent�� a hedonistic neuron if you will. Now, to make the\rsituation as simple as possible, assume that each of these units receives the\rsame reward signal at the same time (although, as indicated above, the\rassumption that dopamine is released at all the corticostriatal synapses under\rthe same conditions and at the same times is likely an oversimplification).\nWhat can reinforcement\rlearning theory tell us about what happens when all mem\u0026shy;bers of a population of\rreinforcement learning agents learn according to a common reward signal? The\rfield of multi-agent reinforcement learningconsiders many as\u0026shy;pects of learning by populations of reinforcement\rlearning agents. Although this field is beyond the scope of this book, we\rbelieve that some of its basic concepts and re\u0026shy;sults are relevant to thinking\rabout the the brain��s diffuse neuromodulatory systems. In multi-agent\rreinforcement learning (and in game theory), the scenario in which all the\ragents try to maximize a common reward signal that they simultaneously receive\ris known as a cooperative gameor a team problem.\nWhat makes a team problem interesting\rand challenging is that the common re\u0026shy;ward signal sent to each agent evaluates\rthe patternof activity produced by\rthe entire population, that is, it evaluates the collective\ractionof the team members.\rThis means that any individual agent has only limited ability to affect the\rreward signal because any single agent contributes just one component of the\rcollective ac\u0026shy;tion evaluated by the common reward signal. Effective learning in\rthis scenario requires addressing a structural credit assignment problem: which team members, or groups of team members, deserve credit for\ra favorable reward signal, or blame for an \n\r\runfavorable\rreward signal? It is a cooperativegame, or a team problem, because the agents are united in seeking\rto increase the same reward signal: there are no conflicts of interest among\rthe agents. The scenario would be a competitive gameif different agents receive different reward\rsignals, where each reward signal again evaluates the collective action of the\rpopulation, and the objective of each agent is to increase its own reward\rsignal. In this case there might be conflicts of interest among the agents,\rmeaning that actions that are good for some agents are bad for others. Even\rdeciding what the best collective action should be is a non-trivial aspect of\rgame theory. This competitive setting might be relevant to neuroscience too\r(for example, to account for heterogeneity of dopamine neuron activity), but\rhere we focus only on the cooperative, or team, case.\nHow can each reinforcement learning agent in a team learn to ��do the\rright thing�� so that the collective action of the team is highly rewarded? An\rinteresting result is that if each agent can learn effectively despite its\rreward signal being corrupted by a large amount of noise, and despite its lack\rof access to complete state infor\u0026shy;mation, then the population as a whole will\rlearn to produce collective actions that improve as evaluated by the common\rreward signal, even when the agents cannot communicate with one another. Each\ragent faces its own reinforcement learning task in which its influence on the\rreward signal is deeply buried in the noise created by the influences of other\ragents. In fact, for any agent, all the other agents are part of its\renvironment because its input, both the part conveying state information and\rthe reward part, depends on how all the other agents are behaving. Furthermore,\rlacking access to the actions of the other agents, indeed lacking access to the\rparam\u0026shy;eters determining their policies, each agent can only partially observe\rthe state of its environment. This makes each team member��s learning task very\rdifficult, but if each uses a reinforcement learning algorithm able to increase\ra reward signal even under these difficult conditions, teams of reinforcement\rlearning agents can learn to produce collective actions that improve over time\ras evaluated by the team��s common reward signal.\nIf the team members are neuron-like units, then each unit has to\rhave the goal of increasing the amount of reward it receives over time, as the\ractor unit does that we described in Section 15.8. Each unit��s learning\ralgorithm has to have two essential features. First, it has to use contingent\religibility traces. Recall that a contingent eligibility trace, in neural\rterms, is initiated (or increased) at a synapse when its presynaptic input\rparticipates in causing the postsynaptic neuron to fire. A non\u0026shy;contingent\religibility trace, in contrast, is initiated or increased by presynaptic input\rindependently of what the postsynaptic neuron does. As explained in Section\r15.8, by keeping information about what actions were taken in what states,\rcontingent eligibility traces allow credit for reward, or blame for punishment,\rto be apportioned to an agent��s policy parameters according to the contribution\rthe values of these parameters made in determining the agent��s action. By\rsimilar reasoning, a team member must remember its recent action so that it can\reither increase or decrease the likelihood of producing that action according\rto the reward signal that is subse\u0026shy;quently received. The action component of a\rcontingent eligibility trace implements this action memory. Because of the\rcomplexity of the learning task, however, con\u0026shy;tingent eligibility is merely a\rpreliminary step in the credit assignment process: the relationship between a\rsingle team member��s action and changes in the team��s re\u0026shy;ward signal is a\rstatistical correlation that has to be estimated over many trials. Contingent\religibility is an essential but preliminary step in this process.\nLearning with non-contingent eligibility traces does not work at all\rin the team setting because it does not provide a way to correlate actions with\rconsequent changes in the reward signal. Non-contingent eligibility traces are\radequate for learning to predict, as the critic component of the actor-critic\ralgorithm does, but they do not support learning to control, as the actor\rcomponent must do. The members of a population of critic-like agents may still\rreceive a common reinforcement signal, but they would all learn to predict the\rsame quantity (which in the case of an actor-critic method, would be the\rexpected return for the current policy). How successful each member of the\rpopulation would be in learning to predict the expected return would depend on\rthe information it receives, which could be very different for different\rmembers of the population. There would be no need for the population to produce\rdifferentiated patterns of activity. This is not a team problem as defined\rhere.\nA second requirement for collective learning in a team problem is\rthat there has to be variability in the actions of the team members in order\rfor the team to explore the space of collective actions. The simplest way for a\rteam of reinforcement learning agents to do this is for each member to\rindependently explore its own action space through persistent variability in\rits output. This will cause the team as a whole to vary its collective actions.\rFor example, a team of the actor units described in Section 15.8 explores the\rspace of collective actions because the output of each unit, being a\rBernoulli-logistic unit, probabilistically depends on the weighted sum of its\rinput vector��s components. The weighted sum biases firing probability up or\rdown, but there is always variability. Because each unit uses a REINFORCE\rpolicy gradient algorithm (Chapter 13), each unit adjusts its weights with the\rgoal of maximizing the average reward rate it experiences while stochastically\rexploring its own action space. One can show, as Williams (1992) did, that a\rteam of Bernoulli-logistic REINFORCE units implements a policy gradient\ralgorithm as a wholewith respect to average rate of the team��s common reward signal,\rwhere the actions are the collective actions of the team.\nFurther, Williams (1992) showed that a team of Bernoulli-logistic\runits using RE\u0026shy;INFORCE ascends the average reward gradient when the units in\rthe team are interconnected to form a multilayer neural network. In this case,\rthe reward signal is broadcast to all the units in the network, though reward\rmay depend only on the collective actions of the network��s output units. This\rmeans that a multilayer team of Bernoulli-logistic REINFORCE units learns like\ra multilayer network trained by the widely-used error backpropagation method,\rbut in this case the backpropagation process is replaced by the broadcasted\rreward signal. In practice, the error backprop- agation method is considerably\rfaster, but the reinforcement learning team method is more plausible as a\rneural mechanism, especially in light of what is being learned about\rreward-modulated STDP as discussed in Section 15.8.\nExploration through independent exploration by team members is only\rthe sim\u0026shy;plest way for a team to explore; more sophisticated methods are\rpossible if the team members communicate with one another so that they can\rcoordinate their actions to focus on particular parts of the collective action\rspace. There are also mechanisms more sophisticated than contingent eligibility\rtraces for addressing structural credit assignment, which is easier in a team\rproblem when the set of possible collective actions is restricted in some way.\rAn extreme case is a winner-take-all arrangement (for example, the result of\rlateral inhibition in the brain) that restricts collective actions to those to\rwhich only one, or a few, team members contribute. In this case the winners get\rthe credit or blame for resulting reward or punishment.\nDetails of\rlearning in cooperative games (or team problems) and non-cooperative game\rproblems are beyond the scope of this book. The Bibliographical and Historical\rRemarks section at the end of this chapter cites a selection of the relevant\rpublica\u0026shy;tions, including extensive references to research on implications for\rneuroscience of collective reinforcement learning.\n15.11\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rModel-based\rMethods in the Brain\nReinforcement\rlearning��s distinction between model-free and model-based algorithms is proving\rto be useful for thinking about animal learning and decision processes. Section\r14.6 discusses how this distinction aligns with that between habitual and\rgoal-directed animal behavior. The hypothesis discussed above about how the\rbrain might implement an actor-critic algorithm is relevant only to an animal��s\rhabitual mode of behavior because the basic actor-critic method is model-free.\rWhat neural mechanisms are responsible for producing goal-directed behavior,\rand how do they interact with those underlying habitual behavior?\nOne way to investigate questions about the brain structures involved\rin these modes of behavior is to inactivate an area of a rat��s brain and then\robserve what the rat does in an outcome-devaluation experiment (Section 14.6).\rResults from experiments like these indicate that the actor-critic hypothesis\rdescribed above is too simple in placing the actor in the dorsal striatum.\rInactivating one part of the dorsal striatum, the dorsolateral striatum (DLS),\rimpairs habit learning, causing the animal to rely more on goal-directed\rprocesses. On the other hand, inactivating the dorsomedial striatum (DMS)\rimpairs goal-directed processes, requiring the animal to rely more on habit\rlearning. Results like these support the view that the DLS in rodents is more\rinvolved in model-free processes, whereas their DMS is more involved in\rmodel-based processes. Results of studies with human subjects in similar\rexperiments using functional neuroimaging, and with non-human primates, support\rthe view that the analogous structures in the primate brain are differentially\rinvolved in habitual and goal-directed modes of behavior.\nOther studies identify activity associated with model-based\rprocesses in the pre- frontal cortex of the human brain, the front-most part of\rthe frontal cortex impli\u0026shy;cated in executive function, including planning and\rdecision making. Specifically implicated is the orbitofrontal cortex (OFC), the\rpart of the prefrontal cortex imme\u0026shy;diately above the eyes. Functional\rneuroimaging in humans, and also recordings of the activities of single neurons\rin monkeys, reveals strong activity in the OFC related to the subjective reward\rvalue of biologically significant stimuli, as well as activity related to the\rreward expected as a consequence of actions. Although not free of controversy,\rthese results suggest significant involvement of the OFC in goal-directed\rchoice. It may be critical for the reward part of an animal��s environment\rmodel.\nAnother structure involved in model-based behavior is the\rhippocampus, a struc\u0026shy;ture critical for memory and spatial navigation. A rat��s\rhippocampus plays a critical role in the rat��s ability to navigate a maze in\rthe goal-directed manner that led Tolman to the idea that animals use models,\ror cognitive maps, in selecting actions (Section 14.5). The hippocampus may\ralso be a critical component of our human ability to imagine new experiences\r(Hassabis and Maguire, 2007; Olafsd6ttir, Barry, Saleem, Hassabis, and Spiers, 2105).\nThe findings that most directly implicate the hippocampus in\rplanning��the pro\u0026shy;cess needed to enlist an environment model in making\rdecisions��come from exper\u0026shy;iments that decode the activity of neurons in the\rhippocampus to determine what part of space hippocampal activity is\rrepresenting on a moment-to-moment basis. When a rat pauses at a choice point\rin a maze, the representation of space in the hippocampus sweeps forward (and not\rbackwards) along the possible paths the ani\u0026shy;mal can take from that point\r(Johnson and Redish, 2007). Furthermore, the spatial trajectories represented\rby these sweeps closely correspond to the rat��s subsequent navigational\rbehavior (Pfeiffer and Foster, 2013). These results suggest that the hip\u0026shy;pocampus\ris critical for the state-transition part of an animal��s environment model, and\rthat it is part of a system that uses the model to simulate possible future\rstate sequences to assess the consequences of possible courses of action: a\rform of planning.\nThe results described above add to a voluminous literature on neural\rmechanisms underlying goal-directed, or model-based, learning and decision\rmaking, but many questions remain unanswered. For example, how can areas as\rstructurally similar as the DLS and DMS be essential components of modes of\rlearning and behavior that are as different as model-free and model-based\ralgorithms? Are separate structures re\u0026shy;sponsible for (what we call) the\rtransition and reward components of an environment model? Is all planning\rconducted at decision time via simulations of possible future courses of action\ras the forward sweeping activity in the hippocampus suggests? In other words,\ris all planning something like a rollout algorithm (Section 8.10)? Or are\rmodels sometimes engaged in the background to refine or recompute value infor\u0026shy;mation\ras illustrated by the Dyna architecture (Section 8.2)? How does the brain\rarbitrate between the use of the habit and goal-directed systems? Is there, in\rfact, a clear separation between the neural substrates of these systems?\nThe evidence is not pointing to a positive answer to this last\rquestion. Summariz\u0026shy;ing the situation, Doll, Simon, and Daw (2012) wrote that\r��model-based influences appear ubiquitous more or less wherever the brain\rprocesses reward information,�� and this is true even in the regions thought to\rbe critical for model-free learning. This includes the dopamine signals\rthemselves, which can exhibit the influence of \n\r\rmodel-based information in addition to the reward prediction errors\rthought to be the basis of model-free processes.\nContinuing neuroscience research informed by reinforcement\rlearning��s model-free and model-based distinction has the potential to sharpen\rour understanding of ha\u0026shy;bitual and goal-directed processes in the brain. A\rbetter grasp of these neural mech\u0026shy;anisms may lead to algorithms combining\rmodel-free and model-based methods in ways that have not yet been explored in\rcomputational reinforcement learning.\n15.12\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rAddiction\nUnderstanding the\rneural basis of drug abuse is a high-priority goal of neuroscience with the\rpotential to produce new treatments for this serious public health problem. One\rview is that drug craving is the result of the same motivation and learning\rprocesses that lead us to seek natural rewarding experiences that serve our\rbiological needs. Addictive substances, by being intensely reinforcing,\reffectively co-opt our natural mechanisms of learning and decision making. This\ris plausible given that many��though not all��drugs of abuse increase levels of\rdopamine either directly or indirectly in regions around terminals of dopamine\rneuron axons in the striatum, a brain structure firmly implicated in normal\rreward-based learning (Section 15.7). But the self-destructive behavior\rassociated with drug addiction is not characteristic of normal learning. What\ris different about dopamine-mediated learning when the reward is the result of\ran addictive drug? Is addiction the result of normal learning in response to\rsubstances that were largely unavailable throughout our evolutionary history,\rso that evolution could not select against their damaging effects? Or do\raddictive substances somehow interfere with normal dopamine-mediated learning?\nThe reward\rprediction error hypothesis of dopamine neuron activity and its con\u0026shy;nection to\rTD learning are the basis of a model due to Redish (2004) of some��but certainly\rnot all��features of addiction. The model is based on the observation that\radministration of cocaine and some other addictive drugs produces a transient\rin\u0026shy;crease in dopamine. In the model, this dopamine surge is assumed to increase\rthe TD error, 5, in a way that cannot be cancelled out by changes in the value\rfunction. In other words, whereas 5 is reduced to the degree that a normal\rreward is pre\u0026shy;dicted by antecedent events (Section 15.6), the contribution to 5\rdue to an addictive stimulus does not decrease as the reward signal becomes\rpredicted: drug rewards cannot be ��predicted away.�� The model does this by\rpreventing 5 from ever becom\u0026shy;ing negative when the reward signal is due to an\raddictive drug, thus eliminating the error-correcting feature of TD learning\rfor states associated with administration of the drug. The result is that the\rvalues of these states increase without bound, making actions leading to these\rstates preferred above all others.\nAddictive behavior is much more complicated than this result from\rRedish��s model, but the model��s main idea may be a piece of the puzzle. Or the\rmodel might be mis\u0026shy;leading. Dopamine appears not to play a critical role in all\rforms of addiction, and not everyone is equally susceptible to developing\raddictive behavior. Moreover, the model does not include the changes in many\rcircuits and brain regions that accom\u0026shy;pany chronic drug taking, for example,\rchanges that lead to a drug��s diminishing effect with repeated use. It is also\rlikely that addiction involves model-based pro\u0026shy;cesses. Still, Redish��s model\rillustrates how reinforcement learning theory can be enlisted in the effort to\runderstand a major health problem. In a similar manner, reinforcement learning\rtheory has been influential in the development of the new field of\rcomputational psychiatry, which aims to improve understanding of mental\rdisorders through mathematical and computational methods.\n15.13\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rSummary\nThe neural\rpathways involved in the brain��s reward system are complex and incom\u0026shy;pletely\runderstood, but neuroscience research directed toward understanding these\rpathways and their roles in behavior is progressing rapidly. This research is\rreveal\u0026shy;ing striking correspondences between the brain��s reward system and the\rtheory of reinforcement learning as presented in this book.\nThe reward prediction error hypothesis\rof dopamine neuron activitywas\rproposed by scientists who recognized striking parallels between the behavior\rof TD errors and the activity of neurons that produce dopamine, a\rneurotransmitter essential in mammals for reward-related learning and behavior.\rExperiments conducted in the late 1980s and 1990s in the laboratory of\rneuroscientist Wolfram Schultz showed that dopamine neurons respond to\rrewarding events with substantial bursts of activity, called phasic responses,\ronly if the animal does not expect those events, suggesting that dopamine\rneurons are signaling reward prediction errors instead of reward itself.\rFurther, these experiments showed that as an animal learns to predict a\rrewarding event on the basis of preceding sensory cues, the phasic activity of\rdopamine neurons shifts to earlier predictive cues while decreasing to later\rpredictive cues. This parallels the backup action of the TD error as a\rreinforcement learning agent learns to predict reward.\nOther experimental results firmly establish that the phasic activity\rof dopamine neurons is a reinforcement signal for learning that reaches\rmultiple areas of the brain by means of profusely branching axons of dopamine\rproducing neurons. These results are consistent with the distinction we make\rbetween a reward signal, Rt, and a reinforcement signal, which is the TD error\r5t in most of the algorithms we present. Phasic responses of dopamine neurons\rare reinforcement signals, not reward signals.\nA prominent hypothesis is that the brain implements something like\ran actor-critic algorithm. Two structures in the brain (the dorsal and ventral\rsubdivisions of the striatum), both of which play critical roles in\rreward-based learning, may function respectively like an actor and a critic.\rThat the TD error is the reinforcement signal for both the actor and the critic\rfits well with the facts that dopamine neuron axons target both the dorsal and\rventral subdivisions of the striatum; that dopamine appears to be critical for\rmodulating synaptic plasticity in both structures; and that the effect on a\rtarget structure of a neuromodulator such as dopamine depends on\nproperties of the target structure and not just on properties of the\rneuromodulator.\nThe actor and the critic can be implemented by artificial neural\rnetworks consist\u0026shy;ing of neuron-like units having learning rules based on the\rpolicy-gradient actor-critic method described in Section 13.5. Each connection\rin these networks is like a synapse between neurons in the brain, and the\rlearning rules correspond to rules governing how synaptic efficacies change as\rfunctions of the activities of the presynaptic and the postsynaptic neurons,\rtogether with neuromodulatory input corresponding to input from dopamine\rneurons. In this setting, each synapse has its own eligibility trace that\rrecords past activity involving that synapse. The only difference between the\ractor and critic learning rules is that they use different kinds of eligibility\rtraces: the critic unit��s traces are non-contingentbecause they do not involve the critic unit��s output, whereas the\ractor unit��s traces are contingentbecause in addition to the actor unit��s input, they\rdepend on the the actor unit��s output. In the hypothetical implementation of an\ractor-critic system in the brain, these learning rules respec\u0026shy;tively correspond\rto rules governing plasticity of corticostriatal synapses that convey signals\rfrom the cortex to the principal neurons in the dorsal and ventral striatal\rsubdivisions, synapses that also receive inputs from dopamine neurons.\nThe learning rule of an actor unit in the actor-critic network\rclosely corresponds to reward-modulated\rspike-timing-dependent plasticity.In spike-timing-dependent plas\u0026shy;ticity (STDP), the\rrelative timing of pre- and postsynaptic activity determines the direction of\rsynaptic change. In reward-modulated STDP, changes in synapses in addition\rdepend on a neuromodulator, such as dopamine, arriving within a time window\rthat can last up to 10 seconds after the conditions for STDP are met. Evi\u0026shy;dence\raccumulating that reward-modulated STDP occurs at corticostriatal synapses,\rwhere the actor��s learning takes place in the hypothetical neural\rimplementation of an actor-critic system, adds to the plausibility of the\rhypothesis that something like an actor-critic system exists in the brains of\rsome animals.\nThe idea of synaptic eligibility and basic features of the actor\rlearning rule de\u0026shy;rive from Klopf��s hypothesis of the ��hedonistic neuron��\r(Klopf, 1972, 1981). He conjectured that individual neurons seek to obtain\rreward and to avoid punishment by adjusting the efficacies of their synapses on\rthe basis of rewarding or punishing consequences of their action potentials. A\rneuron��s activity can affect its later input because the neuron is embedded in\rmany feedback loops, some within the animal��s nervous system and body and\rothers passing through the animal��s external environ\u0026shy;ment. Klopf��s idea of\religibility is that synapses are temporarily marked as eligible for\rmodification if they participated in the neuron��s firing (making this the\rcontin\u0026shy;gent form of eligibility trace). A synapse��s efficacy is modified if a\rreinforcing signal arrives while the synapse is eligible. We alluded to the\rchemotactic behavior of a bacterium as an example of a single cell that directs\rits movements in order to seek some molecules and to avoid others.\nA conspicuous feature of the dopamine system is that fibers\rreleasing dopamine project widely to multiple parts of the brain. Although it\ris likely that only some populations of dopamine neurons broadcast the same\rreinforcement signal, if this signal reaches the synapses of many neurons\rinvolved in actor-type learning, then the situation can be modeled as a team\rproblem.In this type of\rproblem, each agent in a collection of reinforcement learning agents receives\rthe same reinforcement signal, where that signal depends on the activities of\rall members of the collection, or team. If each team member uses a sufficiently\rcapable learning algorithm, the team can learn collectively to improve\rperformance of the entire team as evaluated by the globally-broadcast\rreinforcement signal, even if the team members do not directly communicate with\rone another. This is consistent with the wide dispersion of dopamine signals in\rthe brain and provides a neurally plausible alternative to the widely-used\rerror-backpropagation method for training multilayer networks.\nThe distinction between model-free and model-based reinforcement\rlearning is helping neuroscientists investigate the neural bases of habitual\rand goal-directed learning and decision making. Research so far points to their\rbeing some brain re\u0026shy;gions more involved in one type of process than the other,\rbut the picture remains unclear because model-free and model-based processes do\rnot appear to be neatly separated in the brain. Many questions remain\runanswered. Perhaps most intriguing is evidence that the hippocampus, a\rstructure traditionally associated with spatial navigation and memory, appears\rto be involved in simulating possible future courses of action as part of an\ranimal��s decision-making process. This suggests that it is part of a system\rthat uses an environment model for planning.\nReinforcement learning theory is also influencing thinking about\rneural processes underlying drug abuse. A model of some features of drug\raddiction is based on the reward prediction error hypothesis. It proposes that\ran addicting stimulant, such as cocaine, destabilizes TD learning to produce\runbounded growth in the values of actions associated with drug intake. This is\rfar from a complete model of addiction, but it illustrates how a computational\rperspective suggests theories that can be tested with further research. The new\rfield of computational psychiatry similarly focuses on the use of computational\rmodels, some derived from reinforcement learning, to better understand mental\rdisorders.\nThis chapter only touched the surface of how the neuroscience of\rreinforcement learning and the development of reinforcement learning in\rcomputer science and engineering have influenced one another. Most features of\rreinforcement learning algorithms owe their design to purely computational\rconsiderations, but some have been influenced by hypotheses about neural\rlearning mechanisms. Remarkably, as experimental data has accumulated about the\rbrain��s reward processes, many of the purely computationally-motivated features\rof reinforcement learning algorithms are turning out to be consistent with\rneuroscience data. Other features of compu\u0026shy;tational reinforcement learning,\rsuch eligibility traces and the ability of teams of reinforcement learning\ragents to learn to act collectively under the influence of a globally-broadcast\rreinforcement signal, may also turn out to parallel experimental data as\rneuroscientists continue to unravel the neural basis of reward-based animal\rlearning and behavior.\n\r\rBibliographical and Historical Remarks\nThe number of\rpublications treating parallels between the neuroscience of learning and\rdecision making and the approach to reinforcement learning presented in this\rbook is enormous. We can cite only a small selection. Niv (2009), Dayan and Niv\n(2008)\u0026nbsp;\u0026nbsp;\r, Gimcher\r(2011), Ludvig, Bellemare, and Pearson (2011), and Shah (2012) are good places\rto start.\nTogether with\reconomics, evolutionary biology, and mathematical psychology, re\u0026shy;inforcement\rlearning theory is helping to formulate quantitative models of the neural\rmechanisms of choice in humans and non-human primates. With its focus on learn\u0026shy;ing,\rthis chapter only lightly touches upon the neuroscience of decision making.\rGlimcher (2003) introduced the field of ��neuroeconomics,�� in which\rreinforcement learning contributes to the study of the neural basis of decision\rmaking from an eco\u0026shy;nomics perspective. See also Glimcher and Fehr (2013). The\rtext on computational and mathematical modeling in neuroscience by Dayan and\rAbbott (2001) includes reinforcement learning��s role in these approaches.\rSterling and Laughlin (2015) ex\u0026shy;amined the neural basis of learning in terms of\rgeneral design principles that enable efficient adaptive behavior.\n15.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; There are many good expositions of basic\rneuroscience. Kandel, Schwartz, Jessell, Siegelbaum, and Hudspeth (2013) is an\rauthoritative and very com\u0026shy;prehensive source.\n15.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Berridge and Kringelbach (2008) reviewed the neural\rbasis of reward and pleasure, pointing out that reward processing has many\rdimensions and in\u0026shy;volves many neural systems. Space prevents discussion of the\rinfluential research of Berridge and Robinson (1998), who distinguish between\rthe he\u0026shy;donic impact of a stimulus, which they call ��liking,�� and the\rmotivational effect, which they call ��wanting.�� Hare, O��Doherty, Camerer,\rSchultz, and Rangel (2008) examined the neural basis of value-related signals\rfrom an eco\u0026shy;nomic perspective, distinguishing between goal values, decision\rvalues, and prediction errors. Decision value is goal value minus action cost.\rSee also Rangel, Camerer, and Montague (2008), Rangel and Hare (2010), and\rPeters and Biichel (2010).\n15.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; The reward prediction error hypothesis of dopamine\rneuron activity is most prominently discussed by Schultz, Montague, and Dayan\r(1997). The hy\u0026shy;pothesis was first explicitly put forward by Montague, Dayan,\rand Sejnowski (1996). As they stated the hypothesis, it referred to reward\rprediction errors (RPEs) but not specifically to TD errors; however, their\rdevelopment of the hypothesis made it clear that they were referring to TD\rerrors. The earliest recognition of the TD-error/dopamine connection of which\rwe are aware is that of Montague, Dayan, Nowlan, Pouget, and Sejnowski (1992),\rwho pro\u0026shy;posed a TD-error-modulated Hebbian learning rule motivated by results\ron dopamine signaling from Schultz��s group. The connection was also pointed out\rin an abstract by Quartz, Dayan, Montague, and Sejnowski (1992). Mon\u0026shy;tague and\rSejnowski (1994) emphasized the importance of prediction in the brain and\routlined how predictive Hebbian learning modulated by TD er\u0026shy;rors could be\rimplemented via a diffuse neuromodulatory system, such as the dopamine system.\rFriston, Tononi, Reeke, Sporns, and Edelman (1994) presented a model of\rvalue-dependent learning in the brain in which synaptic changes are mediated by\ra TD-like error provided by a global neuromodula- tory signal (although they\rdid not single out dopamine). Montague, Dayan, Person, and Sejnowski (1995)\rpresented a model of honeybee foraging using the TD error. The model is based\ron research by Hammer, Menzel, and colleagues (Hammer and Menzel, 1995; Hammer,\r1997) showing that the neuromodulator octopamine acts as a reinforcement signal\rin the honeybee. Montague et al. (1995) pointed out that dopamine likely plays\ra similar role in the vertebrate brain. Barto (1995) related the actor-critic\rarchitecture to basal-ganglionic circuits and discussed the relationship\rbetween TD learn\u0026shy;ing and the main results from Schultz��s group. Houk, Adams,\rand Barto (1995) suggested how TD learning and the actor-critic architecture\rmight map onto the anatomy, physiology, and molecular mechanism of the basal\rganglia. Doya and Sejnowski (1998) extended their earlier paper on a model of\rbirdsong learning (Doya and Sejnowski, 1994) by including a TD-like er\u0026shy;ror\ridentified with dopamine to reinforce the selection of auditory input to be\rmemorized. O��Reilly and Frank (2006) and O��Reilly, Frank, Hazy, and Watz (2007)\rargued that phasic dopamine signals are RPEs but not TD er\u0026shy;rors. In support of\rtheir theory they cited results with variable interstimulus intervals that do\rnot match predictions of a simple TD model, as well as the observation that\rhigher-order conditioning beyond second-order condi\u0026shy;tioning is rarely observed,\rwhile TD learning is not so limited. Dayan and Niv (2008) discussed ��the good,\rthe bad, and the ugly�� of how reinforcement learning theory and the reward\rprediction error hypothesis align with exper\u0026shy;imental data. Glimcher (2011)\rreviewed the empirical findings that support the reward prediction error\rhypothesis and emphasized the significance of the hypothesis for contemporary\rneuroscience.\n15.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rGraybiel (2000)\ris a brief primer on the basal ganglia. The experiments mentioned that involve\roptogenetic activation of dopamine neurons were con\u0026shy;ducted by Tsai, Zhang,\rAdamantidis, Stuber, Bonci, de Lecea, and Deisseroth (2009), Steinberg,\rKeiflin, Boivin, Witten, Deisseroth, and Janak (2013), and Claridge-Chang,\rRoorda, Vrontou, Sjulson, Li, Hirsh, and Miesenbock\n(2009)\u0026nbsp;\u0026nbsp; . Fiorillo, Yun, and Song (2013), Lammel, Lim, and\rMalenka (2014), and Saddoris, Cacciapaglia, Wightmman, and Carelli (2015) are\ramong stud\u0026shy;ies showing that the signaling properties of dopamine neurons are\rspecialized for different target regions. RPE-signaling neurons may belong to\rone among multiple populations of dopamine neurons having different targets and\rsub\u0026shy;serving different functions. Eshel, Tian, Bukwich, and Uchida (2016) found\rhomogeneity of reward prediction error responses of dopamine neurons in the\rlateral VTA during classical conditioning in mice, tough their results do not\rrule out response diversity across wider areas. Gershman, Pesaran, and Daw\r(2009) studied reinforcement learning tasks that can be decomposed into\rindependent components with separate reward signals, finding evidence in human\rneuroimaging data suggesting that the brain exploits this kind of structure.\n15.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Schultz��s 1998 survey article (Schultz, 1998) is a\rgood entree into the very extensive literature on reward predicting signaling\rof dopamine neurons. Berns, McClure, Pagnoni, and Montague (2001), Breiter,\rAharon, Kahne- man, Dale, and Shizgal (2001), Pagnoni, Zink, Montague, and\rBerns (2002), and O��Doherty, Dayan, Friston, Critchley, and Dolan (2003)\rdescribed func\u0026shy;tional brain imaging studies supporting the existence of signals\rlike TD errors in the human brain.\n15.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThis section\rroughly follows Barto (1995) in explaining how TD errors mimic the main results\rfrom Schultz��s group on the phasic responses of dopamine neurons.\n15.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThis section is\rlargely based on Takahashi, Schoenbaum, and Niv (2008) and Niv (2009). To the\rbest of our knowledge, Barto (1995) and Houk, Adams, and Barto (1995) first\rspeculated about possible implementations of actor- critic algorithms in the\rbasal ganglia. On the basis of functional magnetic resonance imaging of human\rsubjects while engaged in instrumental condi\u0026shy;tioning, O��Doherty, Dayan,\rSchultz, Deichmann, Friston, and Dolan (2004) suggested that the actor and the\rcritic are most likely located respectively in the dorsal and ventral striatum.\rGershman, Moustafa, and Ludvig (2013) focused on how time is represented in\rreinforcement learning models of the basal ganglia, discussing evidence for,\rand implications of, various computa\u0026shy;tional approaches to time representation.\nThe hypothetical neural\rimplementation of the actor-critic architecture de\u0026shy;scribed in this section\rincludes very little detail about known basal ganglia anatomy and physiology.\rIn addition to the more detailed hypothesis of Houk, Adams, and Barto (1995), a\rnumber of other hypotheses include more specific connections to anatomy and\rphysiology and are claimed to explain additional data. These include hypotheses\rproposed by Suri and Schultz (1998, 1999), Brown, Bullock, and Grossberg\r(1999), Contreras-Vidal and Schultz (1999), Suri, Bargas, and Arbib (2001),\rO��Reilly and Frank (2006), and O��Reilly, Frank, Hazy, and Watz (2007). Joel,\rNiv, and Ruppin (2002) critically evaluated the anatomical plausibility of\rseveral of these models and present an alternative intended to accommodate some\rneglected features of basal ganglionic circuitry.\n15.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; The actor learning rule discussed here is more\rcomplicated than the one in the early actor-critic network of Barto et al.\r(1983). Actor-unit eligi\u0026shy;bility traces in that network were traces of just A x\rx(s) instead of the full (A �� n(A|S, w))x(s). That work did not benefit from\rthe policy-gradient the\u0026shy;ory presented in Chapter 13 or the contributions of\rWilliams (1986, 1992), who showed how an artificial neural network of\rBernoulli-logistic units could implement a policy-gradient method.\nReynolds and\rWickens (2002) proposed a three-factor rule for synaptic plas\u0026shy;ticity in the\rcorticostriatal pathway in which dopamine modulates changes in corticostriatal\rsynaptic efficacy. They discussed the experimental support for this kind of\rlearning rule and its possible molecular basis. The definitive demonstration of\rspike-timing-dependent plasticity (STDP) is attributed to Markram, Liibke,\rFrotscher, and Sakmann (1997), with evidence from earlier experiments by Levy\rand Steward (1983) and others that the relative timing of pre- and postsynaptic\rspikes is critical for inducing changes in synaptic effi\u0026shy;cacy. Rao and\rSejnowski (2001) suggested how STDP could be the result of a TD-like mechanism\rat synapses with non-contingent eligibility traces lasting about 10\rmilliseconds. Dayan (2002) commented that this would require an error as in\rSutton and Barto��s (1981) early model of classical conditioning and not a true\rTD error. Representative publications from the extensive litera\u0026shy;ture on\rreward-modulated STDP are Wickens (1990), Reynolds and Wickens (2002), and\rCalabresi, Picconi, Tozzi and Di Filippo (2007). Pawlak and Kerr (2008) showed\rthat dopamine is necessary to induce STDP at the corticos- triatal synapses of\rmedium spiny neurons. See also Pawlak, Wickens, Kirk\u0026shy;wood, and Kerr (2010).\rYagishita, Hayashi-Takagi, Ellis-Davies, Urakubo, Ishii, and Kasai (2014) found\rthat dopamine promotes spine enlargement of the medium spiny neurons of mice\ronly during a time window of from 0.3 to\n2seconds after STDP stimulation. Izhikevich (2007)\rproposed and explored the idea of using STDP timing conditions to trigger\rcontingent eligibility traces.\n15.9\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Klopf��s hedonistic neuron hypothesis (Klopf 1972,\r1982) inspired our actor- critic algorithm implemented as an artificial neural\rnetwork with a single neuron-like unit, called the actor unit, implementing a\rLaw-of-Effect-like learning rule (Barto, Sutton, and Anderson, 1983). Ideas\rrelated to Klopf��s synaptically-local eligibility have been proposed by others.\rCrow (1968) pro\u0026shy;posed that changes in the synapses of cortical neurons are\rsensitive to the consequences of neural activity. Emphasizing the need to\raddress the time delay between neural activity and its consequences in a\rreward-modulated form of synaptic plasticity, he proposed a contingent form of\religibility, but associated with entire neurons instead of individual synapses.\rAccording to his hypothesis, a wave of neuronal activity\nleads to a\rshort-term change in the cells involved in the wave such that they are picked\rout from a background of cells not so activated.\n... such cells\rare rendered sensitive by the short-term change to a reward signal ... in such\ra way that if such a signal occurs before the end of the decay time of the\rchange the synaptic connexions\n\r\rbetween the cells are made more\reffective. (Crow, 1968)\nCrow argued against previous\rproposals that reverberating neural circuits play this role by pointing out\rthat the effect of a reward signal on such a cir\u0026shy;cuit would ��...establish the\rsynaptic connexions leading to the reverberation (that is to say, those\rinvolved in activity at the time of the reward signal) and not those on the\rpath which led to the adaptive motor output.�� Crow fur\u0026shy;ther postulated that\rreward signals are delivered via a ��distinct neural fiber system,�� presumably\rthe one into which Olds and Milner (1954) tapped, that would transform synaptic\rconnections ��from a short into a long-term form.��\nIn\ranother farsighted hypothesis, Miller (1981) proposed a Law-of-Effect-like\rlearning rule that includes synaptically-local contingent eligibility traces:\n... it is\renvisaged that in a particular sensory situation neurone B, by chance, fires a\r��meaningful burst�� of activity, which is then trans\u0026shy;lated into motor acts,\rwhich then change the situation. It must be supposed that the meaningful burst\rhas an influence, at the neu\u0026shy;ronal level, on all of its own synapses which are active at the\rtime ... thereby making a preliminary selection of the synapses to be\rstrengthened, though not yet actually strengthening them. ...The strengthening\rsignal ... makes the final selection ... and accom\u0026shy;plishes the definitive\rchange in the appropriate synapses. (Miller,\n1981, p. 81)\nMiller��s hypothesis also included\ra critic-like mechanism, which he called a ��sensory analyzer unit,�� that worked\raccording to classical conditioning principles to provide reinforcement signals\rto neurons so that they would learn to move from lower- to higher-valued\rstates, thus anticipating the use of the TD error as a reinforcement signal in\rthe actor-critic architecture. Miller��s idea not only parallels Klopf��s (with\rthe exception of its explicit invocation of a distinct ��strengthening signal��),\rit also anticipated the general features of reward-modulated STDP.\nA related though different idea,\rwhich Seung (2003) called the ��hedonistic synapse,�� is that synapses\rindividually adjust the probability that they re\u0026shy;lease neurotransmitter in the\rmanner of the Law of Effect: if reward follows release, the release probability\rincreases, and decreases if reward follows fail\u0026shy;ure to release. This is\ressentially the same as the learning scheme Minsky used in his 1954 Princeton\rPh.D. dissertation (Minsky, 1954), where he called the synapse-like learning\relement a SNARC (Stochastic Neural-Analog Re\u0026shy;inforcement Calculator).\rContingent eligibility is involved in these ideas too, although it is\rcontingent on the activity of an individual synapse instead of the postsynaptic\rneuron.\nFrey and\rMorris (1997) proposed the idea of a ��synaptic tag�� for the induction of\rlong-lasting strengthening of synaptic efficacy. Though not unlike Klopf��s\religibility, their tag was hypothesized to consist of a temporary strengthening\nof a synapse\rthat could be transformed into a long-lasting strengthening by subsequent\rneuron activation. The model of O��Reilly and Frank (2006) and O��Reilly, Frank,\rHazy, and Watz (2007) uses working memory to bridge temporal intervals instead\rof eligibility traces. Wickens and Kotter (1995) discuss possible mechanisms\rfor synaptic eligibility. He, Huertas, Hong, Tie, Hell, Shouval, Kirkwood\r(2015) provide evidence supporting the existence of contingent eligibility\rtraces in synapses of cortical neurons with time courses like those of the\religibility traces Klopf postulated.\nThe metaphor of a neuron using a\rlearning rule related to bacterial chemo- taxis was discussed by Barto (1989).\rKoshland��s extensive study of bacterial chemotaxis was in part motivated by\rsimilarities between features of bacteria and features of neurons (Koshland,\r1980). See also Berg (1975). Shiman- sky (2009) proposed a synaptic learning\rrule somewhat similar to Seung��s mentioned above in which each synapse\rindividually acts like a chemotactic bacterium. In this case a collection of\rsynapses ��swims�� toward attractants in the high-dimensional space of synaptic\rweight values. Montague, Dayan, Person, and Sejnowski (1995) proposed a chemotactic-like\rmodel of the bee��s foraging behavior involving the neuromodulator octopamine.\n15.10\u0026nbsp;\u0026nbsp;\rResearch on the\rbehavior of reinforcement learning agents in team and game problems has a long\rhistory roughly occurring in three phases. To the best or our knowledge, the\rfirst phase began with investigations by the Russian math\u0026shy;ematician and\rphysicist M. L. Tsetlin. A collection of his work was published as Tsetlin\r(1973) after his death in 1966. Our Sections 1.7 and 4.8 refer to his study of\rlearning automata in connection to bandit problems. The Tsetlin collection also\rincludes studies of learning automata in team and game prob\u0026shy;lems, which led to\rlater work in this area using stochastic learning automata as described by\rNarendra and Thathachar (1974), Viswanathan and Narendra (1974), Lakshmivarahan\rand Narendra (1982), Narendra and Wheeler (1983), Narendra (1989), and\rThathachar and Sastry (2002). Thathachar and Sastry\n(2011)is a more\rrecent comprehensive account. These studies were mostly restricted to\rnon-associative learning automata, meaning that they did not address\rassociative, or contextual, bandit problems (Section 2.9).\nThe second\rphase began with the extension of learning automata to the associative, or\rcontextual, case. Barto, Sutton, and Brouwer (1981) and Barto and Sutton (1981)\rexperimented with associative stochastic learning automata in single-layer\rartificial neural networks to which a global reinforce\u0026shy;ment signal was\rbroadcast. They called neuron-like elements implementing this kind of learning associative\rsearch elements(ASEs). Barto\rand Anan- dan (1985) introduced a more sophisticated associative reinforcement\rlearn\u0026shy;ing algorithm called the associative\rreward-penalty(Ar-p) algorithm. They\rproved a convergence result by combining theory of stochastic learning au\u0026shy;tomata\rwith theory of pattern classification. Barto (1985, 1986) and Barto and Jordan\r(1987) described results with teams of Ar_p units connected into multi-layer neural networks, showing that they\rcould learn nonlinear\nfunctions, such as XOR and\rothers, with a globally-broadcast reinforcement signal. Barto (1985)\rextensively discussed this approach to artificial neural networks and how this\rtype of learning rule is related to others in the litera\u0026shy;ture at that time.\rWilliams (1992) mathematically analyzed and broadened this class of learning\rrules and related their use to the error backpropagation method for training\rmultilayer artificial neural networks. Williams (1988) de\u0026shy;scribed several ways\rthat backpropagation and reinforcement learning can be combined for training\rartificial neural networks. Williams (1992) showed that a special case of the Ar-palgorithm is a REINFORCE algorithm, although better results were\robtained with the general Ar-palgorithm (Barto,1985).\nThe third phase of interest in\rteams of reinforcement learning agents was influenced by increased\runderstanding of the role of dopamine as a widely broadcast neuromodulator and\rspeculation about the existence of reward- modulated STDP. Much more so than\rearlier research, this research considers details of synaptic plasticity and\rother constraints from neuroscience. Pub\u0026shy;lications include the following\r(chronologically and alphabetically): Bartlett and Baxter (1999, 2000), Xie and\rSeung (2004), Baras and Meir (2007), Far- ries and Fairhall (2007), Florian\r(2007), Izhikevich (2007), Pecevski, Maass, and Legenstein (2007), Legenstein,\rPecevski, and Maass (2008), Kolodziejski, Porr, and Worgotter (2009), Urbanczik\rand Senn (2009), and Vasilaki, Fremaux, Urbanczik, Senn, and Gerstner (2009).\rNowe, Vrancx, and De Hauwere (2012) reviewed more recent developments in the wider field of\rmulti-agent reinforcement learning\n15.11\u0026nbsp;\u0026nbsp;\rYin and\rKnowlton (2006) reviewed findings from outcome-devaluation ex\u0026shy;periments with\rrodents supporting the view that habitual and goal-directed behavior (as\rpsychologists use the phrase) are respectively most associated with processing\rin the dorsolateral striatum (DLS) and the dorsomedial stria\u0026shy;tum (DMS). Results\rof functional imaging experiments with human subjects in the outcome-devaluation\rsetting by Valentin, Dickinson, and O��Doherty (2007) suggest that the\rorbitofrontal cortex (OFC) is an important compo\u0026shy;nent of goal-directed choice.\rSingle unit recordings in monkeys by Padoa- Schioppa and Assad (2006) support\rthe role of the OFC in encoding values guiding choice behavior. Rangel,\rCamerer, and Montague (2008) and Rangel and Hare (2010) reviewed findings from\rthe perspective of neuroeconomics about how the brain makes goal-directed\rdecisions. Pezzulo, van der Meer, Lansink, and Pennartz (2014) reviewed the\rneuroscience of internally gen\u0026shy;erated sequences and presented a model of how\rthese mechanisms might be components of model-based planning. Daw and Shohamy\r(2008) proposed that while dopamine signaling connects well to habitual, or\rmodel-free, be\u0026shy;havior, other processes are involved in goal-directed, or\rmodel-based, behav\u0026shy;ior. Data from experiments by Bromberg-Martin, Matsumoto,\rHong, and Hikosaka (2010) indicate that dopamine signals contain information perti\u0026shy;nent to\rboth habitual and goal-directed behavior. Doll, Simon, and Daw\n(2012)argued that\rthere may not a clear separation in the brain between\nmechanisms\rthat subserve habitual and goal-directed learning and choice.\n\r\r\r15.12\n\r\r\r\r\rKeiflin and Janak (2015) reviewed connections between TD errors and addic\u0026shy;tion. Nutt,\rLingford-Hughes, Erritzoe, and Stokes (2015) critically evaluated the\rhypothesis that addiction is due to a disorder of the dopamine system.\rMontague, Dolan, Friston, and Dayan (2012) outlined the goals and early ef\u0026shy;forts\rin the field of computational psychiatry, and Adams, Huys, and Roiser (2015) reviewed\rmore recent progress.\n\r\rChapter 16\nApplications and Case Studies\nIn this final chapter we present\ra few case studies of reinforcement learning. Several of these are substantial\rapplications of potential economic significance. One, Samuel��s checkers player,\ris primarily of historical interest. Our presentations are intended to\rillustrate some of the trade-offs and issues that arise in real applications.\rFor example, we emphasize how domain knowledge is incorporated into the\rformulation and solution of the problem. We also highlight the representation\rissues that are so often critical to successful applications. The algorithms\rused in some of these case studies are substantially more complex than those we\rhave presented in the rest of the book. Applications of reinforcement learning\rare still far from routine and typically require as much art as science. Making\rapplications easier and more straightforward is one of the goals of current\rresearch in reinforcement learning.\n16.1\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rTD-Gammon\nOne of the\rmost impressive applications of reinforcement learning to date is that by\rGerald Tesauro to the game of backgammon (Tesauro, 1992, 1994, 1995, 2002).\rTesauro��s program, TD-Gammon,required little backgammon knowledge, yet learned\rto play extremely well, near the level of the world��s strongest grandmasters.\rThe learning algorithm in TD-Gammon was a straightforward combination of the\rTD(A) algorithm and nonlinear function approximation using a multilayer neural\rnetwork trained by backpropagating TD errors.\nBackgammon is a major game in the sense that it is played throughout\rthe world, with numerous tournaments and regular world championship matches. It\ris in part a game of chance, and it is a popular vehicle for waging significant\rsums of money. There are probably more professional backgammon players than\rthere are profes\u0026shy;sional chess players. The game is played with 15 white and 15\rblack pieces on a board of 24 locations, called points. Figure 16.1 shows a typical position early in the\rgame, seen from the perspective of the white player.\nIn this figure, white has just rolled the dice and obtained a 5 and\ra 2. This means that he can move one of his pieces 5 steps and one (possibly\rthe same piece) 2 steps.\n\r\n\r\r\r\r\r\u0026nbsp;\nFor example,\rhe could move two pieces from the 12 point,\rone to the 17 point, and one to the 14 point. White��s\robjective is to advance all of his pieces into the last quadrant (points 19-24)\rand then off the board. The first player to remove all his pieces wins. One\rcomplication is that the pieces interact as they pass each other going in\rdifferent directions. For example, if it were black��s move in Figure 16.1, he\rcould use the dice roll of 2 to move a piece from the 24 point to the 22 point,\r��hitting�� the white piece there. Pieces that have been hit are placed on the ��bar��\rin the middle of the board (where we already see one previously hit black\rpiece), from whence they reenter the race from the start. However, if there are\rtwo pieces on a point, then the opponent cannot move to that point; the pieces\rare protected from being hit. Thus, white cannot use his 5-2 dice roll to move\reither of his pieces on the 1point, because their possible resulting points are occupied by\rgroups of black pieces. Forming contiguous blocks of occupied points to block\rthe opponent is one of the elementary strategies of the game.\nBackgammon\rinvolves several further complications, but the above description gives the\rbasic idea. With 30 pieces and 24 possible locations (26, counting the bar and\roff-the-board) it should be clear that the number of possible backgammon\rpositions is enormous, far more than the number of memory elements one could\rhave in any physically realizable computer. The number of moves possible from\reach position is also large. For a typical dice roll there might be 20\rdifferent ways of playing. In considering future moves, such as the response of\rthe opponent, one must consider the possible dice rolls as well. The result is\rthat the game tree has an effective branching factor of about 400. This is far\rtoo large to permit effective use of the conventional heuristic search methods\rthat have proved so effective in games like chess and checkers.\nOn the other\rhand, the game is a good match to the capabilities of TD learning methods.\rAlthough the game is highly stochastic, a complete description of the game��s\rstate is available at all times. The game evolves over a sequence of moves and\rpositions until finally ending in a win for one player or the other, ending the\rgame. The outcome can be interpreted as a final reward to be predicted. On theother hand, the theoretical results we have described so far cannot be usefully\rapplied to this task. The number of states is so large that a lookup table\rcannot be used, and the opponent is a source of uncertainty and time variation.\nTD-Gammon used a nonlinear form of TD(A). The estimated value,\rv(s,w), of any state (board position) swas meant to estimate the probability of winning\rstarting from state s. To achieve this, rewards were defined as zero for all\rtime steps except those on which the game is won. To implement the value\rfunction, TD-Gammon used a standard multilayer neural network, much as shown in\rFigure 16.2. (The real network had two additional units in its final layer to\restimate the probability of each player��s winning in a special way called a\r��gammon�� or ��backgammon.��) The network consisted of a layer of input units, a\rlayer of hidden units, and a final output unit. The input to the network was a\rrepresentation of a backgammon position, and the output was an estimate of the\rvalue of that position.\n\r\r\rpredictedprobability\n\r\r\r\r\r\r\r\rhidden units(40-80)\n\r\r\r\r\r\r\r\rFigure 16.2: The\rneural network used in TD-Gammon\n\r\r\r\r\r\r\r\rbackgammon position (198 input units)\n\r\r\r\r\rIn the first version of TD-Gammon,\rTD-Gammon 0.0, backgammon positions were represented to the network in a\rrelatively direct way that involved little backgammon knowledge. It did,\rhowever, involve substantial knowledge of how neural networks work and how information\ris best presented to them. It is instructive to note the exact representation\rTesauro chose. There were a total of 198 input units to the network. For each\rpoint on the backgammon board, four units indicated the number of white pieces\ron the point. If there were no white pieces, then all four units took on the\rvalue zero. If there was one piece, then the first unit took on the value 1.\rThis encoded the elementary concept of a ��blot,�� i.e., a piece that can be hit\rby the opponent. If there were two or more pieces, then the second unit was set\rto 1. This encoded the basic concept of a ��made point�� on which the opponent\rcannot land. If there were exactly three pieces on the point, then the third\runit was set to 1. This encoded the basic concept of a ��single spare,�� i.e., an\rextra piece in addition to the two pieces that made the point. Finally, if\rthere were more than three pieces, the fourth unit was set to a value\rproportionate to the number of additional pieces beyond three. Letting ndenote the total number of pieces on the point, if n\u0026gt; 3, then the fourth unit took on the value (n ��3)/2.This encoded a linear representationof ��multiple spares�� at the given\rpoint.\nWith four units for white and four for black at\reach of the 24 points, that made a total of 192 units. Two additional units\rencoded the number of white and black pieces on the bar (each took the value n/2, where n is the number of pieces on the\rbar), and two more encoded the number of black and white pieces already\rsuccessfully removed from the board (these took the value n/15, where n is the\rnumber of pieces already borne off). Finally, two units indicated in a binary\rfashion whether it was white��s or black��s turn to move. The general logic\rbehind these choices should be clear. Basically, Tesauro tried to represent the\rposition in a straightforward way, while keeping the number of units relatively\rsmall. He provided one unit for each conceptually distinct possibility that\rseemed likely to be relevant, and he scaled them to roughly the same range, in\rthis case between 0and 1.\nGiven a representation of a backgammon position, the network\rcomputed its esti\u0026shy;mated value in the standard way. Corresponding to each\rconnection from an input unit to a hidden unit was a real-valued weight.\rSignals from each input unit were multiplied by their corresponding weights and\rsummed at the hidden unit. The output, h(j), of hidden unit j\rwas a nonlinear sigmoid function of the weighted sum:\n\r\n\r\r\r\r\r\u0026nbsp;\nwhere x is\rthe value of the ith input unit and Wj- is the weight of its connection to the\rjth hidden unit (all the weights in the network together make up the parameter\rvector w). The output of the sigmoid is always between 0 and 1, and has a\rnatural interpretation as a probability based on a summation of evidence. The\rcomputation from hidden units to the output unit was entirely analogous. Each\rconnection from a hidden unit to the output unit had a separate weight. The\routput unit formed the weighted sum and then passed it through the same sigmoid\rnonlinearity.\nTD-Gammon used the semi-gradient form of the TD(A) algorithm\rdescribed in Section 12.2, with the gradients computed by the error\rbackpropagation algorithm (Rumelhart, Hinton, and Williams, 1986). Recall that\rthe general update rule for this case is\nwtʮ1== wt + a Rt+1+ Y^(St+1,wt) �� v(St,wt) eu\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (16.1)\nwhere wt is the vector of all modifiable parameters (in this case,\rthe weights of the network) and et is a vector of eligibility traces, one for\reach component of wt, updated by\net == 7Aet-i + W(St,wt),\nwith eo == 0. The gradient in this equation can\rbe computed efficiently by the backpropagation procedure. For the backgammon\rapplication, in which 7= 1 and the reward is\ralways zero except upon winning, the TD error portion of the learning rule is\rusually just v(St+i,w) �� {)(St,w), as suggested in Figure 16.2.\nTo apply the learning rule we need a source of backgammon games.\rTesauro obtained an unending sequence of games by playing his learning\rbackgammon player against itself. To choose its moves, TD-Gammon considered\reach of the 20 or so ways it could play its dice roll and the corresponding\rpositions that would result. The resulting positions are afterstatesas discussed in Section 6.8. The network was consulted to estimate each of their values. The\rmove was then selected that would lead to the position with the highest\restimated value. Continuing in this way, with TD-Gammon making the moves for\rboth sides, it was possible to easily generate large numbers of backgammon\rgames. Each game was treated as an episode, with\nthe sequence of positions acting as the states, So, Si, S2,\u0026nbsp;\u0026nbsp;\u0026nbsp; \rTesauro applied the\nnonlinear TD rule\r(16.1) fully incrementally, that is, after each individual move.\nThe weights of the network were set initially to small random\rvalues. The initial evaluations were thus entirely arbitrary. Since the moves\rwere selected on the basis of these evaluations, the initial moves were inevitably\rpoor, and the initial games often lasted hundreds or thousands of moves before\rone side or the other won, almost by accident. After a few dozen games however,\rperformance improved rapidly.\nAfter playing about 300,000 games against itself, TD-Gammon 0.0 as\rdescribed above learned to play approximately as well as the best previous\rbackgammon com\u0026shy;puter programs. This was a striking result because all the\rprevious high-performance computer programs had used extensive backgammon\rknowledge. For example, the reigning champion program at the time was,\rarguably, Neurogammon,another pro\u0026shy;gram written by Tesauro that used a neural network but\rnot TD learning. Neu\u0026shy;rogammon��s network was trained on a large training corpus\rof exemplary moves provided by backgammon experts, and, in addition, started\rwith a set of features specially crafted for backgammon. Neurogammon was a\rhighly tuned, highly effec\u0026shy;tive backgammon program that decisively won the\rWorld Backgammon Olympiad in 1989. TD-Gammon 0.0, on the other hand, was\rconstructed with essentially zero backgammon knowledge. That it was able to do\ras well as Neurogammon and all other approaches is striking testimony to the\rpotential of self-play learning methods.\nThe tournament success of TD-Gammon 0.0 with zero expert backgammon\rknowl\u0026shy;edge suggested an obvious modification: add the specialized backgammon\rfeatures but keep the self-play TD learning method. This produced TD-Gammon\r1.0. TD- Gammon 1.0 was clearly substantially better than all previous\rbackgammon pro\u0026shy;grams and found serious competition only among human experts.\rLater versions of the program, TD-Gammon 2.0 (40 hidden units) and TD-Gammon\r2.1 (80 hidden units), were augmented with a selective two-ply search\rprocedure. To select moves, these programs looked ahead not just to the\rpositions that would immediately result, but also to the opponent��s possible\rdice rolls and moves. Assuming the opponent always took the move that appeared\rimmediately best for him, the expected value of each candidate move was computed\rand the best was selected. To save computer time, the second ply of search was\rconducted only for candidate moves that were ranked highly after the first ply,\rabout four or five moves on average. Two-ply search affected only the moves\rselected; the learning process proceeded exactly as before. The final versions\rof the program, TD-Gammon 3.0 and 3.1, used 160 hidden units\nProgram\n\rHidden\nUnits\n\rTraining\nGames\n\rOpponents\n\rResults\n\r\rTD-Gam 0.0\n\r40\n\r300,000\n\rother programs\n\rtied for best\n\r\rTD-Gam 1.0\n\r80\n\r300,000\n\rRobertie, Magriel, ...\n\r��13 pts / 51 games\n\r\rTD-Gam 2.0\n\r40\n\r800,000\n\rvarious Grandmasters\n\r��7 pts / 38 games\n\r\rTD-Gam 2.1\n\r80\n\r1,500,000\n\rRobertie\n\r��1 pt / 40 games\n\r\rTD-Gam 3.0\n\r80\n\r1,500,000\n\rKazaros\n\r+6pts / 20games\n\r\r\r\r\rTable 16.1: Summary of TD-Gammon Results\n\r\r\r\r\r\u0026nbsp;\nand a selective\rthree-ply search. TD-Gammon illustrates the combination of learned value\rfunctions and decision-time search as in heuristic search and MCTS methods. In\rfollow-on work, Tesauro and Galperin (1997) explored trajectory sampling meth\u0026shy;ods\ras an alternative to full-width search, which reduced the error rate of live\rplay by large numerical factors (4x-6x) while keeping the think time reasonable\rat \u0026#12316;5-10 seconds per move.\nDuring the 1990s, Tesauro was able to play his programs in a\rsignificant number of games against world-class human players. A summary of the\rresults is given in Table 16.1. Based on these results and analyses by\rbackgammon grandmasters (Robertie, 1992; see Tesauro, 1995), TD-Gammon 3.0\rappeared to play at close to, or possibly better than, the playing strength of\rthe best human players in the world. Tesauro reported in a subsequent article\r(Tesauro, 2002) the results of an extensive rollout analysis of the move\rdecisions and doubling decisions of TD-Gammon relative to top human players. The\rconclusion was that TD-Gammon 3.1 had a ��lopsided advantage�� in piece-movement\rdecisions, and a ��slight edge�� in doubling decisions, over top humans.\nTD-Gammon had\ra significant impact on the way the best human players play the game. For\rexample, it learned to play certain opening positions differently than was the\rconvention among the best human players. Based on TD-Gammon��s success and\rfurther analysis, the best human players now play these positions as TD-Gammon\rdoes (Tesauro, 1995). The impact on human play was greatly accelerated when sev\u0026shy;eral\rother self-teaching neural net backgammon programs inspired by TD-Gammon, such\ras Jellyfish, Snowie, and GNUBackgammon, became widely available. These\rprograms enabled wide dissemination of new knowledge generated by the neural\rnets, resulting in great improvements in the overall caliber of human\rtournament play (Tesauro, 2002).\n16.2\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rSamuel,s\rCheckers Player\nAn important\rprecursor to Tesauro��s TD-Gammon was the seminal work of Arthur Samuel (1959,\r1967) in constructing programs for learning to play checkers. Samuel was one of\rthe first to make effective use of heuristic search methods and of what we\rwould now call temporal-difference learning. His checkers players are\rinstructive \n\r\rcase studies\rin addition to being of historical interest. We emphasize the relationship of\rSamuel��s methods to modern reinforcement learning methods and try to convey\rsome of Samuel��s motivation for using them.\nSamuel first wrote a checkers-playing program for the IBM 701 in\r1952. His first learningprogram was completed in 1955 and was demonstrated\ron television in 1956. Later versions of the program achieved good, though not\rexpert, playing skill. Samuel was attracted to game-playing as a domain for\rstudying machine learning because games are less complicated than problems\r��taken from life�� while still allowing fruitful study of how heuristic\rprocedures and learning can be used together. He chose to study checkers\rinstead of chess because its relative simplicity made it possible to focus more\rstrongly on learning.\nSamuel��s programs played by performing a lookahead search from each\rcurrent position. They used what we now call heuristic search methods to\rdetermine how to expand the search tree and when to stop searching. The\rterminal board positions of each search were evaluated, or ��scored,�� by a value\rfunction, or ��scoring polynomial,�� using linear function approximation. In this\rand other respects Samuel��s work seems to have been inspired by the suggestions\rof Shannon (1950). In particular, Samuel��s program was based on Shannon��s\rminimax procedure to find the best move from the current position. Working\rbackward through the search tree from the scored terminal positions, each\rposition was given the score of the position that would result from the best\rmove, assuming that the machine would always try to maximize the score, while\rthe opponent would always try to minimize it. Samuel called this the backed-up\rscore of the position. When the\rminimax procedure reached the search tree��s root��the current position��it\ryielded the best move under the assumption that the opponent would be using the\rsame evaluation criterion, shifted to its point of view. Some versions of\rSamuel��s programs used sophisticated search control methods analogous to what\rare known as ��alpha-beta�� cutoffs (e.g., see Pearl, 1984).\nSamuel used two main learning methods, the simplest of which he\rcalled rote learn\u0026shy;ing. It consisted simply of saving a description of each board position\rencountered during play together with its backed-up value determined by the\rminimax procedure. The result was that if a position that had already been\rencountered were to occur again as a terminal position of a search tree, the\rdepth of the search was effectively amplified since this position��s stored\rvalue cached the results of one or more searches conducted earlier. One initial\rproblem was that the program was not encouraged to move along the most direct\rpath to a win. Samuel gave it a ��a sense of direc\u0026shy;tion�� by decreasing a\rposition��s value a small amount each time it was backed up a level (called a\rply) during the minimax analysis. ��If the program is now faced with a choice of\rboard positions whose scores differ only by the ply number, it will\rautomatically make the most advantageous choice, choosing a low-ply alternative\rif winning and a high-ply alternative if losing�� (Samuel, 1959, p. 80). Samuel\rfound this discounting-like technique essential to successful learning. Rote\rlearning produced slow but continuous improvement that was most effective for\ropening and endgame play. His program became a ��better-than-average novice��\rafter learning from many games against itself, a variety of human opponents,\rand from book games in a su-\n\r\nFigure 16.3: The backup diagram for\rSamuel��s checkers player.\n\r\r\r\r\r\u0026nbsp;\npervised\rlearning mode.\nRote learning and other aspects of Samuel��s work strongly suggest\rthe essential idea of temporal-difference learningһthat the\rvalue of a state should equal the value of likely following states. Samuel came\rclosest to this idea in his second learning method, his ��learning by\rgeneralization�� procedure for modifying the parameters of the value function.\rSamuel��s method was the same in concept as that used much later by Tesauro in\rTD-Gammon. He played his program many games against another version of itself\rand performed a backup operation after each move. The idea of Samuel��s backup\ris suggested by the diagram in Figure 16.3. Each open circle represents a\rposition where the program moves next, an on-moveposition, and each solid circle represents a\rposition where the opponent moves next. A backup was made to the value of each\ron-move position after a move by each side, resulting in a second on- move\rposition. The backup was toward the minimax value of a search launched from the\rsecond on-move position. Thus, the overall effect was that of a backup\rconsisting of one full move of real events and then a search over possible\revents, as suggested by Figure 16.3. Samuel��s actual algorithm was\rsignificantly more complex than this for computational reasons, but this was\rthe basic idea.\nSamuel did not include explicit rewards. Instead, he fixed the\rweight of the most important feature, the piece advantagefeature, which measured the number of pieces the\rprogram had relative to how many its opponent had, giving higher weight to\rkings, and including refinements so that it was better to trade pieces when\rwinning than when losing. Thus, the goal of Samuel��s program was to improve its\rpiece advantage, which in checkers is highly correlated with winning.\nHowever, Samuel��s learning method may have been missing an essential\rpart of a sound temporal-difference algorithm. Temporal-difference learning can\rbe viewed as a way of making a value function consistent with itself, and this\rwe can clearly see in Samuel��s method. But also needed is a way of tying the\rvalue function to the true value of the states. We have enforced this via\rrewards and by discounting or giving a fixed value to the terminal state. But\rSamuel��s method included no rewards and no special treatment of the terminal\rpositions of games. As Samuel himself pointed out, his value function could\rhave become consistent merely by giving a constant value to all positions. He\rhoped to discourage such solutions by giving his piece-advantage term a large,\rnonmodifiable weight. But although this may decrease the likelihood of finding\ruseless evaluation functions, it does not prohibit them. For example, a\rconstant function could still be attained by setting the modifiable weights so\ras to cancel the effect of the nonmodifiable one.\nSince Samuel��s\rlearning procedure was not constrained to find useful evaluation functions, it\rshould have been possible for it to become worse with experience. In fact,\rSamuel reported observing this during extensive self-play training sessions. To\rget the program improving again, Samuel had to intervene and set the weight\rwith the largest absolute value back to zero. His interpretation was that this\rdrastic intervention jarred the program out of local optima, but another\rpossibility is that it jarred the program out of evaluation functions that were\rconsistent but had little to do with winning or losing the game.\nDespite these potential problems, Samuel��s checkers player using the\rgeneraliza\u0026shy;tion learning method approached ��better-than-average�� play. Fairly\rgood amateur opponents characterized it as ��tricky but beatable�� (Samuel,\r1959). In contrast to the rote-learning version, this version was able to\rdevelop a good middle game but remained weak in opening and endgame play. This\rprogram also included an ability to search through sets of features to find those\rthat were most useful in forming the value function. A later version (Samuel,\r1967) included refinements in its search procedure, such as alpha-beta pruning,\rextensive use of a supervised learning mode called ��book learning,�� and\rhierarchical lookup tables called signature tables (Grif\u0026shy;fith, 1966) to\rrepresent the value function instead of linear function approximation. This\rversion learned to play much better than the 1959 program, though still not at\ra master level. Samuel��s checkers-playing program was widely recognized as a\rsignificant achievement in artificial intelligence and machine learning.\n16.3\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThe Acrobot\nReinforcement learning\rhas been applied to a wide variety of physical control tasks (e.g., for a\rcollection of robotics applications, see Kober and Peters, 2012). One such task\ris the acrobot, a two-link, underactuated robot roughly analogous to a gymnast\rswinging on a high bar (Figure 16.4). The first joint (corresponding to the\rgymnast��s hands on the bar) cannot exert torque, but the second joint (corresponding\rto the gymnast bending at the waist) can. The system has four continuous state\rvariables: two joint positions and two joint velocities. The equations of\rmotion are given in Figure 16.5. This system has been widely studied by control\rengineers (e.g., Spong, 1994) and machine-learning researchers (e.g., Dejong\rand Spong, 1994; Boone, 1997).\nOne objective\rfor controlling the acrobot is to swing the tip (the ��feet��) above the first\rjoint by an amount equal to one of the links in minimum time. In this task, the\rtorque applied at the second joint is limited to three choices: positive torque\rof a fixed magnitude, negative torque of the same magnitude, or no torque. A\rreward of ��1 is given on all time steps until the goal is reached, which ends\rthe episode. No discounting is used (7= 1). Thus, the optimal value, v^(s),of any state, s, is the minimum time to reach the goal (an integer\rnumber of steps) starting from s.\nSutton (1996) addressed the acrobot swing-up task in an on-line,\rmodelfree con\u0026shy;text. Although the acrobot was simulated, the simulator was not\ravailable for use by the agent/controller in any way. The training and\rinteraction were just as if a real, physical acrobot had been used. Each\repisode began with both links of the ac\u0026shy;robot hanging straight down and at\rrest. Torques were applied by the reinforcement learning agent until the goal\rwas reached, which always happened eventually. Then the acrobot was restored to\rits initial rest position and a new episode was begun.\nThe learning algorithm used was Sarsa(A) with linear function\rapproximation, tile coding, and replacing traces as on page 319. With a small,\rdiscrete action set, it is natural to use a separate set of tilings for each\raction. The next choice is of the continuous variables with which to represent\rthe state. A clever designer would probably represent the state in terms of the\rangular position and velocity of the center of mass and of the second link,\rwhich might make the solution simpler and consistent with broad generalization.\rBut since this was just a test problem, a more naive, direct representation was\rused in terms of the positions and velocities of the links: 0i,6i,02,\rand 62.\rThe two angles are restricted to a limited range by the physics of the acrobot\r(see Figure 16.5) and the two angles are naturally restricted to [0, 2n]. Thus,\rthe state space in this task is a bounded rectangular region in four\rdimensions.\nThis leaves\rthe question of what tilings to use. There are many possibilities, as discussed\rin Chapter 9. One is to use a complete grid, slicing the four-dimensional space\ralong all dimensions, and thus into many small four-dimensional tiles. Alterna\u0026shy;tively,\rone could slice along only one of the dimensions, making hyperplanar stripes.\rIn this case one has to pick which dimension to slice along. And of course in\rall\nGoal: Raise\rtip above line\n\r\n\r\r\r\r\r\rtip\n\r\r\r\r\r\rFigure 16.4: The acrobot.\n\r\r\r\r\r\u0026nbsp;\n-d- (d202+ 0i) f72\r\\-\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; i\n\r\r\r0i =\r6*2=\nwhere\ndi = d2=\n�li =\n�l2=\n\r\r\r\r\rd2\nm21^2+ h �� di J( T + di�li �� m2li1c2^?2sin 6 ���l2\nmil^i + צ2��+ 1^2+ 21i1c2cos 02) + h+ h m2(1^2+ 1i 1c2cos 6*2) + h ��m2li1c2^?2sin 02�� 2m2li1c2^62^6i sin 02 + (milci + m2li)g cos(0i �� n/2) + �l2 m2lc2g cos(0i + 02�� n/2).\nFigure 16.5: The equations of\rmotions of the simulated acrobot. A time step of 0.05 seconds was used in the\rsimulation, with actions chosen after every four time steps. The torque applied\rat the second joint is denoted by t G {+1, ��1, 0}. There were no constraints on\rthe joint positions, but the angular velocities were limited to G [��4n, 4n] and\r02G\r[��9n, 9n]. The constants were mi = m2= 1 (masses of the links), 1i = I2= 1 (lengths of links), 1ci = 1C2= 0.5 (lengths to center of mass of links), Ii = I2= 1 (moments of inertia of links), and g = 9.8\r(gravity).\ncases one has to pick the width of the slices,\rthe number of tilings of each kind, and, if there are multiple tilings, how to\roffset them. One could also slice along pairs or triplets of dimensions to get\rother tilings. For example, if one expected the velocities of the two links to\rinteract strongly in their effect on value, then one might make many tilings\rthat sliced along both of these dimensions. If one thought the region around\rzero velocity was particularly critical, then the slices could be more closely\rspaced there.\nSutton used tilings that sliced in a variety of\rsimple ways. Each of the four di\u0026shy;mensions was divided into six equal intervals.\rA seventh interval was added to the angular velocities so that tilings could be\roffset by a random fraction of an inter\u0026shy;val in all dimensions (see Chapter 9,\rsubsection ��Tile Coding����. Of the total of 48 tilings, 12sliced along all four dimensions as discussed above, dividing the\rspace into\n6\u0026nbsp;\u0026nbsp;\u0026nbsp; x 7 x 6x 7 = 1764 tiles each. Another 12tilings sliced along three dimensions (3 randomly offset tilings\reach for each of the 4 sets of three dimensions), and another 12sliced along two dimensions (2tilings\rfor each of the 6sets of two dimensions. Finally, a set of 12 tilings depended each\ron only one dimension (3 tilings for each of the 4 dimensions). This resulted\rin a total of approximately 25, 000 tiles for each action. This number is small\renough that hashing was not necessary. All tilings were offset by a random\rfraction of an interval in all relevant dimensions.\n\r\r\rFigure 16.6: Learning\rcurves for Sarsa(A) on the acrobot task.\n\r\r\r\r\r\r\r\rEpisodes\n\r\r\r\r\rThe remaining parameters of the learning algorithm\rwere a = 0.2/48, A = 0.9, e = 0, and wo = 0. The use of a greedy policy (^ = 0)\rseemed preferable on this task because long sequences of correct actions are\rneeded to do well. One exploratory action could spoil a whole sequence of good\ractions. Exploration was ensured instead \n\r\rby starting the action values\roptimistically, at the low value of 0. As discussed in Section 2.7 and Example 9.2, this makes the agent\rcontinually disappointed with whatever rewards it initially experiences,\rdriving it to keep trying new things.\nFigure 16.6\rshows learning curves for the acrobot task and the learning algorithm described\rabove. Note from the single-run curve that single episodes were sometimes\rextremely long. On these episodes, the acrobot was usually spinning repeatedly\rat the second joint while the first joint changed only slightly from vertical\rdown. Although this often happened for many time steps, it always eventually\rended as the action values were driven lower. All runs ended with an efficient\rpolicy for solving the problem, usually lasting about 75 steps. A typical final\rsolution is shown in\n\r\r\r\r\r%\n\r\r\r\r\r\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\r\r\n\r\r\r\r\r\rFigure 16.7: A typical learned behavior of the acrobot. Each group\ris a series of consecutive positions, the thicker line being the first. The\rarrow indicates the torque applied at the second joint.\n\r\r\r\r\r\n\r\u0026nbsp;\nFigure 16.7. First the acrobot\rpumps back and forth several times symmetrically, with the second link always\rdown. Then, once enough energy has been added to the system, the second link is\rswung upright and stabbed to the goal height.\n16.4\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rWatson��s\rDaily-Double Wagering\nIBM Watson[31]\ris the system developed by a\rteam of IBM researchers to play the popular TV quiz show Jeopardy!.[32]It gained fame in 2011 by winning first prize in an\rexhibition match against human champions. Although the main technical\rachievement demonstrated by Watson was its ability to quickly and accurately answer\rnatural language questions over broad areas of general knowledge, its win\u0026shy;ning Jeopardy!performance also relied on sophisticated\rdecision-making strategies for critical parts of the game. Tesauro, Gondek,\rLechner, Fan, and Prager (2012, 2013) adapted Tesauro��s TD-Gammon system\rdescribed above to create the strategy used by Watson in ��Daily-Double��\r(DD) wagering in its celebrated winning perfor\u0026shy;mance against human champions.\rThese authors report that the effectiveness of this wagering strategy went well\rbeyond what human players are able to do in live game play, and that it, along\rwith other advanced strategies, was an important contributor to Watson��s impressive\rwinning performance. Here we focus only on DD wager\u0026shy;ing because it is the\rcomponent of Watson that owes the most to reinforcement learning.\nJeopardy!is played by three contestants who face a board\rshowing 30 squares, each of which hides a clue and has a dollar value. The\rsquares are arranged in six columns, each corresponding to a different\rcategory. A contestant selects a square, the host reads the square��s clue, and\reach contestant may choose to respond to the clue by sounding a buzzer\r(��buzzing in��). The first contestant to buzz in gets to try responding to the\rclue. If this contestant��s response is correct, their score increases by the\rdollar value of the square; if their response is not correct, or if they do not\rrespond within five seconds, their score decreases by that amount, and the\rother contestants get a chance to buzz in to respond to the same clue. One or\rtwo squares (depending on the game��s current round) are special DD squares. A\rcontestant who selects one of these gets an exclusive opportunity to respond to\rthe square��s clue and has to decideһbefore the clue is revealedһon how much\rto wager, or bet. The bet has to be greater than five dollars but not greater\rthan the contestant��s current score. If the contestant responds correctly to\rthe DD clue, their score increases by the bet amount; otherwise it decreases by\rthe bet amount. At the end of each game is a ��Final Jeopardy�� (FJ) round in\rwhich each contestant writes down a sealed bet and then writes an answer after\rthe clue is read. The contestant with the highest score after three rounds of\rplay (where a round consists of revealing all 30 clues) is the winner. The game\rhas many other details, but these are enough to appreciate the importance of DD\rwagering. Winning or losing often depends on a contestant��s DD wagering\rstrategy.\nWhenever Watson selected a DD square, it chose its bet by comparing action values,\rq(s, bet), that estimated the probability of a win from the current game state,\rs, for each round-dollar legal bet. Except for some risk-abatement measures\rdescribed below, Watson selected the bet with the maximum action value. Action values were\rcomputed whenever a betting decision was needed by using two types of estimates\rthat were learned before any live game play took place. The first were\restimated values of the afterstates (Section 6.8) that would result from selecting each legal bet. These estimates\rwere obtained from a state-value function, {)(-,w), defined by parameters w, that gave estimates of\rthe probability of a win for Watson from any game state. The second estimates used to compute action\rvalues gave the ��in\u0026shy;category DD confidence,�� pdd, which estimated the likelihood that Watson would respond correctly to the as-yet unrevealed DD\rclue.\nTesauro et al. used the reinforcement learning approach of TD-Gammon\rdescribed above to learn v(-,w):a\rstraightforward combination of nonlinear TD(A) using a multilayer neural\rnetwork with weights w trained by backpropagating TD errors during many\rsimulated games. States were represented to the network by feature vectors\rspecifically designed for Jeopardy!.Features included the current scores of the three\rplayers, how many DDs remained, the total dollar value of the remaining clues,\rand other information related to the amount of play left in the game. Unlike\rTD-Gammon, which learned by self-play, Watson��s Vwas learned over millions of simulated games against\rcarefully-crafted models of human players. In-category confidence estimates\rwere conditioned on the number of right responses rand wrong responses wthat Watson gave in previously-played clues in the current category. The\rdependencies on (r, w) were estimated from Watson��s actual accuracies over many thousands of\rhistorical categories.\nWith the previously learned value function v and in-category DD\rconfidence pdd , Watson computed q(s, bet) for each legal round-dollar bet as follows:\nq(s,\rbet) = pdd x V(Sw + bet,...)\r+ (1�� pdd) x v(Sw �� bet,...),\u0026nbsp; (16.2)\nwhere Sw is Watson��s current score, and v gives the estimated value for the game state\rafter Watson��s response to the\rDD clue, which is either correct or incorrect. Computing an action value this\rway corresponds to the insight from Exercise 3.12 that an action value is the\rexpected next state value given the action (except that here it is the expected\rnext afterstatevalue because the full next state of the entire game depends on the next square\rselection).\nTesauro et al. found that selecting bets by maximizing action values\rincurred ��a frightening amount of risk,�� meaning that if Watson��sresponse to the clue happened to be wrong, the loss could be disastrous for its\rchances of winning. To decrease the downside risk of a wrong answer, Tesauro et\ral. adjusted (16.2) by subtracting a small fraction of the standard deviation\rover Watson��scorrect/incorrect afterstate evaluataions. They\rfurther reduced risk by prohibiting bets that would cause the wrong-answer\rafterstate value to decrease below a certain limit. These measures slightly\rreduced Watson��sexpectation of winning, but they significantly\rreduced downside risk, not only in terms of average risk per DD bet, but even\rmore so in extreme-risk scenarios where a risk-neutral Watson would bet most or\rall of its bankroll.\nWhy was the TD-Gammon method of self-play not used to learn the\rcritical value function v? Learning from self-play in Jeopardy!would not have worked very well because Watson was so different\rfrom any human contestant. Self-play would have led to exploration of state\rspace regions that are not typical for play against human opponents,\rparticularly human champions. In addition, unlike backgammon, Jeop\u0026shy;ardy!is a game of imperfect information because\rcontestants do not have access to all the information influencing their\ropponents�� play. In particular, Jeopardy!con\u0026shy;testants do not know how much confidence their\ropponents have for responding to clues in the various categories. Self-play\rwould have been something like playing poker with someone who is holding the\rsame cards that you hold.\nAs a result of these complications, much of the effort in developing\rWatson��s DD-wagering strategy\rwas devoted to creating good models of human opponents. The models did not\raddress the natural language aspect of the game, but were instead stochastic\rprocess models of events that can occur during play. Statistics were extracted\rfrom an extensive fan-created archive of game information from the beginning of\rthe show to the present day. The archive includes information such as the\rordering of the clues, right and wrong contestant answers, DD locations, and DD\rand FJ bets for nearly 300,000 clues. Three models were constructed: an Average\rContestant model (based on all the data), a Champion model (based on statistics\rfrom games with the 100 best players), and a Grand Champion model (based on\rstatistics from games with the 10 best players). In addition to serving as\ropponents during learning, the models were used to asses the benefits produced\rby the learned DD-wagering strategy. Watson��s win rate in simulation when it used a baseline heuristic DD-wagering\rstrategy was 61%; when it used the learned values and a default confidence\rvalue, its win rate increased to 64%; and with live in-category confidence, it\rwas 67%. Tesauro et al. regarded this as a significant improvement, given that\rthe DD wagering was needed only about 1.5 to 2 times in each game.\nBecause Watson had only a few seconds to bet, as well as to select\rsquares and decide whether or not to buzz in, the computation time needed to\rmake these decisions was a critical factor. The neural network implementation\rof v allowed DD bets to be made quickly enough to meet the time constraints of\rlive play. However, once games could be simulated fast enough through\rimprovements in the simulation software, near the end of a game it was feasible\rto estimate the value of bets by averaging over many Monte-Carlo trials in which\rthe consequence of each bet was determined by simulating play to the game��s\rend. Selecting endgame DD bets in live play based on Monte-Carlo trials instead\rof the neural network significantly improved Watson��s performance because errors in value estimates in endgames could\rseriously affect its chances of winning. Making all the decisions via\rMonte-Carlo trials might have led to better wagering decisions, but this was\rsimply impossible given the complexity of the game and the time constraints of\rlive play.\nAlthough its ability to quickly and accurately answer natural\rlanguage questions stands out as Watson��s major achievement, all of its sophisticated decision strategies \n\r\rcontributed to its impressive defeat of human champions. According\rto Tesauro et al. (2012):\n... it is plainly evident that\rour strategy algorithms achieve a level of quantitative precision and real-time\rperformance that exceeds human ca\u0026shy;pabilities. This is particularly true in the\rcases of DD wagering and endgame buzzing, where humans simply cannot come close\rto matching the precise equity and confidence estimates and complex decision\rcalcu\u0026shy;lations performed by Watson.\n16.5\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rOptimizing\rMemory Control\nMost computers\ruse dynamic random access memory (DRAM) as their main memory because of its low\rcost and high capacity. The job of a DRAM memory controller is to efficiently\ruse the interface between the processor chip and an off-chip DRAM system to\rprovide the high-bandwidth and low-latency data transfer necessary for high-speed\rprogram execution. A memory controller needs to deal with dynamically changing\rpatterns of read/write requests while adhering to a large number of timing and\rresource constraints required by the hardware. This is a formidable scheduling\rproblem, especially with modern processors with multiple cores sharing the same\rDRAM.\nIpek, Mutlu, Martinez, and Caruana (2008) (also Martinez and Ipek,\r2009) de\u0026shy;signed a reinforcement learning memory controller and demonstrated\rthat it can significantly improve the speed of program execution over what was\rpossible with conventional controllers at the time of their research. They were\rmotivated by lim\u0026shy;itations of existing state-of-the-art controllers that used\rpolicies that did not take advantage of past scheduling experience and did not\raccount for long-term conse\u0026shy;quences of scheduling decisions. Ipek et al.��s\rproject was carried out by means of simulation, but they designed the\rcontroller at the detailed level of the hardware needed to implement\rit��including the learning algorithm��directly on a processor chip.\nAccessing DRAM involves a number of steps that have to be done\raccording to strict time constraints. DRAM systems consist of multiple DRAM\rchips, each con\u0026shy;taining multiple rectangular arrays of storage cells arranged\rin rows and columns. Each cell stores a bit as the charge on a capacitor. Since\rthe charge decreases over time, each DRAM cell needs to be\rrecharged��refreshed��every few milliseconds to prevent memory content from being\rlost. This need to refresh the cells is why DRAM is called ��dynamic.��\nEach cell array has a row buffer that holds a row of bits that can\rbe transferred into or out of one of the array��s rows. An activatecommand ��opens a row,�� which means moving the\rcontents of the row whose address is indicated by the command into the row\rbuffer. With a row open, the controller can issue readand writecommands to the cell array. Each read command\rtransfers a word (a short sequence of consecutive bits) in the row buffer to\rthe external data bus, and each write command transfers a word in the external\rdata bus to the row buffer. Before a different row can be opened, a prechargecommand must be issued which transfers the\r(possibly updated) data in the row buffer back into the addressed row of the\rcell array. After this, another activate command can open a new row to be\raccessed. Read and write commands are column commandsbecause they sequentially transfer bits into or out\rof columns of the row buffer; multiple bits can be transferred without\rre-opening the row. Read and write commands to the currently-open row can be\rcarried out more quickly than accessing a different row, which would involve\radditional row commands: precharge and activate; this is sometimes referred to as ��row\rlocality.�� A memory controller maintains a memory transaction queuethat stores memory-access requests from the\rprocessors sharing the memory system. The controller has to process requests by\rissuing commands to the memory system while adhering to a large number of\rtiming constraints.\nA controller��s policy for scheduling access requests can have a\rlarge effect on the performance of the memory system, such as the average\rlatency with which requests can be satisfied and the throughput the system is\rcapable of achieving. The simplest scheduling strategy handles access requests\rin the order in which they arrive by issu\u0026shy;ing all the commands required by the\rrequest before beginning to service the next one. But if the system is not\rready for one of these commands, or executing a command would result in\rresources being underutilized (e.g., due to timing constraints arising from\rservicing that one command), it makes sense to begin servicing a newer request\rbefore finishing the older one. Policies can gain efficiency by reordering\rrequests, for example, by giving priority to read requests over write requests,\ror by giving priority to read/write commands to already open rows. The policy\rcalled First-Ready, First- Come-First-Serve (FR-FCFS), gives priority to column\rcommands (read and write) over row commands (activate and precharge), and in\rcase of a tie gives priority to the oldest command. FR-FCFS was shown to\routperform other scheduling policies in terms of average memory-access latency\runder conditions commonly encountered (Rixner, 2004).\nFigure 16.8 is a high-level view of Ipek et al.��s reinforcement\rlearning memory controller. They modeled the DRAM access process as an MDP\rwhose states are the contents of the transaction queue and whose actions are\rcommands to the DRAM system: precharge, activate, read, write, and NoOp. The reward signal is 1 whenever the action is reador write, and otherwise it is 0. State transitions were\rconsidered to be stochastic because the next state of the system not only\rdepends on the scheduler��s command, but also on aspects of the system��s\rbehavior that the scheduler cannot control, such as the workloads of the\rprocessor cores accessing the DRAM system.\nCritical to this MDP are constraints on the actions available in\reach state. Recall from Chapter 3 that the set of available actions can depend\ron the state: At G A(St), where At is the action at time step tand A(St) is the set of actions available in state\rSt. In this application, the integrity of the DRAM system was assured by not\rallowing actions that would violate timing or resource constraints. Although\rIpek et al. did not make it explicit, they effectively accomplished this by\rpre-defining the sets A(St) for all possible states St.\n\r\n\r\r\r\r\r\rFigure 16.8:\rHigh-level view of the reinforcement learning DRAM controller. The scheduler\ris the reinforcement learning agent. Its environment is represented by\rfeatures of the trans\u0026shy;action queue, and its actions are commands to the DRAM\rsystem. \u0026copy;2009 IEEE. Reprinted, with permission, from J. F. Martinez and E.\rIpek, Dynamic multicore resource management: A machine learning approach,\rMicro, IEEE, 29(5), p. 12.\n\r\r\r\r\r\rlAJVQCa\n\r\r\r\r\r\u0026nbsp;\nThese constraints explain why the MDP has a NoOpaction\rand why the reward signal is 0 except when a reador writecommand is issued. NoOpis issued when it is the sole legal action in a\rstate. To maximize utilization of the memory system, the controller��s task is\rto drive the system to states in which either a reador a write action can be\rselected: only these actions result in sending data over the external data bus,\rso it is only these that contribute to the throughput of the system. Although prechargeand activateproduce no immediate reward, the agent needs to\rselect these actions to make it possible to later select the rewarded readand writeactions.\nThe scheduling agent used Sarsa (Figure 6.4) to learn an\raction-value function. States were represented by six integer-valued features.\rTo approximate the action- value function, the algorithm used linear function\rapproximation implemented by tile coding with hashing (Section 9.5.4). The tile\rcoding had 32 tilings, each storing 256 action values as 16-bit fixed point\rnumbers. Exploration was e-greedy with e = 0.05.\nState features included the number of read requests in the\rtransaction queue, the number of write requests in the transaction queue, the\rnumber of write requests in the transaction queue waiting for their row to be\ropened, and the number of read requests in the transaction queue waiting for\rtheir row to be opened that are the oldest issued by their requesting\rprocessors. (The other features depended on how the DRAM interacts with cache\rmemory, details we omit here.) The selection of the state features was based on\rIpek et al.��s understanding of factors that impact DRAM performance. For\rexample, balancing the rate of servicing reads and writes based on how many of\reach are in the transaction queue can help avoid stalling the DRAM system��s\rinteraction with cache memory. The authors in fact generated a relatively long\rlist of potential features, and then pared them down to a handful using\rsimulations guided by stepwise feature selection.\nAn interesting aspect of this formulation of the scheduling problem\ras an MDP is that the features input to the tile coding for defining the\raction-value function were different from the features used to specify the\raction-constraint sets A(St). Whereas the tile coding input was derived from\rthe contents of the transaction queue, the constraint sets depended on a host\rof other features related to timing and resource constraints that had to be\rsatisfied by the hardware implementation of the entire system. In this way, the\raction constraints ensured that the learning algorithm��s ex\u0026shy;ploration could not\rendanger the integrity of the physical system, while learning was effectively\rlimited to a ��safe�� region of the much larger state space of the hardware\rimplementation.\nSince an\robjective of this work was that the learning controller could be imple\u0026shy;mented\ron a chip so that learning could occur on-line while a computer is running,\rhardware implementation details were important considerations. The design\rincluded two five-stage pipelines to calculate and compare two action values at\revery processor clock cycle, and to update the appropriate action value. This\rincluded accessing the tile coding which was stored on-chip in static RAM. For\rthe configuration Ipek et al. simulated, which was a 4GHz 4-core chip typical\rof high-end workstations at the time of their research, there were 10 processor\rcycles for every DRAM cycle. Consid\u0026shy;ering the cycles needed to fill the pipes,\rup to 12actions could be evaluated in each DRAM cycle. Ipek et al. found that the\rnumber of legal commands for any state was rarely greater than this, and that\rperformance loss was negligible if enough time was not always available to\rconsider all legal commands. These and other clever design details made it\rfeasible to implement the complete controller and learning algorithm on a\rmulti-processor chip.\nIpek et al.\revaluated their learning controller in simulation by comparing it with three\rother controllers: 1) the FR-FCFS controller mentioned above that produces the\rbest on-average performance, 2) a conventional controller that processes each request in order,\rand 3) an unrealizable ideal controller, called the Optimistic con\u0026shy;troller,\rable to sustain 100% DRAM throughput if given enough demand by ignoring all\rtiming and resource constraints, but otherwise modeling DRAM latency (as row\rbuffer hits) and bandwidth. They simulated nine memory-intensive parallel work\u0026shy;loads\rconsisting of scientific and data-mining applications. Figure 16.9 shows the\rperformance (the inverse of execution time normalized to the performance of FR-\rFCFS) of each controller for the nine applications, together with the geometric\rmean of their performances over the applications. The learning controller,\rlabeled RL in the figure, improved over that of FR-FCFS by from 7% to 33% over\rthe nine ap\u0026shy;plications, with an average improvement of 19%. Of course, no\rrealizable controller can match the performance of Optimistic, which ignores\rall timing and resource con\u0026shy;straints, but the learning controller��s performance\rclosed the gap with Optimistic��s upper bound by an impressive 27%.\nBecause the\rrationale for on-chip implementation of the learning algorithm was to allow the\rscheduling policy to adapt on-line to changing workloads, Ipek et al. analyzed\rthe impact of on-line learning compared to a previously-learned fixed policy.\rThey trained their controller with data from all nine benchmark applications\rand then held the resulting action values fixed throughout the simulated\rexecution of the applications. They found that the average performance of the\rcontroller that learned\n\r\r\r\nFigure 16.9: Performances of four\rcontrollers over a suite of 9 simulated benchmark ap\u0026shy;plications. The\rcontrollers are: the simplest ��in-order�� controller, FR-FCFS, the learning\rcontroller RL, and the unrealizable Optimistic controller which ignores all\rtiming and re\u0026shy;source constraints to provide a performance upper bound.\rPerformance, normalized to that of FR-FCFS, is the inverse of execution time.\rAt far right is the geometric mean of perfor\u0026shy;mances over the 9 benchmark\rapplications for each controller. Controller RL comes closest to the ideal\rperformance. @2009 IEEE. Reprinted, with permission, from J. F. Martinez and\rE. Ipek, Dynamic multicore resource management: A machine learning approach,\rMicro, IEEE, 29(5), p. 13.\n\r\r\r\r\r\u0026nbsp;\non-line was 8% better than that of the controller using the fixed policy, leading\rthem to conclude that on-line learning is an important feature of their\rapproach.\nThis learning memory controller was never committed to physical\rhardware be\u0026shy;cause of the large cost of fabrication. Nevertheless, Ipek et al.\rcould convincingly argue on the basis of their simulation results that a memory\rcontroller that learns on-line via reinforcement learning has the potential to\rimprove performance to levels that would otherwise require more complex and\rmore expensive memory systems, while removing from human designers some of the\rburden required to manually de\u0026shy;sign efficient scheduling policies. Mukundan and\rMartinez (2012) took this project forward by investigating learning controllers\rwith additional actions, other perfor\u0026shy;mance criteria, and more complex reward\rfunctions derived using genetic algorithms. They considered additional\rperformance criteria related to energy efficiency. The re\u0026shy;sults of these\rstudies surpassed the earlier results described above and significantly\rsurpassed the 2012state-of-the-art for all of the performance criteria they consid\u0026shy;ered. The\rapproach is especially promising for developing sophisticated power-aware DRAM\rinterfaces.\n16.6\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rHuman-level\rVideo Game Play\nOne of the greatest\rchallenges in applying reinforcement learning to real-world prob\u0026shy;lems is\rdeciding how to represent and store value functions and/or policies. Unless the\rstate set is finite and small enough to allow exhaustive representation by a\rlookup table��as in many of our illustrative examples��one must use a\rparameterized func\u0026shy;tion approximation scheme. Whether linear or non-linear,\rfunction approximation relies on features that have to be readily accessible to\rthe learning system and able to convey the information necessary for skilled\rperformance. Most successful appli\u0026shy;cations of reinforcement learning owe much\rto sets of features carefully handcrafted based on human knowledge and\rintuition about the specific problem to be tackled.\nA team of researchers at Google DeepMind developed an impressive\rdemonstra\u0026shy;tion that a deep multi-layer artificial neural network (ANN) can\rautomate the feature design process (Mnih et al., 2015). Multi-layer ANNs have\rbeen used for function ap\u0026shy;proximation in reinforcement learning ever since the\r1986 popularization of the back- propagation algorithm as a method for learning\rinternal representations (Rumelhart, Hinton, and Williams, 1986; see Section 9.6).Striking results have been obtained by coupling reinforcement learning with\rbackpropagation. The results obtained by Tesauro and colleages with TD-Gammon\rand Watson discussed above\rare notable examples. These and other applications benefited from the ability\rof multi-layer ANNs to learn task-relevant features. However, in all the\rexamples of which we are aware, the most impressive demonstrations required the\rnetwork��s input to be rep\u0026shy;resented in terms of specialized features handcrafted\rfor the given problem. This is vividly apparent in the TD-Gammon results.\rTD-Gammon 0.0, whose network in\u0026shy;put was essentially a ��raw�� representation of\rhe backgammon board, meaning that it involved very little knowledge of\rbackgammon, learned to play approximately as well as the best previous\rbackgammon computer programs. Adding specialized backgam\u0026shy;mon features produced\rTD-Gammon 1.0 which was substantially better than all previous backgammon\rprograms and competed well against human experts.\nMnih et al. developed a reinforcement learning agent called deep\rQ-network(DQN) that combined\rQ-learning with a deep convolutionalANN, a many-layered, or deep, ANN specialized for\rprocessing spatial arrays of data such as images. We describe deep\rconvolutional ANNs in Section 9.6. By the time of Mnih et al.��s work with DQN,\rdeep ANNs, including deep convolutional ANNs, had produced impressive results\rin many applications, but they had not been widely used in reinforcement\rlearning.\nMnih et al. used DQN to show how a single reinforcement learning agent\rcan achieve high levels of performance in many different problems without\rrelying on different problem-specific feature sets. To demonstrate this, they\rlet DQN learn to play 49 different Atari 2600 video games by interacting with a\rgame emulator. For learning each game, DQN used the same raw input, the same\rnetwork architecture, and the same parameter values (e.g., step-size, discount\rrate, exploration parame\u0026shy;ters, and many more specific to the implementation).\rDQN achieved levels of play at or beyond human level on a large fraction of\rthese games. Although the games were alike in being played by watching streams\rof video images, they varied widely in other respects. Their actions had\rdifferent effects, they had different state-transition dynamics, and they\rneeded different policies for earning high scores. The deep con\u0026shy;volutional ANN\rlearned to transform the raw input common to all the games into features\rspecialized for representing the action values required for playing at the high\rlevel DQN achieved for most of the games.\nThe Atari 2600 is a home video game console that was sold in various\rversions by Atari Inc. from 1977 to 1992. It introduced or popularized many\rarcade video games that are now considered classics, such as Pong, Breakout,\rSpace Invaders, and Asteroids. Although much simpler than modern video games,\rAtari 2600 games are still entertaining and challenging for human players, and\rthey have been attractive as testbeds for developing and evaluating\rreinforcement learning methods (Diuk, Co\u0026shy;hen, Littman, 2008; Naddaf, 2010;\rCobo, Zang, Isbell, and Thomaz, 2011; Bellemare, Veness, and Bowling, 2012).\rBellemare, Naddaf, Veness, and Bowling (2012) devel\u0026shy;oped the publicly available\rArcade Learning Environment (ALE) to encourage and simplify using Atari 2600\rgames to study learning and planning algorithms.\nThese previous studies and the availability of ALE made the Atari\r2600 game collection a good choice for Mnih et al.��s demonstration, which was\ralso influenced by the impressive human-level performance that TD-Gammon was\rable to achieve in backgammon. DQN is similar to TD-Gammon in using a\rmulti-layer ANN as the function approximation method for a semi-gradient form\rof a TD algorithm, with the gradients computed by the backpropagation\ralgorithm. However, instead of using TD(A) as TD-Gammon did, DQN used the\rsemi-gradient form of Q-learning. TD-Gammon estimated the values of\rafterstates, which were easily obtained from the rules for making backgammon\rmoves. To use the same algorithm for the Atari games would have required\rgenerating the next states for each possible action (which would not have been\rafterstates in that case). This could have been done by using the game emulator\rto run single-step simulations for all the possible actions (which ALE makes\rpossible). Or a model of each game��s state-transition function could have been\rlearned and used to predict next states (Oh, Guo, Lee, Lewis, and Singh, 2015).\rWhile these methods might have produced results comparable to DQN��s, they would\rhave been more complicated to implement and would have significantly increased\rthe time needed for learning. Another motivation for using Q-learning was that\rDQN used the experience replaymethod, described below, which requires an\roff-policy algorithm. Being model-free and off-policy made Q-learning a natural\rchoice.\nBefore describing the details of DQN and how the experiments were\rconducted, we look at the skill levels DQN was able to achieve. Mnih et al.\rcompared the scores of DQN with the scores of the best performing learning\rsystem in the literature at the time, the scores of a professional human games\rtester, and the scores of an agent that selected actions at random. The best\rsystem from the literature used linear function approximation with features\rhand designed using some knowledge about Atari 2600 games (Bellemare, Naddaf,\rVeness, and Bowling, 2012). DQN learned on each game by interacting with the\rgame emulator for 50 million frames, which corresponds to about 38 days of\rexperience with the game. At the start of learning on each game, the weights of\rDQN��s network were reset to random values. To evaluate DQN��s skill level after\rlearning, its score was averaged over 30 sessions on each game, each lasting up\rto 5 minutes and beginning with a random initial game state. The professional\rhuman tester played using the same emulator (with the sound turned off to\rremove any possible advantage over DQN which did not process audio). After 2\rhours of practice, the human played about 20 episodes of each game for up to 5\rminutes each and was not allowed to take any break during this time. DQN\rlearned to play better than the best previous reinforcement learning systems on\rall but 6of\rthe games, and played better than the human player on 22 of the games. By\rconsidering any performance that scored at or above 75% of the human score to\rbe comparable to, or better than, human-level play, Mnih et al. concluded that\rthe levels of play DQN learned reached or exceeded human level on 29 of the 46\rgames. See Mnih et al. (2015) for a more detailed account of these results.\nFor an artificial learning system\rto achieve these levels of play would be impressive enough, but what makes\rthese results remarkable��and what many at the time con\u0026shy;sidered to be\rbreakthrough results for artificial intelligence��is that the very same learning\rsystem achieved these levels of play on widely varying games without relying on\rany game-specific modifications.\nA human playing any of these 46\rAtari games sees 210 x 160 pixel image frames with 128 colors at 60Hz. In\rprinciple, exactly these images could have formed the raw input to DQN, but to\rreduce memory and processing requirements, Mnih et al. preprocessed each frame\rto produce an 84 x 84 array of luminance values. Since the full states of many\rof the Atari games are not completely observable from the image frames, Mnih et\ral. ��stacked�� the four most recent frames so that the inputs to the network had\rdimension 84x84x4. This did not eliminate partial observability for all of the\rgames, but it was helpful in making many of them more Markovian.\nAn essential point here is that\rthese preprocessing steps were exactly the same for all 46 games. No\rgame-specific prior knowledge was involved beyond the gen\u0026shy;eral understanding\rthat it should still be possible to learn good policies with this reduced\rdimension and that stacking adjacent frames should help with the partial\robservability of some of the games. Since no game-specific prior knowledge\rbeyond this minimal amount was used in preprocessing the image frames, we can\rthink of the 84x84x 4 input vectors as being ��raw�� input to DQN.\nThe basic architecture of DQN is\rsimilar to the deep convolutional ANN illustrated in Figure 9.15 (though unlike\rthat network, subsampling in DQN is treated as part of each convolutional\rlayer, with feature maps consisting of units having only a selection of the\rpossible receptive fields). DQN has three hidden convolutional layers, followed\rby one fully connected hidden layer, followed by the output layer. The three\rsuccessive hidden convolutional layers of DQN produce 32 20 x 20 feature maps,\r64 9x9 feature maps, and 64 7x7 feature maps. The activation function of the\runits of each feature map is a rectifier nonlinearity (max(0, x)). The 3,136\r(64x7x7) units in this third convolutional layer all connect to each of 512\runits in the fully connected hidden layer, which then each connect to all 18\runits in the output layer, one for each possible action in an Atari game.\nThe activation levels of DQN��s output units were the estimated\roptimal action values (optimal Q-values) of the corresponding state-action\rpairs, for the state rep\u0026shy;resented by the network��s input. The assignment of\routput units to a game��s actions varied from game to game, and since the number\rof valid actions varied between 4 and 18 for the games, not all output units\rhad functional roles in all of the games. It helps to think of the network as\rif it were 18 separate networks, one for estimating the optimal action value of\reach possible action. In reality, these networks shared their initial layers,\rbut the output units learned to use the features extracted by these layers in\rdifferent ways.\nDQN��s reward signal indicated how\ra games��s score changed from one time step to the next: +1whenever it increased, ��1whenever it decreased, and 0otherwise. This standardized the reward signal\racross the games and made a single step-size parameter work well for all the\rgames despite their varying ranges of scores. DQN used an e-greedy policy, with e decreasing linearly over the first million frames\rand remaining at a low value for the rest of the learning session. The values\rof the various other parameters, such as the learning step-size, discount rate,\rand others specific to the implementation, were selected by performing informal\rsearches to see which values worked best for a small selection of the games.\rThese values were then held fixed for all of the games.\nAfter DQN selected an action, the action was\rexecuted by the game emulator, which returned a reward and the next video\rframe. The frame was preprocessed and added to the four-frame stack that became\rthe next input to the network. Skipping for the moment the changes to the basic\rQ-learning procedure made by Mnih et al., DQN used the following semi-gradient\rform of Q-learning to update the network��s weights:\nwt+i = wt + a Rt+i + 7maxq(St+i, a, wt) �� q(St, At, wt) Vwtq(St, At, wt), (16.3)\nLa\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nwhere wt is the vector of the network��s weights, At is the action selected at time step t, and St and St+i are respectively the preprocessed image stacks input\rto the network at time steps t and t + 1.\nThe gradient in (16.3) was\rcomputed by backpropagation. Imagining again that there was a separate network\rfor each action, for the update at time step t, back- propagation was applied only to the network\rcorresponding to At. Mnih et al. took advantage of techniques shown to improve the\rbasic backpropagation algorithm when applied to large networks. They used a mini-batch\rmethodthat updated weights only\rafter accumulating gradient information over a small batch of images (here\rafter 32 images). This yielded smoother sample gradients compared to the usual\rprocedure that updates weights after each action. They also used a gradient-ascent\ralgorithm called RMSProp (Tieleman and Hinton, 2012) that accelerates learning\rby adjusting the step-size parameter for each weight based on a running average\rof the magnitudes of recent gradients for that weight.\nMnih et al. modified the basic\rQ-learning procedure in three ways. First, they used a method called experience\rreplayfirst studied by Lin\r(1992). This method stores the agent��s experience at each time step in a replay\rmemory that is accessed to perform the weight updates. It worked like this in\rDQN. After the game emulator executed action At in a state represented by the image stack St, and returned reward Rt+i and image stack St+i, it added the tuple (St, At, Rt+i, St+i) to the replay memory. This memory accumulated\rexperiences over many plays of the same game. At each time step multiple\rQ-learning updates��a mini-batch��were performed based on experiences sampled\runiformly at random from the replay memory. Instead of St+i becoming the new St for the next update as it would in the usual form\rof Q-learning, a new unconnected experience was drawn from the replay memory to\rsupply data for the next update. Since Q-learning is an off-policy algorithm,\rit does not need to be applied along connected trajectories.\nQ-learning with experience replay provided several\radvantages over the usual form of Q-learning. The ability to use each stored\rexperience for many updates allowed DQN to learn more efficiently from its\rexperiences. Experience replay reduced the variance of the updates because\rsuccessive updates were not correlated with one another as they would be with\rstandard Q-learning. And by removing the dependence of successive experiences\ron the current weights, experience replay eliminated one source of instability.\nMnih et al. modified standard Q-learning in a second\rway to improve its stability. As in other methods that bootstrap, the target\rfor a Q-learning update depends on the current action-value function estimate.\rWhen a parameterized function approx\u0026shy;imation method is used to represent action\rvalues, the target is a function of the same parameters that are being updated.\rFor example, the target in the update given by (16.3) is 7maxa q(St+i,a, wt). Its dependence on wt complicates the pro\u0026shy;cess compared to the simpler\rsupervised-learning situation in which the targets do not depend on the\rparameters being updated. As discussed in Chapter 11this can lead to oscillations and/or divergence.\nTo address this problem Mnih et\ral. used a technique that brought Q-learning closer to the simpler\rsupervised-learning case while still allowing it to bootstrap. Whenever a\rcertain number, C, of updates had been done to the weights w of the action value network, they inserted the\rnetwork��s current weights into another network and held these duplicate weights\rfixed for the next C updates of w. The outputs of this duplicate network over the\rnext C updates of w were used as the Q-learning targets. Letting qdenote the output of this duplicate network, then\rinstead of (16.3) the update rule was:\nwt+i = wt + a Rt+i + Ymaxq(St+i,a, wt) - q(St, At, wt) Vw\u0026pound;q(St,\rAt, wt).\nLa\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ��\nA final modification of standard Q-learning was also found to\rimprove stability. They clipped the error term Rt+i + Y maxa �T(St+i, a, wt) �� q(St, At, wt) so that it remained in the interval [��1,1].\nMnih et al. conducted a large number of learning\rruns on 5 of the games to gain insight into the effect that various of DQN��s\rdesign features had on its performance. They ran DQN with the four combinations\rof experience replay and the duplicate target network being included or not\rincluded. Although the results varied from game to game, each of these features\ralone significantly improved performance, and very dramatically improved\rperformance when used together. Mnih et al. also studied the role played by the\rdeep convolutional ANN in DQN��s learning ability by comparing the deep\rconvolutional version of DQN with a version having a network of just one linear\rlayer, both receiving the same stacked preprocessed video frames. Here, the\rimprovement of the deep convolutional version over the linear version was\rparticularly striking across all 5 of the test games.\nCreating artificial agents that\rexcel over a diverse collection of challenging tasks has been an enduring goal\rof artificial intelligence. The promise of machine learning \n\r\ras a means for achieving this has\rbeen frustrated by the need to craft problem-specific representations.\rDeepMind��s DQN stands as a major step forward by demonstrating that a single\ragent can learn problem-specific features enabling it to acquire human-\rcompetitive skills over a range of tasks. But as Mnih et al. point out, DQN is\rnot a complete solution to the problem of task-independent learning. Although\rthe skills needed to excel on the Atari games were markedly diverse, all the\rgames were played by observing video images, which made a deep convolutional\rANN a natural choice for this collection of tasks. In addition, DQN��s\rperformance on some of the Atari 2600 games fell considerably short of human\rskill levels on these games. The games most difficult for DQN��especially\rMontezuma��s Revenge on which DQN learned to perform about as well as the random\rplayer��require deep planning beyond what DQN was designed to do. Further,\rlearning control skills through extensive practice, like DQN learned how to\rplay the Atari games, is just one of the types of learning humans routinely\raccomplish. Despite these limitations, DQN advanced the state-of- the-art in\rmachine learning by impressively demonstrating the promise of combining\rreinforcement learning with modern methods of deep learning.\n16.7\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rMastering the Game of Go\nThe ancient Chinese game of Go has challenged artificial\rintelligence researchers for many decades. Methods that achieve human-level\rskill, or even superhuman-level skill, in other games have not been successful\rin producing strong Go programs. Thanks to a very active community of Go\rprogrammers and international competi\u0026shy;tions, the level of Go program play has\rimproved significantly over the years. Until recently, however, no Go program\rhad been able to play anywhere near the level of a human Go master. Here we\rdescribe a program called AlphaGo developed by a Google DeepMind team (Silver\ret al., 2016) that broke this barrier by combin\u0026shy;ing deep artificial neural\rnetworks (deep ANNs, Section 9.6), supervised learning, Monte Carlo tree search\r(MCTS, Section 8.11), and reinforcement learning. By the time of Silver et\ral.��s 2016 publication, AlphaGo had been shown to be decisively stronger than\rother current Go programs, and it had defeated the human European Go champion 5\rgames to 0. These were the first victories of a Go program over a human professional\rGo player without handicap in full Go games. Shortly thereafter, AlphaGo went\ron to stunning victories over an 18-time world champion Go player, winning 4\rout of a 5 games in a challenge match, making worldwide headline news.\rArtificial intelligence researchers thought that it would be many more years,\rperhaps decades, for a program to reach this level of play.\nIn many ways, AlphaGo is a descendant of Tesauo��s\rTD-Gammon (Section 16.1), itself a descendant of Samuel��s checkers player\r(Section 16.2). AlphaGo, like these earlier programs, used reinforcement\rlearning with function approximation over many simulated games. AlphaGo also\rbuilt upon the progress made by Google DeepMind on playing Atari games with the\rprogram DQN (Section 16.6) by approximating value functions with deep\rconvolutional ANNs. By using a novel variant of MCTS, AlphaGo extended the\rtechnology responsible for the impressive gains of the most successful\rpreceding Go programs. But AlphaGo was not a simple combination of these\rtechnologies: it combined them in a highly-engineered way that was perhaps\rcritical for AlphaGo��s impressive performance. Another element of AlphaGo was\rits distributed architecture: many of its computations were executed in\rparallel on many processors so that it could select moves quickly enough to\rmeet the time constraints of live play. Although this contributed to AlphaGo��s\rsuccess, most of its playing skill was due to algorithmic innovations, and here\rwe neglect AlphaGo��s distributed architecture.\nGo is a game between two players who alternately\rplace black and white ��stones�� on unoccupied intersections, or ��points,�� on a\rboard with a grid of 19 horizontal and 19 vertical lines (Figure 16.10). The\rgame��s goal is to capture an area of the board larger than that captured by the\ropponent. Stones are captured according to simple rules. A player��s stones are\rcaptured if they are completely surrounded by the other player��s stones,\rmeaning that there is no horizontally or vertically adjacent point that is\runoccupied. For example, Figure 16.11 shows on the left three white stones with\ran unoccupied adjacent point (labeled X). If player black places a stone on X,\rthe three white stones are captured and taken off the board (Figure 16.11\rmiddle). However, if player white were to place a stone on point X first, than\rthe possibility of this capture would be blocked (Figure 16.11 right). Other\rrules are needed to prevent infinite capturing/re-capturing loops. The game\rends when neither player wishes to place another stone. These rules are simple,\rbut they produce a very complex game that has had wide appeal for thousands of\ryears.\n\r\nFigure 16.10: A Go board\rconfiguration.\n\r\r\r\r\r\u0026nbsp;\nMethods that produce strong play\rfor other games, such as chess, have not worked as well for Go. The search\rspace for Go is significantly larger than that of chess because Go has a larger\rnumber of legal moves per position than chess (^ 250 versus ��35) and Go games tend to involve more moves\rthan chess games (^ 150 versus ^ 80). But the size of the search space is not\rthe major factor that makes Go so difficult. Exhaustive search is infeasible\rfor both chess and Go, and Go on smaller\n_�s_\nFigure 16.11: Go capturing rule. Left: the three white stones are\rnot surrounded because point X is unoccupied. Middle: if black places a stone\ron X, the three white stones are captured and removed from the board. Right: if\rwhite places a stone on point X first, the capture is blocked.\nboards, e.g., 9 x 9, has proven to be exceedingly\rdifficult as well. Experts agree that the major stumbling block to creating\rstronger-than-amateur Go programs is the difficulty of defining an adequate\rposition evaluation function. A good evaluation function allows search to be\rtruncated at a feasible depth by providing relatively easy- to-compute\rpredictions of what deeper search would likely yield. According to Muller (2002): ��No simple yet reasonable evaluation function\rwill ever be found for Go.�� A major step forward was the introduction of MCTS\rto Go programs, which does not attempt to store an evaluation function, instead\revaluating moves at decision time by running many Monte Carlo simulations of\rentire games. The strongest programs at the time of AlphaGo��s development all\rused MCTS, but master-level skill remained elusive.\nThe main innovation of AlphaGo is\rits use of a variant of MCTS called ��asyn\u0026shy;chronous policy and value MCTS,�� or\rAPV-MCTS. APV-MCTS selects moves via basic MCTS as described in Section 8.11\rand illustrated in Figure 8.13, but with some twists in terms of how policies\rand value functions are computed and ultimately how each move is chosen. In\raddition to the tree policyfor traversing the search tree, and a rollout\rpolicy,pn, used in the Monte\rCarlo simulations, there is a supervised learning\rpolicy(SL policy), pa,\ra reinforcement learning policy(RL policy), pp, and a state-value function,v^. These policies and value function were all\rlearned off-line before actual play. The SL policy is used in the expansion\rphase of APV-MCTS iterations, the phase in which a node in the search tree is\rselected as a promising node from which to explore further. In contrast to\rbasic MCTS, which expands the selected node by choosing an unexplored action\rbased on stored action-values, APV- MCTS chooses an action according to prior\rprobabilities supplied by the SL-policy. Both the rollout policy and the SL\rpolicy were learned before play via supervised learning using large databases\rof expert human moves. The RL policy was learned via self-play reinforcement\rlearning and was used to derive the state-value function ��.Below we\rexplain how this was done.\nIn contrast to basic MCTS, which evaluates the\rexpanded node solely on the basis of the return of the simulation initiated\rfrom it, APV-MCTS evaluates the node in two ways: by this return of the\rsimulation, but also by the value function v: if s is the node selected for expansion, its value\rbecomes\nV\u0026nbsp; (s) = (1�� n)ve (s) + nG,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (16.4)\nwhere Gis the return of the simulation and n controls the mixing of the\rvalues\n\r\rluation methods.\nhe rollout policy,\rthe SL policy, the RL policy, and the Ns came into play. These policies were\rlearned by deep tively called the rollout policy network,the SL policy\rork, and the value network. Figure 16.12 illustrates the works in what the DeepMind team\rcalled the ��AlphaGo ained before any live game play took place and their ighout\rAlphaGo��s live play.\n-play process was\rto train the SL policy network. This ��s deep convolutional network described in\rSection 16.6 pt that it had 13 layers with the final layer consisting of a n\rthe 19 x 19 board. The network��s input was a 19 x 19 ch point on the Go board\rwas represented by the values ed features. For example, for each point, one\rfeature upied by one of AlphaGo��s stones, one of its opponent��s thus providing\rthe ��raw�� representation of the board es were based on the rules of Go, such as\rthe number empty, the number of opponent stones that would be here, the number\rof turns since a stone was placed there, at the design team considered to be\rimportant. The SL rvised learning to predict moves contained in a data base\rnoves. Its output, which was a probability distribution the SL policy that\rsupplied the action probabilities in -MCTS. Training took approximately 3 weeks\rusing a\n\r\r\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rdistributed implementation of stochastic gradient\rascent on 50 processors. The SL network achieved 57% accuracy, compared to best\raccuracy achieved by other groups at the time of publication of 44.4%.\nThe goal for the rollout policy\rnetwork was that it should select actions quickly while still being reasonably\raccurate. In principle, the SL policy could have served as the rollout policy,\rbut the forward propagation through the SL policy network took too much time\rfor this network to be used in rollout simulations, a great many of which had\rto be carried out for each move decision during live play. For this reason, the\rrollout policy network was less complex than the SL policy network, and its\rinput features could be computed more quickly than the features used for the SL\rpolicy network. This network was trained by supervised learning on a corpus of 8 million human moves. The resulting rollout policy\rnetwork allowed approximately 1,000complete game simulations per second to be run on\reach of the processing threads that AlphaGo used.\nNext, the value function, ,\rneeded by APV-MCTS had to be created. Ideally, would be the optimal state-value\rfunction. It might have been possible to approx\u0026shy;imate the optimal value\rfunction along the lines of TD-Gammon described above: self-play with nonlinear\rTD(A) coupled to a deep convolutional ANN. But the Deep- Mind team took a\rdifferent approach that held more promise for a game as complex as Go. They\rdivided the process into two stages. In the first stage, they created the best\rpolicy they could by training a deep convolutional network by means of a\rpolicy-gradient reinforcement learning algorithm. This resulted in the RL\rpolicy network implementing the RL policy pp.\nThe RL policy network had the\rsame architecture as the SL policy network and was initialized with the final\rweights of the SL policy network that were learned via supervised learning.\rReinforcement learning was then used to improve upon the SL policy. Learning\rwas by means of simulated games between the network��s current policy and\ropponents using policies randomly selected from policies produced by earlier\riterations of the learning algorithm. Playing against a randomly selected\rcollection of opponents prevented overfitting to the current policy. The reward\rsignal was +1 if the current policy won, ��1 if it lost, and zero otherwise.\rThese games directly pitted two policies against one another without involving\rany search. By simulating many games in parallel on 50 processors, the DeepMind\rteam trained the RL policy network on a million games in a single day. In\rtesting the final RL policy Pp, they found that it won more than 80% of games\rplayed against the SL policy , and it won 85% of games played against a Go\rprogram using Monte Carlo search that simulated 100,000games per move.\nFinally, the value function v^\rwas learned by the value network: a deep convolu\u0026shy;tional ANN whose structure was\rlike that of the SL and RL policy networks except that its single output unit\rproduced state values (the right-most network in Fig\u0026shy;ure 16.12) instead of\rprobability distributions over legal actions. The value network was trained by\rMonte Carlo policy evaluation on data obtained from a large number of simulated\rgames in which each player used the RL policy pp. The team found\rthat the values produced by this network were more accurate than values\rproduced by multiple simulations using the rollout policy pn, and in fact\rcompared well with val\u0026shy;ues produced by simulations using the higher-performing\rRL policy ppһeven though they could be computed 15,000 times more\rquickly.\nOne may wonder why APV-MCTS used\rthe SL policy pa instead of the better RL policy pp to select\ractions in the expansion phase. These policies took the same amount of time to\rcompute since they used the same network architecture. The team actually found\rthat AlphaGo worked better against human opponents when APV-MCTS used as its SL\rpolicy pa instead of pp. They conjectured that the reason for this\rwas that the pp was tuned to respond to optimal moves rather than to the\rbroader set of moves characteristic of human play. Interestingly, the situation\rwas reversed for the value function, v^, used by APV-MCTS. They found that when\rAPV-MCTS used the value function derived from the RL policy, it performed\rbetter than if it used the value function derived from the SL policy.\nSeveral methods worked together\rto produce AlphaGo��s impressive playing skill. The DeepMind team evaluated\rdifferent versions of AlphaGo in order to asses the contributions made by these\rvarious components. The parameter n in (16.4) controls the mixing of game state\revaluations produced by the value network and by rollouts. With n = 0, AlphaGo\rused just the value network without rollouts, and with n = 1, evaluation relied\rjust on rollouts. They found that AlphaGo using just the value network played\rbetter than the rollout-only AlphaGo, and in fact played better than the\rstrongest of all other Go programs. The best play resulted from setting n =\r0.5, indicating that combining the value network with rollouts was particularly\rimportant to AlphaGo��s success. These evaluation methods complemented one\ranother: the value network evaluated the high-performance policy pp that was\rtoo slow to be used in live play, while rollouts using the weaker but much\rfaster rollout policy p^ were able to add precision to the value network��s\revaluations for specific states that occurred during games.\nOverall, AlphaGo��s remarkable success helped fuel a\rnew round of enthusiasm for the promise of artificial intelligence,\rspecifically for systems combining reinforcement learning with deep ANNs, to\raddress problems in many other challenging domains.\n16.8\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rPersonalized Web Services\nPersonalizing web services such as the delivery of\rnews articles or advertisements is one approach to increasing users��\rsatisfaction with a website or to increase the yield of a marketing campaign. A\rpolicy can recommend content considered to be the best for each particular user\rbased on a profile of that user��s interests and preferences inferred from their\rhistory of online activity. This is a natural domain for machine learning, and\rin particular, for reinforcement learning. A reinforcement learning system can\rimprove a recommendation policy by making adjustments in response to user\rfeedback. One way to obtain user feedback is by means of website satisfaction\rsurveys, but for acquiring feedback in real time it is common to monitor user\rclicks as indicators of interest in a link.\nA method long used in marketing\rcalled A/B testingis a simple type of reinforce\u0026shy;ment learning used to decide which of two\rversions, A or B, of a website users prefer. Because it is non-associative,\rlike a two-armed bandit problem, this approach does not personalize content\rdelivery. Adding context consisting of features describing individual users and\rthe content to be delivered allows personalizing service. This has been\rformalized as a contextual bandit problem (or an associative reinforcement\rlearning problem, Section 2.9) with the objective of maximizing the total\rnumber of user clicks. Li, Chu, Langford, and Schapire (2010) applied a\rcontextual bandit al\u0026shy;gorithm to the problem of personalizing the Yahoo! Front\rPage Today webpage (one of the most visited pages on the internet at the time\rof their research) by selecting the news story to feature. Their objective was\rto maximize the click-through rate (CTR), which is the ratio of the total number of\rclicks all users make on a webpage to the total number of visits to the page.\rTheir contextual bandit algorithm improved over a standard non-associative\rbandit algorithm by 12.5%.\nTheocharous, Thomas, and\rGhavamzadeh (2015) argued that better results are possible by formulating\rpersonalized recommendation as a Markov decision problem (MDP) with the\robjective of maximizing the total number of clicks users make over repeated\rvisits to a website. Policies derived from the contextual bandit formulation\rare greedy in the sense that they do not take long-term effects of actions into\raccount. These policies effectively treat each visit to a website as if it were\rmade by a new visitor uniformly sampled from the population of the website��s\rvisitors. By not using the fact that many users repeatedly visit the same\rwebsites, greedy policies do not take advantage of possibilities provided by\rlong-term interactions with individual users.\nAs an example of how a marketing\rstrategy might take advantage of long-term user interaction, Theocharous et al.\rcontrasted a greedy policy with a longer-term policy for displaying ads for\rbuying a product, say a car. The ad displayed by the greedy policy might offer\ra discount if the user buys the car immediately. A user either takes the offer\ror leaves the website, and if they ever return to the site, they would likely\rsee the same offer. A longer-term policy, on the other hand, can transition the\ruser ��down a sales funnel�� before presenting the final deal. It might start by describing\rthe availability of favorable financing terms, then praise an excellent service\rdepartment, and then, on the next visit, offer the final discount. This type of\rpolicy can result in more clicks by a user over repeated visits to the site,\rand if the policy is suitably designed, more eventual sales.\nWorking at Adobe Systems\rIncorporated, Theocharous et al. conducted experi\u0026shy;ments to see if policies\rdesigned to maximize clicks over the long term could in fact improve over\rshort-term greedy policies. The Adobe Marketing Cloud, a set of tools that many\rcompanies use to to run digital marketing campaigns, provides infrastruc\u0026shy;ture\rfor automating user-targed advertising and fund-raising campaigns. Actually\rdeploying novel policies using these tools entails significant risk because a\rnew policy may end up performing poorly. For this reason, the research team\rneeded to assess what a policy��s performance would be if it were to be actually\rdeployed, but to do so on the basis of data collected under the execution of\rother policies. A critical aspect of this research, then, was off-policy\revaluation. Further, the team wanted to do this with high confidence to reduce\rthe risk of deploying a new policy. Although high confidence off-policy\revaluation was a central component of this research (see also Thomas, 2015;\rThomas, Theocharous, and Ghavamzadeh, 2015), here we focus only on the\ralgorithms and their results.\nTheocharous et al. compared the\rresults of two algorithms for learning ad recom\u0026shy;mendation policies. The first\ralgorithm, which they called greedy optimization,had the goal of maximizing only the probability of\rimmediate clicks. As in the standard contextual bandit formulation, this\ralgorithm did not take the long-term effects of recommendations into account.\rThe other algorithm, a reinforcement learning algo\u0026shy;rithm based on an MDP\rformulation, aimed at improving the number of clicks users made over multiple\rvisits to a website. They called this latter algorithm life-time\rvalue(LTV) optimization. Both\ralgorithms faced challenging problems because the reward signal in this domain\ris very sparse since users usually do not click on ads, and user clicking is\rvery random so that returns have high variance.\nData sets from the banking\rindustry were used for training and testing these algo\u0026shy;rithms. The data sets\rconsisted of many complete trajectories of customer interaction with a bank��s\rwebsite that showed each customer one out of a collection of possible offers.\rIf a customer clicked, the reward was 1, and otherwise it was 0. One data set\rcontained approximately 200,000interactions from a month of a bank��s campaign that\rrandomly offered one of 7 offers. The other data set from another bank��s cam\u0026shy;paign\rcontained 4,000,000 interactions involving 12possible offers. All interactions included customer\rfeatures such as the time since the customer��s last visit to the website, the\rnumber of their visits so far, the last time the customer clicked, geo\u0026shy;graphic\rlocation, one of a collection of interests, and features giving demographic\rinformation.\nGreedy optimization was based on\ra mapping estimating the probability of a click as a function of user features.\rThe mapping was learned via supervised learning from one of the data sets by\rmeans of a random forest (RF) algorithm (Breiman, 2001). RF algorithms have\rbeen widely used for large-scale applications in industry because they are\reffective predictive tools that tend not to overfit and are relatively\rinsensitive to outliers and noise. Theocharous et al. then used the mapping to\rdefine an e-greedy policy that selected with probability 1-e the offer\rpredicted by the RF algorithm to have the highest probability of producing a\rclick, and otherwise selected from the other offers uniformly at random.\nLTV optimization used a\rbatch-mode reinforcement learning algorithm called fitted\rQ iteration(FQI). It is a\rvariant of fitted value iteration(Gordon, 1999) adapted to Q-learning. Batch mode means that the\rentire data set for learning is available from the start, as opposed to the\ron-line mode of the algorithms we focus on in this book in which data are\racquired sequentially while the learning algorithm executes. Batch-mode\rreinforcement learning algorithms are sometimes necessary when on\u0026shy;line learning\ris not practical, and they can use any batch-mode supervised learning\rregression algorithm, including algorithms known to scale well to high-dimensional\rspaces. The convergence of FQI depends on properties of the function\rapproximation \n\r\ralgorithm (Gordon, 1999). For\rtheir application to LTV optimi et al. used the same RF algorithm they used for\rthe greedy opt Since in this case FQI convergence is not monotonic, Theochar of\rthe best FQI policy by off-policy evaluation using a validati( final policy for\rtesting the LTV approach was the e-greedy poli( policy produced by FQI with the\rinitial action-value function produced by the RF for the greedy optimization\rapproach.\nTo measure the performance of the\rpolicies produced by the proaches, Theocharous et al. used the CTR metric and a\rmetric metric. These metrics are similar, except that the LTV metric cr between\rindividual website visitors:\nTotal # of Clicks\nCTR\nTotal \u0026#8226;of Visits5Total # of Clicks\nLTV\nTotal # of Visitors Figure 16.13 illustrates how\rthese metrics differ. Each circle re to the site; black circles are visits at\rwhich the user clicks. Each by a particular user. By not distinguishing between\rvisitors, sequences is 0.35, whereas the LTV is 1.5. Because LTV is larg extent\rthat individual users revisit the site��it is an\rindicator policy is in encouraging users to engage in extended interaction\nVisit 1 Visit\r2 Visit 3 Visit 4 Visit 5\n\r\r\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rFigure 16.13: Click through rate (CTR) versus life-time value (LTV).\ra user visit; black circles are visits at which the user clicks. Adapted al.\r(2015) permission pending.\nTesting the policies produced by\rthe greedy and LTV appr ing a high confidence off-policy evaluation method on a\rtest of real-world interactions with a bank website served by a ran pected,\rresults showed that greedy optimization performed b the CTR metric, while LTV\roptimization performed best as m metric. Furthermoreһalthough we\rhave omitted its details�� off-policy evaluation method provided probabilistic\rguarantees mization method would, with high probability, produce policie\rpolicies currently deployed. Assured by these probabilistic gu nounced in 2016\rthat the new LTV algorithm would be a standa\n\r\rAdobe Marketing Cloud so that a retailer could issue a sequence of\roffers following a policy likely to yield higher return than a policy that is\rinsensitive to long-term results.\n16.9\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\rThermal Soaring\nBirds and gliders take advantage of upward air\rcurrents��thermals��to gain altitude in order maintain flight while expending\rlittle, or no, energy. Thermal soaring, as this behavior is called, is a\rcomplex skill requiring responding to subtle environmental cues to increase\raltitude by exploiting a rising column of air for as long as possi\u0026shy;ble. Reddy,\rCelani, Sejnowski, and Vergassola (2016) used reinforcement learning to\rinvestigate thermal soaring policies that are effective in the strong\ratmospheric tur\u0026shy;bulence usually accompanying rising air currents. Their primary\rgoal was to provide insight into the cues birds sense and how they use them to\rachieve their impressive thermal soaring performance, but the results also\rcontribute to technology relevant to autonomous gliders. Reinforcement learning\rhad previously been applied to the problem of navigating efficiently to the\rvicinity of a thermal updraft (Woodbury, Dunn, and Valasek, 2014) but not to\rthe more challenging problem of soaring within the turbulence of the updraft\ritself.\nReddy et al. modeled the soaring\rproblem as an MDP. The agent interacted with a detailed model of a glider\rflying in turbulent air. They devoted significant effort toward making the\rmodel generate realistic thermal soaring conditions, including investigating\rseveral different approaches to atmospheric modeling. For the learn\u0026shy;ing\rexperiments, air flow in a three-dimensional box with one kilometer sides, one\rof which was at ground level, was modeled by a sophisticated physics-based set\rof partial differential equations involving air velocity, temperature, and\rpressure. Intro\u0026shy;ducing small random perturbations into the numerical simulation\rcaused the model to produce analogs of thermal updrafts and accompanying\rturbulence (Figure 16.14 Left) Glider flight was modeled by aerodynamic\requations involving velocity, lift, drag, and other factors governing powerless\rflight of a fixed-wing aircraft. Maneu\u0026shy;vering the glider involved changing its\rangle of attack (the angle between the glider��s wing and the direction of air\rflow) and its bank angle (Figure 16.14 Right).\nThe interface between the agent\rand the environment required defining the agent��s actions, the state\rinformation the agent receives from the environment, and the reward signal. By\rexperimenting with various possibilities, Reddy et al. decided that three\ractions each for the angle of attack and the bank angle were enough for their\rpurposes: increment or decrement the current bank angle and angle of attack by\r5�� and 2.5��, respectively, or leave them unchanged. This resulted in 32possible actions. The bank angle was bounded to\rremain between -15�� and +15��.\nBecause a goal of their study was\rto try to determine what minimal set of sensory cues are necessary for\reffective soaring, both to shed light on the cues birds might use for soaring\rand to minimize the sensing complexity required for automated glider soaring,\rthe authors tried various sets of signals as input to the reinforcement\rlearning\n\rz\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Lift\u0026nbsp; L\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ,\n\nFigure 16.14: Thermal soaring\rmodel: Left: snapshot of the vertical velocity field of the simulated cube of\rair: in light (dark) grey is a region of large upward (downward) flow. Right:\rdiagram of powerless flight showing bank angle \\iand angle of attack a.Adapted with\rpermission From PNAS vol. 113(22), p. E4879, 2016, Reddy, Celani, Sejnowski,\rand Vergassola, Learning to Soar in Turbulent Environments.\n\r\r\r\r\r\u0026nbsp;\nagent. They started by using state aggregation\r(Chapter 9) of a four-dimensional state space with dimensions giving local\rvertical wind speed, local vertical wind acceleration, torque depending on the\rdifference between the vertical wind velocities at the left and right wing\rtips, and the local temperature. Each dimension was discretized into three\rbins: positive high, negative high, and small. Results, described below, showed\rthat only two of these dimensions were critical for effective soaring behavior.\nThe overall objective of thermal\rsoaring is to gain as much altitude as possible from each rising column of air.\rReddy et al. tried a straightforward reward signal that rewarded the agent at\rthe end of each episode based on the altitude gained over the episode, a large\rnegative reward signal if the glider touched the ground, and zero otherwise.\rThey found that learning was not successful with this reward signal for\repisodes of realistic duration and that eligibility traces did not help. By\rexperimenting with various reward signals, they found that learning was best\rwith a reward signal that at each time step linearly combined the vertical wind\rvelocity and vertical wind acceleration observed on the previous time step.\nLearning was by Sarsa with action\rselection using softmax applied to action values normalized to the interval\r[0,1]. The temperature parameter was initialized to 2.0 and incrementally\rdecreased to 0.2 during learning. The step-size and discount-rate parameters\rwere fixed at 0.1 and 0.98 respectively. Each learning episode took place with\rthe agent controlling simulated flight in an independently generated period of\rsimulated turbulent air currents. Each episode lasted 2.5 minutes simulated\rwith a 1 second time step. Learning effectively converged after a few hundred\repisodes. The left panel of Figure 16.15 shows a sample trajectory before\rlearning where the agent selects actions randomly. Starting at the top of the\rvolume shown, the glider��strajectory is in the direction indicated by the arrow and quickly loses\raltitude. Fig\u0026shy;ure 16.15��s right panel is a trajectory after learning. The\rglider starts at the same place (here appearing at the bottom of the volume)\rand gains altitude by spiraling within the rising column of air. Although Reddy\rat al. found that performance varied widely over different simulated periods of\rair flow, the number of times the glider touched the ground consistently\rdecreased to nearly zero as learning progressed.\n\r\r\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\rAfter experimenting with\rdifferent sets of features available to the learning agent, it turned out that\rthe combination of just vertical wind acceleration and torques worked best. The\rauthors conjectured that because these features give information about the\rgradient of vertical wind velocity in two different directions, they allow the\rcontroller to select between turning by changing the bank angle or continuing\ralong the same course by leaving the bank angle alone. This allows the glider\rto stay within a rising column of air. Vertical wind velocity is indicative of\rthe strength of the thermal but does not help in staying within the flow. They\rfound that sensitivity to temperature was of little help. They also found that\rcontrolling the angle of attack is not helpful in staying within a particular\rthermal, being useful instead for traveling between thermals when covering\rlarge distances, as in cross-country gliding and bird migration.\nSince soaring in different levels\rof turbulence requires different policies, training was done in conditions\rranging from weak to strong turbulence. In strong turbulence the rapidly changing\rwind and glider velocities allowed less time for the controller to react. This\rreduced the amount of control possible compared to what was possible for\rmaneuvering when fluctuations were weak. Reddy at al. examined the policies\nSarsa learned under these different conditions. Common to policies\rlearned in all regimes were these features: when sensing negative wind\racceleration, bank sharply in the direction of the wing with the higher lift;\rwhen sensing large positive wind acceleration and no torque, do nothing.\rHowever, different levels of turbulence led to policy differences. Policies\rlearned in strong turbulence were more conservative in that they preferred\rsmall bank angles, whereas in weak turbulence, the best ac\u0026shy;tion was to turn as\rmuch as possible by banking sharply. Systematic study of the bank angles\rpreferred by the policies learned under the different conditions led the\rauthors to suggest that by detecting when vertical wind acceleration crosses a\rcer\u0026shy;tain threshold the controller can adjust its policy to cope with different\rturbulence regimes.\nReddy et al. also conducted experiments to\rinvestigate the effect of the discount- rate parameter Y on the performance of\rthe learned policies. They found that the altitude gained in an episode increased\ras Y increased, reaching a maximum for\nY\u0026nbsp; = .99, suggesting that effective thermal soaring\rrequires taking into account long\u0026shy;term effects of control decisions.\nThis computational study of thermal soaring\rillustrates how reinforcement learning can further progress toward different\rkinds of objectives. Learning policies having access to different sets of\renvironmental cues and control actions contributes to both the engineering\robjective of designing autonomous gliders and the scientific objective of\rimproving understanding of the soaring skills of birds. In both cases,\rhypotheses resulting from the learning experiments can be tested in the field\rby instrumenting real gliders and by comparing predictions with observed bird\rsoaring behavior.\n\r\rChapter 17\nFrontiers\n\r\r478\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; CHAPTER\u0026nbsp; 17.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; FRONTIERS\nReferences\nAbbeel, P., and Ng, A. Y.\r(2004). Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international\rconference on Machine learning.ACM.\nAbramson, B. (1990).\rExpected-outcome: A general model of static evaluation. IEEE transactions on pattern analysis and\rmachine intelligence 12(2):182-193.\nAdams, C. (1982). Variations\rin the sensitivity of instrumental responding to reinforcer devaluation. The Quarterly Journal of Experimental\rPsychology,34(2):77-98.\nAdams, C. D. and Dickinson, A.\r(1981). Instrumental responding following reinforcer deval\u0026shy;uation. The Quarterly Journal of Experimental Psychology33(2):109-121.\nAdams, R. A., Huys, Q. J. M.,\rand Roiser, J. P. (2015). Computational Psychiatry: to\u0026shy;wards a mathematically\rinformed understanding of mental illness. Journal of Neurology, Neurosurgery \u0026amp; Psychiatry,doi:10.1136/jnnp-2015-310737.\nAgrawal, R. (1995). Sample\rmean based index policies with O(logn) regret for the multi\u0026shy;armed bandit\rproblem. Advances in Applied\rProbability,27:1054-1078.\nAgre, P. E. (1988). The Dynamic Structure of Everyday Life.Ph.D. thesis, Massachusetts Institute of Technology.\rAI-TR 1085, MIT Artificial Intelligence Laboratory.\nAgre, P. E., Chapman, D.\r(1990). What are plans for? Robotics\rand Autonomous Systems, 6:17-34.\nAizerman, M. A., Braverman, E. f., and Rozonoer, L. I. (1964).\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Probability\rproblem\nof\rpattern recognition learning and potential functions method.\u0026nbsp; Avtomat. i Telemekh\n25(9):1307-1323.\nAlbus, J. S. (1971). A theory of cerebellar\rfunction. Mathematical\rBiosciences,10:25-61.\nAlbus, J. S. (1981). Brain, Behavior, and Robotics.Byte Books, Peterborough, NH.\nAleksandrov, V. M., Sysoev, V.\rI., Shemeneva, V. V. (1968). Stochastic optimization of systems. Izv. Akad. Nauk SSSR, Tekh. Kibernetika,14-19.\nAmari, S. I. (1998). Natural gradient works efficiently in learning.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Neural Computation\n10(2), 251-276.\nAn, P.-C. E. (1991). An Improved Multi-dimensional CMAC Neural\rnetwork: Receptive Field Function and Placement(Doctoral dissertation, PhD Thesis, Dept.\rElectrical and Computer Engineering, New Hampshire Univ., New Hampshire, USA).\nAn, P. C. E., Miller, W. T.,\rParks, P. C. (1991). Design improvements in associative mem\u0026shy;ories for\rcerebellar model articulation controllers (CMAC). Artificial Neural Networks, pp. 1207-1210, Elsvier North-Holland.\nAnderson, C. W. (1986). Learning and Problem Solving with Multilayer\rConnectionist Sys\u0026shy;tems.Ph.D.\rthesis, University of Massachusetts, Amherst.\nAnderson,\rC. W. (1987). Strategy learning with multilayer connectionist representations.\n\r\rProceedings of\rthe Fourth International Workshop on Machine Learning,pp.\r103��114. Morgan Kaufmann, San Mateo, CA.\nAnderson, C. W. (1989). Learning\rto control an inverted pendulum using neural networks. IEEE\rControl Systems Magazine 9(3):31-37.\nAnderson, J. A., Silverstein, J.\rW., Ritz, S. A., Jones, R. S. (1977). Distinctive features, categorical\rperception, and probability learning: Some applications of a neural model. Psychological\rReview,84:413-451.\nAndreae, J. H. (1963). STELLA: A scheme for a learning machine. In Proceedings\rof the 2nd IFAC Congress, Basle,pp. 497-502. Butterworths, London.\nAndreae, J. H. (1969a). A\rlearning machine with monologue. International Journal of\rMan-Machine Studies,1:1-20.\nAndreae, J. H. (1969b). Learning\rmachines��a unified view. In A. R. Meetham and R. A. Hudson (eds.), Encyclopedia\rof Information, Linguistics, and Control,pp. 261\u0026shy;270. Pergamon, Oxford.\nAndreae, J. H. (1977). Thinking\rwith the Teachable Machine.Academic Press, London.\nArthur, W. B. (1991). Designing\reconomic agents that act like human agents: A behavioral approach to bounded\rrationality. The American Economic Review 81(2):353-359.\nAtkeson, C. G. (1992).\rMemory-based approaches to approximating continuous functions. In Sante\rFe Institute Studies in the Sciences of Complexity, Proceedings Vol. 12, pp. 521\u0026shy;521. Addison-Wesley\rPublishing Co.\nAtkeson, C. G., Moore, A. W., and\rSchaal, S. (1997). Locally weighted learning. Artificial\rIntelligence Review 11:11һ73.\nAuer, P.,\rCesa-Bianchi, N., Fischer, P. (2002). Finite-time analysis of the multiarmed\rbandit problem. Machine learning, 47(2-3):235-256.\nBae, J., Chhatbar, P.,\rFrancis, J. T., Sanchez, J. C., and Principe, J. C. (2011). Reinforce\u0026shy;ment\rlearning via kernel temporal difference. In Annual\rInternational Conference of the IEEE Engineering in Medicine and Biology\rSociety, pp. 5662-5665. IEEE.\nBaird, L. C. (1995).\rResidual algorithms: Reinforcement learning with function approxi\u0026shy;mation. In Proceedings\rof the Twelfth International Conference on Machine Learning, pp. 30-37. Morgan Kaufmann, San Francisco.\nBaird, L. C., and Klopf, A. H.\r(1993). Reinforcement learning with high-dimensional, con\u0026shy;tinuous actions.\rWright Laboratory, Wright-Patterson Air Force Base, Tech. Rep. WL- TR-93-1147.\nBaird, L., Moore, A. W. (1999).\rGradient descent for general reinforcement learning. Ad\u0026shy;vances\rin Neural Information Processing Systems,pp. 968-974.\nBaldassarre, G. and Mirolli, M.,\reditors (2013). Intrinsically Motivated Learning in Natural\rand Artificial Systems.\rSpringer-Verlag, Berlin.\nBalke, A., Pearl, J. (1994).\rCounterfactual probabilities: Computational methods, bounds and applications.\rIn Proceedings of the Tenth International Conference on\rUncertainty in Artificial Intelligence(pp. 46-54). Morgan Kaufmann.\nBao, G., Cassandras, C. G.,\rDjaferis, T. E., Gandhi, A. D., Looze, D. P. (1994). Elevator dispatchers for\rdown peak traffic. Technical report. ECE Department, University of\rMassachusetts, Amherst.\nBaras, D. and Meir, R. (2007).\rReinforcement learning, spike-time-dependent plasticity, and the BCM rule. Neural\rComputation, 19(8):2245-2279.\nBarnard, E. (1993). Temporal-difference methods and\rMarkov models. IEEE Transactions on Systems, Man,\rand Cybernetics23:357-365.\nBarnhill,\rR. E. (1977). Representation and approximation of surfaces. Mathematical Software\n3:69-120.\nBarreto, A. S., Precup, D.,\rand Pineau, J. (2011). Reinforcement learning using kernel- based stochastic\rfactorization. In Advances\rin Neural Information Processing Systems, pp. 720-728.\nBartlett, P. L. and Baxter, J.\r(1999). Hebbian synaptic modifications in spiking neurons that learn. Technical\rreport, Research School of Information Sciences and Engineering, Australian\rNational University.\nBartlett, P. L. and Baxter, J.\r(2000). A biologically plausible and locally optimal learning algorithm for\rspiking neurons. Rapport technique, Australian National University.\nBarto, A. G. (1985). Learning\rby statistical cooperation of self-interested neuron-like com\u0026shy;puting elements. Human Neurobiology,4:229-256.\nBarto, A. G. (1986).\rGame-theoretic cooperativity in networks of self-interested units. In J. S.\rDenker (ed.), Neural\rNetworks for Computing,pp.\r41-46. American Institute of Physics, New York.\nBarto, A. G. (1989). From\rchemotaxis to cooperativity: Abstract exercises in neuronal learning\rstrategies. In Durbin, R., Maill, R., and Mitchison, G., editors, The Computing Neuron, pages 73-98. Addison-Wesley, Reading, MA.\nBarto, A. G. (1990).\rConnectionist learning for control: An overview. In T. Miller, R. S. Sutton,\rand P. J. Werbos (eds.), Neural\rNetworks for Control,pp.\r5-58. MIT Press, Cambridge, MA.\nBarto, A. G.\r(1991). Some learning tasks from a control perspective. In L. Nadel and D. L.\rStein (eds.), 1990 Lectures\rin Complex Systems,pp.\r195-223. Addison-Wesley, Redwood City, CA.\nBarto, A. G. (1992).\rReinforcement learning and adaptive critic methods. In D. A. White and D. A.\rSofge (eds.), Handbook of\rIntelligent Control: Neural, Fuzzy, and Adaptive Approaches, pp. 469-491. Van Nostrand Reinhold, New York.\nBarto, A. G. (1995a). Adaptive\rcritics and the basal ganglia. In J. C. Houk, J. L. Davis, and D. G. Beiser\r(eds.), Models of\rInformation Processing in the Basal Ganglia,pp. 215-232. MIT Press, Cambridge, MA.\nBarto, A. G. (1995b).\rReinforcement learning. In M. A. Arbib (ed.), Handbook of Brain Theory and Neural Networks, pp. 804-809. MIT Press, Cambridge, MA.\nBarto, A. G.\r(2011). Adaptive real-time dynamic programming. In Sammut, C. and Webb, G. I.\r(Eds.) Encyclopedia of\rmachine learning,pp. 19-22.\rSpringer Science and Business Media.\nBarto, A. G. (2013). Intrinsic\rmotivation and reinforcement learning. In Intrinsically Moti\u0026shy;vated Learning in Natural and Artificial Systems,pp. 17-47. Springer Berlin Heidelberg\nBarto, A. G., Anandan, P.\r(1985). Pattern recognizing stochastic learning automata. IEEE Transactions on Systems, Man, and\rCybernetics,15:360-375.\nBarto, A. G., Anderson, C. W.\r(1985). Structural learning in connectionist systems. In Program of the Seventh Annual Conference of the\rCognitive Science Society,\rpp. 43-54.\nBarto, A. G., Anderson, C. W.,\rSutton, R. S. (1982). Synthesis of nonlinear control surfaces by a layered\rassociative search network. Biological\rCybernetics, 43:175-185.\nBarto, A. G., Bradtke, S. J.,\rSingh, S. P. (1991). Real-time learning and control using asynchronous dynamic\rprogramming. Technical Report 91-57. Department of Computer\nand Information Science,\rUniversity of Massachusetts, Amherst.\nBarto, A. G., Bradtke, S. J.,\rSingh, S. P. (1995). Learning to act using real-time dynamic programming. Artificial\rIntelligence,72:81-138.\nBarto, A. G., Duff, M. (1994).\rMonte Carlo matrix inversion and reinforcement learning. In J. D. Cohen, G.\rTesauro, and J. Alspector (eds.), Advances in Neural\rInformation Pro\u0026shy;cessing Systems: Proceedings of the 1993 Conference,pp. 687-694. Morgan Kaufmann, San Francisco.\nBarto, A. G., Jordan, M. I.\r(1987). Gradient following without back-propagation in layered networks. In M.\rCaudill and C. Butler (eds.), Proceedings of the IEEE\rFirst Annual Conference on Neural Networks, pp. II629-II636. SOS Printing, San Diego, CA.\nBarto, A. G., and Mahadevan, S.\r(2003). Recent advances in hierarchical reinforcement learning. Discrete\rEvent Dynamic Systems 13(4):341-379.\nBarto, A. G., Singh, S., and\rChentanez, N. (2004). Intrinsically motivated learning of hierarchical collections\rof skills. In International Conference on\rDevelopmental Learning (ICDL),LaJolla, CA.\nBarto, A. G., Sutton, R. S.\r(1981a). Goal seeking components for adaptive intelligence: An initial\rassessment. Technical Report AFWAL-TR-81-1070. Air Force Wright Aeronau\u0026shy;tical\rLaboratories/Avionics Laboratory, Wright-Patterson AFB, OH.\nBarto, A. G., Sutton, R. S.\r(1981b). Landmark learning: An illustration of associative search. Biological\rCybernetics,42:1-8.\nBarto, A. G., Sutton, R. S.\r(1982). Simulation of anticipatory responses in classical condi\u0026shy;tioning by a\rneuron-like adaptive element. Behavioural Brain\rResearch,4:221-235.\nBarto, A. G., Sutton, R. S.,\rAnderson, C. W. (1983). Neuronlike elements that can solve difficult learning\rcontrol problems. IEEE Transactions on Systems, Man,\rand Cybernet\u0026shy;ics,13:835-846. Reprinted in J. A. Anderson and E. Rosenfeld (eds.), Neurocomputing:\rFoundations of Research,pp. 535-549. MIT Press, Cambridge, MA, 1988.\nBarto, A. G., Sutton, R. S.,\rBrouwer, P. S. (1981). Associative search network: A reinforce\u0026shy;ment learning\rassociative memory. Biological Cybernetics, 40:201-211.\nBarto, A. G., Sutton, R. S., and\rWatkins, C. J. C. H. (1990). Learning and sequential decision making. In M.\rGabriel and J. Moore (eds.), Learning and\rComputational Neuroscience: Foundations of Adaptive Networks, pp. 539-602. MIT Press, Cambridge, MA.\nBellemare, M. G.,\rNaddaf, Y., Veness, J., and Bowling, M. (2012a). The arcade learning\renvironment: An evaluation platform for general agents. Journal\rof Artificial Intelligence Research, 47:253-279.\nBellemare, M. G.,\rVeness, J., and Bowling, M. (2012b). Investigating contingency aware\u0026shy;ness using\rAtari 2600 games. In Proceedings of the Twenty-Sixth AAAI\rConference on Artificial Intelligence (AAAI 2012),pages 864-871, Palo Alto, CA. The AAAI Press.\nBellman, R. E. (1956).\rA problem in the sequential design of experiments. Sankhya, 16:221-229.\nBellman, R. E. (1957a). Dynamic\rProgramming.Princeton University Press, Princeton.\nBellman, R. E. (1957b). A Markov\rdecision process. Journal of Mathematical Mechanics, 6:679-684.\nBellman, R. E., Dreyfus, S. E.\r(1959). Functional approximations and dynamic program\u0026shy;ming. Mathematical\rTables and Other Aids to Computation,13:247-251.\nBellman, R. E., Kalaba, R.,\rKotkin, B. (1973). Polynomial approximation��A new com\u0026shy;putational technique in\rdynamic programming: Allocation processes. Mathematical\nComputation,17:155-161.\nBengio, Y. (2009). Learning deep\rarchitectures for AI. Foundations and Trends in Machine\rLearning,2(1):1-27.\nBengio, Y., Courville, A. C., and\rVincent, P. (2012). Unsupervised feature learning and deep learning: A review\rand new perspectives. CoRR 1, arXiv 1206.5538.\nBentley, J. L. (1975).\rMultidimensional binary search trees used for associative searching. Communications\rof the ACM 18(9):509-517.\nBerg, H. C. (1975).\rChemotaxis in bacteria. Annual review of biophysics and\rbioengineering, 4(1):119-136.\nBernoulli, D. (1954). Exposition\rof a new theory on the measurement of risk. Econometrica,\r22(1):23-36. English translation\rof the 1738 paper.\nBerns, G. S., McClure,\rS. M., Pagnoni, G., and Montague, P. R. (2001). Predictability modulates human\rbrain response to reward. The journal of neuroscience,21(8):2793- 2798.\nBerridge, K. C. and\rKringelbach, M. L. (2008). Affective neuroscience of pleasure: reward in humans\rand animals. Psychopharmacology,199(3):457-480.\nBerridge, K. C. and Robinson, T.\rE. (1998). What is the role of dopamine in reward: hedonic impact, reward\rlearning, or incentive salience? Brain Research Reviews, 28(3):309-369.\nBerry, D. A., Fristedt, B. (1985). Bandit\rProblems.Chapman\rand Hall, London.\nBertsekas, D. P. (1982).\rDistributed dynamic programming. IEEE Transactions on Auto\u0026shy;matic\rControl,27:610-616.\nBertsekas, D. P. (1983).\rDistributed asynchronous computation of fixed points. Mathemat\u0026shy;ical\rProgramming,27:107-120.\nBertsekas, D. P. (1987). Dynamic\rProgramming: Deterministic and Stochastic Models. Prentice-Hall, Englewood Cliffs, NJ.\nBertsekas, D. P. (2005). Dynamic\rProgramming and Optimal Control, Volume 1,third edition. Athena Scientific, Belmont, MA.\nBertsekas,\rD. P. (2012). Dynamic Programming\rand Optimal Control, Volume 2: Approxi\u0026shy;mate Dynamic Programming,\rfourth edition. Athena Scientific, Belmont, MA.\nBertsekas, D. P.\r(2013). Rollout algorithms for discrete optimization: A survey. In Handbook\rof Combinatorial Optimization,pp. 2989-3013. Springer New York.\nBertsekas, D. P.,\rTsitsiklis, J. N. (1989). Parallel and Distributed Computation:\rNumerical Methods.Prentice-Hall, Englewood Cliffs, NJ.\nBertsekas, D. P.,\rTsitsiklis, J. N. (1996). Neuro-Dynamic Programming.Athena Scientific, Belmont, MA.\nBertsekas, D. P., Tsitsiklis, J.\rN., and Wu, C. (1997). Rollout algorithms for combinatorial optimization. Journal\rof Heuristics 3(3):245-262.\nBertsekas, D. P., Yu, H. (2009).\rProjected equation methods for approximate solution of large linear systems. Journal\rof Computational and Applied Mathematics,227(1):27-50.\nBhat, N., Farias, V., and Moallemi,\rC. C. (2012). Non-parametric approximate dynamic programming via the kernel\rmethod. In Advances in Neural Information\rProcessing Systems, pp. 386-394.\nBhatnagar, S., Sutton, R.,\rGhavamzadeh, M., Lee, M. (2009). Natural actor-critic algo\u0026shy;rithms. Automatica\r45(11).\nBiermann, A. W., Fairfield, J. R.\rC., Beres, T. R. (1982). Signature table systems and\nlearning. IEEE Transactions on Systems, Man, and Cybernetics, 12:635-648.\nBishop, C.\rM. (1995). Neural Networks\rfor Pattern Recognition.Clarendon, Oxford.\nBishop, C. M. (2006). Pattern Recognition and Machine Learning.Springer.\nBlodgett, H. C. (1929). The\reffect of the introduction of reward upon the maze performance of rats. University of California Publications in\rPsychology,4:113-134.\nBoakes, R. A. and Costa, D. S.\rJ. (2014). Temporal contiguity in associative learning: Iinter- ference and\rdecay from an historical perspective. Journal of Experimental Psychology: Animal Learning and Cognition,40(4):381-400.\nBooker, L. B. (1982). Intelligent Behavior as an Adaptation to the\rTask Environment. Ph.D.\rthesis, University of Michigan, Ann Arbor.\nBoone, G. (1997). Minimum-time\rcontrol of the acrobot. In 1997\rInternational Conference on Robotics and Automation, pp. 3281-3287. IEEE Robotics and Automation\rSociety.\nBottou, L., and Vapnik, V.\r(1992). Local learning algorithms. Neural Computation 4(6):888- 900.\nBoutilier, C., Dearden, R.,\rGoldszmidt, M. (1995). Exploiting structure in policy con\u0026shy;struction. In Proceedings of the Fourteenth International\rJoint Conference on Artificial Intelligence,pp. 1104-1111. Morgan Kaufmann.\nBoyan, J. A., (1999).\rLeast-squares temporal difference learning. International Conference on Machine Learning 16,pp. 49-56.\nBoyan, J. (2002). Technical\rupdate: Least-squares temporal difference learning. Machine Learning49:233-246.\nBoyan, J. A., Moore, A. W.\r(1995). Generalization in reinforcement learning: Safely ap\u0026shy;proximating the\rvalue function. In G. Tesauro, D. S. Touretzky, and T. Leen (eds.), Advances in Neural Information Processing\rSystems: Proceedings of the 1994 Confer\u0026shy;ence, pp. 369-376. MIT Press, Cambridge, MA.\nBradtke, S. J.\r(1993). Reinforcement learning applied to linear quadratic regulation. In S. J.\rHanson, J. D. Cowan, and C. L. Giles (eds.), Advances in Neural Information Pro\u0026shy;cessing Systems: Proceedings of\rthe 1992 Conference,pp.\r295-302. Morgan Kaufmann, San Mateo, CA.\nBradtke, S. J.\r(1994). Incremental Dynamic\rProgramming for On-Line Adaptive Optimal Control.Ph.D. thesis, University of Massachusetts,\rAmherst. Appeared as CMPSCI Technical Report 94-62.\nBradtke, S. J., Barto, A. G.\r(1996). Linear least-squares algorithms for temporal difference learning. Machine Learning, 22:33-57.\nBradtke, S. J.,\rYdstie, B. E., Barto, A. G. (1994). Adaptive linear quadratic control using\rpolicy iteration. In Proceedings\rof the American Control Conference, pp. 3475-3479. American Automatic Control Council, Evanston, IL.\nBradtke, S. J., Duff, M. O.\r(1995). Reinforcement learning methods for continuous-time Markov decision\rproblems. In G. Tesauro, D. Touretzky, and T. Leen (eds.), Advances in Neural Information Processing\rSystems: Proceedings of the 1994 Conference,pp. 393\u0026shy;400. MIT Press, Cambridge, MA.\nBrafman, R. I., Tennenholtz,\rM. (2003). R-max - a general polynomial time algorithm for near-optimal\rreinforcement learning. Journal\rof Machine Learning Research,3, 213-231.\nBreiter, H. C., Aharon, I.,\rKahneman, D., Dale, A., and Shizgal, P. (2001). Functional imaging of neural\rresponses to expectancy and experience of monetary gains and losses. Neuron,30(2):619-639.\nBreland, K. and Breland, M.\r(1961). The misbehavior of organisms. American Psychologist, 16(11):681-684.\nBridle, J. S. (1990). Training\rstochastic model recognition algorithms as networks can lead to maximum mutual\rinformation estimates of parameters. In D. S. Touretzky (ed.), Advances\rin Neural Information Processing Systems: Proceedings of the 1989 Conference, pp. 211-217. Morgan Kaufmann, San Mateo, CA.\nBroomhead, D. S., Lowe, D.\r(1988). Multivariable functional interpolation and adaptive networks. Complex\rSystems,2:321-355.\nBromberg-Martin, E. S.,\rMatsumoto, M., Hong, S., and Hikosaka, O. (2010). A pallidus- habenula-dopamine\rpathway signals inferred stimulus values. Journal of Neurophysiol\u0026shy;ogy,104(2):1068-1076.\nBrowne, C.B., Powley, E.,\rWhitehouse, D., Lucas, S.M., Cowling, P.I., Rohlfshagen, P., Tavener, S.,\rPerez, D., Samothrakis, S. and Colton, S. (2012). A survey of monte carlo tree\rsearch methods. IEEE Transactions on Computational\rIntelligence and AI in Games\n4(1):1-43.\nBrown, J., Bullock,\rD., and Grossberg, S. (1999). How the basal ganglia use parallel exci\u0026shy;tatory\rand inhibitory learning pathways to selectively respond to unexpected rewarding\rcues. The Journal of Neuroscience,19(23):10502-10511.\nBryson, A. E., Jr. (1996).\rOptimal control��1950 to 1985. IEEE Control Systems,13(3):26- 33.\nBuchanan, B. G., Mitchell, T.,\rSmith, R. G., and Jr., C. R. J. (1978). Models of learning\nsystems. Encyclopeadia of Computer Science and technology,\r11.\nBuhusi, C. V. and\rSchmajuk, N. A. (1999). Timing in simple conditioning and occasion setting: A\rneural network approach. Behavioural processes, 45(1):33-57.\nBurke, C. J., Dreher, J.-C.,\rSeymour, B., and Tobler, P. N. (2014). State-dependent value representation:\revidence from the stiatum. Frontiers in Neuroscience, 8.\nBusoniu, L., Lazaric,\rA., Ghavamzadeh, M., Munos, R., Babuska, R., and De Schutter, B.\n(2012)\u0026nbsp;\r. Least-squares\rmethods for policy iteration. In Wiering and van Otterlo (Eds.) Reinforcement\rLearning: State-of-the Art,pp. 75-109. Springer Berlin Heidelberg.\nBush, R. R., Mosteller, F. (1955). Stochastic\rModels for Learning.Wiley, New York.\nByrne, J. H.,\rGingrich, K. J., Baxter, D. A. (1990). Computational capabilities of single neu\u0026shy;rons:\rRelationship to simple forms of associative and nonassociative learning in aplysia.\rIn R. D. Hawkins and G. H. Bower\r(eds.), Computational Models of Learning,pp. 31-63. Academic Press, New York.\nCalabresi, P.,\rPicconi, B., Tozzi, A., and Filippo, M. D. (2007). Dopamine-mediated regu\u0026shy;lation\rof corticostriatal synaptic plasticity. Trends in Neuroscience, 30(5):211-219.\nCamerer,\rC. (2003). Behavioral game\rtheory: Experiments in strategic interaction.Princeton\rUniversity Press.\nCampbell, D. T.\r(1960). Blind variation and selective survival as a general strategy in\rknowledge-processes. In M. C. Yovits and S. Cameron (eds.), Self-Organizing\rSystems, pp. 205-231. Pergamon,\rNew York.\nCao, X. R. (2009). Stochastic\rlearning and optimization��A sensitivity-based approach. Annual\rReviews in Control 33(1):11-24.\nCarlstrom, J.,\rNordstrom, E. (1997). Control of self-similar ATM call traffic by reinforce\u0026shy;ment\rlearning. In Proceedings of the International\rWorkshop on Applications of Neural Networks to Telecommunications 3,pp. 54-62. Erlbaum, Hillsdale, NJ.\nChapman, D., Kaelbling, L. P. (1991). Input generalization in delayed reinforcement learn\u0026shy;ing: An\ralgorithm and performance comparisons. In Proceedings of the\rTwelfth In\u0026shy;ternational Conference on Artificial Intelligence, pp. 726-731. Morgan\rKaufmann, San Mateo, CA.\nChow, C.-S., Tsitsiklis, J. N.\r(1991). An optimal one-way multigrid algorithm for discrete\u0026shy;time stochastic\rcontrol. IEEE Transactions on Automatic Control, 36:898-914.\nChrisman, L. (1992).\rReinforcement learning with perceptual aliasing: The perceptual distinctions\rapproach. In Proceedings of the Tenth National\rConference on Artificial Intelligence,pp. 183-188. AAAI/MIT Press, Menlo Park, CA.\nChristensen, J., Korf,\rR. E. (1986). A unified theory of heuristic evaluation functions and its\rapplication to learning. In Proceedings of the Fifth\rNational Conference on Artificial Intelligence,pp. 148-152. Morgan Kaufmann, San Mateo, CA.\nCichosz, P. (1995). Truncating\rtemporal differences: On the efficient implementation of TD(A) for\rreinforcement learning. Journal of Artificial Intelligence\rResearch,2:287-318.\nClaridge-Chang, A., Roorda, R.\rD., Vrontou, E., Sjulson, L., Li, H., Hirsh, J., and Miesenbock,\nG.\u0026nbsp;\r(2009). Writing\rmemories with light-addressable reinforcement circuitry.Cell,\n139(2):405-415.\nClark, R. E. and\rSquire, L. R. (1998). Classical conditioning and brain systems: the role of\rawareness. Science, 280(5360):77-81.\nClark, W. A., Farley, B. G.\r(1955). Generalization of pattern recognition in a self-organizing system. In Proceedings\rof the 1955 Western Joint Computer Conference,pp. 86-91.\nClouse,\rJ. (1996). On Integrating\rApprentice Learning and Reinforcement Learning TITLE2.\nPh.D. thesis,\rUniversity of Massachusetts, Amherst. Appeared as CMPSCI Technical Report\r96-026.\nClouse, J., Utgoff, P.\r(1992). A teaching method for reinforcement learning systems. In Pro\u0026shy;ceedings\rof the Ninth International Machine Learning Conference,pp. 92-101. Morgan Kaufmann, San Mateo, CA.\nCobo, L. C., Zang, P.,\rIsbell, C. L., and Thomaz, A. L. (2011). Automatic state abstrac\u0026shy;tion from\rdemonstration. In IJCAAI Proceedings: International\rJoint Conference on Artificial Intelligence, volume 22, page 1243.\nCohen, J. Y., Haesler, S., Vong,\rL., Lowell, B. B., and Uchida, N. (2012). Neuron-type-\nspecific signals for reward and punishment in the\rventral tegmental area. Nature 482(7383):85- 88.\nColombetti, M., Dorigo, M.\r(1994). Training agent to perform sequential behavior. Adaptive\rBehavior, 2(3):247-275.\nConnell, J. (1989). A colony\rarchitecture for an artificial creature. Technical Report AI- TR-1151. MIT\rArtificial Intelligence Laboratory, Cambridge, MA.\nConnell, J., Mahadevan, S. (1993). Robot\rLearning.Kluwer\rAcademic, Boston.\nConnell, M. E., and Utgoff, P. E.\r(1987). Learning to control a dynamic physical system. Computational\rintelligence 3(1):330-337.\nContreras-Vidal, J. L. and\rSchultz, W. (1999). A predictive reinforcement model of dopamine neurons for\rlearning approach behavior. Journal of computational\rneuroscience,6(3):191- 214.\nCoulom, R. (2006). Efficient\rselectivity and backup operators in Monte-Carlo tree search. In Proceedings\rof the 5th International Conference on Computers and Games,pp. 72-83.\nCourville, A. C., Daw, N. D., and\rTouretzky, D. S. (2006). Bayesian theories of conditioning\nin a changing world. Trends\rin Cognitive Science,10(7):294-300.\nCraik, K. J. W. (1943). The\rNature of Explanation.Cambridge University Press, Cambridge.\nCrites,\rR. H. (1996). Large-Scale Dynamic\rOptimization Using Teams of Reinforcement Learning Agents.Ph.D.\rthesis, University of Massachusetts, Amherst.\nCrites, R. H., Barto,\rA. G. (1996). Improving elevator performance using reinforcement learning. In\rD. S. Touretzky, M. C. Mozer, and M. E. Hasselmo (eds.), Advances\rin Neural Information Processing Systems: Proceedings of the 1995 Conference,pp. 1017\u0026shy;1023. MIT Press, Cambridge, MA.\nCross, J. G. (1973). A stochastic\rlearning model of economic behavior. The Quarterly Journal of Economics\r87(2):239-266.\nCrow, T. J. (1968). Cortical synapses and\rreinforcement: a hypothesis. Nature,219:736-737.\nCurtiss, J. H. (1954).\rA theoretical comparison of the efficiencies of two classical methods and a\rMonte Carlo method for computing one component of the solution of a set of\rlinear algebraic equations. In H. A. Meyer (ed.), Symposium\ron Monte Carlo Methods, pp.\r191-233. Wiley, New York.\nCybenko, G. (1989). Approximation\rby superpositions of a sigmoidal function. Mathematics of control,\rsignals and systems,\r2(4):303-314.\nCziko,\rG. (1995). Without Miracles:\rUniversal Selection Theory and the Second Darvinian Revolution.MIT\rPress, Cambridge, MA.\nDaniel, J. W. (1976).\rSplines and efficiency in dynamic programming. Journal\rof Mathe\u0026shy;matical Analysis and Applications, 54:402-407.\nDann, C., Neumann, G., Peters, J.\r(2014). Policy evaluation with temporal differences: A survey and comparison. Journal\rof Machine Learning Research 15:809-883.\nDaw, N. D., Courville,\rA. C., and Touretzky, D. S. (2003). Timing and partial observability in the\rdopamine system. In Advances in neural information\rprocessing systems, pages\r99-106.\nDaw, N. D., Courville, A. C., and\rTouretzky, D. S. (2006). Representation and timing in theories of the dopamine\rsystem. Neural Computation,18(7):1637-1677.\nDaw, N., Niv, Y., and\rDayan, P. (2005). Uncertainty based competition between pre- frontal and\rdorsolateral striatal systems for behavioral control. Nature\rNeuroscience, 8(12):1704-1711.\nDaw, N. D. and\rShohamy, D. (2008). The cognitive neuroscience of motivation and learning. Social\rCognition, 26(5):593-620.\nDayan, P. (1991).\rReinforcement comparison. In D. S. Touretzky, J. L. Elman, T. J. Se- jnowski,\rand G. E. Hinton (eds.), Connectionist Models: Proceedings of\rthe 1990 Summer School,pp. 45-51. Morgan Kaufmann, San Mateo, CA.\nDayan, P. (1992). The convergence of TD(A) for\rgeneral A. Machine Learning,8:341-362.\nDayan, P. (2008). The\rrole of value systems in decision making. In Engel, C. and Singer, W., editors,\rBetter Than Conscious?: Decision Making, the Human Mind,\rand Implications For Institutions (Strungmann Forum Reports),pages 51-70. MIT Press, Cambridge, MA.\nDayan,\rP. and Abbott, L. F. (2001). Theoretical\rNeuroscience: Computational and Mathe\u0026shy;matical Modeling of Neural Systems.\rMIT Press, Cambridge, MA.\nDayan, P., and\rBerridge, K. C. (2014). Model-based and model-free Pavlovian reward learning:\rRevaluation, revision, and revaluation. Cognitive, Affective, \u0026amp; Behavioral Neuroscience, 14(2):473-492.\nDayan, P., and Hinton, G. E.\r(1993). Feudal reinforcement learning. In S. J. Hanson, J. D. Cohen, and C. L.\rGiles (eds.), Advances in Neural Information\rProcessing Systems: Proceedings of the 1992 Conference,pp. 271-278. Morgan Kaufmann, San Mateo, CA.\nDayan, P. and Niv, Y. (2008).\rReinforcement learning: the good, the bad and the ugly. Current\rOpinion in Neurobiology,\r18(2):185-196.\nDayan, P., Niv, Y., Seymour, B.,\rand Daw, N. D. (2006). The misbehavior of value and the discipline of the will.\rNeural Networks 19(8):1153-1160.\nDayan, P., and Sejnowski, T.\r(1994). TD(A) converges with probability 1. Machine\rLearning, 14:295-301.\nDe Asis, K., Hernandez-Garcia, J.\rF., Holland, G. Z., and Sutton, R. S. (2017). Multi-step Reinforcement\rLearning: A Unifying Algorithm. arXiv preprint arXiv:1703.01327.\nDean, T., Lin, S.-H. (1995).\rDecomposition techniques for planning in stochastic domains. In Proceedings\rof the Fourteenth International Joint Conference on Artificial Intelli\u0026shy;gence,pp. 1121-1127. Morgan Kaufmann. See also Technical\rReport CS-95-10, Brown University, Department of Computer Science, 1995.\nDegris, T., White, M., Sutton, R.\rS. (2012). Off-policy actor-critic. Proceedings of the 29th\rInternational Conference on Machine Learning.\nDeJong, G., Spong, M. W. (1994).\rSwinging up the acrobot: An example of intelligent control. In Proceedings\rof the American Control Conference,pp. 2158-2162. American Automatic Control Council,\rEvanston, IL.\nDenardo, E. V. (1967).\rContraction mappings in the theory underlying dynamic program\u0026shy;ming. SIAM\rReview,9:165-177.\nDennett, D. C. (1978). Brainstorms,pp. 71-89. Bradford/MIT Press, Cambridge, MA.\nDerthick, M. (1984). Variations\ron the Boltzmann machine learning algorithm. Carnegie- Mellon University\rDepartment of Computer Science Technical Report No. CMU-CS-84- 120.\nDeutsch, J. A. (1953). A new type\rof behaviour theory. British Journal of Psychology.\rGeneral Section,44(4):304-317.\nDeutsch, J. A. (1954). A machine\rwith insight. Quarterly Journal of Experimental\rPsychol\u0026shy;ogy,6(1):6-11.\nDick, T. (2015). Policy\rGradient Reinforcement Learning Without Regret. MSc Thesis, University of Alberta.\nDickinson, A. (1980). Contemporary\rAnimal Learning Theory.Cambridge University Press, Cambridge.\nDickinson, A. (1985).\rActions and habits: the development of behavioral autonomy. Phil.\rTrans. R. Soc. Lond. B,308(1135):67-78.\nDickinson, A. and\rBalleine, B. W. (2002). The role of learning in motivation. In Gallistel,\nC.\u0026nbsp; R., editor, Stevens handbook of experimental\rpsychology,volume 3, pages 497-533. Wiley, NY.\nDietterich, T. and\rBuchanan, B. G. (1984). The role of the critic in learning systems. In\rSelfridge, O. G., Rissland, E. L., and Arbib, M. A., editors, Adaptive\rControl of Ill- Defined Systems,pages 127-147. Plenum Press, NY. Proceedings of the\rNATO Advanced Research Institute on Adaptive Control of Ill-defined Systems,\rNATO Conference Series II, Systems Science, Vol. 16.\nDietterich, T. G., Flann, N. S.\r(1995). Explanation-based learning and reinforcement learn\u0026shy;ing: A unified view.\rIn A. Prieditis and S. Russell (eds.), Proceedings of the\rTwelfth\nInternational\rConference on Machine Learning,pp. 176-184. Morgan\rKaufmann, San Francisco.\nDietterich, T. G. and Wang, X.\r(2002). Batch value function approximation via support vec\u0026shy;tors. In Advances\rin Neural Information Processing Systems 14,pp. 1491-1498. Cam\u0026shy;bridge, MA: MIT Press.\nDiuk, C., Cohen, A., and Littman,\rM. L. (2008). An object-oriented representation for efficient reinforcement\rlearning. In Proceedings of the 25th\rinternational conference on machine learning, pages 240-247. ACM New York, NY.\nDolan, R. J. and Dayan, P. (2013). Goals and habits\rin the brain. Neuron,80(2):312-325.\nDoll, B. B., Simon, D. A., and\rDaw, N. D. (2012). The ubiquity of model-based reinforcement learning. Current\rOpinion in Neurobiology,22:1-7.\nDonahoe, J. W. and Burgos, J. E.\r(2000). Behavior analysis and revaluation. Journal of the\rExperimental Analysis of Behavior, 74(3):331-346.\nDorigo, M. and Colombetti, M.\r(1994). Robot shaping: Developing autonomous agents through learning. Artificial\rIntelligence,71(2):321-370.\nDoya, K. (1996). Temporal\rdifference learning in continuous time and space. In D. S. Touret- zky, M. C.\rMozer, and M. E. Hasselmo (eds.), Advances in Neural\rInformation Processing Systems: Proceedings of the 1995 Conference,pp. 1073-1079. MIT Press, Cambridge, MA.\nDoya, K. and Sejnowski, T. J.\r(1995). A novel reinforcement model of birdsong vocalization learning. In\rTesauro, G., Touretzky, D. S., and Leen, T., editors, Advances\rin Neural Information Processing Systems: Proceedings of the 1994 Conference,pages 101-108, Cambridge, MA. MIT Press.\nDoya, K. and Sejnowski, T. J.\r(1998). A computational model of birdsong learning by auditory experience and\rauditory feedback. In Central auditory processing and\rneural modeling, pages 77-88.\rSpringer US.\nDoyle, P. G., Snell, J. L.\r(1984). Random Walks and Electric Networks.The Mathematical Association of America. Carus\rMathematical Monograph 22.\nDreyfus, S. E., Law, A. M.\r(1977). The Art and Theory of Dynamic Programming.Academic Press, New York.\nDuda, R. O., Hart, P. E. (1973). Pattern\rClassification and Scene Analysis.Wiley, New York.\nDuff, M. O. (1995). Q-learning\rfor bandit problems. In A. Prieditis and S. Russell (eds.), Proceedings\rof the Twelfth International Conference on Machine Learning,pp. 209-217. Morgan Kaufmann, San Francisco.\nEgger, D. M. and Miller, N. E.\r(1962). Secondary reinforcement in rats as a function of information value and\rreliability of the stimulus. Journal of Experimental\rPsychology, 64:97-104.\nEshel, N., Bukwich, M., Rao, V.,\rHemmelder, V., Tian, J., and Uchida, N. (2015). Arithmetic and local circuitry\runderlying dopamine prediction errors. Nature 525(7568):243-246.\nEshel, N., Tian, J., Bukwich, M.,\rand Uchida, N. (2016). Dopamine neurons share common response function for\rreward prediction error. Nature Neuroscience 19(3):479-486.\nEstes, W. K. (1943). Discriminative\rconditioning. I. A discriminative property of conditioned anticipation. Journal\rof Experimental Psychology 32(2):150-155.\nEstes, W. K. (1948).\rDiscriminative conditioning. II. Effects of a Pavlovian conditioned stimulus\rupon a subsequently established operant response. Journal\rof experimental\npsychology 38(2):173-177.\nEstes, W. K. (1950). Toward a\rstatistical theory of learning. Psychololgical Review,57:94\u0026shy;107.\nFarley, B. G., Clark, W. A.\r(1954). Simulation of self-organizing systems by digital computer.\nIRE\rTransactions on Information Theory,4:76-84.\nFarries, M. A. and\rFairhall, A. L. (2007). Reinforcement learning with modulated spike\rtimingdependent synaptic plasticity. Journal of\rneurophysiology,98(6):3648-3665.\nFeldbaum, A. A. (1965). Optimal\rControl Systems.Academic Press, New York.\nFinch, G., and Culler, E. (1934).\rHigher order conditioning with constant motivation. The\rAmerican Journal of Psychology,\r596-602.\nFinnsson, H., Bjornsson, Y.\r(2008). Simulation-based approach to general game playing. In Proceedings\rof the Association for the Advancement of Artificial Intelligence,259-264.\nFiorillo, C. D., Tobler, P. N.,\rand Schultz, W. (2003). Discrete coding of reward probability and uncertainty\rby dopamine neurons. Science, 299(5614):1898-1902.\nFiorillo, C. D., Yun, S. R., and\rSong, M. R. (2013). Diversity and homogeneity in responses of midbrain dopamine\rneurons. The Journal of Neuroscience,33(11):4693-4709.\nFlorian, R. V. (2007).\rReinforcement learning through modulation of spike-timing-dependent synaptic\rplasticity. Neural Computation, 19(6):1468-1502.\nFogel, L. J., Owens, A. J.,\rWalsh, M. J. (1966). Artificial intelligence through\rsimulated evolution. John Wiley\rand Sons.\nFrey, U. and Morris, R. G. M.\r(1997). Synaptic tagging and long-term potentiation. Nature,\r385(6616):533-536.\nFriedman, J. H., Bentley, J. L.,\rand Finkel, R. A. (1977). An algorithm for finding best matches in logarithmic\rexpected time. ACM Transactions on Mathematical\rSoftware 3(3):209-226.\nFriston, K. J., Tononi, G.,\rReeke, G. N., Sporns, O., Edelman, G. M. (1994). Value- dependent selection in\rthe brain: Simulation in a synthetic neural model. Neuroscience,\r59:229-243.\nFu, K. S. (1970). Learning\rcontrol systems��Review and outlook. IEEE Transactions on\rAutomatic Control,15:210-221.\nGalanter, E., Gerstenhaber, M.\r(1956). On thought: The extrinsic theory. Psychological Review,63:218-227.\nGallant, S. I. (1993). Neural\rNetwork Learning and Expert Systems.MIT Press, Cambridge, MA.\nGallistel, C. R. (2005).\rDeconstructing the law of effect. Games and Economic\rBehavior 52(2), 410-423.\nGallmo, O., Asplund,\rL. (1995). Reinforcement learning by construction of hypothetical targets. In\rJ. Alspector, R. Goodman, and T. X. Brown (eds.), Proceedings\rof the International Workshop on Applications of Neural Networks to\rTelecommunications2, pp. 300-307. Erlbaum, Hillsdale, NJ.\nGardner, M. (1973). Mathematical games. Scientific\rAmerican,228(1):108-115.\nGeist, M., Scherrer,\rB. (2014). Off-policy learning with eligibility traces: A survey. Journal\rof Machine Learning Research 15:289-333.\nGelperin, A.,\rHopfield, J. J., Tank, D. W. (1985). The logic of Umaxlearning. In A. Selver- ston (ed.), Model\rNeural Networks and Behavior,pp. 247-261. Plenum Press, New York.\nGenesereth, M., Thielscher, M. (2014). General game playing. Synthesis Lectures on\rArtifi\u0026shy;cial Intelligence and Machine Learning, 8(2), 1-229.\nGershman, S. J., Moustafa, A. A.,\rand Ludvig, E. A. (2013). Time representation in rein\u0026shy;forcement learning models\rof the basal ganglia. Frontiers in computational\rneuroscience, 7.\nGershman, S. J., Pesaran, B., and\rDaw, N. D. (2009). Human reinforcement learning sub\u0026shy;divides structured action\rspaces by learning effector-specific values. Journal\rof Neuro\u0026shy;science 29(43):13524-13531.\nGershman, S. J. and Niv, Y.\r(2010). Learning latent structure: Carving nature at its joints. Current\rOpinions in Neurobiology,\r20:251-256.\nGhiassian, S., Rafiee, B.,\rSutton, R. S. (2016). A first empirical study of emphatic tem\u0026shy;poral difference\rlearning. Workshop on Continual Learning and Deep Learning at the Conference on\rNeural Information Processing Systems. ArXiv:1705.04185.\nGibbs, C. M., Cool, V., Land, T.,\rKehoe, E. J., and Gormezano, I. (1991). Second-order conditioning of the\rrabbits nictitating membrane response. Integrative Physiological\rand Behavioral Science 26(4):282-295.\nGittins, J. C., Jones,\rD. M. (1974). A dynamic allocation index for the sequential design of\rexperiments. In J. Gani, K. Sarkadi, and I. Vincze (eds.), Progress\rin Statistics, pp. 241-266.\rNorth-Holland, Amsterdam-London.\nGlimcher, P. W.\r(2011). Understanding dopamine and reinforcement learning: The dopamine reward\rprediction error hypothesis. Proceedings of the\rNational Academy of Sciences,\r108(Supplement 3):15647-15654.\nGlimcher,\rP. W. (2003). Decisions,\runcertainty, and the brain: The science of neuroeco\u0026shy;nomics.\rMIT Press, Cambridge, MA.\nGlimcher, P. W. and Fehr, E.,\reditors (2013). Neuroeconomics: Decision making and\rthe brain, Second Edition.\rAcademic Press.\nGoldberg,\rD. E. (1989). Genetic Algorithms\rin Search, Optimization, and Machine Learning. Addison-Wesley,\rReading, MA.\nGoldstein, H. (1957). Classical\rMechanics.Addison-Wesley, Reading, MA.\nGoodfellow, I., Bengio, Y., and Courville, A.\r(2016). Deep Learning.MIT Press.\nGoodwin, G. C., Sin, K. S.\r(1984). Adaptive Filtering Prediction and Control. Prentice-Hall, Englewood Cliffs, NJ.\nGopnik, A., Glymour, C., Sobel,\rD., Schulz, L. E., Kushnir, T., and Danks, D. (2004). A theory of causal\rlearning in children: Causal maps and Bayes nets. Psychological\rReview, 111(1):3-32.\nGordon, G. J. (1995). Stable\rfunction approximation in dynamic programming. In A. Priedi\u0026shy;tis and S. Russell\r(eds.), Proceedings of the Twelfth International Conference\ron Machine Learning, pp.\r261-268. Morgan Kaufmann, San Francisco. An expanded version was published as\rTechnical Report CMU-CS-95-103. Carnegie Mellon University, Pittsburgh, PA,\r1995.\nGordon, G. J. (1996a). Chattering in SARSA(A). CMU\rlearning lab internal report.\nGordon, G. J. (1996b). Stable\rfitted reinforcement learning. In D. S. Touretzky, M. C. Mozer, M. E. Hasselmo\r(eds.), Advances in Neural Information Processing Systems:\rProceedings of the 1995 Conference,pp. 1052-1058. MIT Press, Cambridge, MA.\nGordon, G. J. (1999). Approximate\rsolutions to Markov decision processes.PhD thesis, School of Computer Science, Carnegie\rMellon University, Pittsburgh, PA.\nGordon, G. J. (2001).\rReinforcement learning with function approximation converges to a region. Advances\rin neural information processing systems.\nGraybiel, A. M. (2000). The basal ganglia. Current\rBiology,10(14):R509-R511.\nGreensmith, E., Bartlett, P. L.,\rBaxter, J. (2001). Variance reduction techniques for gradi\u0026shy;ent estimates in\rreinforcement learning. In Advances in Neural\rInformation Processing Systems: Proceedings of the 2000 Conference, pp. 1507-1514.\nGreensmith, E., Bartlett, P. L.,\rBaxter, J. (2004). Variance reduction techniques for gradient estimates in\rreinforcement learning. Journal of Machine Learning Research\r5(Nov), 1471-1530.\nGriffith, A. K. (1966). A new\rmachine learning technique applied to the game of checkers. Technical Report\rProject MAC, Artificial Intelligence Memo 94. Massachusetts Institute of\rTechnology, Cambridge, MA.\nGriffith, A. K. (1974). A\rcomparison and evaluation of three machine learning procedures as applied to\rthe game of checkers. Artificial Intelligence,5:137-148.\nGrondman, I., Busoniu, L., Lopes,\rG. A., Babuska, R. (2012). A survey of actor-critic reinforcement learning:\rStandard and natural policy gradients. IEEE Transactions on\rSystems, Man, and Cybernetics, Part C (Applications and Reviews) 42(6), 1291-1307.\nGrossberg, S. (1975). A neural\rmodel of attention, reinforcement, and discrimination learn\u0026shy;ing. International\rReview of Neurobiology,18:263-327.\nGrossberg, S. and Schmajuk, N. A.\r(1989). Neural dynamics of adaptive timing and temporal discrimination during\rassociative learning. Neural Networks, 2(2):79-102.\nGullapalli, V. (1990). A\rstochastic reinforcement algorithm for learning real-valued functions. Neural\rNetworks, 3:671-692.\nGullapalli, V. and Barto, A. G.\r(1992). Shaping as a method for accelerating reinforcement learning. In Proceedings\rof the 1992 IEEE International Symposium on Intelligent Control, pages 554-559. IEEE.\nGurney, K., Prescott, T. J., and\rRedgrave, P. (2001). A computational model of action selection in the basal\rganglia I. A new functional anatomy. Biological cybernetics, 84(6):401-410.\nGurvits, L., Lin, L.-J., Hanson,\rS. J. (1994). Incremental learning of evaluation functions for absorbing Markov\rchains: New methods and theorems. Preprint.\nHackman, L. (2012). Faster\rGradient-TD Algorithms(MSc dissertation, University of Al\u0026shy;berta).\nHallak, A., Tamar, A., Munos, R.,\rMannor, S. (2016). Generalized emphatic temporal difference learning:\rBias-variance analysis. In Thirtieth AAAIConference on Artificial Intelligence.\nHammer, M. (1997). The neural\rbasis of associative reward learning in honeybees. Trends\rin Neuroscience, 20:245-252.\nHammer, M. and Menzel, R. (1995).\rLearning and memory in the honeybee. Journal of Neuroscience, 15(3):1617-1630.\nHampson, S. E. (1983). A\rNeural Model of Adaptive Behavior. Ph.D. thesis, University of California, Irvine.\nHampson,\rS. E. (1989). Connectionist\rProblem Solving: Computational Aspects of Biological Learning.\rBirkhauser, Boston.\nHare, T. A., O,Doherty,\rJ., Camerer, C. F., Schultz, W., and Rangel, A. (2008). Dissociating the role\rof the orbitofrontal cortex and the striatum in the computation of goal values\nand prediction errors. The\rJournal of Neuroscience,28(22):5623-5630.\nHassabis, D. and Maguire, E. A.\r(2007). Deconstructing episodic memory with construction. Trends\rin cognitive sciences,\r11(7):299-306.\nHawkins, R. D., Kandel, E. R.\r(1984). Is there a cell-biological alphabet for simple forms of learning? Psychological\rReview,91:375-391.\nHe, K., Huertas, M.,\rHong, S. Z., Tie, X., Hell, J. W., Shouval, H., and Kirkwood, A. (2015).\rDistinct eligibility traces for LTP and LTD in cortical synapses. Neuron,88(3):528-538.\nHe, K., Zhang, X.,\rRen, S., and Sun, J. (2016). Deep residual learning for image recog\u0026shy;nition. In Proceedings\rof the 1992 IEEE Conference on Computer Vision and Pattern Recognition, pages 770-778.\nHebb, D. O. (1949). The\rorganization of behavior: A neuropsychological theory.John Wiley and Sons Inc., New York. Reissued by\rLawrence Erlbaum Associates Inc., Mahwah NJ, 2002.\nHengst, B. (2012). Hierarchical\rapproaches. In Wiering and van Otterlo (Eds.) Reinforce\u0026shy;ment\rLearning: State-of-the Art,pp. 293-323. Springer Berlin Heidelberg.\nHerrnstein, R. J. (1970). On the\rLaw of Effect. Journal of the Experimental Analysis\rof Behavior 13(2), 243-266.\nHersh, R., Griego, R.\rJ. (1969). Brownian motion and potential theory. Scientific\rAmerican, 220:66-74.\nHester, T., and Stone,\rP. (2012). Learning and using models. In Wiering and van Otterlo (Eds.) Reinforcement\rLearning: State-of-the Art,pp. 111-141. Springer Berlin Heidel\u0026shy;berg.\nHesterberg, T. C.\r(1988), Advances in importance sampling, Ph.D. Dissertation, Statistics Department,\rStanford University.\nHilgard, E. R. (1956).\rTheories of Learning, Second Edition.Appleton-Century-Cofts, Inc., New York.\nHilgard, E. R., Bower, G. H.\r(1975). Theories of Learning.Prentice-Hall, Englewood Cliffs, NJ.\nHinton, G. E. (1984). Distributed\rrepresentations. Technical Report CMU-CS-84-157. Department of Computer\rScience, Carnegie-Mellon University, Pittsburgh, PA.\nHinton, G. E., Osindero, S., and\rTeh, Y. (2006). A fast learning algorithm for deep belief nets. Neural\rComputation, 18(7):1527-1554.\nHochreiter, S., Schmidhuber, J.\r(1997). LTSM can solve hard time lag problems. In Advances\rin Neural Information Processing Systems: Proceedings of the 1996 Conference,pp. 473\u0026shy;479. MIT Press, Cambridge, MA.\nHolland, J. H. (1975). Adaptation\rin Natural and Artificial Systems. University of Michigan Press, Ann Arbor.\nHolland, J. H. (1976).\rAdaptation. In R. Rosen and F. M. Snell (eds.), Progress\rin Theoretical Biology,vol. 4, pp. 263-293. Academic Press, New York.\nHolland, J. H. (1986).\rEscaping brittleness: The possibility of general-purpose learning algorithms\rapplied to rule-based systems. In R. S. Michalski, J. G. Carbonell, and T. M.\rMitchell (eds.), Machine Learning: An Artificial\rIntelligence Approach,vol. 2, pp. 593-623. Morgan Kaufmann, San Mateo, CA.\nHollerman, J. R. and Schultz, W.\r(1998). Dopmine neurons report an error in the temporal prediction of reward\rduring learning. Nature Neuroscience,1:304-309.\nHouk, J. C., Adams, J. L., Barto,\rA. G. (1995). A model of how the basal ganglia generates and uses neural\rsignals that predict reinforcement. In J. C. Houk, J. L. Davis, and\nD.G. Beiser (eds.), Models\rof Information Processing in the Basal Ganglia,pp. 249-270. MIT Press, Cambridge, MA.\nHoward, R. (1960). Dynamic\rProgramming and Markov Processes.\rMIT Press, Cambridge, MA.\nHull, C. L. (1932). The\rgoal-gradient hypothesis and maze learning. Psychological\rReview, 39(1):25-43.\nHull, C. L. (1943). Principles\rof Behavior.Appleton-Century, New York.\nHull, C. L. (1952). A\rBehavior System.Wiley, New York.\nIoffe, S., and Szegedy, C.\r(2015). Batch normalization: Accelerating deep network training by reducing\rinternal covariate shift. arXiv:1502.03167.\nipek, E., Mutlu, O.,\rMartinez, J. F., and Caruana, R. (2008). Self-optimizing memory controllers: A\rreinforcement learning approach. In 35th International\rSymposium on Computer Architecture, ISCA?08,pages 39-50. IEEE.\nIzhikevich, E. M.\r(2007). Solving the distal reward problem through linkage of STDP and dopamine\rsignaling. Cerebral cortex, 17(10):2443-2452.\nJaakkola, T., Jordan, M. I.,\rSingh, S. P. (1994). On the convergence of stochastic iterative dynamic\rprogramming algorithms. Neural Computation,6:1185-1201.\nJaakkola, T., Singh,\rS. P., Jordan, M. I. (1995). Reinforcement learning algorithm for partially\robservable Markov decision problems. In G. Tesauro, D. S. Touretzky, T. Leen\r(eds.), Advances in Neural Information Processing Systems:\rProceedings of the 1994 Conference, pp. 345-352. MIT Press, Cambridge, MA.\nJoel, D., Niv, Y., and\rRuppin, E. (2002). Actor-critic models of the basal ganglia: New anatomical and\rcomputational perspectives. Neural networks, 15(4):535-547.\nJohanson, E. B.,\rKilleen, P. R., Russell, V. A., Tripp, G., Wickens, J. R., Tannock, R.,\rWilliams, J., and Sagvolden, T. (2009). Origins of altered reinforcement\reffects in ADHD. Behavioral and Brain Functions,5(7).\nJohnson, A. and\rRedish, A. D. (2007). Neural ensembles in CA3 transiently encode paths forward\rof the animal at a decision point. The Journal of\rneuroscience,27(45):12176- 12189.\nKaelbling, L. P. (1993a).\rHierarchical learning in stochastic domains: Preliminary results. In Proceedings\rof the Tenth International Conference on Machine Learning,pp. 167-173. Morgan Kaufmann, San Mateo, CA.\nKaelbling, L. P. (1993b). Learning\rin Embedded Systems.MIT Press, Cambridge, MA.\nKaelbling, L. P. (Ed.)\r(1996). Special triple issue on reinforcement learning, Machine\rLearning 22(1/2/3).\nKaelbling, L. P.,\rLittman, M. L., Moore, A. W. (1996). Reinforcement learning: A survey. Journal\rof Artificial Intelligence Research, 4:237-285.\nKahneman, D. and Tversky, A.\r(1979). Prospect theory: An analysis of decision under risk. Econometrica:\rJournal of the Econometric Society,47:263-291.\nKakade, S. (2002). A natural\rpolicy gradient. Advances in neural information\rprocessing systems 2,1531-1538.\nKakade, S. M. (2003). On the\rSample Complexity of Reinforcement Learning (Doctoral dissertation, University\rof London).\nKakutani, S. (1945). Markov\rprocesses and the Dirichlet problem. Proceedings of the Japan\rAcademy,21:227-233.\nKalos, M. H., Whitlock, P. A. (1986). Monte\rCarlo Methods.Wiley, New York.\nKamin, L. J. (1968).\r��Attention-like�� processes in classical conditioning. In Jones, M. R. editor, Miami\rSymposium on the Prediction of Behavior, 1967: Aversive Stimulation pages 9-31. University of Miami Press, Coral Gables,\rFlorida.\nKamin, L. J. (1969). Predictability,\rsurprise, attention, and conditioning. In Campbell B. A. and Church, R. M.,\reditors, Punishment and Aversive Behavior, pages 279-296 Appleton-Century-Crofts, New York,\rNY.\nKandel, E. R., Schwartz, J. H.,\rJessell, T. M., Siegelbaum, S. A., and Hudspeth, A. J. editors (2013). Principles\rof Neural Science, Fifth Edition.McGraw-Hill Companies Inc.\nKanerva, P. (1988). Sparse\rDistributed Memory.MIT Press, Cambridge, MA.\nKanerva, P. (1993). Sparse\rdistributed memory and related models. In M. H. Hassoun (ed.), Associative\rNeural Memories: Theory and Implementation,pp. 50-76. Oxford University Press, New York.\nKarampatziakis, N., and Langford,\rJ. (2010). Online importance weight aware updates. ArXiv:1011.1576.\nKashyap, R. L., Blaydon, C. C.,\rFu, K. S. (1970). Stochastic approximation. In J. M. Mendel and K. S. Fu\r(eds.), Adaptive, Learning, and Pattern Recognition Systems:\rTheory and Applications,pp. 329-355. Academic Press, New York.\nKearns, M., Singh, S. (2002).\rNear-optimal reinforcement learning in polynomial time. Ma\u0026shy;chine\rLearning, 49(2-3), 209-232.\nKeerthi, S. S., Ravindran, B.\r(1997). Reinforcement learning. In E. Fiesler and R. Beale (eds.), Handbook\rof Neural Computation,C3. Oxford University Press, New York.\nKehoe, E. J. (1982). Conditioning\rwith serial compound stimuli: Theoretical and empirical issues. Experimental\rAnimal Behavior,1:30-65.\nKehoe, E. J., Schreurs, B. G.,\rand Graham, P. (1987). Temporal primacy overrides prior training in serial\rcompound conditioning of the rabbits nictitating membrane response. Animal\rLearning \u0026amp; Behavior,15(4):455-464.\nKeiflin, R. and Janak, P. H.\r(2015). Dopamine prediction errors in reward learning and addiction: Ffrom\rtheory to neural circuitry. Neuron,88(2):247- 263.\nKimble, G. A. (1961). Hilgard\rand Marquis��Conditioning and Learning.Appleton-Century- Crofts, New York.\nKimble, G. A. (1967). Foundations\rof Conditioning and Learning.Appleton-Century-Crofts, New York.\nKlopf, A. H. (1972).\rBrain function and adaptive systems��A heterostatic theory. Technical Report\rAFCRL-72-0164, Air Force Cambridge Research Laboratories, Bedford, MA. A\rsummary appears in Proceedings of the International\rConference on Systems, Man, and Cybernetics. IEEE Systems, Man, and Cybernetics Society, Dallas, TX, 1974.\nKlopf, A. H. (1975). A comparison\rof natural and artificial intelligence. SIGART Newsletter, 53:11-13.\nKlopf,\rA. H. (1982). The Hedonistic\rNeuron: A Theory of Memory, Learning, and Intelli\u0026shy;gence.Hemisphere,\rWashington, DC.\nKlopf, A. H. (1988). A neuronal model of classical\rconditioning. Psychobiology, 16:85-125.\nKober, J. and Peters,\rJ. (2012). Reinforcement learning in robotics: A survey. In Wiering, M. and van\rOtterlo, M., editors, Reinforcement Learning:\rState-of-the-Art, pages 579\u0026shy;610.\rSpringer-Verlag, Berlin.\nKocsis, L., Szepesvari, Cs. (2006). Bandit based Monte-Carlo\rplanning. In Proceedings of the European\rConference on Machine Learning,\r282-293. Springer Berlin Heidelberg.\nKohonen,\rT. (1977). Associative Memory: A\rSystem Theoretic Approach.Springer-Verlag, Berlin.\nKoller,\rD., Friedman, N. (2009). Probabilistic\rGraphical Models: Principles and Techniques. MIT\rPress, 2009.\nKolodziejski, C., Porr, B., and\rWorgotter, F. (2009). On the asymptotic equivalence between differential\rHebbian and temporal difference learning. Neural computation, 21(4):1173- 1202.\nKolter, J. Z. (2011). The fixed points of off-policy TD. Advances\rin Neural Information Processing Systems 24,pp. 2169-2177.\nKonidaris, G. D., Osentoski, S.,\rThomas, P. S. (2011). Value function approximation in rein\u0026shy;forcement learning\rusing the Fourier basis, Proceedings of the Twenty-Fifth\rConference of the Association for the Advancement of Artificial Intelligence,pp. 380-385.\nKorf, R. E. (1988). Optimal path\rfinding algorithms. In L. N. Kanal and V. Kumar (eds.), Search\rin Artificial Intelligence,pp. 223-267. Springer Verlag, Berlin.\nKorf, R. E.\r(1990). Real-time heuristic search. Artificial Intelligence\r42(2-3), 189-211.\nKoshland, D. E.\r(1980). Bacterial Chemotaxis as a Model Bhavioral System. Raven Press, New York.\nKoza,\rJ. R. (1992). Genetic\rprogramming: On the programming of computers by means of natural selection(Vol.\r1). MIT press.\nKraft, L. G.,\rCampagna, D. P. (1990). A summary comparison of CMAC neural network and\rtraditional adaptive control systems. In T. Miller, R. S. Sutton, and P. J.\rWerbos (eds.), Neural Networks for Control,pp. 143-169. MIT Press, Cambridge, MA.\nKraft, L. G., Miller, W. T., Dietz, D. (1992). Development and\rapplication of CMAC neural network-based control. In D. A. White and D. A.\rSofge (eds.), Handbook of Intelligent Control:\rNeural, Fuzzy, and Adaptive Approaches,pp. 215-232. Van Nostrand Reinhold, New York.\nKumar, P. R., Varaiya, P. (1986). Stochastic\rSystems: Estimation, Identification, and Adaptive Control.Prentice-Hall, Englewood Cliffs, NJ.\nKumar, P. R. (1985). A survey of some results in stochastic adaptive\rcontrol. SIAM Journal of Control and Optimization, 23:329-380.\nKumar, V., Kanal, L. N. (1988).\rThe CDP: A unifying formulation for heuristic search, dynamic programming, and\rbranch-and-bound. In L. N. Kanal and V. Kumar (eds.), Search\rin Artificial Intelligence, pp.\r1-37. Springer-Verlag, Berlin.\nKushner,\rH. J., Dupuis, P. (1992). Numerical\rMethods for Stochastic Control Problems in Continuous Time.Springer-Verlag,\rNew York.\nLagoudakis, M., Parr, R. (2003).\rLeast squares policy iteration. Journal of Machine\rLearning Research 4:1107-1149.\nLai, T. L., Robbins,\rH. (1985). Asymptotically efficient adaptive allocation rules. Advances\rin applied mathematics,6(1):4-22.\nLakshmivarahan, S. and Narendra, K. S. (1982). Learning algorithms\rfor two-person zero- sum stochastic games with incomplete information: A\runified approach. SIAM Journal of Control and\rOptimization, 20:541-552.\nLammel, S., Lim, B.\rK., and Malenka, R. C. (2014). Reward and aversion in a heterogeneous midbrain\rdopamine system. Neuropharmacology,76:353-359.\nLane, S. H., Handelman, D. A.,\rGelfand, J. J. (1992). Theory and development of higher- order CMAC neural\rnetworks. IEEE Control Systems 12(2):23-30.\nLang, K. J., Waibel, A. H.,\rHinton, G. E. (1990). A time-delay neural network architecture for isolated\rword recognition. Neural Networks, 3:33-43.\nLange, S., Gabel, T., and\rRiedmiller, M. (2012). Batch reinforcement learning. In Wiering and van Otterlo\r(Eds.) Reinforcement Learning: State-of-the Art,pp. 45-73. Springer Berlin Heidelberg.\nLeCun, Y. (1985). Une procdure d,apprentissage\rpour rseau a seuil asymmetrique (a learn\u0026shy;ing scheme for asymmetric threshold\rnetworks). In Proceedings of Cognitiva 85,Paris, France.\nLeCun, Y., Bottou, L., Bengio,\rY., and Haffner, P. (1998). Gradient-based learning applied to document\rrecognition. Proceedings of the IEEE,86(11):2278-2324.\nLegenstein, R. and andW. Maass,\rD. P. (2008). A learning theory for reward-modulated spike-timing-dependent\rplasticity with application to biofeedback. PLoS\rComputational Biology, 4(10).\nLevy, W. B. and Steward, D.\r(1983). Temporal contiguity requirements for long-term associative\rpotentiation/depression in thehippocampus. Neuroscience,8:791-797.\nLewis,\rF. L., Liu, D. (Eds.). (2013). Reinforcement\rLearning and Approximate Dynamic Programming for Feedback Control.John\rWiley and Sons.\nLewis, R. L., Howes, A., and\rSingh, S. (2014). Computational rationality: Linking mecha\u0026shy;nism and behavior\rthrough utility maximization. Topics in Cognitive\rScience,6(2):279-\n311.\nLi, L. (2012). Sample complexity\rbounds of exploration. In Wiering and van Otterlo (Eds.) Reinforcement\rLearning: State-of-the Art,pp. 175-204. Springer Berlin Heidelberg.\nLi, L., Chu, W., Langford, J.,\rand Schapire, R. E. (2010). A contextual-bandit approach to personalized news\rarticle recommendation. In Proceedings of the 19th\rInternational Conference on World Wide Web,pages 661-670. ACM.\nLin, C.-S., Kim, H. (1991).\rCMAC-based adaptive critic self-learning control. IEEE\rTransactions on Neural Networks,\r2:530-533.\nLin, L.-J. (1992). Self-improving\rreactive agents based on reinforcement learning, planning and teaching. Machine\rLearning,8:293-321.\nLin, L.-J., Mitchell, T. (1992).\rReinforcement learning with hidden states. In Proceedings\rof the Second International Conference on Simulation of Adaptive Behavior: From\rAnimals to Animats, pp. 271-280.\rMIT Press, Cambridge, MA.\nLittman, M. L. (1994). Markov\rgames as a framework for multi-agent reinforcement learning. In Proceedings\rof the Eleventh International Conference on Machine Learning,pp. 157\u0026shy;163. Morgan Kaufmann, San Francisco.\nLittman, M. L.,\rCassandra, A. R., Kaelbling, L. P. (1995). Learning policies for partially\robservable environments: Scaling up. In A. Prieditis and S. Russell (eds.), Proceedings\rof the Twelfth International Conference on Machine Learning,pp. 362-370. Morgan Kaufmann, San Francisco.\nLittman, M. L., Dean,\rT. L., Kaelbling, L. P. (1995). On the complexity of solving Markov decision\rproblems. In Proceedings of the Eleventh Annual\rConference on Uncertainty in Artificial Intelligence, pp. 394-402.\nLiu, J. S. (2001). Monte Carlo strategies in scientific computing.Berlin,\rSpringer-Verlag.\nLiu, W., Pokharel, P. P., and\rPrincipe, J. C. (2008). The kernel least-mean-square algorithm.\nIEEE Transactions on Signal\rProcessing 56(2):543-554.\nLjung, L. (1998). System\ridentification. In Prochazka, A., Uhli^, J., Rayner, P. W. J., and Kingsbury,\rN. G., editors, Signal Analysis and Prediction,pages 163-173. Springer Science ʮBusiness Media New York, LLC.\nLjung, L., Soderstrom, T. (1983).\rTheory and Practice of Recursive Identification.MIT Press, Cambridge, MA.\nLjungberg, T., Apicella, P., and\rSchultz, W. (1992). Responses of monkey dopamine neurons during learning of\rbehavioral reactions. Journal of Neurophysiology,67(1):145-163.\nLovejoy, W. S. (1991). A survey\rof algorithmic methods for partially observed Markov decision processes. Annals\rof Operations Research,28:47-66.\nLuce, D. (1959). Individual\rChoice Behavior.Wiley, New York.\nLudvig, E. A.,\rBellemare, M. G., and Pearson, K. G. (2011). A primer on reinforcement learning\rin the brain: Psychological, computational, and neural perspectives. In Alonso,\nE.\u0026nbsp;\rand Mondragon, E.,\reditors, Computational\rneuroscience for advancing artificial in\u0026shy;telligence: Models, methods and\rapplications,pages 111-44. Medical Information\rScience Reference, Hershey PA.\nLudvig, E. A., Sutton, R. S., and\rKehoe, E. J. (2008). Stimulus representation and the timing of\rreward-prediction errors in models of the dopamine system. Neural\rComputation, 20(12):3034-3054.\nLudvig, E. A., Sutton, R. S., and\rKehoe, E. J. (2012). Evaluating the TD model of classical conditioning. Learning\r\u0026amp; behavior, 40(3):305-319.\nMachado, A. (1997). Learning the\rtemporal dynamics of behavior. Psychological review, 104(2):241-265.\nMackintosh, N. J. (1975). A\rtheory of attention: Variations in the associability of stimuli with\rreinforcement. Psychological Review,82(4):276-298.\nMackintosh, N. J. (1983). Conditioning\rand Associative Learning.Oxford: Clarendon Press.\nMaclin, R., and\rShavlik, J. W. (1994). Incorporating advice into agents that learn from rein\u0026shy;forcements.\rIn Proceedings of the Twelfth National Conference on\rArtificial Intelligence, pp.\r694-699. AAAI Press, Menlo Park, CA.\nMaei, H. R. (2011). Gradient\rtemporal-difference learning algorithms.PhD thesis, University of Alberta.\nMaei, H. R., and\rSutton, R. S. (2010). GQ(A): A general gradient algorithm for temporal-\rdifference prediction learning with eligibility traces. In Proceedings\rof the Third Confer\u0026shy;ence on Artificial General Intelligence,pp. 91-96.\nMaei, H. R.,\rSzepesvari, Cs., Bhatnagar, S., Precup, D., Silver, D., and Sutton, R. S.\r(2009). Convergent temporal-difference learning with arbitrary smooth function\rapproximation. In Advances in Neural Information\rProcessing Systems,pp. 1204-1212.\nMaei, H. R., Szepesvari, Cs.,\rBhatnagar, S., and Sutton, R. S. (2010). Toward off-policy learning control\rwith function approximation. In Proceedings of the 27th\rInternational Conference on Machine Learning,pp. 719-726).\nMahadevan, S. (1996). Average\rreward reinforcement learning: Foundations, algorithms, and empirical results. Machine\rLearning,22:159-196.\nMahadevan, S., Liu, B., Thomas,\rP., Dabney, W., Giguere, S., Jacek, N., Gemp, I., Liu, J. (2014). Proximal\rreinforcement learning: A new theory of sequential decision making in\rprimal-dual spaces. ArXiv preprint arXiv:1405.6757.\nMahadevan, S., and Connell, J.\r(1992). Automatic programming of behavior-based robots\nusing reinforcement learning. Artificial\rIntelligence,55:311-365.\nMahmood, A. R. (2017).\rIncremental Off-policy Reinforcement Learning Algorithms. Uni\u0026shy;versity of\rAlberta PhD thesis.\nMahmood, A. R., and Sutton, R. S.\r(2015). Off-policy learning based on weighted importance sampling with linear\rcomputational complexity. In Proceedings of the 31st\rConference on Uncertainty in Artificial Intelligence, Amsterdam, Netherlands.\nMahmood, A. R., Sutton, R. S.,\rDegris, T., and Pilarski, P. M. (2012). Tuning-free step-size adaptation. In Acoustics,\rSpeech and Signal Processing (ICASSP), 2012 IEEE Interna\u0026shy;tional Conference on(pp. 2121-2124). IEEE.\nMahmood, A. R., Yu, H, Sutton, R.\rS. (2017). Multi-step off-policy learning without impor\u0026shy;tance sampling ratios.\rArXiv 1702.03006.\nMahmood, A. R., van Hasselt, H.,\rand Sutton, R. S. (2014). Weighted importance sam\u0026shy;pling for off-policy learning\rwith linear function approximation. Advances in Neural\rInformation Processing Systems 27.\nMarbach, P., Tsitsiklis, J. N.\r(2001). Simulation-based optimization of Markov reward pro\u0026shy;cesses. IEEE\rTransactions on Automatic Control 46(2), 191-209. Also MIT Technical Report LIDS-P-2411 (1998).\nMarkey, K. L. (1994). Efficient\rlearning of multiple degree-of-freedom control problems with quasi-independent\rQ-agents. In M. C. Mozer, P. Smolensky, D. S. Touretzky, J. L. Elman, and A. S.\rWeigend (eds.), Proceedings of the 1990\rConnectionist Models Summer School. Erlbaum, Hillsdale, NJ.\nMarkram, H., Liibke, J.,\rFrotscher, M., and Sakmann, B. (1997). Regulation of synaptic efficacy by\rcoincidence of postsynaptic APs and EPSPs. Science,275:213-215.\nMartinez, J. F. and ipek, E.\r(2009). Dynamic multicore resource management: A machine learning approach. Micro,\rIEEE, 29(5):8-17.\nMataric, M. J. (1994). Reward\rfunctions for accelerated learning. In Machine Learning:\rProceedings of the Eleventh international conference,pages 181-189.\nMatsuda, W., Furuta, T.,\rNakamura, K. C., Hioki, H., Fujiyama, F., Arai, R., and Kaneko, T. (2009).\rSingle nigrostriatal dopaminergic neurons form widely spread and highly dense\raxonal arborizations in the neostriatum. The Journal of\rNeuroscience,29(2):444-453.\nMazur, J. E. (1994). Learning\rand Behavior,3rd ed. Prentice-Hall, Englewood Cliffs, NJ.\nMcCallum, A. K. (1993).\rOvercoming incomplete perception with utile distinction memory. In Proceedings\rof the Tenth International Conference on Machine Learning,pp. 190-196. Morgan Kaufmann, San Mateo, CA.\nMcCallum, A. K. (1995). Reinforcement\rLearning with Selective Perception and Hidden State. Ph.D. thesis, University of Rochester, Rochester,\rNY.\nMcCulloch, W. S., and Pitts, W.\r(1943). A logical calculus of the ideas immanent in nervous activity. Bulletin\rof Mathematical Biophysics 5(4):115-133.\nMelo, F. S., Meyn, S. P.,\rRibeiro, M. I. (2008). An analysis of reinforcement learning with function\rapproximation. In Proceedings of the 25th\rinternational conference on Machine learning(pp. 664-671).\nMendel, J. M. (1966). A survey of learning control\rsystems. ISA Transactions,5:297-303.\nMendel, J. M., McLaren, R. W.\r(1970). Reinforcement learning control and pattern recog\u0026shy;nition systems. In J.\rM. Mendel and K. S. Fu (eds.), Adaptive, Learning and\rPattern Recognition Systems: Theory and Applications, pp. 287-318. Academic Press, New York.\nMichie, D. (1961). Trial and\rerror. In S. A. Barnett and A. McLaren (eds.), Science\rSurvey,\nPart 2, pp. 129-145. Penguin, Harmondsworth.\nMichie, D. (1963). Experiments on\rthe mechanisation of game learning. 1. characterization of the model and its\rparameters. Computer Journal,1:232-263.\nMichie, D. (1974). On\rMachine Intelligence.Edinburgh University Press, Edinburgh.\nMichie, D., Chambers, R. A.\r(1968). BOXES: An experiment in adaptive control. In E. Dale and D. Michie (eds.),\rMachine Intelligence 2,pp. 137-152. Oliver and Boyd, Edinburgh.\nMiller,\rR. (1981). Meaning and Purpose in\rthe Intact Brain: A Philosophical, Psychological, and Biological Account of\rConscious Process. Clarendon Press, Oxford.\nMiller, W. T., An, E., Glanz, F.,\rCarter, M. (1990). The design of CMAC neural networks for control. Adaptive\rand Learning Systems 1:140-145.\nMiller, W. T., Glanz, F. H.\r(1996). UNH��CMAC verison 2.1: The University of New Hamp\u0026shy;shire\rImplementation of the Cerebellar Model Arithmetic Computer - CMAC.Robotics Laboratory Technical Report, University of\rNew Hampshire, Durham, New Hampshire.\nMiller, S., Williams,\rR. J. (1992). Learning to control a bioreactor using a neural net Dyna- Q\rsystem. In Proceedings of the Seventh Yale\rWorkshop on Adaptive and Learning Systems,pp. 167-172. Center for Systems Science, Dunham\rLaboratory, Yale University, New Haven.\nMiller, W. T.,\rScalera, S. M., Kim, A. (1994). Neural network control of dynamic balance for a\rbiped walking robot. In Proceedings of the Eighth Yale\rWorkshop on Adaptive and Learning Systems, pp. 156-161. Center for Systems Science, Dunham Laboratory, Yale\rUniversity, New Haven.\nMinsky,\rM. L. (1954). Theory of\rNeural-Analog Reinforcement Systems and Its Application to the Brain-Model\rProblem.Ph.D. thesis, Princeton University.\nMinsky, M. L. (1961). Steps\rtoward artificial intelligence. Proceedings of the\rInstitute of Radio Engineers,49:8-30. Reprinted in E. A. Feigenbaum and J.\rFeldman (eds.), Computers and Thought,pp. 406-450. McGraw-Hill, New York, 1963.\nMinsky, M. L. (1967). Computation:\rFinite and Infinite Machines.Prentice-Hall, Englewood Cliffs, NJ.\nMnih, V., Kavukcuoglu, K.,\rSilver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller,\rM., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A.,\rAntonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D.\r(2015). Human-level control through deep reinforcement learning. Nature, 518(7540):529-533.\nModayil, J., and Sutton, R. S.\r(2014). Prediction driven behavior: Learning predictions that drive fixed\rresponses. In AAAI-14 Workshop on Artificial\rIntelligence and Robotics, Quebec\rCity, Canada.\nModayil, J., White, A., and\rSutton, R. S. (2014). Multi-timescale nexting in a reinforcement learning\rrobot. Adaptive Behavior,22(2):146-160.\nMontague, P. R., Dayan, P.,\rNowlan, S. J., Pouget, A., and Sejnowski, T. J. (1992). Using aperiodic\rreinforcement for directed self-organization during development. In Advances\rin neural information processing systems 5, pages 969-976.\nMontague, P. R., Dayan, P.,\rPerson, C., and Sejnowski, T. J. (1995). Bee foraging in uncertain environments\rusing predictive hebbian learning. Nature, 377(6551):725-728.\nMontague, P. R., Dayan, P.,\rSejnowski, T. J. (1996). A framework for mesencephalic dopamine systems based\ron predictive Hebbian learning. Journal of Neuroscience, 16:1936-1947.\nMontague, P.\rR., Dolan, R. J., Friston, K. J., and Dayan, P. (2012). Computational psychi\u0026shy;atry.\rTrends in\rCognitive Sciences 16(1):72-80.\nMontague, P. R. and Sejnowski, T.\rJ. (1994). The predictive brain: Temporal coincidence and temporal order in\rsynaptic learningmechanisms. Learning \u0026amp; Memory,1:1-33.\nMoore, A. W. (1990). Efficient\rMemory-Based Learning for Robot Control.Ph.D. thesis, University of Cambridge.\nMoore, A. W. (1994).\rThe parti-game algorithm for variable resolution reinforcement learn\u0026shy;ing in multidimensional\rspaces. In J. D. Cohen, G. Tesauro and J. Alspector (eds.), Advances\rin Neural Information Processing Systems: Proceedings of the 1993 Confer\u0026shy;ence,pp. 711-718. Morgan Kaufmann, San Francisco.\nMoore, A. W., Atkeson, C. G.\r(1993). Prioritized sweeping: Reinforcement learning with less data and less\rreal time. Machine Learning,13:103-130.\nMoore, A. W., Schneider, J., and\rDeng, K. (1997). Efficient locally weighted polynomial regression predictions.\rIn Proceedings of the 1997 International Machine Learning Con\u0026shy;ference.Morgan Kaufmann.\nMoore, J. W. and Blazis, D. E. J.\r(1989). Simulation of a classically conditioned response: A cerebellar\rimplementation of the sutton-barto-desmond model. In Byrne, J. H. and Berry, W.\rO., editors, Neural Models of Plasticity,pages 187-207. Academic Press, San Diego, CA.\nMoore, J. W., Choi,\rJ.-S., and Brunzell, D. H. (1998). Predictive timing under temporal un\u0026shy;certainty:\rThe time derivative model of the conditioned response. In Rosenbaum, D. A. and\rCollyer, C. E., editors, Timing of Behavior, pages 3-34. MIT Press, Cambridge, MA.\nMoore, J. W., Desmond,\rJ. E., Berthier, N. E., Blazis, E. J., Sutton, R. S., and Barto, A. G. (1986).\rSimulation of the classically conditioned nictitating membrane response by a\rneuron-like adaptive element: I. Response topography, neuronal firing, and\rinterstimulus intervals. Behavioural Brain Research, 21:143-154.\nMoore, J. W., Marks, J. S.,\rCastagna, V. E., and Polewan, R. J. (2001). Parameter stability in the TD model\rof complex CR topographies. Society for Neuroscience Abstract 642.2.\nMoore, J. W. and Schmajuk, N. A. (2008). Kamin\rblocking. Scholarpedia,3(5):3542.\nMoore, J. W. and\rStickney, K. J. (1980). Formation of attentional-associative networks in real\rtime:Role of the hippocampus and implications for conditioning. Physiological\rPsychology, 8(2):207-217.\nMukundan, J. and\rMartinez, J. F. (2012). MORSE: Multi-objective reconfigurable self- optimizing\rmemory scheduler. In IEEE 18th International Symposium on\rHigh Perfor\u0026shy;mance Computer Architecture (HPCA),pages 1-12.\nMuller, M. (2002). Computer Go. Artificial\rIntelligence,134(1):145-179.\nMunos, R., Stepleton,\rT., Harutyunyan, A., and Bellemare, M. (2016). Safe and efficient off- policy\rreinforcement learning. In Advances in Neural\rInformation Processing Systems,\rpp. 1046-1054.\nNaddaf,\rY. (2010). Game-independent AI\ragents for playing Atari 2600 console games.PhD\rthesis, University of Alberta.\nNarendra, K. S., Thathachar, M.\rA. L. (1974). Learning automata��A survey. IEEE Transactions on\rSystems, Man, and Cybernetics,\r4:323-334.\nNarendra, K. S., Thathachar, M.\rA. L. (1989). Learning Automata: An Introduction. Prentice-Hall, Englewood Cliffs, NJ.\nNarendra, K. S. and Wheeler, R.\rM. (1983). An n-player sequential stochastic game with\nidentical payoffs. IEEE Transactions on Systems, Man, and Cybernetics,13:1154-1158.\nNarendra, K. S., Wheeler, R. M.\r(1986). Decentralized learning in finite Markov chains. IEEE\rTransactions on Automatic Control,AC31(6):519-526.\nNedic, A., Bertsekas, D. P.\r(2003). Least squares policy evaluation algorithms with linear function\rapproximation. Discrete Event Dynamic Systems 13(1-2):79-110.\nNg, A. Y. (2003). Shaping\rand policy search in reinforcement learning.PhD thesis, Univer\u0026shy;sity of California, Berkeley,\rBerkeley, CA.\nNg, A. Y., Harada, D.,\rand Russell, S. (1999). Policy invariance under reward trans\u0026shy;formations: Theory\rand application to reward shaping. In Bratko, I. and Dzeroski, S., editors, Proceedings\rof the Sixteenth International Conference on Machine Learning (ICML 1999),volume 99, pp. 278-287.\nNg, A. Y., and Russell, S. J.\r(2000). Algorithms for inverse reinforcement learning. In International\rConference on Machine Learning,pp. 663-670.\nNie, J., Haykin, S. (1996). A\rdynamic channel assignment policy through Q-learning. CRL Report 334.\rCommunications Research Laboratory, McMaster University, Hamilton, Ontario.\nNiv, Y. (2009). Reinforcement\rlearning in the brain. Journal of Mathematical Psychology, 53(3):139-154.\nNiv, Y., Daw, N. D., and Dayan,\rP. (2005). How fast to work: Response vigor, motivation and tonic dopamine. In\rYeiss, Y., Scholkopft, B., and Platt, J., editors, Advances\rin Neural Information Processing Systems 18 (NIPS 2005),pages 1019-1026. MIT Press, Cambridge, MA.\nNiv, Y., Daw, N. D., Joel, D.,\rand Dayan, P. (2007). Tonic dopamine: opportunity costs and the control of\rresponse vigor. Psychopharmacology,191(3):507-520.\nNiv, Y., Joel, D., and Dayan, P.\r(2006). A normative perspective on motivation. Trends\rin Cognitive Sciences,10(8):375-381.\nNowe, A., Vrancx, P., and\rHauwere, Y.-M. D. (2012). Game theory and multi-agent reinforce\u0026shy;ment learning.\rIn Wiering, M. and van Otterlo, M., editors, Reinforcement\rLearning: State-of-the-Art,\rpages 441-467. Springer-Verlag, Berlin.\nNutt, D. J., Lingford-Hughes, A., Erritzoe, D., and Stokes, P. R. A.\r(2015). The dopamine theory of addiction: 40 years of highs and lows. Nature\rReviews Neuroscience,16:305\u0026shy;\n312.\nO,Doherty, J. P.,\rDayan, P., Friston, K., Critchley, H., and Dolan, R. J. (2003). Temporal\rdifference models and reward-related learning in the human brain. Neuron,38(2):329- 337.\nO,Doherty, J. P.,\rDayan, P., Schultz, J., Deichmann, R., Friston, K., and Dolan, R. J. (2004).\rDissociable roles of ventral and dorsal striatum in instrumental conditioning. Science, 304(5669):452-454.\n(Olafsdottir, H. F.,\rBarry, C., Saleem, A. B., Hassabis, D., and Spiers, H. J. (2015). Hip\u0026shy;pocampal\rplace cells construct reward related sequences through unexplored space. Elife, 4:e06063.\nOh, J., Guo, X., Lee,\rH., Lewis, R. L., and Singh, S. (2015). Action-conditional video prediction\rusing deep networks in Atari games. In Advances in Neural\rInformation Processing Systems,\rpages 2845-2853.\nOlds, J. and Milner, P. (1954). Positive reinforcement produced by\relectrical stimulation of the septal area and other regions of rat brain. Journal\rof Comparative and Physiological\nPsychology, 47(6):419-427.\nOliehoek, F. A. (2012).\rDecentralized POMDPs. In Wiering and van Otterlo (Eds.) Rein\u0026shy;forcement\rLearning: State-of-the Art,pp. 471-503. Springer Berlin Heidelberg.\nO��Reilly, R. C. and Frank, M. J.\r(2006). Making working memory work: A computational model of learning in the\rprefrontal cortex and basal ganglia. Neural Computation, 18(2):283-328.\nO��Reilly, R. C., Frank, M. J.,\rHazy, T. E., and Watz, B. (2007). PVLV: the primary value and learned value\rPavlovian learning algorithm. Behavioral neuroscience,121(1):31-49.\nOmohundro, S. M. (1987). Efficient\ralgorithms with neural network behavior. Technical Report, Department of\rComputer Science, University of Illinois at Urbana-Champaign.\nOrenstein, J. A.\r(1982). Multidimensional tries used for associative searching. Information\rProcessing Letters 14(4):150-157.\nOrmoneit, D., and Sen, S. (2002).\rKernel-based reinforcement learning. Machine learning 49(2-3):161-178.\nOudeyer, P.-Y. and\rKaplan, F. (2007). What is intrinsic motivation? A typology of compu\u0026shy;tational\rapproaches. Frontiers in Neurorobotics, 1.\nOudeyer, P.-Y.,\rKaplan, F., and Hafner, V. V. (2007). Intrinsic motivation systems for\rautonomous mental development. IEEE Transactions on\rEvolutionary Computation, 11(2):265-286.\nPadoa-Schioppa, C.,\rand Assad, J. A. (2006). Neurons in the orbitofrontal cortex encode economic\rvalue. Nature 441(7090):223-226.\nPage, C. V. (1977). Heuristics\rfor signature table analysis as a pattern recognition technique. IEEE\rTransactions on Systems, Man, and Cybernetics,7:77-86.\nPagnoni, G., Zink, C. F.,\rMontague, P. R., and Berns, G. S. (2002). Activity in human ventral striatum\rlocked to errors of reward prediction. Nature neuroscience,5(2):97-98.\nPan, W.-X., Schmidt, R., Wickens,\rJ. R., and Hyland, B. I. (2005). Dopamine cells respond to predicted events\rduring classical conditioning: Evidence for eligibility traces in the\rreward-learning network. The Journal of Neuroscience,25(26):6235-6242.\nPark, J., Kim, J., Kang, D.\r(2005). An RLS-based natural actor-critic algorithm for loco\u0026shy;motion of a\rtwo-linked robot arm. Computational Intelligence and\rSecurity,65-72.\nParker, D. B. (1985). Learning\rLogic.???\nParks, P. C., Militzer, J.\r(1991). Improved allocation of weights for associative memory storage in\rlearning control systems. IFAC Design Methods of Control\rSystems,Zurich,\rSwitzerland, 507-512.\nParr, R., Russell, S. (1995).\rApproximating optimal policies for partially observable stochas\u0026shy;tic domains. In\rProceedings of the Fourteenth International Joint\rConference on Artifi\u0026shy;cial Intelligence,pp. 1088-1094. Morgan Kaufmann.\nPavlov, P. I. (1927). Conditioned\rReflexes.Oxford\rUniversity Press, London.\nPawlak, V. and Kerr, J. N. D.\r(2008). Dopamine receptor activation is required for corti\u0026shy;costriatal\rspike-timing-dependent plasticity. The Journal of\rNeuroscience,28(10):2435- 2446.\nPawlak, V., Wickens, J. R.,\rKirkwood, A., and Kerr, J. N. D. (2010). Timing is not everything:\rneuromodulation opens the STDP gate. Frontiers in synaptic\rneuroscience, 2.\nPearce, J. M. and Hall, G.\r(1980). A model for Pavlovian learning: Variation in the effective\u0026shy;ness of\rconditioning but not unconditioned stimuli. Psychological\rReview,87(6):532-552.\nPearl,\rJ. (1984). Heuristics:\rIntelligent Search Strategies for Computer Problem Solving. Addison-Wesley,\rReading, MA.\nPearl, J. (1995). Causal diagrams for empirical\rresearch. Biometrika,82(4), 669-688.\nPecevski, D., Maass, W., and\rLegenstein, R. A. (2007). Theoretical analysis of learning with\rreward-modulated spike-timing-dependent plasticity. In Advances\rin Neural Information Processing Systems,pp. 881-888.\nPeng, J. (1993). Efficient\rDynamic Programming-Based Learning for Control.Ph.D. thesis, Northeastern University, Boston.\nPeng, J. (1995). Efficient\rmemory-based dynamic programming. In 12th International Con\u0026shy;ference\ron Machine Learning,pp. 438-446.\nPeng, J., Williams, R.\rJ. (1993). Efficient learning and planning within the Dyna framework. Adaptive\rBehavior,1(4):437-454.\nPeng, J., Williams, R.\rJ. (1994). Incremental multi-step Q-learning. In W. W. Cohen and H. Hirsh\r(eds.), Proceedings of the Eleventh International Conference\ron Machine Learning,pp. 226-232. Morgan Kaufmann, San Francisco.\nPeng, J., Williams, R. J. (1996).\rIncremental multi-step Q-learning. Machine Learning, 22:283-290.\nPerkins, T. J.,\rPendrith, M. D. (2002). On the existence of fixed points for Q-learning and\rSarsa in partially observable domains. In Proceedings of the\rInternational Conference on Machine Learning,pp. 490-497.\nPerkins, T. J., Precup, D.\r(2003). A convergent form of approximate policy iteration. In\nAdvances in\rneural information processing systems, proceedings of the 2002 conference,\rpp. 1595-1602.\nPeters, J. and Buchel, C. (2010).\rNeural representations of subjective reward value. Behav\u0026shy;ioral\rbrain research,213(2):135-141.\nPeters, J., Schaal, S. (2008). Natural actor-critic.\rNeurocomputing 71(7), 1180-1190.\nPeters, J., Vijayakumar, S.,\rSchaal, S. (2005). Natural actor-critic. In European\rConference on Machine Learning(pp. 280-291). Springer Berlin Heidelberg.\nPeterson, G. B. (2004). A day of\rgreat illumination: B.F. Skinner��s discovery of shaping. Journal\rof the Experimental Analysis of Behavior,82(3):317-28.\nPezzulo, G., van der\rMeer, M. A. A., Lansink, C. S., and Pennartz, C. M. A. (2014). Inter\u0026shy;nally\rgenerated sequences in learning and executing goal-directed behavior. Trends\rin Cognitive Science,18(12):647-657.\nPfeiffer, B. E. and Foster, D. J.\r(2013). Hippocampal place-cell sequences depict future paths to remembered\rgoals. Nature,497(7447):74-79.\nPhansalkar, V. V., Thathachar, M.\rA. L. (1995). Local and global optimization algorithms for generalized learning\rautomata. Neural Computation, 7:950-973.\nPoggio, T., Girosi, F. (1989). A theory\rof networks for approximation and learning. A.I. Memo 1140. Artificial\rIntelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA.\nPoggio, T., Girosi, F. (1990).\rRegularization algorithms for learning that are equivalent to multilayer\rnetworks. Science,247:978-982.\nPolyak, B. T. (1990). New\rstochastic approximation type procedures. Automat. i Telemekh 7(98-107), 2 (in Russian).\nPolyak, B. T., Juditsky, A. B.\r(1992). Acceleration of stochastic approximation by averaging. SIAM\rJournal on Control and Optimization 30(4), 838-855.\nPowell, M.\rJ. D. (1987). Radial basis functions for multivariate interpolation: A review.\nIn J. C. Mason and M. G. Cox (eds.), Algorithms for Approximation,pp. 143-167. Clarendon Press, Oxford.\nPowell, W. B. (2011). Approximate Dynamic Programming: Solving the Curses\rof Dimen\u0026shy;sionality, Second edition. John Wiley and Sons.\nPowers, W. T. (1973). Behavior: The Control of Perception.Aldine de Gruyter, Chicago. 2nd expanded edition\r2005.\nPrecup, D., Sutton, R. S.,\rDasgupta, S. (2001). Off-policy temporal-difference learning with function\rapproximation. In Proceedings\rof the 18th International Conference on Machine Learning.\nPrecup, D., Sutton, R. S.,\rPaduraru, C., Koop, A., and Singh, S. (2005). Off-policy learning with options\rand recognizers. In Advances\rin Neural Processing Systems,pp. 1097-1104.\nPrecup, D.,\rSutton, R. S., Singh, S. (2000). Eligibility traces for off-policy policy\revaluation.\nIn Proceedings of the 17th International Conference on Machine Learning, pp. 759-766. Morgan\rKaufmann.\nPuterman, M. L. (1994). Markov Decision Problems.Wiley, New York.\nPuterman, M. L., Shin, M. C.\r(1978). Modified policy iteration algorithms for discounted Markov decision\rproblems. Management Science, 24:1127-1137.\nQuartz, S., Dayan, P.,\rMontague, P. R., and Sejnowski, T. J. (1992). Expectation learning in the brain\rusing diffuse ascending connections. In Society for Neuroscience Abstracts, volume 18, page 1210.\nRandl0v, J. and Alstr0m, P.\r(1998). Learning to drive a bicycle using reinforcement learning and shaping.\rIn Proceedings of the\rFifteenth International Conference on Machine Learning,pages 463-471.\nRangel, A., Camerer, C., and\rMontague, P. R. (2008). A framework for studying the neurobiology of\rvalue-based decision making. Nature Reviews Neuroscience,9(7):545- 556.\nRangel, A. and Hare, T.\r(2010). Neural computations associated with goal-directed choice. Current opinion in neurobiology, 20(2):262-270.\nReddy, G., Celani, A.,\rSejnowski, T. J., and Vergassola, M. (2016). Learning to soar in tur\u0026shy;bulent\renvironments. Proceedings of\rthe National Academy of Sciences,113(33):E4877- E4884.\nRedgrave, P. and Gurney, K.\r(2006). The short-latency dopamine signal: a role in discovering novel actions?\rNature Reviews Neuroscience, 7:967-975.\nRedish, D. A. (2004). Addiction as a computational process gone\rawry. Science,306(5703):1944- 1947.\nReetz, D. (1977). Approximate\rsolutions of a discounted Markovian decision process. Bonner Mathematische Schriften,98:77-92.\nRescorla, R. A. and Wagner, A.\rR. (1972). A theory of Pavlovian conditioning: Variations in the effectiveness\rof reinforcement and nonreinforcement. In Black, A. H. and Prokasy,\nW. F., editors, Classical\rConditioningII, pages 64-99.\rAppleton-Century-Crofts, New York.\nRevusky, S. and Garcia, J.\r(1970). Learned associations over long delays. In Bower, G., editor, The psychology of learning and motivation,volume 4, pages 1-84. Academic Press, Inc., New\rYork.\nReynolds,\rJ. N. J. and Wickens, J. R. (2002). Dopamine-dependent plasticity of\rcorticostri- atal synapses. Neural\rNetworks,\r15(4):507-521.\nRing, M. B. (1994). Continual\rLearning in Reinforcement Environments.Ph.D. thesis, University of Texas, Austin.\nRipley, B. D. (2007). Pattern\rRecognition and Neural Networks.Cambridge University Press.\nRivest, R. L.,\rSchapire, R. E. (1987). Diversity-based inference of finite automata. In Pro\u0026shy;ceedings\rof the Twenty-Eighth Annual Symposium on Foundations of Computer Science, pp. 78-87. Computer Society Press of the IEEE,\rWashington, DC.\nRixner, S. (2004).\rMemory controller optimizations for web servers. In Proceedings\rof the 37th annual IEEE/A CM International Symposium on Microarchitecture,pages 355-366. IEEE Computer Society.\nRobbins, H. (1952).\rSome aspects of the sequential design of experiments. Bulletin\rof the American Mathematical Society, 58:527-535.\nRobertie, B. (1992).\rCarbon versus silicon: Matching wits with TD-Gammon. Inside\rBackgammon,2:14-22.\nRoesch, M. R., Calu,\rD. J., and Schoenbaum, G. (2007). Dopamine neurons encode the better option in\rrats deciding between differently delayed or sized rewards. Nature\rNeuroscience,10(12):1615-1624.\nRomo, R. and Schultz,\rW. (1990). Dopamine neurons of the monkey midbrain: Contin\u0026shy;gencies of responses\rto active touch during self-initiated arm movements. Journal\rof Neurophysiology,63(3):592-624.\nRosenblatt,\rF. (1962). Principles of\rNeurodynamics: Perceptrons and the Theory of Brain Mechanisms.\rSpartan Books, Washington, DC.\nRoss,\rS. (1983). Introduction to\rStochastic Dynamic Programming. Academic Press, New\rYork.\nRoss, T. (1933). Machines that think. Scientific\rAmerican,pages\r206-208.\nRubinstein, R. Y. (1981). Simulation\rand the Monte Carlo Method.Wiley, New York.\nRumelhart, D. E., Hinton, G. E.,\rWilliams, R. J. (1986). Learning internal representations by error propagation.\rIn D. E. Rumelhart and J. L. McClelland (eds.), Parallel\rDis\u0026shy;tributed Processing: Explorations in the Microstructure of Cognition,vol. I, Foundations. Bradford/MIT Press, Cambridge, MA.\nRummery, G. A. (1995). Problem\rSolving with Reinforcement Learning.Ph.D. thesis, Cambridge University.\nRummery, G. A., Niranjan, M.\r(1994). On-line Q-learning using connectionist systems. Technical Report CUED/F-INFENG/TR\r166. Engineering Department, Cambridge University.\nRuppert, D. (1988).\rEfficient estimations from a slowly convergent Robbins-Monro process. Cornell\rUniversity Operations Research and Industrial Engineering Technical Report No.\r781.\nRussell, S., Norvig, P. (2010). Artificial\rIntelligence: A Modern Approach,3rd edition. Prentice-Hall, Englewood Cliffs, NJ.\nRust, J. (1996).\rNumerical dynamic programming in economics. In H. Amman, D. Kendrick, and J.\rRust (eds.), Handbook of Computational Economics,pp. 614-722. Elsevier, Am\u0026shy;sterdam.\nRyan, R. M. and Deci, E. L.\r(2000). Intrinsic and extrinsic motivations: Classic definitions and new\rdirections. Contemporary Educational Psychology,25(1):54-67.\nSaddoris, M. P., Cacciapaglia,\rF., Wightmman, R. M., and Carelli, R. M. (2015). Differential dopamine release\rdynamics in the nucleus accumbens core and shell reveal complemen\u0026shy;tary signals\rfor error prediction and incentive motivation. The\rJournal of Neuroscience, 35(33):11572-11582.\nSaksida, L. M., Raymond, S. M.,\rand Touretzky, D. S. (1997). Shaping robot behavior using principles from\rinstrumental conditioning. Robotics and Autonomous\rSystems, 22(3):231-249.\nSamuel, A. L. (1959). Some\rstudies in machine learning using the game of checkers. IBM\rJournal on Research and Development,3:211-229. Reprinted in E. A. Feigenbaum and J.\rFeldman (eds.), Computers and Thought, pp. 71-105. McGraw-Hill, New York, 1963.\nSamuel, A. L. (1967). Some\rstudies in machine learning using the game of checkers. II�� Recent progress. IBM\rJournal on Research and Development, 11:601-617.\nSchaal, S., and Atkeson, C. G.\r(1994). Robot juggling: Implementation of memory-based learning. IEEE\rControl Systems 14(1):57-71.\nSchmajuk, N. A. (2008).\rComputational models of classical conditioning. Scholarpedia,\r3(3):1664.\nSchmidhuber, J. (1991a). Adaptive\rconfidence and adaptive curiosity. Technical Report FKI-149-91, Institut fiir\rInformatik, Technische Universitat Munchen, Arcisstr. 21, 800 Miinchen 2,\rGermany.\nSchmidhuber, J.\r(1991b). A possibility for implementing curiosity and boredom in model-\rbuilding neural controllers. In From Animals to Animats:\rProceedings of the First In\u0026shy;ternational Conference on Simulation of Adaptive\rBehavior,pages\r222-227, Cambridge, MA. MIT Press.\nSchmidhuber, J.\r(2009). Driven by compression progress: A simple principle explains essen\u0026shy;tial\raspects of subjective beauty, novelty, surprise, interestingness, attention,\rcuriosity, creativity, art, science, music, jokes. In Pezzulo, G., Butz, M. V.,\rSigaud, O., and Baldassarre, G., editors, Anticipatory Behavior in\rAdaptive Learning Systems. From Psychological Theories to Artificial Cognitive\rSystems, pages 48-76. Springer,\rBerlin.\nSchmidhuber, J. (2015). Deep\rlearning in neural networks: An overview. Neural Networks 61,85-117.\nSchmidhuber, J., Storck, J., and\rHochreiter, S. (1994). Reinforcement driven information acquisition in\rnondeterministic environments. Technical report, Fakultat fiir Informatik,\rTechnische Universitat Miinchen, Miinchen, Germany.\nSchultz, D. G., Melsa, J. L.\r(1967). State Functions and Linear Control Systems.McGraw- Hill, New York.\nSchultz, W. (1998). Predictive\rreward signal of dopamine neurons. Journal of Neurophysi\u0026shy;ology,80:1-27.\nSchultz, W., Apicella, P., and\rLjungberg, T. (1993). Responses of monkey dopamine neurons to reward and\rconditioned stimuli during successive steps of learning a delayed response\rtask. Journal of Neuroscience 13(3):900-913.\nSchultz, W., Dayan, P., Montague,\rP. R. (1997). A neural substrate of prediction and reward. Science,275:1593-1598.\nSchultz, W. and Romo, R. (1990).\rDopamine neurons of the monkey midbrain: contingencies of responses to stimuli\reliciting immediate behavioral reactions. Journal of Neurophysi\u0026shy;ology,63(3):607-624.\nSchultz, W., Romo, R., Ljungberg,\rT., Mirenowicz, J., Hollerman, J. R., and Dickinson, A. (1995). Reward-related\rsignals carried by dopamine neurons. In Houk, Davis, and Beiser (Eds.) Models\rof Information Processing in the Basal Ganglia,pp. 233-248. MIT Press\n\r\rSchumaker, L. L. (1976). Fitting\rSurfaces to Scattered Data.University of Texas at Austin,\nDept. of Mathematics.\nSchwartz, A. (1993). A\rreinforcement learning method for maximizing undiscounted rewards.\nIn Proceedings of the Tenth International Conference on Machine\rLearning,pp. 298-305.\nMorgan Kaufmann, San Mateo, CA.\nSchweitzer, P. J., Seidmann, A.\r(1985). Generalized polynomial approximations in Marko\u0026shy;vian decision processes.\rJournal of Mathematical Analysis and Applications, 110:568\u0026shy;582.\nSelfridge, O. G. (1978). Tracking\rand trailing: Adaptation in movement strategies. Technical report, Bolt Beranek\rand Newman, Inc. Unpublished report.\nSelfridge, O. G. (1984). Some\rthemes and primitives in ill-defined systems. In Selfridge,\nO. G., Rissland, E. L., and Arbib, M. A., editors, Adaptive\rControl of Ill-Defined Systems, pages\r21-26. Plenum Press, NY. Proceedings of the NATO Advanced Research Institute on\rAdaptive Control of Ill-defined Systems, NATO Conference Series II, Systems\rScience,\nVol. 16.\nSelfridge, O. J., Sutton, R. S.,\rBarto, A. G. (1985). Training and tracking in robotics.\nIn A. Joshi (ed.), Proceedings of the Ninth International Joint\rConference on Artificial Intelligence, pp. 670-672. Morgan\rKaufmann, San Mateo, CA.\nSeo, H., Barraclough, D., and\rLee, D. (2007). Dynamic signals related to choices and outcomes in the\rdorsolateral prefrontal cortex. Cerebral Cortex,17(suppl 1):110-117.\nSeung, H. S. (2003). Learning in\rspiking neural networks by reinforcement of stochastic synaptic transmission. Neuron, 40(6):1063-1073.\nShah, A. (2012). Psychological\rand neuroscientific connections with reinforcement learning.\nIn Wiering, M. and van Otterlo, M., editors, Reinforcement\rLearning: State of the Art,\rpages 507-537. Springer-Verlag, Berlin.\nShannon, C. E. (1950).\rProgramming a computer for playing chess. Philosophical Magazine, 41:256-275.\nShannon, C. E. (1951).\rPresentation of a maze-solving machine. In Forester, H. V., editor, Cybernetics.\rTransactions of the Eighth Conference,pages 173-180. Josiah Macy Jr. Foundation.\nShannon, C. E. (1952). ��Theseus�� maze-solving mouse. http://cyberneticzoo.com/mazesolvers/1952--theseus-maze-solving-mouse--claude-shannon-american/.\nShelton,\rC. R. (2001). Importance Sampling\rfor Reinforcement Learning with Multiple Ob\u0026shy;jectives.PhD\rthesis, Massachusetts Institute of Technology.\nShepard, D. (1968). A\rtwo-dimensional interpolation function for irregularly-spaced data.\nIn Proceedings of the 23rd ACM National Conference,pp.\r517-524. ACM.\nSherman, J., Morrison, W. J.\r(1949). Adjustment of an inverse matrix corresponding to changes in the elements\rof a given column or a given row of the original matrix (abstract).\nAnnals of\rMathematical Statistics 20:621.\nShewchuk, J., Dean, T. (1990).\rTowards learning time-varying functions with high input dimensionality. In Proceedings\rof the Fifth IEEE International Symposium on Intelligent Control,pp. 383-388. IEEE Computer Society Press, Los\rAlamitos, CA.\nShimansky, Y. P. (2009).\rBiologically plausible learning in neural networks: a lesson from bacterial\rchemotaxis. Biological Cybernetics,101(5-6):379-385.\nSi, J., Barto,\rA., Powell, W., Wunsch, D. (Eds.). (2004). Handbook of learning and\rapproxi\u0026shy;mate dynamic programming.John Wiley and Sons.\nSilver,\rD. (2009). Reinforcement learning\rand simulation based search in the game of Go. University\rof Alberta Doctoral dissertation.\nSilver, D., Huang, A., Maddison,\rC. J., Guez, A., Sifre, L., van den Driessche, G., Schrit- twieser, J.,\rAntonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe, D., Nham,\rJ., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K.,\rGraepel, T., and Hassabis, D. (2016). Mastering the game of go with deep neural\rnetworks and tree search. Nature,529(7587):484-489.\nSilver, D., Lever, G., Heess, N.,\rDegris, T., Wierstra, D., Riedmiller, M. (2014). Determin\u0026shy;istic policy gradient\ralgorithms. In Proceedings of the 31st\rInternational Conference on Machine Learning(ICML-14) (pp. 387-395).\nSimsek, O., Algorta, S., and\rKothiyal, A. (2016). Why most decisions are easy in tetris-and perhaps in other\rsequential decision problems, as well. Proceedings of 33rd\rInternational Conference on Machine Learning.\nSingh, S. P. (1992a).\rReinforcement learning with a hierarchy of abstract models. In Proceedings\rof the Tenth National Conference on Artificial Intelligence, pp. 202-207. AAAI/MIT Press, Menlo Park, CA.\nSingh, S. P. (1992b).\rScaling reinforcement learning algorithms by learning variable tem\u0026shy;poral resolution\rmodels. In Proceedings of the Ninth\rInternational Machine Learning Conference,pp. 406-415. Morgan Kaufmann, San Mateo, CA.\nSingh, S. P. (1993). Learning\rto Solve Markovian Decision Processes.Ph.D. thesis, Univer\u0026shy;sity of Massachusetts, Amherst.\rAppeared as CMPSCI Technical Report 93-77.\nSingh, S. P. (Ed.) (2002).\rSpecial double issue on reinforcement learning, Machine\rLearning\n49(2/3).\nSingh, S., Barto, A.\rG., and Chentanez, N. (2005). Intrinsically motivated reinforcement learning.\rIn Advances in Neural Information Processing Systems 17:\rProceedings of the 2004 Conference,pages 1281-1288, Cambridge MA. MIT Press.\nSingh, S. P.,\rBertsekas, D. (1997). Reinforcement learning for dynamic channel allocation in\rcellular telephone systems. In Advances in Neural\rInformation Processing Systems: Proceedings of the 1996 Conference,pp. 974-980. MIT Press, Cambridge, MA.\nSingh, S. P.,\rJaakkola, T., Jordan, M. I. (1994). Learning without state-estimation in\rpartially observable Markovian decision problems. In W. W. Cohen and H. Hirsch\r(eds.), Proceedings of the Eleventh International Conference\ron Machine Learning, pp.\r284-292. Morgan Kaufmann, San Francisco.\nSingh, S. P., Jaakkola, T.,\rJordan, M. I. (1995). Reinforcement learing with soft state aggregation. In G.\rTesauro, D. S. Touretzky, T. Leen (eds.), Advances in Neural In\u0026shy;formation\rProcessing Systems: Proceedings of the 1994 Conference,pp. 359-368. MIT Press, Cambridge, MA.\nSingh, S., Lewis, R. L., and\rBarto, A. G. (2009). Where do rewards come from? In Taatgen, N. and van Rijn,\rH., editors, Proceedings of the 31st Annual\rConference of the Cognitive Science Society, pages 2601-2606. Cognitive Science Society.\nSingh, S., Lewis, R.\rL., Barto, A. G., and Sorg, J. (2010). Intrinsically motivated reinforce\u0026shy;ment\rlearning: An evolutionary perspective. IEEE Transactions on\rAutonomous Mental Development,2(2):7082. Special issue on Active Learning and\rIntrinsically Motivated Exploration in Robots: Advances and Challenges.\nSingh, S. P., Sutton,\rR. S. (1996). Reinforcement learning with replacing eligibility traces. Machine\rLearning, 22:123-158.\nSivarajan, K. N., McEliece, R.\rJ., Ketchum, J. W. (1990). Dynamic channel\rassignment in cellular radio. In Proceedings of the 40th Vehicular\rTechnology Conference,pp. 631-637.\nSkinner,\rB. F. (1938). The Behavior of\rOrganisms: An Experimental Analysis.Appleton-\rCentury, New York.\nSkinner, B. F. (1958). Reinforcement today. American\rPsychologist,13(3):94-99.\nSkinner, B. F. (1981). Selection by consequences. Science\r213(4507):501-504.\nSmith, K. S. and Greybiel, A. M.\r(2013). A dual operator view of habitual behavior reflecting cortical and\rstriatal dynamics. Neuron,79(2):361-374.\nSofge, D. A., White, D. A. (1992).\rApplied learning: Optimal control for manufacturing. In D. A. White and D. A.\rSofge (eds.), Handbook of Intelligent Control:\rNeural, Fuzzy, and Adaptive Approaches,pp. 259-281. Van Nostrand Reinhold, New York.\nSorg, J. D. (2011). The\rOptimal Reward Problem:Designing Effective Reward for Bounded Agents.PhD thesis, Computer Science and Engineering, The\rUniversity of Michigan.\nSorg, J., Lewis, R. L., and\rSingh, S. P. (2010). Reward design via online gradient ascent. In Advances\rin Neural Information Processing Systems,pp. 2190-2198.\nSorg, J., Singh, S., and Lewis,\rR. (2010). Internal rewards mitigate agent boundedness. In Proceedings\rof the 27th International Conference on Machine Learning (ICML),pages 1007-1014.\nSpaan, M. T. (2012).\rPartially observable Markov decision processes. In Wiering and van Otterlo\r(Eds.) Reinforcement Learning: State-of-the Art,pp. 387-414. Springer Berlin Heidelberg.\nSpence, K. W. (1947). The role of\rsecondary reinforcement in delayed reward learning. Psychological\rReview,54(1):1-8.\nSpong, M. W. (1994).\rSwing up control of the acrobot. In Proceedings of the 1994\rIEEE Conference on Robotics and Automation,pp. 2356-2361. IEEE Computer Society Press, Los\rAlamitos, CA.\nSrivastava, N., Hinton,\rG., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: A\rsimple way to prevent neural networks from overfitting. The\rJournal of Machine Learning Research,15(1):1929-1958.\nStaddon, J. E. R.\r(1983). Adaptive Behavior and Learning.Cambridge University Press, Cambridge.\nStanfill, C., and\rWaltz, D. (1986). Toward memory-based reasoning. Communications\rof the ACM 29(12):1213-1228.\nSteinberg, E. E.,\rKeiflin, R., Boivin, J. R., Witten, I. B., Deisseroth, K., and Janak, P. H.\n(2013)\u0026nbsp;\r. A causal link\rbetween prediction errors, dopamine neurons and learning. Nature\rNeuroscience,16(7):966-973.\nSterling, P. and\rLaughlin, S. (2015). Principles of Neural Design.MIT Press, Cambridge, MA.\nStorck, J.,\rHochreiter, S., and Schmidhuber, J. (1995). Reinforcement-driven information\racquisition in non-deterministic environments. In Proceedings\rof ICANN��95, Paris, France, volume 2, pages 159-164.\nSugiyama,\rM., Hachiya, H., Morimura, T. (2013). Statistical Reinforcement Learning: Mod\u0026shy;ern Machine Learning\rApproaches.Chapman \u0026amp; Hall/CRC.\nSuri, R. E., Bargas, J., and\rArbib, M. A. (2001). Modeling functions of striatal dopamine modulation in\rlearning and planning. Neuroscience,103(1):65-85.\nSuri, R. E. and Schultz, W.\r(1998). Learning of sequential movements by neural net\u0026shy;work model with\rdopamine-like reinforcement signal. Experimental Brain\rResearch, 121(3):350-354.\nSuri, R. E. and Schultz, W.\r(1999). A neural network model with dopamine-like reinforce\u0026shy;ment signal that\rlearns a spatial delayed response task. Neuroscience,91(3):871-890.\nSutton, R. S. (1978a). Learning\rtheory support for a single channel theory of the brain. Unpublished report.\nSutton, R. S. (1978b). Single\rchannel theory: A neuronal theory of learning. Brain\rTheory Newsletter,4:72-75. Center for Systems Neuroscience, University of\rMassachusetts, Amherst, MA.\nSutton, R. S. (1978c). A unified\rtheory of expectation in classical and instrumental condi\u0026shy;tioning. Bachelors\rthesis, Stanford University.\nSutton, R. S. (1984). Temporal\rCredit Assignment in Reinforcement Learning.Ph.D. thesis, University of Massachusetts, Amherst.\nSutton, R. S. (1988). Learning to\rpredict by the method of temporal differences. Machine\rLearning,3:9-44.\nSutton, R. S. (1990). Integrated\rarchitectures for learning, planning, and reacting based on approximating\rdynamic programming. In Proceedings of the Seventh\rInternational Conference on Machine Learning,pp. 216-224. Morgan Kaufmann, San Mateo, CA.\nSutton, R. S. (1991a). Dyna, an\rintegrated architecture for learning, planning, and reacting. SIGART\rBulletin,2:160-163.\rACM Press.\nSutton, R. S. (1991b). Planning\rby incremental dynamic programming. In L. A. Birnbaum and G. C. Collins (eds.),\rProceedings of the Eighth International Workshop on Machine\rLearning,pp.\r353-357. Morgan Kaufmann, San Mateo, CA.\nSutton, R. S. (Ed.) (1992). Reinforcement\rLearning.Kluwer\rAcademic Press. Reprinting of a special double issue on reinforcement learning,\rMachine Learning 8(3/4).\nSutton, R. S. (1995a). TD models:\rModeling the world at a mixture of time scales. In A. Prieditis and S. Russell\r(eds.), Proceedings of the Twelfth International Conference\ron Machine Learning, pp.\r531-539. Morgan Kaufmann, San Francisco.\nSutton, R. S. (1995). On the\rvirtues of linear learning and trajectory distributions. Proceed\u0026shy;ings\rof the Workshop on Value Function Approximationat the International Conference on Machine Learning.\nSutton, R. S. (1996).\rGeneralization in reinforcement learning: Successful examples using sparse\rcoarse coding. In D. S. Touretzky, M. C. Mozer and M. E. Hasselmo (eds.), Advances\rin Neural Information Processing Systems: Proceedings of the 1995 Conference, pp. 1038-1044. MIT Press, Cambridge, MA.\nSutton,\rR. S. (2009). The grand challenge of predictive empirical abstract knowledge. Work\u0026shy;ing Notes of the IJCAI-09 Workshop on Grand\rChallenges for Reasoning from Experi\u0026shy;ences.\nSutton, R. S. (2015a)\rIntroduction to reinforcement learning with function approximation. Tutorial at\rthe Conference on Neural Information Processing Systems, Montreal, De\u0026shy;cember 7,\r2015.\nSutton, R. S. (2015b)\rTrue online Emphatic TD(A): Quick reference and implementation guide.\rArXiv:1507.07147. Code is available in Python and C++ by downloading the source\rfiles of this arXiv paper as a zip archive.\nSutton, R. S., Barto,\rA. G. (1981a). Toward a modern theory of adaptive networks: Expec\u0026shy;tation and\rprediction. Psychological Review, 88:135-170.\nSutton, R. S., Barto, A. G.\r(1981b). An adaptive network that constructs and uses an internal model of its\rworld. Cognition and Brain Theory,3:217-246.\nSutton, R. S., Barto, A. G.\r(1987). A temporal-difference model of classical conditioning. In Proceedings\rof the Ninth Annual Conference of the Cognitive Science Society, pp. 355\u0026shy;378. Erlbaum, Hillsdale, NJ.\nSutton, R. S., Barto, A. G. (1990).\rTime-derivative models of Pavlovian reinforcement. In M. Gabriel and J. Moore\r(eds.), Learning and Computational Neuroscience: Foundations\rof Adaptive Networks, pp.\r497-537. MIT Press, Cambridge, MA.\nSutton, R. S., Maei,\rH. R., Precup, D., Bhatnagar, S., Silver, D., Szepesvari, Cs., and Wiewiora, E.\r(2009). Fast gradient-descent methods for temporal-difference learning with\rlinear function approximation. In Proceedings of the 26th\rAnnual International Conference on Machine Learning, pp. 993-1000. ACM.\nSutton, R. S., Maei,\rH. R., and Szepesvari, Cs. (2009). A convergent O(d2) temporal-\rdifference algorithm for off-policy learning with linear function\rapproximation. In Ad\u0026shy;vances in Neural Information\rProcessing Systems, pp.\r1609-1616.\nSutton, R. S., Mahmood,\rA. R., Precup, D., van Hasselt, H. (2014). A new Q(A) with interim forward view\rand Monte Carlo equivalence. International Conference\ron Machine Learning 31. JMLR W\u0026amp;CP 32(2).\nSutton, R. S., Mahmood, A. R.,\rWhite, M. (2016). An emphatic approach to the problem of off-policy\rtemporal-difference learning. Journal of Machine\rLearning Research 17(73):1- 29.\nSutton, R. S., McAllester, D. A.,\rSingh, S. P., Mansour, Y. (2000). Policy gradient methods for reinforcement\rlearning with function approximation. In Advances in Neural Infor\u0026shy;mation\rProcessing Systems 99, pp.\r1057-1063.\nSutton, R. S., Modayil, J., Delp,\rM., Degris, T., Pilarski, P. M., White, A., Precup, D. (2011). Horde: A\rscalable real-time architecture for learning knowledge from unsuper\u0026shy;vised\rsensorimotor interaction. In Proceedings of the Tenth\rInternational Conference on Autonomous Agents and Multiagent Systems,pp. 761-768, Taipei, Taiwan.\nSutton, R. S., Pinette, B.\r(1985). The learning of world models by connectionist networks. In Proceedings\rof the Seventh Annual Conference of the Cognitive Science Society,pp. 54\u0026shy;64.\nSutton, R. S., Singh,\rS. (1994). On bias and step size in temporal-difference learning. In Proceedings\rof the Eighth Yale Workshop on Adaptive and Learning Systems,pp. 91-96. Center for Systems Science, Dunham\rLaboratory, Yale University, New Haven.\nSutton, R. S.,\rWhitehead, D. S. (1993). Online learning with random representations. In Proceedings\rof the Tenth International Machine Learning Conference,pp. 314-321. Morgan Kaufmann, San Mateo, CA.\nSzepesvari, C. (2010).\rAlgorithms for reinforcement learning. Synthesis Lectures on Artificial\rIntelligence and Machine Learning 4(1), 1-103.\nSzita, I. (2012). Reinforcement\rlearning in games. In Reinforcement Learning(pp. 539-577). Springer Berlin Heidelberg.\nTadepalli, P., Ok, D.\r(1994). H-learning: A reinforcement learning method to optimize undiscounted\raverage reward. Technical Report 94-30-01. Oregon State University, Computer\rScience Department, Corvallis.\nTadepalli, P., and Ok,\rD. (1996). Scaling up average reward reinforcement learning by approximating\rthe domain models and the value function. In International\rConference on Machine Learning,\rpp. 471-479.\nTakahashi, Y., Schoenbaum, G.,\rand Niv, Y. (2008). Silencing the critics: understanding the effects of cocaine\rsensitization on dorsolateral and ventral striatum in the context of an\ractor/critic model. Frontiers in Neuroscience,2(1):86-99.\nTan, M. (1991). Learning a\rcost-sensitive internal representation for reinforcement learning. In L. A.\rBirnbaum and G. C. Collins (eds.), Proceedings of the Eighth\rInternational Workshop on Machine Learning,pp. 358-362. Morgan Kaufmann, San Mateo, CA.\nTan, M. (1993). Multi-agent\rreinforcement learning: Independent vs. cooperative agents. In Proceedings\rof the Tenth International Conference on Machine Learning, pp. 330-337. Morgan Kaufmann, San Mateo, CA.\nTaylor, G., and Parr,\rR. (2009). Kernelized value function approximation for reinforce\u0026shy;ment learning.\rIn Proceedings of the 26th Annual International Conference on\rMachine Learning, pp. 1017-1024.\rACM.\nTaylor, M. E., and Stone, P.\r(2009). Transfer learning for reinforcement learning domains: A survey. Journal\rof Machine Learning Research 10:1633-1685.\nTesauro, G. J. (1986). Simple\rneural models of classical conditioning. Biological Cybernetics, 55:187-200.\nTesauro, G. J. (1992). Practical\rissues in temporal difference learning. Machine Learning, 8:257-277.\nTesauro, G. J. (1994). TD-Gammon,\ra self-teaching backgammon program, achieves master- level play. Neural\rComputation,6(2):215-219.\nTesauro, G. J. (1995). Temporal\rdifference learning and TD-Gammon. Communications of the\rACM,38:58-68.\nTesauro, G. (2002). Programming\rbackgammon using self-teaching neural nets. Artificial\rIntelligence, 134(1):181-199.\nTesauro, G. J., Galperin, G. R.\r(1997). On-line policy improvement using Monte-Carlo search. In Advances\rin Neural Information Processing Systems: Proceedings of the 1996 Conference, pp. 1068-1074. MIT Press, Cambridge, MA.\nTesauro, G., Gondek, D. C.,\rLechner, J., Fan, J., and Prager, J. M. (2012). Simulation, learning, and\roptimization techniques in watson��s game strategies. IBM\rJournal of Research and Development, 56(3.4):16-1-16-11.\nTesauro, G., Gondek, D. C.,\rLenchner, J., Fan, J., and Prager, J. M. (2013). Analysis of WATSON��s strategies for playing Jeopardy! Journal\rof Artificial Intelligence Research, 21:205-251.\nTham,\rC. K. (1994). Modular On-Line\rFunction Approximation for Scaling up Reinforcement Learning.\rPhD thesis, Cambridge University.\nThathachar, M. A. L. and Sastry,\rP. S. (1985). A new approach to the design of reinforcement schemes for\rlearning automata. IEEE Transactions on Systems, Man,\rand Cybernetics, 15:168-175.\nThathachar, M. and Sastry, P. S.\r(2002). Varieties of learning automata: an overview. IEEE\rTransactions on Systems, Man, and Cybernetics, Part B: Cybernetics,36(6):711-722.\nThathachar,\rM. and Sastry, P. S. (2011). Networks\rof learning automata: Techniques for online stochastic optimization.Springer\rScience \u0026amp; Business\rMedia.\nTheocharous, G., Thomas, P. S.,\rand Ghavamzadeh, M. (2015). Personalized ad recommen\u0026shy;dation for life-time value\roptimization guarantees. In Proceedings of the\rTwenty-Fourth International Joint Conference on Artificial Intelligence\r(IJCAI-15).\nThistlethwaite, D. (1951). A\rcritical review of latent learning and related experiments.\nPsychological\rBulletin,48(2):97-129.\nThomas, P. (2014). Bias in\rnatural actor-critic algorithms. International Conference\ron Machine Learning 31. JMLR W\u0026amp;CP 32(1):441-448.\nThomas, P. S. (2015). Safe\rReinforcement Learning.PhD thesis, University of Massachusetts Amherst.\nThomas, P. S.,\rTheocharous, G., and Ghavamzadeh, M. (2015). High-confidence off-policy\revaluation. In Proceedings of the Twenty-Ninth AAAIConference on Artificial Intelli\u0026shy;gence,pages 3000-3006. The AAAI Press, Palo Alto, CA.\nThompson, W. R. (1933). On the\rlikelihood that one unknown probability exceeds another in view of the evidence\rof two samples. Biometrika, 25:285-294.\nThompson, W. R.\r(1934). On the theory of apportionment. American Journal of Mathe\u0026shy;matics,57:450-457.\nThorndike, E. L.\r(1898). Animal intelligence: An experimental study of the associative processes\rin animals. The Psychological Review, Series of\rMonograph Supplements, II(4).\nThorndike, E. L. (1911). Animal\rIntelligence. Hafner, Darien,\rCT.\nThorp,\rE. O. (1966). Beat the Dealer: A\rWinning Strategy for the Game of Twenty-One. Random\rHouse, New York.\nTian, T. (2017) Empirical Study\rof Sliding-Step Methods in Temporal Difference Learning. University of Alberta\rMSc thesis.\nTieleman, T. and Hinton, G.\r(2012). Lecture 6.5-rmsprop. COURSERA: Neural networks for machine learning.\nTobler, P. N., Fiorillo, C. D.,\rand Schultz, W. (2005). Adaptive coding of reward value by dopamine neurons. Science, 307(5715):1642-1645.\nTolman, E. C. (1932). Purposive\rBehavior in Animals and Men.Century, New York.\nTolman, E. C. (1948). Cognitive maps in rats and\rmen. Psychological Review,55(4):189-208.\nTsai, H.-S., Zhang,\rF., Adamantidis, A., Stuber, G. D., Bonci, A., de Lecea, L., and Deis- seroth,\rK. (2009). Phasic firing in dopaminergic neurons is sufficient for behavioral\rconditioning. Science, 324(5930):1080-1084.\nTsetlin,\rM. L. (1973). Automaton Theory\rand Modeling of Biological Systems.Academic\rPress, New York.\nTsitsiklis, J. N.\r(1994). Asynchronous stochastic approximation and Q-learning. Machine\rLearning,16:185-202.\nTsitsiklis, J. N.\r(2002). On the convergence of optimistic policy iteration. Journal\rof Machine Learning Research,3:59-72.\nTsitsiklis, J. N. and Van Roy, B.\r(1996). Feature-based methods for large scale dynamic programming. Machine\rLearning, 22:59-94.\nTsitsiklis, J. N., Van Roy, B.\r(1997). An analysis of temporal-difference learning with function\rapproximation. IEEE Transactions on Automatic\rControl,42:674-690.\nTsitsiklis, J. N., Van Roy, B.\r(1999). Average cost temporal-difference learning. Automatica, 35:1799-1808.\nTuring, A. M. (1950). Computing machinery and\rintelligence. Mind433-460.\nTuring, A. M. (1948). Intelligent\rMachinery, A Heretical Theory. The Turing Test: Verbal\rBehavior as the Hallmark of Intelligence, 105.\nUngar, L. H. (1990). A bioreactor\rbenchmark for adaptive network-based process control.\nIn W. T. Miller, R. S. Sutton, and P. J. Werbos\r(eds.), Neural Networks for Control, pp. 387-402. MIT Press, Cambridge, MA.\nUrbanczik, R. and Senn, W.\r(2009). Reinforcement learning in populations of spiking neurons. Nature\rneuroscience, 12(3):250-252.\nUrbanowicz, R. J., Moore, J. H.\r(2009). Learning classifier systems: A complete introduction, review, and\rroadmap. Journal of Artificial Evolution and Applications.\nValentin, V. V., Dickinson, A.,\rand O��Doherty, J. P. (2007). Determining the neural sub\u0026shy;strates of\rgoal-directed learning in the human brain. The Journal of\rNeuroscience, 27(15):4019-4026.\nvan Hasselt, H. (2010). Double\rQ-learning. In Advances in Neural Information\rProcessing Systems, pp.\r2613-2621.\nvan\rHasselt, H. (2011). Insights in\rReinforcement Learning: Formal Analysis and Empircal Evaluation of\rTemporal-difference Learning.SIKS dissertation series\rnumber 2011-04.\nvan Hasselt, H. (2012).\rReinforcement learning in continuous state and action spaces. In Wiering and\rvan Otterlo (Eds.) Reinforcement Learning: State-of-the\rArt,pp.\r207-251. Springer Berlin Heidelberg.\nvan Hasselt, H., and Sutton, R.\rS. (2015). Learning to predict independent of span. ArXiv 1508.04582.\nvan Otterlo, M. (2009). The\rLogic of Adaptive Behavior.IOS Press.\nvan Otterlo, M.\r(2012). Solving relational and first-order logical markov decision processes: A\rsurvey. In Wiering and van Otterlo (Eds.) Reinforcement Learning:\rState-of-the Art, pp. 253-292.\rSpringer Berlin Heidelberg.\nVan Roy, B.,\rBertsekas, D. P., Lee, Y., Tsitsiklis, J. N. (1997). A neuro-dynamic pro\u0026shy;gramming\rapproach to retailer inventory management. In Proceedings\rof the 36th IEEE Conference on Decision and Control, Vol. 4, pp. 4052-4057.\nvan Seijen, H. (2016). Effective\rmulti-step temporal-difference learning for non-linear func\u0026shy;tion approximation.\rarXiv preprint arXiv:1608.05151.\nvan Seijen, H., and\rSutton, R. S. (2014). True online TD(A). In Proceedings\rof the 31st International Conference on Machine Learning.JMLR W\u0026amp;CP 32(1):692-700.\nVan Seijen, H., Mahmood,\rA. R., Pilarski, P. M., Machado, M. C., and Sutton, R. S. (2016). True online\rtemporal-difference learning. Journal of Machine\rLearning Research 17(145), 1-40.\nvan Seijen, H., Van Hasselt, H.,\rWhiteson, S., Wiering, M. (2009). A theoretical and empiri\u0026shy;cal analysis of\rExpected Sarsa. In IEEE Symposium on Adaptive Dynamic\rProgramming and Reinforcement Learning,pp. 177-184.\nVarga, R. S. (1962). Matrix\rIterative Analysis.Englewood\rCliffs, NJ: Prentice-Hall.\nVasilaki, E., Fremaux, N., Urbanczik, R., Senn, W., and Gerstner, W.\r(2009).\u0026nbsp;\u0026nbsp;\u0026nbsp; Spike-\nbased reinforcement learning in continuous state and\raction space: when policy gradient methods fail. PLoS\rComputational Biology,5(12).\nViswanathan, R. and Narendra, K. S. (1974). Games of stochastic\rautomata.\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; IEEE\nTransactions on\rSystems, Man, and Cybernetics,4:131-135.\nVlassis, N.,\rGhavamzadeh, M., Mannor, S., and Poupart, P. (2012). Bayesian reinforcement\rlearning. In Wiering and van Otterlo (Eds.) Reinforcement\rLearning: State-of-the Art, pp.\r359-386. Springer Berlin Heidelberg.\nWalter, W. G. (1950). An imitation of life. Scientific\rAmerican,pages\r42-45.\nWalter, W. G. (1951). A machine\rthat learns. Scientific American,185(2):60-63.\nWaltz, M. D., Fu, K. S. (1965). A\rheuristic approach to reinforcement learning control systems. IEEE\rTransactions on Automatic Control,10:390-398.\nWatkins, C. J. C. H. (1989). Learning\rfrom Delayed Rewards. Ph.D.\rthesis, Cambridge University.\nWatkins, C. J. C. H., Dayan, P. (1992). Q-learning. Machine\rLearning,8:279-292.\nWiering, M., Van Otterlo, M. (2012). Reinforcement\rLearning.Springer\rBerlin Heidelberg.\nWerbos, P. (1974). Beyond\rregression: New tools for prediction and analysis in the behavioral sciences.\rPhd Thesis, Harvard University, Cambridge, Massachusetts.\nWerbos, P. J. (1977). Advanced\rforecasting methods for global crisis warning and models of intelligence. General\rSystems Yearbook,22:25-38.\nWerbos, P. J. (1982).\rApplications of advances in nonlinear sensitivity analysis. In R. F. Drenick\rand F. Kozin (eds.), System Modeling and Optimization,pp. 762-770. Springer-Verlag, Berlin.\nWerbos, P. J. (1987).\rBuilding and understanding adaptive systems: A statistical/numerical approach\rto factory automation and brain research. IEEE Transactions on\rSystems, Man, and Cybernetics,17:7-20.\nWerbos, P. J. (1988).\rGeneralization of back propagation with applications to a recurrent gas market\rmodel. Neural Networks, 1:339-356.\nWerbos, P. J. (1989).\rNeural networks for control and system identification. In Proceedings\rof the 28th Conference on Decision and Control, pp. 260-265. IEEE Control Systems Society.\nWerbos, P. J. (1990). Consistency\rof HDP applied to a simple reinforcement learning problem. Neural\rNetworks,3:179-189.\nWerbos, P. J. (1992).\rApproximate dynamic programming for real-time control and neural modeling. In\rD. A. White and D. A. Sofge (eds.), Handbook of Intelligent\rControl: Neural, Fuzzy, and Adaptive Approaches, pp. 493-525. Van Nostrand Reinhold, New York.\nWerbos,\rP. J. (1994). The Roots of\rBackpropagation: From Ordered Derivatives to Neural Networks and Political\rForecasting(Vol. 1). John Wiley and Sons.\nWhite, A. (2015). Developing\ra Predictive Approach to Knowledge.Phd thesis, University of Alberta.\nWhite, D. J. (1969). Dynamic\rProgramming. Holden-Day, San\rFrancisco.\nWhite, D. J. (1985). Real applications of Markov\rdecision processes. Interfaces,15:73-83.\nWhite, D. J. (1988). Further real\rapplications of Markov decision processes. Interfaces, 18:55-61.\nWhite, D. J. (1993). A survey of\rapplications of Markov decision processes. Journal of the\rOperational Research Society,\r44:1073-1096.\nWhite, A., and White, M. (2016).\rInvestigating practical linear temporal difference learn\u0026shy;ing. In Proceedings\rof the 2016 International Conference on Autonomous Agents and Multiagent\rSystems, pp. 494-502.\nWhitehead, S. D., Ballard, D. H.\r(1991). Learning to perceive and act by trial and error. Machine\rLearning,7:45-83.\nWhitt, W. (1978). Approximations\rof dynamic programs I. Mathematics of Operations Research,3:231-243.\nWhittle, P. (1982). Optimization\rover Time, vol. 1. Wiley, New\rYork.\nWhittle, P. (1983). Optimization\rover Time, vol. 2. Wiley, New York.\nWickens, J. and Kotter, R.\r(1995). Cellular models of reinforcement. In Houk, J. C., Davis, J. L., and\rBeiser, D. G., editors, Models of Information Processing in\rthe Basal Ganglia, pages\r187-214. MIT Press, Cambridge, MA.\nWidrow, B., Gupta, N. K., Maitra,\rS. (1973). Punish/reward: Learning with a critic in adaptive threshold systems.\rIEEE Transactions on Systems, Man, and Cybernetics, 3:455-465.\nWidrow, B., Hoff, M. E. (1960).\rAdaptive switching circuits. In 1960 WESCON Convention\rRecord Part IV,pp. 96-104. Institute of Radio Engineers, New York. Reprinted in J.\rA. Anderson and E. Rosenfeld, Neurocomputing:\rFoundations of Research,pp. 126-134. MIT Press, Cambridge, MA, 1988.\nWidrow, B., Smith, F. W. (1964).\rPattern-recognizing control systems. In J. T. Tou and R. H. Wilcox (eds.), Computer\rand Information Sciences,pp. 288-317. Spartan, Washington, DC.\nWidrow, B., Stearns, S. D.\r(1985). Adaptive Signal Processing. Prentice-Hall, Englewood Cliffs, NJ.\nWiewiora, E. (2003). Potential-based\rshaping and Q-value initialization are equivalent. Jour\u0026shy;nal\rof Artificial Intelligence Research 19:205-208.\nWilliams, R. J. (1986).\rReinforcement learning in connectionist networks: A mathematical analysis.\rTechnical Report ICS 8605. Institute for Cognitive Science, University of\rCalifornia at San Diego, La Jolla.\nWilliams, R. J. (1987).\rReinforcement-learning connectionist systems. Technical Report NU-CCS-87-3.\rCollege of Computer Science, Northeastern University, Boston.\nWilliams, R. J. (1988). On the\ruse of backpropagation in associative reinforcement learning. In Proceedings\rof the IEEE International Conference on Neural Networks,pp. I263-I270. IEEE San Diego section and IEEE TAB\rNeural Network Committee.\nWilliams, R. J. (1992). Simple\rstatistical gradient-following algorithms for connectionist reinforcement\rlearning. Machine Learning,8:229-256.\nWilliams, R. J.,\rBaird, L. C. (1990). A mathematical analysis of actor-critic architectures for\rlearning optimal controls through incremental dynamic programming. In Proceedings\rof the Sixth Yale Workshop on Adaptive and Learning Systems,pp. 96-101. Center for Systems Science, Dunham\rLaboratory, Yale University, New Haven.\nWilson, R. C.,\rTakahashi, Y. K., Schoenbaum, G., and Niv, Y. (2014). Orbitofrontal cortex as a\rcognitive map of task space. Neuron, 81(2):267-279.\nWilson, S. W. (1994).\rZCS: A zeroth order classifier system. Evolutionary Computation,\r2:1-18.\nWise, R. A. (2004). Dopamine,\rlearning, and motivation. Nature Reviews Neuroscience, 5(6):1-12.\nWitten, I. H. (1976). The\rapparent conflict between estimation and control��A survey of the two-armed problem.\rJournal of the Franklin Institute,301:161-189.\nWitten, I. H. (1977). An adaptive\roptimal controller for discrete-time Markov environments. Information\rand Control, 34:286-295.\nWitten, I. H., Corbin, M. J.\r(1973). Human operators and automatic adaptive controllers: A comparative study\ron a particular control task. International Journal of\rMan-Machine Studies,5:75-104.\nWoodbury, T., Dunn, C., and\rValasek, J. (2014). Autonomous soaring using reinforcement\nlearning for\rtrajectory generation. In 52nd Aerospace Sciences Meeting, page 0990.\nWoodworth, R. S., Schlosberg, H.\r(1938). Experimental psychology.New York: Henry Holt and Company.\nXie, X. and Seung, H. S. (2004).\rLearning in neural networks by reinforcement of irregular spiking. Physical\rReview E,69(4).\nXu, X., Xie, T., Hu,\rD., and Lu, X. (2005). Kernel least-squares temporal difference learning. International\rJournal of Information Technology 11(9):54-63.\nYagishita, S., Hayashi-Takagi, A., Ellis-Davies, G.\rC. R., Urakubo, H., Ishii, S., and Kasai,\nH.\u0026nbsp;\r(2014). A\rcritical time window for dopamine actions on the structural plasticity of\rdendritic spines. Science, 345(6204):1616-1619.\nYee, R. C., Saxena,\rS., Utgoff, P. E., Barto, A. G. (1990). Explaining temporal differences to\rcreate useful concepts for evaluating states. In Proceedings\rof the Eighth National Conference on Artificial Intelligence, pp. 882-888. AAAI Press, Menlo Park, CA.\nYin, H. H. and Knowlton, B. J.\r(2006). The role of the basal ganglia in habit formation. Nature\rReviews Neuroscience,7(6):464-476.\nYoung, P. (1984). Recursive Estimation and Time-Series Analysis.\rSpringer-Verlag, Berlin.\nYu, H. (2010). Convergence of least\rsquares temporal difference methods under general conditions. International\rConference on Machine Learning 27,pp. 1207-1214.\nYu, H. (2012). Least squares\rtemporal difference methods: An analysis under general con\u0026shy;ditions. SIAM\rJournal on Control and Optimization,50(6), 3310-3343.\nYu, H. (2015a). On convergence of\remphatic temporal-difference learning. ArXiv:1506.02582. A shorter version\rappeared in Conference on Learning Theory 18,\rJMLR W\u0026amp;CP 40.\nYu, H. (2015b). Weak convergence\rproperties of constrained emphatic temporal-difference learning with constant\rand slowly diminishing stepsize. ArXiv:1511.07471.\nZhang, M., Yum, T. P. (1989). Comparisons of\rchannel-assignment strategies in cellular mobile telephone systems. IEEE\rTransactions on Vehicular Technology,38:211-215.\nZhang, W. (1996). Reinforcement\rLearning for Job-shop Scheduling.\rPh.D. thesis, Oregon State University. Technical Report CS-96-30-1.\nZhang, W., Dietterich, T. G.\r(1995). A reinforcement learning approach to job-shop schedul\u0026shy;ing. In Proceedings\rof the Fourteenth International Joint Conference on Artificial Intel\u0026shy;ligence, pp. 1114-1120. Morgan Kaufmann.\nZhang, W., Dietterich, T. G.\r(1996). High-performance job-shop scheduling with a time- delay TD(A) network.\rIn D. S. Touretzky, M. C. Mozer, M. E. Hasselmo (eds.), Ad\u0026shy;vances\rin Neural Information Processing Systems: Proceedings of the 1995 Conference, pp. 1024-1030. MIT Press, Cambridge, MA.\nZweben, M., Daun, B.,\rDeale, M. (1994). Scheduling and rescheduling with iterative re\u0026shy;pair. In M.\rZweben and M. S. Fox (eds.), Intelligent Scheduling,pp. 241-255. Morgan Kaufmann, San Francisco.\n\r\riht=IHT(4096) and tiles(iht, 8, [8*x/(0.5+1.2),\r8*xdot/(0.07+0.07)], A) to get the indices of the ones in the feature vector\rfor state (x, xdot) and action A.\n\r\r[1]The\rnotation (a, b] as a set denotes the real interval between a\rand b including b but not including\n\ra.Thus, here we are saying that 0 \u0026lt; a \u0026lt;\r1.\n\r[3]Associative search tasks are often now termed contextual banditsin the literature.\n\r[4]We use the terms agent, environment,and actioninstead of the engineers�� terms controller, controlled system(or plant), and control\rsignalbecause they are\rmeaningful to a wider audience.\n\r[5]We restrict attention todiscrete time tokeep things as simple as possible, even though many of the ideas can\rbe extended to the continuous-time case (e.g., see Bertsekas and Tsitsiklis,\r1996; Werbos, 1992; Doya, 1996).\n\r[6]We use Rt+iinstead of Rtto denote the reward due to Atbecause it emphasizes that the next reward and next state, Rt+i and St+i,are jointly determined. Unfortunately, both conventions are widely\rused in the literature.\n\r[7]Better places for imparting this kind of prior\rknowledge are the initial policy or value function, or in influences on these.\rSee Lin (1992), Maclin and Shavlik (1994), and Clouse (1996).\n\r[8]Episodes are sometimes called ��trials�� in the\rliterature.\n\r[9]This algorithm has a subtle bug, in that it may never\rterminate if the policy continually switches between two or more policies that\rare equally good. The bug can be fixed by adding additional flags, but it makes\rthe pseudocode so ugly that it is not worth it.\nPolicy iteration often\rconverges in surprisingly few iterations. This is illustrated by the example in\rFigure 4.1. The bottom-left diagram shows the value function for the\requiprobable random policy, and the bottom-right diagram shows a greedy policy\rfor this value function. The policy improvement theorem assures us that these\rpolicies are better than the original random policy. In this case, however,\rthese policies are not just better, but optimal, proceeding to the terminal states\rin the minimum number of steps. In this example, policy iteration would find\rthe optimal policy after just one iteration.\nExample 4.2: Jack��s Car Rental\rJack manages two locations for a nationwide car rental company. Each day, some\rnumber of customers arrive at each location to rent cars. If Jack has a car\ravailable, he rents it out and is credited $10 by the national company. If he\ris out of cars at that location, then the business is lost. Cars become\ravailable for renting the day after they are returned. To help ensure that cars\rare available where they are needed, Jack can move them between the two\rlocations overnight, at a cost of $2 per car moved. We assume that the number\rof cars requested and returned at each location are Poisson random variables,\rmeaning that the probability that the number is n is ��e-A, where ��is the expected\rnumber. Suppose ��is 3 and 4 for rental requests at the first and second locations and\r3 and 2 for returns. To simplify the problem slightly, we assume that there\rcan be no more than 20cars at each location (any additional\rcars are returned to the nationwide company, and thus disappear from the\rproblem) and a maximum of five cars can\n\r[10]If this were a control problem with the objective of\rminimizing travel time, then we would of course make the rewards the negativeof the elapsed time. But since we are concerned here only with\rprediction (policy evaluation), we can keep things simple by using positive\rnumbers.\n\r[11]-step Sarsa\naka Sarsa(0) 2-step Sarsa 3-step Sarsa\nPseudocode is shown in the box below, and an\rexample of why it can speed up learning compared to one-step methods is given\rin Figure 7.4.\nWhat about Expected Sarsa? The backup diagram for\rthe n-step version of Ex\u0026shy;pected Sarsa is shown on the far right in Figure 7.3.\rIt consists of a linear string of sampled actions and states, just as in n-step\rSarsa, except that its last element is a branch over all action possibilities\rweighted, as always, by their probability under n. This algorithm can be\rdescribed by the same equation as n-step Sarsa (above) except with the n-step\rreturn redefined as\nGt:t+n = Rt+i + ��+ Yn\riRt+n +\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; أ(a|St+n)Qt+n-i(St+n,\ra),\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; (7.6)\na\nfor all n and t such that n 1\rand 0 t T n.\n\r[12]The Dyna-Q+ agent was changed in two other ways as\rwell. First, actions that had never been tried before from a state were allowed\rto be considered in the planning step (f) of the Tabular Dyna-Q algorithm in\rthe box above. Second, the initial model for such actions was that they would\nlead back to the same state with a reward of zero.\n\r[13]This policy might be stochastic because RTDP\rcontinues to randomly select among all the\n\r[14]There are interesting exceptions to this. See, e.g.,\rPearl (1984).\n\r[15]In particular, in the episodic case with discounting (7\u0026lt; 1) it can be argued that we should be more\rconcerned about accurately valuing the states that occur early in the episode\rthan those that occur later. This can be expressed by altering the on-policy\rdistibution ��to include a factor of 7in the second term of (9.2). Although this might be\rmore general, it would complicate the following\npresentation of algorithms, and concerns a rare case, so we omit it\rhere.\n\r[16]The T denotes transpose, needed here to\rturn the horizontal row vector in the text into a vertical column vector; in\rthis book vectors are generally taken to be column vectors unless explicitly\rwritten out horizontally, as here, or transposed.\n\r[17]This data is actually from the ��semi-gradient\rSarsa(A)�� algorithm that we will not meet until Chapter 12, but semi-gradient\rSarsa behaves similarly.\n\r[18]For state values there remains a small difference in\rthe treatment of the importance sampling ratio pt.In the analagous action-value\rcase (which is the most important case for control algorithms), the residual\rgradient algorithm would reduce exactly to the naive version.\n\r[19]They would of course be estimateable if the statesequence were observed rather than only the corresponding feature vectors.\n\r[20]These MRPs can equivalently be considered MDPs with a\rsingle action in all states; what we conclude about them here applies as well\rto MDPs.\n\r[21]The lone exception is the gradient bandit algorithms\rof Section 2.8. In fact, that section goes through many of the same steps, in\rthe single-state bandit case, as we go through here for full MDPs. Reviewing\rthat section would be good preparation for fully understanding this chapter.\n\rThe vector\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; in\rthe REINFORCE update is the only place the policy\nparameterization appears in the\ralgorithm. This vector has been given several names and notations in the\rliterature; we will refer to it simply as the eligibility vector.The eligibility vector is often written in the compact form Ve logn(At|St, 6), using the identity V log x =��.Thisform is used in\rall the boxed pseudocode in this chapter. In earlier examples in this chapter\rwe considered exponential softmax policies (13.2) with linear action\rpreferences (13.3). For this parameterization, the eligibility vector is\nVe logn(a|s, 6) = x(s, a) - ^ n(b|s, 6)x(s, b).\nb\n\r[23]Technically, this is only true if each episode��s\rupdates are done off-line,meaning they are accumulated on the side during the\repisode and only used to change 6by their sum at the episode��s end. However, this\rwould probably be a worse algorithm in practice, and its desireable theoretical\rproperties would probably be shared by the algorithm as given (although this\rhas not been proved).\n\rThe\rgeneralizations to the forward view of multi-step methods and then to a\rX-return algorithm are straightforward. The one-step return in (13.10) is\rmerely replaced by G^��t+fc and G^\rrespectively. The backward views are also straightforward, using separate\religibility traces for the actor and critic, each after the patterns in Chapter\r12. Pseudocode for the complete algorithm is given in the box on the next page.\n\r[25]What control means for us is different from what it\rtypically means in animal learning theories; there the environment controls the\ragent instead of the other way around. See our comments on terminology at the\rend of this chapter.\n\r[26]Comparison with a control group is necessary to show\rthat the previous conditioning to the tone is responsible for blocking learning\rto the light. This is done by trials with the tone/light CS but with no prior\rconditioning to the tone. Learning to the light in this case is unimpaired. Moore\rand Schmajuk (2008) give a full account of this procedure.\n\r[27]The only differences between the LMS rule and the\rRescorla-Wagner model are that for LMS the input vectors xt can have any real\rnumbers as components, and��at least in the simplest version of the LMS rule��the\rstep-size parameter adoes not depend on the input vector or the identity\rof the stimulus setting the prediction target.\n\r[28]In our formalism, there is a different state, St,for\reach time step tduring a trial, and for a trial in which a compound\rCS consists of ncomponent CSs of various durations occurring at\rvarious times\n\r[29]As we mentioned in Section 6.1, Stin\rour notation is defined to be ֻt+i + 7V(St+i) �� V(St),so St is not available until time t+1. The TD error availableat tis\ractually St-i = Rt+ 7V(St) �� V(St-i). Since we are thinking of time steps as very small,\ror even infinitesimal, time intervals, one should not attribute undue\rimportance to this one-step time shift.\n\r[30]In the literature relating TD errors to the activity\rof dopamine neurons, their Stis the same as our St-i = Rt+ YV\r(St) - V (St-i).\n\r[31]Registered trademark of IBM Corp.\n\r[32]Registered trademark of Jeopardy Productions Inc.\n\r\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"92aec05bb0b0676c87c7485839c3431a","permalink":"https://wormcode.github.io/post/reinforcement-learning-an-introduction-translate-copy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/reinforcement-learning-an-introduction-translate-copy/","section":"post","summary":"v\\:* {behavior:url(#default#VML);}\ro\\:* {behavior:url(#default#VML);}\rw\\:* {behavior:url(#default#VML);}\r.shape {behavior:url(#default#VML);}\r\r\r\rcommon\rNormal\rcommon\r2\r233\r2019-06-01T06:56:00Z\r2019-06-01T06:56:00Z\r544\r199385\r1136498\rMicrosoft Corporation\r9470\r2666\r1333217\r14.00\r\r\r\r\r\r\r\r\rfalse\r\r\r9.05 pt\r9.05 pt\r\rfalse\rfalse\rfalse\r\rEN-US\rZH-CN\rX-NONE\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r/* Style Definitions */\rtable.","tags":null,"title":"","type":"post"}]